# Modelos de probabilidad para variables aletorias discretas

## Objetivo

En este capítulo veremos algunas funciones de masa de probabilidad que se utilizan para describir experimentos aleatorios comunes.

Introduciremos el concepto de parámetro y por tanto de modelos paramétricos.

En particular, discutiremos las funciones de probabilidad uniforme y de Bernoulli y cómo se usan para derivar las funciones de probabilidad binomial y binomial negativa. También hableramos del modelo hipergeométrico. 



## Función de probabilidad

Recordemos que una función de masa de probabilidad de una **variable aleatoria discreta** $X$ con valores posibles $x_1 , x_2 , .. , x_M$ es **cualquier función** tal que

1) Nos permite calcular probabilidades para todos los resultados

$$f(x_i)=P(X=x_i)$$

2) Siempre es positiva:

$$f(x_i)\geq 0$$

3) La probabilidad de obtener algo en el experimento aleatorio es $1$

$$\sum_{i=1}^M f(x_i)=1$$

Estudiamos dos **propiedades importantes:**

1) La media como medida de tendencia central:

$$E(X)= \sum_{i=1}^M x_i f(x_i)$$

2) La varianza como medida de dispersión:

$$V(X)= \sum_{i=1}^M (x_i-\mu)^2 f(x_i)$$



## Modelo de probabilidad

Un **modelo de probabilidad** es una función de masa de probabilidad que puede representar las probabilidades de un experimento aleatorio.


**Ejemplos:**


1) La función de masa de probabilidad definida por

| $X$ | $f(x)$ |
|:--------:|:-------:|
| $-2$ | $1/8$ |
| $-1$ | $2/8$ |
| $0$ | $2/8$ |
| $1$ | $2/8$ |
| $2$ | $1/8$ |


Representa la probabilidad de sacar **una** bola de una urna donde hay dos bolas con etiquetas: $-1, 0, 1$ y una bola con etiquetas: $-2, 2$.

2) $f(x)=P(X=x)=1/6$ representa la probabilidad de los resultados de **un** lanzamiento de un dado.


## Modelos paramétricos

Cuando tenemos un experimento aleatorio con $M$ resultados posibles, necesitamos encontrar $M$ números para determinar la función de masa de probabilidad. Como en el ejemplo 1 anterior, necesitábamos $5$ valores en la columna $f(x)$ de la tabla de probabilidad.

Sin embargo, **en muchos casos**, podemos formular funciones de probabilidad $f(x)$ que dependen únicamente de **muy pocos** números. Al igual que en el ejemplo 2 anterior, solo necesitábamos saber cuántos resultados posibles puede dar un dado.

**Ejemplo (probabilidad clásica):**

Un experimento aleatorio con $M$ resultados igualmente probables tiene una función de masa de probabilidad:
$$f(x)=P(X=x)=1/M$$

Sólo necesitamos saber $M$.

Los números que **necesitamos saber** para determinar completamente una función de probabilidad se llaman **parámetros**.


## Distribución uniforme (un parámetro)

El ejemplo anterior es la interpretación clásica de la probabilidad y define nuestro primer modelo paramétrico.

**Definición**

Una variable aleatoria $X$ con resultados $\{1,...M\}$ tiene una **distribución uniforme** discreta si todos sus resultados $M$ tienen la misma probabilidad

$$f(x)=\frac{1}{M}$$

$M$ es el parámetro natural del modelo. Una vez que definimos $M$ para un experimento, elegimos una función de masa de probabilidad particular. La función anterior es realmente una **familia** de funciones que dependen de $M$: $f(x; M)$.

La media y la varianza de una variable que sigue una distribución uniforme son:

$$E(X)= \frac{M+1}{2}$$

y

$$V(X)= \frac{M^2-1}{12}$$

Nota: $E(X)$ y $V(X)$ también son **parámetros**. Si conocemos alguno de ellos, entonces podemos determinar completamente la distribución. Por ejemplo:

$$f(x)=\frac{1}{2E(X)-1}$$

Veamos algunas funciones de masa de probabilidad en la familia de modelos paramétricos uniformes:

```{r, echo=FALSE}
par(mfrow=c(1,4))

for(i in seq(2,8,2))
{
  outcome <- 1:i
  probability <- rep(1/i, i)
  plot(outcome, probability, pch=16,col="red",   main=paste("M=",i), ylab="f(x)", ylim=c(0,1), xlim=c(0,10))
  for(i in 1:length(outcome))
  {lines(c(outcome[i], outcome[i]), c(0,      probability[i]), col="red")}

  abline(v=(i+1)/2, lty=2)
}
  
```

## Distribución uniforme (dos parámetros)

Consideremos ahora un nuevo modelo de probabilidad **uniforme** con **dos parámetros**: los resultados mínimo y máximo.


Si la variable aleatoria toma valores en $\{a, a+1, ...b\}$, donde $a$ y $b$ son números enteros y todos los resultados son igualmente probables, entonces

$$f(x)=\frac{1}{b-a+1}$$

porque $M=b-a+1$.

Entonces decimos que $X$ se distribuye uniformemente entre $a$ y $b$ y escribimos

$$X \rightarrow Unif(a,b)$$

**Propiedades:**

Si $X$ se distribuye uniformemente entre $a$ y $b$

$$X \rightarrow Unif(a,b)$$

1) Su media es

$$E(X)= \frac{b+a}{2}$$


2) Su varianza es

$$V(X)= \frac{(b-a+1)^2-1}{12}$$

Para probar esto cambia las variables $X=Y+a-1$, $y \in \{1,...M\}$.


**Funciones de masa de probabilidad**

Veamos algunas funciones de masa de probabilidad en la familia de modelos paramétricos uniformes:

```{r, echo=FALSE}
par(mfrow=c(3,3))

for(j in c(10, 8, 6))
{
for(i in c(0, 2, 4))
{
  outcome <- i:j
  probability <- rep(1/length(outcome), length(outcome))
  plot(outcome, probability, pch=16,col="red",   main=paste(c("a", "b"),c(i,j), sep="="), ylab="f(x)", ylim=c(0,0.5), xlim=c(0,10))
  for(x in 1:length(outcome))
  {lines(c(outcome[x], outcome[x]), c(0,      probability[x]), col="red")

  }
  #  abline(v=(i+j)/2, lty=2)    
}
  }
```



**Ejemplo (clases escolares):**

¿Cuál es la probabilidad de observar a un niño de una edad particular en una escuela primaria (si todas las clases tienen la misma cantidad de niños)?

Del diseño del experimento sabemos: $a=6$ y $b=11$ entonces

$$X \rightarrow Unif(a=6, b=11)$$ eso es

$$f(x)=\frac{1}{6}$$ para $x\in \{6,7,8,9,10,11\}$, y $0$ en caso contrario.

La media y la varianza de esta función de masa de probabilidad es:

- $E(X)=8.5$
- $V(X)=2.916667$

Recuerda 

- El valor esperado es la **media** $\mu=8.5$

- La **desviación estándar** $\sigma=1.707825$ es la distancia promedio desde la media y se calcula a partir de la raíz cuadrada de la varianza.


```{r, echo=FALSE}
outcome <- 1:6
probability <- rep(1/6, 6)
plot(outcome+5, probability, pch=16,col="red", xlab="Age", ylab="f(x)", ylim=c(0,0.4), main="Unif(a=6, b=11)", xlim=c(5,12))
for(i in 1:length(outcome))
{lines(c(outcome[i]+5, outcome[i]+5), c(0, probability[i]), col="red")}

lines(c(outcome[1]+5, outcome[1]+5), c(0, probability[i]), col="black", lwd=2)

lines(c(outcome[6]+5, outcome[6]+5), c(0, probability[i]), col="black", lwd=2)

abline(v=8.5, col="black", lty=2)

lines( c(8.5, 8.5+sqrt((6^2-1)/12)), c(1/6/2, 1/6/2), col="black", lty=2)

points(outcome+5, probability, pch=16,col="red")

text(6,0.2, "a")
text(11,0.2, "b")

text(8.75,0.38, expression(mu), col="black")

text(9.5,0.125, expression(sigma), col="black")


```  


**Parámetros y Modelos:**

Un **modelo** es una función particular $f(x)$ que **describe** nuestro experimento.

Si el modelo es una función **conocida** que depende de algunos parámetros, al cambiar el valor de los parámetros producimos una **familia de modelos**: $f(x; a,b)$.

El conocimiento de $f(x)$ se reduce al conocimiento del valor de los parámetros $a$, $b$.

Idealmente, el modelo y los parámetros son **interpretables**.


En nuestro ejemplo, $a$ representa la edad mínima en la escuela y $b$ la edad máxima. Pueden considerarse como las **propiedades físicas** del experimento.


## Ensayo de Bernoulli

Ahora considermos un modelo con solo dos resultados posibles ($A$ y $B$) que tienen probabilidades **desiguales**

**Ejemplos:**

- Anotar el sexo de un paciente que acude a urgencias de un hospital ($A:masculino$ y $B:femenino$).

- Registrar si una máquina fabricada es defectuosa o no ($A:defectuosa$ y $B:no\,\,defectuosa$).

- Dar en el blanco ($A:éxito$ y $B:fracaso$).

- Transmitir un píxel correctamente ($A:sí$ y $B:no$).

En estos ejemplos, la probabilidad del resultado $A$ suele ser **desconocida**.

**Modelo de probabilidad:**

Introduciremos la probabilidad de un resultado ($A$) como el **parámetro** del modelo. El modelo se puede escribir en diferentes formas.

1) Como una tabla de probabilidad:

| $Resultado$ | $P_i$ |
|:--------:|:--------:|
| $A$ | $p$ |
| $B$ | $1-p$ |

- $i \in \{A,B\}$
- resultado $A$ (éxito): tiene probabilidad $p$ (parámetro)
- resultado $B$ (fracaso): tiene una probabilidad $1-p$


2) Como función de masa de probabilidad de la variable aleatoria $K$ tomando valores $\{0, 1\}$ para $B$ y $A$, respectivamente.


\[
    f(k)= 
\begin{cases}
    1-p,&  k=0\, (event\, B)\\
    p,& k=1\, (event\, A) 
\end{cases}
\]

3) Como una función de $k$ 

$$f(k; p)=p^k(1-p)^{1-k} $$

para $k=(0,1)$.


Entonces decimos que $K$ sigue una distribución de Bernoulli con parámetro $p$
$$K \rightarrow Bernoulli(p)$$

**Propiedades:**

Si $K$ sigue una distribución de Bernoulli, entonces

1) su media es

$$E(K)=p$$

2) su varianza es

$$V(K)=(1-p)p$$

Ten en cuenta que la probabilidad del resultado $A$ es el parámetro $p$
que es lo mismo que su valor en $k=1$: $f(1)=P(k=1)$. El parámetro determina completamente la función de masa de probabilidad, incluidas su media y varianza.

Veamos algunas funciones de masa de probabilidad en la familia de modelos paramétricos uniformes:

```{r, echo=FALSE}
par(mfrow=c(1,4))

probs <- c(0.2, 0.4, 0.6, 0.8)

for(i in 1:4)
{
  outcome <- 0:1
  probability <- c(1-probs[i], probs[i]) 
  plot(outcome, probability, pch=16,col="red",   main=paste("p=",probs[i]), ylab="f(x)", ylim=c(0,1), xlim=c(0,1))
  for(i in 1:length(outcome))
  {lines(c(outcome[i], outcome[i]), c(0,      probability[i]), col="red")}
}
  
```


## Experimento binomial

Si estamos interesados en predecir **frecuencias absolutas** cuando conocemos el parámetro $p$ de un ensayo particular de Bernoulli, entonces

1) **repetimos** el ensayo de Bernoulli $n$ veces y contamos cuantas veces obtuvimos $A$; es decir, calculamos la frecuencia absoluta de $A$: $N_A$.

2) definimos una **variable aleatoria** $X=N_A$ tomando valores $x \in {0,1,...n}$

Cuando repetimos $n$ veces una ensayo de Bernoulli, observamos un valor para $n_A$. Si realizamos otros $n$ ensayos de Bernoulli, entonces $n_A$ cambia de valor. $X=N_A$ es por lo tanto una variable aleatoria y $X=n_A$ es su observación.



**Ejemplos (Algunos experimentos binomiales):**

- Anotamos el sexo de $n=10$ pacientes que acuden a urgencias de un hospital. ¿Cuál es la probabilidad de que $9$ ($X=9$) pacientes sean hombres cuando $p=0.8$?


- Intentamos $n=5$ veces de dar en un blanco ($A:éxito$ y $B:fracaso$). ¿Cuál es la probabilidad de que alcancemos el objetivo $5$ ($X=5$) veces cuando normalmente lo hacemos el $25\%$ de las veces ($p=0.25$)?

- Transmitimos $n=100$ píxeles correctamente ($A:sí$ y $B:no$). ¿Cuál es la probabilidad de que $2$ ($X=2$) píxeles sean errores, cuando la probabilidad de error es $p=0.1$?

## Función de probabilidad binomial

Supongamos que **sabemos** el valor real del parámetro del ensayo de Bernoulli $p$.

Cuando repetimos un ensayo de Bernoulli y paramos en la repetición número $n$, ¿el valor $x$ que obtenemos es un valor común o es raro? ¿cuál es su función de masa de probabilidad $P(X=x)=f(x)$?

**Ejemplo (transmisión de píxeles):**

¿Cuál es la probabilidad de observar errores $X=x$ al transmitir $n=4$ píxeles, si la probabilidad de error es $p$?

Consideremos que

1) Una variable aleatoria del **experimento de transmisión** es el vector $$(K_1, K_2, K_3, K_4)$$ donde una observación puede ser $(K_1=0, K_2=1, K_3=0, K_4= 1)$ o $(0, 1, 0, 1)$.

2) Cada $$K_i \rightarrow Bernoulli(p)$$ $k_i \in \{0, 1\}$

3) $X=N_A$ se puede calcular como la suma $$X=\sum_{i=1}^4 K_i$$ $x\in \{0,1,2,3,4\}$. Por ejemplo $X=2$ para el resultado $(0, 1, 0, 1)$.


Ahora veamos las probabilidades del número de **errores** y luego las generalizaremos.

1) ¿Cuál es la probabilidad de observar $4$ **errores** ($X=4$)?

La probabilidad de observar $4$ errores es la probabilidad de observar un error en $1^{er}$ **y** $2^{o}$ **y** $3^{o}$ **y** $4 ^{o}$ píxel:

$$P(X=4)=P(1,1,1,1)=p*p*p*p=p^4$$

porque $K_i$ son **independientes**.

2) ¿Cuál es la probabilidad de observar $0$ **errores** ($X=0$)?

La probabilidad de errores $0$ es la probabilidad conjunta de observar **ningún error** en **cualquier** transmisión:

$$P(X=0)=P(0,0,0,0)=(1-p)(1-p)(1-p)(1-p)=(1-p)^4$$

3) ¿Cuál es la probabilidad de observar $3$ **errores**?

La probabilidad de $3$ errores es la **suma** de la probabilidad de observar $3$ errores en **eventos diferentes**:

$$P(X=3)=P(0,1,1,1)+P(1,0,1,1)+P(1,1,0,1)+P(1,1,1, 0)=4p^3(1-p)^1$$
porque todos estos eventos son **mutuamente excluyentes**.

4) Por lo tanto, la probabilidad de $x$ **errores** es

\[
    f(x)= 
\begin{cases}
    1*p^0(1-p)^4,&  x=0 \\
    4*p^1(1-p)^3,&  x=1 \\
    6*p^2(1-p)^2,&  x=2 \\
    4*p^3(1-p)^1,&  x=3 \\
    1*p^4(1-p)^0,&  x=4 \\
\end{cases}
\]

o más en breve

$$f(x)=\binom 4 xp^x(1-p)^{4-x}$$
para $x=0,1,2,3,4$

donde $\binom 4 x$ es el número de **posibles resultados** (transmisiones de $4$ píxeles) con $x$ errores.

**Definición:**

La función de **probabilidad binomial** es la función de masa de probabilidad de observar $x$ resultados de tipo $A$ en $n$ ensayos independientes de Bernoulli, donde $A$ tiene la misma probabilidad $p$ en cada ensayo.

La función está dada por

$f(x)=\binom nxp^x(1-p)^{nx}$, $x=0,1,...n$

$\binom nx= \frac{n!}{x!(nx)!}$ se denomina **coeficiente binomial** y da el número de formas en que se pueden obtener $x$ eventos de tipo $A$ en un conjunto de $n$.

Cuando una variable $X$ tiene una función de probabilidad binomial decimos que se distribuye binomialmente y escribimos

$$X\rightarrow Bin(n,p)$$

donde $n$ y $p$ son parámetros.

Veamos algunas funciones de masa de probabilidad en la familia de modelos paramétricos binomiales:

```{r, echo=FALSE}
par(mfrow=c(2,3))

outcome <- 0:4
probability <- dbinom(outcome,4,0.1)
plot(outcome, probability, pch=16,col="red", main="Bin(n=4,p=0.1)")
for(i in 1:length(outcome))
{lines(c(outcome[i], outcome[i]), c(0, probability[i]), col="red")}


outcome <- 0:4
probability <- dbinom(outcome,4,0.5)
plot(outcome, probability, pch=16,col="red", main="Bin(n=4,p=0.5)")
for(i in 1:length(outcome))
{lines(c(outcome[i], outcome[i]), c(0, probability[i]), col="red")}


outcome <- 0:4
probability <- dbinom(outcome,4,0.9)
plot(outcome, probability, pch=16,col="red", main="Bin(n=4,p=0.9)")
for(i in 1:length(outcome))
{lines(c(outcome[i], outcome[i]), c(0, probability[i]), col="red")}


outcome <- 0:10
probability <- dbinom(outcome,10,0.1)
plot(outcome, probability, pch=16,col="red", main="Bin(n=10,p=0.1)", xlab="x")
for(i in 1:length(outcome)) 
{lines(c(outcome[i], outcome[i]), c(0, probability[i]), col="red")}


outcome <- 0:10
probability <- dbinom(outcome,10,0.5)
plot(outcome, probability, pch=16,col="red", main="Bin(n=10,p=0.5)", xlab="x")
for(i in 1:length(outcome))
{lines(c(outcome[i], outcome[i]), c(0, probability[i]), col="red")}


outcome <- 0:10
probability <- dbinom(outcome,10,0.9)
plot(outcome, probability, pch=16,col="red", main="Bin(n=10,p=0.9)", xlab="x")
for(i in 1:length(outcome))
{lines(c(outcome[i], outcome[i]), c(0, probability[i]), col="red")}


```

**Propiedades:**

Si una variable aleatoria $X\rightarrow Bin(n,p)$ entonces

1) su media es

$$E(X)=np$$

2) su varianza es

$$V(X)=np(1-p)$$


Estas propiedades se pueden demostrar por el hecho de que $X$ es la suma de $n$ variables de Bernoulli independientes. Por lo tanto,

$E(X)=E(\sum_{i=1}^n K_i)=np$

y

$V(X)=V(\sum_{i=1}^n K_i)=n(1-p)p$





**Ejemplo (transmisión de píxeles):**

- El valor esperado para el número de errores en la transmisión de $4$ píxeles es $np=4*0.1=0.4$ cuando la probabilidad de error es $0.1$.

- La varianza es $n(1-p)p=0.36$

- ¿Cuál es la probabilidad de observar $4$ errores?

Dado que estamos repitiendo una ensayo de Bernoulli $n=4$ veces y contando el número de eventos de tipo $A$ (errores), cuando $P(A)=p=0.1$ entonces

$$X \rightarrow Bin(n=4, p=0.1)$$
Eso es $$f(x)=\binom 4 x 0.1^x(1-0.1)^{4-x}$$

$P(X=4)=f(4)=\binom 4 4 0.1^4 0.9^{0}=0.1^4=10^{-4}$

En R <code>dbinom(4,4,0.1)</code>

- ¿Cuál es la probabilidad de observar $2$ errores?

$P(X=2)=\binom 4 2 0.1^2 0.9^2=0.0486$

En R <code>dbinom(2,4,0.1)</code>


**Ejemplo (encuestas de opinión):**

- ¿Cuál es la probabilidad de observar **como máximo** $8$ votantes del partido de gobierno en una encuesta electoral de tamaño $10$, si la probabilidad de un voto para el partido es de $0.9$?

Para este caso

$$X \rightarrow Bin(n=10, p=0.9)$$

Eso es $$f(x)=\binom {10} x 0.9^x(0.1)^{4-x}$$

Queremos calcular:
$P(X\le 8)=F(8)= \sum_{i=1..8} f(x_i)=0.2639011$

en R <code>pbinom(8,10, 0.9)</code>


## Función de probabilidad binomial negativa

Ahora imaginemos que estamos interesados en contar los píxeles bien transmitidos antes de que ocurra un **número dado** de errores. Digamos que podemos **tolerar** $r$ errores en la transmisión.

Nuestro experimento aleatorio ahora es: Repetir las pruebas de Bernoulli hasta que observemos que el resultado $A$ aparece $r$ veces.

El resultado del experimento es el número de eventos $B$ es decir $n_B=y$.

Estamos interesados en encontrar la probabilidad de observar un número particular de eventos $B$, $P(Y=y)$, donde $Y=N_B$ es la variable aleatoria.

**Ejemplo (transmisión de píxeles):**

¿Cuál es la probabilidad de observar $y$ píxeles bien transmitidos ($B$) antes de $r$ errores ($A$)?

Primero encontremos la probabilidad de **un** evento de transmisión **en particular** con $y$ número de píxeles correctos ($B$) y $r$ número de errores ($A$).

$$(0,0,1,., 0,1,...0,1)$$

donde consideramos que hay $y$ ceros y $r$ unos. Por lo tanto, observamos $y$ píxeles correctos en un total de $y + r$ píxeles.

La probabilidad de este evento es:

$$P(0,0,1,., 0,1,...0,1)=p^r(1-p)^y$$

Recuerda que $p$ es la probabilidad de error ($A$).

¿Cuántos **eventos de transmisión** pueden tener $y$ píxeles correctos (0) antes de $r$ errores (1)?

Ten en cuenta que

1) El último pixel es fijo (marca el final de la transmisión)

2) El número total de formas en que $y$ el número de ceros se puede asignar en $y + r-1$ píxeles es: $\binom {y + r-1} y$


Por lo tanto, la probabilidad de observar $y$ 1 antes de $r$ 0 (cada 1 con probabilidad $p$) es

$$P(Y=y)=f(y)=\binom {y+r-1} yp^r(1-p)^y$$

para $y=0,1,...$

Entonces decimos que $Y$ sigue una distribución binomial negativa y escribimos

$$Y\rightarrow NB(r,p)$$

donde $r$ y $p$ son parámetros que representan la tolerancia y la probabilidad de un solo error (evento $A$).


**Propiedades:**

Una variable aleatoria $Y\rightarrow NB(r,p)$ tiene

1) media $$E(Y)= r\frac{1-p}{p}$$

2) y varianza $$V(Y)= r\frac{1-p}{p^2}$$

Veamos algunas funciones de masa de probabilidad en la familia de modelos paramétricos binomiales negativos:

```{r, echo=FALSE}

par(mfrow=c(2,2))

outcome <- 0:50
probability <- dnbinom(outcome, 2, 0.1)
plot(outcome, probability, pch=16,col="red", main="NB(r= 2, p=0.1)", xlab="y")
for(i in 1:length(outcome))
{lines(c(outcome[i], outcome[i]), c(0, probability[i]), col="red")}

outcome <- 0:50
probability <- dnbinom(outcome, 2, 0.5)
plot(outcome, probability, pch=16,col="red", main="NB(r= 2, p=0.5)", xlab="y")
for(i in 1:length(outcome))
{lines(c(outcome[i], outcome[i]), c(0, probability[i]), col="red")}

outcome <- 0:100
probability <- dnbinom(outcome, 4, 0.1)
plot(outcome, probability, pch=16,col="red", main="NB(r=4, p=0.1)", xlab="y")
for(i in 1:length(outcome))
{lines(c(outcome[i], outcome[i]), c(0, probability[i]), col="red")}


outcome <- 0:100
probability <- dnbinom(outcome, 4, 0.5)
plot(outcome, probability, pch=16,col="red", main="NB(r=4, p=0.5)", xlab="y")
for(i in 1:length(outcome))
{lines(c(outcome[i], outcome[i]), c(0, probability[i]), col="red")}

```

**Ejemplo (sitio web)**

Un sitio web tiene tres servidores. Un servidor opera a la vez y solo cuando falla una solicitud se usa otro servidor.

Si se sabe que la probabilidad de que falle una solicitud es $p=0.0005$, entonces

- ¿Cuál es el número esperado de solicitudes exitosas antes de que las tres computadoras fallen?

Ya que estamos repitiendo un ensayo de Bernoulli hasta $r=3$ se observan eventos de tipo $A$ (fallo) (cada uno con $P(A)=p=0.0005$) y estamos contando el número de eventos de tipo $B$ (solicitudes exitosas) entonces


$$Y \rightarrow NB(r=3, p=0.0005)$$

Por lo tanto, el número esperado de solicitudes antes de que el sistema falle es:

$E(Y)=r\frac{1-p}{p}=3\frac{1-0.0005}{0.0005}=5997$


Ten en cuenta que en realidad hay pruebas de $6000$.

- ¿Cuál es la probabilidad de observar  $5$ solicitudes exitosas antes de que el sistema falle?

$f(5)=\binom {7} 5 0.0005^3 0.9995^5=2.618444 \times 10^{-9}$


- ¿Cuál es la probabilidad de tratar con un máximo de $5$ solicitudes exitosas antes de que el sistema falle?

En R esto se calcula con <code>dnbinom(5,3,0.0005)</code>


Por lo tanto, queremos calcular la distribución de probabilidad en $5$:

$F(5)=P(Y\leq 5)=\Sigma_{y=0}^5 f(y)$

$=\sum_{y=0}^5\binom {y+2} y 0.0005^r0.9995^y$

$=\binom{2} 0 0.0005^3 0.9995^0 +\binom{3} 1 0.0005^3 0.9995^1$

$+\binom {4} 2 0.0005^3 0.9995^2 +\binom {5} 3 0.0005^3 0.9995^3$

$+\binom {6} 4 0.0005^3 0.9995^4 +\binom {7} 5 0.0005^3 0.9995^5$

$= 6.9\times 10^{-9}$


En R esto se calcula con <code>pnbinom(5,3,0.0005)</code>


**Ejemplos**


- ¿Cuál es la probabilidad de observar $10$ píxeles correctos antes de $2$ errores, si la probabilidad de error es $0.1$?

$f(10; r=2, p=0.1)=0.03835463$

en R <code>dnbinom(10, 2, 0.1)</code>

- ¿Cuál es la probabilidad de que entren $2$ chicas antes que $4$ chicos entren a clase si la probabilidad de que entre un chico es de $0.55$?

$f(2; r=4, p=0.55)=0.1853$


en R <code>dnbinom(2, 4, 0.55)</code>


## Distribución geométrica

Llamamos **distribución geométrica** a la distribución **binomial negativa** con $r=1$

La probabilidad de observar $B$ eventos antes de observar el **primer** evento de tipo $A$ es

$$P(Y=y)=f(y)= p(1-p)^y$$


$$Y\rightarrow Geom(p)$$
que tiene

1) media $$E(Y)= \frac{1-p}{p}$$

2) y varianza $$V(Y)= \frac{1-p}{p^2}$$



## Modelo hipergeométrico

El **modelo hipergeométrico** surge cuando queremos contar el número de eventos de tipo $A$ que se extraen de una población finita.

El modelo general es considerar $N$ bolas totales en una urna. Marquemos $K$ con la etiqueta $A$ y $NK$ con la etiqueta $B$. Saquemos $n$ bolas una por una sin reemplazo en la urna y luego contemos cuántos $A$ obtuvimos.

El modelo **Binomial** se puede derivar del modelo **Hipergeométrico** cuando consideramos que $N$ es infinito, o que cada vez que sacamos una bola la volvemos a colocar en la urna.

**Ejemplo (varicela):**

Una escuela de $N=600$ niños tiene una epidemia de varicela. Testamos a $n=200$ niños y observamos que $x=17$ dieron positivo. Si supiéramos que un total de $K=64$ estaban realmente infectados en la escuela, ¿cuál es la probabilidad de nuestra observación?

**Definición:**

La probabilidad de obtener $x$ casos (tipo $A$) en una muestra de $n$ extraída de una población de $N$ donde $K$ son casos (tipo $A$).

$P(X=x)=P(una\,muestra) \times (Número\, de\, formas\, de\, obteniendo\, x)$

$$=\frac{1}{\binom N n}\binom K x \binom {NK} {nx}$$

donde $k \in \{\max(0, n+KN), ... \min(K, n) \}$

Entonces decimos que $X$ sigue una distribución hipergeométrica y escribimos

$$X \rightarrow Hipergeom(N,K,n)$$
El modelo hipergeométrico tiene tres parámetros.

**Propiedades:**

Si $X \rightarrow Hypergeometric(N,K,n)$ entonces tiene

1) media $$E(X) = n \frac{K}{N} = np$$

2) y varianza $$V(X) = np(1-p)\frac{Nn}{N-1}$$

cuando $p=\frac{K}{N}$ es la proporción de casos ($A$) en una población de tamaño $N$. Ten en cuenta que cuando $N \rightarrow \infty$ recuperamos las propiedades binomiales.

Veamos algunas funciones de masa de probabilidad en la familia de modelos paramétricos hipergeométricos:

```{r, echo=FALSE}
outcome <- 0:5
probability <- dhyper(outcome, m=5, n=5, k=5)
plot(outcome, probability, pch=16,col="red", main="Hyper(N=10,K=5, n=5)", xlab="x")
for(i in 1:length(outcome))
{lines(c(outcome[i], outcome[i]), c(0, probability[i]), col="red")}
```



```{r, echo=FALSE}
outcome <- 0:5
probability <-dhyper(outcome, m=45, n=5, k=5)
plot(outcome, probability, pch=16,col="red", main="Hyper(N=50, K=45, n=5)", xlab="x")
for(i in 1:length(outcome))
{lines(c(outcome[i], outcome[i]), c(0, probability[i]), col="red")}
```


**Ejemplo (varicela):**

- ¿Cuál es la probabilidad ver como mucho $17$ casos de varicela en una muestra de $200$ alumnos de una ecuela de $600$ alumnos donde $64$ están infectados?

La probabilidad que necesitamos calcular es
$P(X \leq 17)=F(17)$

donde $X \rightarrow Hypergeometric(N=600,K=64,n=200)$

en R <code>phyper(17, 64, 600-64, 200)=0.140565</code>

La solución es la adición de las agujas azules en el gráfico.


```{r, echo=FALSE}
outcome <- 10:35
probability <-dhyper(outcome, 64, 536, 200)
plot(outcome, probability, pch=16,col="red", main="Hyper(N=600, K=64, n=200)", xlab="x")
for(i in 1:length(outcome))
{lines(c(outcome[i], outcome[i]), c(0, probability[i]), col="red")}

for(i in 1:8)
{lines(c(outcome[i], outcome[i]), c(0, probability[i]), col="blue")}

```

## Preguntas

**1)** ¿Cuál es el valor esperado y la varianza del número de fallos en $100$ prototipos, cuando la probabilidad de un fallo es de $0.25$?

**$\qquad$a:** $0.25$, $0.1875$;
**$\qquad$b:** $25$, $0.1875$;
**$\qquad$c:** $0.25$, $18.75$;
**$\qquad$d:** $25$, $18.75$


**2)** ¿Qué modelo de probabilidad describe mejor el número de mesas disponibles a la hora de la cena en un restaurante?

**$\qquad$a:** Binomial;
**$\qquad$b:** Uniforme;
**$\qquad$c:** Binomial negativo;
**$\qquad$d:** Hipergeométrico

**3)** El valor esperado de una distribución Binomial no es

**$\qquad$a:** $n$ veces el valor esperado de un Bernoulli;
**$\qquad$b:** el valor esperado de un Hipergeométrico, cuando la población es muy grande;
**$\qquad$c:** $np$;
**$\qquad$d:** el límite de la frecuencia relativa cuando el número de repeticiones es grande

**4)** Las encuestas de opinión para las elecciones de EE. UU. dan una probabilidad de $0.55$ de que un votante esté a favor del partido republicano. Si realizamos nuestra propia encuesta y preguntamos a 100 personas al azar en la calle, ¿cómo calcularías la probabilidad de que en nuestra encuesta los demócratas ganen las elecciones?

**$\qquad$a:**<code>pbinom(x=49, n=100, p=0.55)=0.13</code>;
**$\qquad$b:**<code>1-pbinom(x=49, n=100, p=0.55)=0.86</code>;
**$\qquad$c:**<code>pbinom(x=51, n=100, p=0.45)=0.90</code>; **$\qquad$d:**<code>1-pbinom(x=51, n=100, p=0.45)=0.095</code>


**5)** En un examen un alumno cuando no sabe la respuesta elige al azar una de las cuatro respuestas en una pregunta de selección múltiple. Si no sabe $10$ preguntas, ¿cuál es la probabilidad de que al mas de $5$ preguntas ($>5$) sean correctas?

**$\qquad$a:**<code>dbinom(x=4, n=10, p=0.25)</code>; **$\qquad$b:**<code>pbinom(x=4, n=10, p=0.75)</code>; **$\qquad$c:**<code>dbinom(x=4, n=10, p=0.75)</code>; **$\qquad$d:**<code>1-pbinom(x=4, n=10, p=0.25)</code>
  

## Ejercicios


#### Ejercicio 1

Si el 25% de los tornillo producidos por una máquina son defectuosos, determina la probabilidad de que, de
5 tornillos elegidos al azar

- ningún tornillo sea defectuoso (R:0.2373)
- 1 tornillo sea defectuoso (R:0.3955)
- 2 tornillos sean defectuosos (R:0.2636)
- como máximo 2 tornillos sean defectuosos (R:0.8964)



#### Ejercicio 2

En una población, la probabilidad de que nazca un niño es $p=0.51$. Considera una familia de 4 hijos.

- ¿Cuál es la probabilidad de que una familia tenga un solo niño? (R: 0.240)
- ¿Cuál es la probabilidad de que una familia tenga una sola niña? (R: 0.259)
- ¿Cuál es la probabilidad de que una familia tenga solo un niño o solo una niña? (R: 0.4999)
- ¿Cuál es la probabilidad de que la familia tenga por mucho dos niños? (R:  0.6723)
- ¿Cuál es la probabilidad de que la familia tenga al menos dos niños? (R: 0.7023)
- ¿Cuál es el mínimo número de hijos que debe tener una familia para que la probabilidad de tener al menos una niña sea mayor a $0.75$?(R:$n=3 \geq\log(0.25)/\log(0.51)=2.05$)


#### Ejercicio 3

Un motor de búsqueda falla al recuperar información con una probabilidad de $0.1$

- Si nuestro sistema recibe $50$ solicitudes de búsqueda, ¿cuál es la probabilidad de que el sistema no responda a tres de ellas? (R: 0.1385651)

- ¿Cuál es la probabilidad de que el motor complete con éxito $15$ búsquedas antes del primer fallo? (R:0.020)

- Consideramos que un motor de búsqueda funciona suficientemente bien cuando es capaz de encontrar información como mínimo para $10$  solicitudes por cada $2$ fallos. ¿Cuál es la probabilidad de que en un ensayo de fiabilidad nuestro buscador sea satisfactorio? (R: 0.659)



