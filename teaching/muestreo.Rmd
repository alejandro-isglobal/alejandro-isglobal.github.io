---
title: "Distribuciones de muestreo"
output: html_document
---
*Las distribuciones de muestreo son la puerta de entrada a la inferencia estadística y de salida a la predicción. Las distribuciones de muestreo son las intermediarias entre los datos y las distribuciones poblacionales; aquellas que queremos conocer. Los estadísticos son las variables aleatorias de las distribuciones de muestreo, y por lo tanto son fundamentales para entender cualquier razonamiento estadístico. Mediante un ejemplo de un contador Geiger ilustro el papel central de las distribuciones de muestreo para el razonamiento estadístico.*


<img src="Geiger.jpg" style="width:30%;  margin-right: 20px" align="left"> Un contador Geiger mide el número de partículas radioactivas por segundo. Es escencial para saber que tan expuesta está una persona en un lugar con alta radiación como por ejemplo en una central nuclear o en lugares con alta radiación como Fukushima o Chernovil. Analizaremos los datos de un contador Geiger imaginando que son los datos que obtenemos al seguir a un trabajador que entra en una central nuclear a hacer una labor de limpieza. Nuestro trabajo entonces es hacer una lectura remota de los datos, para alertar al trabajador cuando se encuentre en una zona de alto riesgo y que abandone el lugar. En [mightyohm.com](http://mightyohm.com/files/geiger/capture.txt) hay una muestra real de un contadorr Geiger, que también se puede encontar [aquí](https://alejandro-isglobal.github.io/data/capture.txt). Usaremos estos datos imaginando que son los obtenidos en la situación descrita, y suponeniendo que el promedio de partículas detectadas por segundo debe ser menor de 0.4 para la salud del trabajador. Nuestro objetivo es estimar el valor medio de partículas detectadas por segundo, a medida que el trabajador va pasando más tiempo en el reactor y nosotros vamos obteniendo mas datos. El propósito de plantear esta situación ficticia sobre datos reales es dramatizar cómo nuestro conocimiento y decisiones cambian en función de la cantidad de datos que disponemos. 

Empecemos cargando los datos    

```{r}
library(RCurl)
text <- getURL('https://alejandro-isglobal.github.io/data/capture.txt')
geiger <- read.table(text=text, sep = ",")
head(geiger)
```

En la columna 2 se encuentran los conteos de partículas en cada segundo (CPS, counts per second). Imaginemos que comenzamos observando las detecciones del contador Geiger en los primeros 10 segundos depués de que el trabajador entra al reactor.

```{r}
detecciones <- geiger[,2]
detecciones10 <- detecciones[1:10]
detecciones10
```

Podemos ver que en el primer segundo se detectó una partícula, así como en los segundos 4, 5, 7 y 8. Así pues el número de promedio de partículas detectadas por segundo es

```{r}
xbar <- mean(detecciones10)
xbar
```

el promedio *xbar* lo denotamos como $\bar{x}$, que definido sobre $n$ datos $x_1,..x_n$ tiene la forma

$$\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i.$$

Esta es la suma ponderada de los datos, o su centro de gravedad cuando cada dato tiene el mismo peso, y nos da una idea de la centralidad de las mediciones. Observamos que el promedio de detección en número de partículos es mayor al recomendado $0.4$ *partículas/segundo*. Según estos datos deberíamos sugerirle al trabajador que salga inmediatamente. 

Sin embargo ¿Qué tan confiados estamos de que el trabajador realmente esté en una zona de peligro? Al fin y al cabo, si esperamos las detecciones de los siguientes 10 segundos (11,...20) nos dará otro valor (*xbar=0.2*) que nos hará cambiar de opinión. Por lo tanto $\bar{x}=0.5$ (con x minúscula) es el resultado de un experimento aleatorio sobre la variable aleatoria $\bar{X}$ (con x mayúscula). De tal forma que nos interesa conocer la probabilidad de que $\bar{X}<0.4$. Si conocemos cómo se distribuye $\bar{X}$ entonces podremos **predecir**  qué tan probable es obtener mediciones  $\bar{x}<0.4$ en un futuros muestreos de $n=10$ segundos. Es decir, podríamos calcular 

$$P(\bar{X} < 0.4)=F(0.4; n=10)=\int_0^{0.4} f(t; n=10) dt$$

donde $F(0.4; n=10)$ es la función de distribución o acumulación de probabilidad de $\bar{X}$ para $\bar{X}=0.4$ y $f(t; n=10)$ la densidad de probabilidad de $\bar{X}$ para $t$. La función de distribución para $\bar{X}$ se conoce como la **distribución se muestreo** para la media (en nuestro caso para una muestra de tamaño  $n=10$). La pregunta es entonces ¿Cómo podemos conocer $F_{\bar{X}}$ o equivalentemente $f_{\bar{X}}$?

###Modelización de la probabilidad poblacional
Primero hagamos una gran consideracón. Nuestras mediciones $x_1,..x_n$, o conteos, son eventos independientes que provienen de un proceso común. Este proceso puede ser descrito por una única variable aleatoria $X$. Denotaremos como $f_X$ la función de probabilidad de $X$, que llamaremos función de probabilidad **poblacional**. Esto es por el hecho de que $f_X$ será la función que nos de las probabilidades de obtener cada uno de los resultados posibles que resulten de una medición. Usamos en término poblacional inspirados en un censo, donde las mediciones no pueden ser separadas de los individuos que representan y por lo tanto $f_x$, en ese caso, da la probabilidad a todos y cada uno de los posibles valores que puede tomar $X$ en la población. $f_x$ es abstracta y tiene la libertad de representar individuos que todavía no existen o, partículas radioctivas de átomos que todaía no ha decaído. Aún así suponemos que el proceso de creación que representa existe y que lo podemos conocer, o como mínimo modelar. 

La suposición de que $x_1,..x_n$ son independientes no está siempre garantizada, ya sea porque el conteo de nuestras partículas en un momento dependa de cuantas se detectan previamente, o porque los individuos en el caso de un censo estén relacionados y no representen a la población. Sin embargo, como no tenemos razones para pensar que  nuestras partículas radioactivas dependen de una ley de oferta y demanda, nos vamos a permitir suponer un **modelo** de probabillidad para los datos $x_i$ independientes. Queremos derivar características generales de cómo se producen diferentes conteos en el detector segundo a segundo por diez segundos; por qué ahora uno, después ninguno y tal vez en 10 segundos dos. El objetivo es usar la forma de $f_X$ para deducir las posibles formas que puede tomar $f_{\bar{X}}$. 

Para modelar una función de probabilidad poblacional solemos empezar considerando una circunstancia para la cual nuestra ignorancia sobre los eventos que dan diferentes mediciones es razonablemente total. De este punto de patida construimos la situación que queremos describir. Imaginemos pues que tuviésemos un contador que pudiese detectar partículas en milésimas de segúndo. Seguramente en un milisegundo detectaríamos o una o cero partículas: no se detectarían más de una. No es mucho pedirle a nuestro proceso radiactivo. Si es así entonces detectaríamos una partícula con una probabilidad $p$ que sería igual al número promedio de detecciones por segundo que llamamos $\lambda$, dividido por $m=1000$. $\lambda$ es ideal, una característica del proceso imaginado. Con esta perspectiva, volviendo al mundo de nuestro contador Geiger real, si contamos todas las partículas detectadas en un segundo, sería un total de $x$ partículas detectadas en $m=1000$ ensayos, uno por cada milisegundo. Este tipo de conteo probabilístico sigue una distribución binomial

$$P(X=x)=\binom m x p^x(1-p)^{m-x}$$
que describo en otro tutorial, pero que usando $p=\lambda/m$

$$P(X=x)=\binom m x \big(\frac{\lambda}{m}\big)^x(1-\frac{\lambda}{m})^{m-x}$$
podemos escribir en términos de $\lambda$. El intervalo en milisegunodos es todavía una aproximación, útil para anclarnos en un proceso binomial, pero insuficente para destilar la idea de que las partículas son independientes entre sí. El nuestro no es un proceso, como la emisión de un par electrón-positrón, que genere múltiples partículas a la vez. Entonces imaginemos ahora contadores infinitamente rápidos, en tiempo de detección de 0 segundos, o cuando el número de ensayos en la distribución binomial crece a infinito ($m \rightarrow \infty$). En el límite de rapidez, nos acegurarnos que las partículas no se crean exactamente al mismo tiempo, es decir que el proceso las crea de una a una y con cierta cadencia $\lambda$. Recordémonos de nuestro trabajador que ha quedado en una zona de posible riesgo. Las partículas que él detecta son de un átomo que decae aquí y otro que decae allá. El proceso que queremos describir no es otro que la densidad de partículas radioctivas de dónde él se encuentra, siendo $\lambda$ una medida de esa densidad. Recordemos que nuestro objetivo es describir la peligrocidad del lugar. Tomando ese límite llegamos a la distribución de Poisson       

$$f_{X}=f(x; \lambda)=P(X=x)= \frac{\lambda^x e^{-\lambda}}{x!}$$

La distribución de Poisson sirve para modelar cualquier proceso con el cual podamos recorrer el mismo camino de supociciones: conteos independientes en un intervalo de tiempo o espacio que se generan uno a uno con un promedio $\lambda$ por unidad de intervalo. Nosotros lo usaremos para modelar la probabilidad de contar $x$ partículas radioactivas con una media de detección por segundo dada por $\lambda$. 

Veamos la forma tendría $f_{X}$ si considereamos los primeros 10 segundos de detección, pero no conocemos el valor de $\lambda$. Vamos a asumir que $\bar{X}$ es un estimador de $\lambda$, es decir que en últimas podemos remplazar $\lambda$ por el resulatdo de nuestro experimento $\bar{x}=0.5$. ¿Por qué podemos hacer esto?. Por un lado la media de la distribución de Poisson es

$$E(X)= \sum_{x=0}^\infty x f_{x}=\lambda$$
es decir su centro de gravedad es $\lambda$, que se demuestra usando la forma para $f_{x}$ y expandiendo el exponencial $e^{-\lambda}$ en series de potencia de $\lambda$.  Por otro lado el promedio de los datos también se puede escribir como

$$\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i=\sum_{x=0}^\infty x \frac{n_x}{n}  $$
donde $n_x$ es el número veces en n=10 segundos que observamos x=1, 2, 3, .. partículas. Para nuestros datos esto es 


```{r}
nx <- table(detecciones10)
nx
x <- 0:1
n <- 10
sum(nx/n*x)
```

que es *xbar*. Al escribir el promedio en esta forma, nos damos cuenta que cuando $n \rightarrow \infty$, $n_x \rightarrow \infty$ pero su ratio $\frac{n_x}{n} \rightarrow f_x$. Así recuperamos la noción de probabilidad como la fracción de veces que observamos un evento si hacemos infinitos experimentos. Por lo tanto $\bar{x}$ es un de las versiones aproximadas de $\lambda$, cuando sólo disponemos de 10 mediciones, esto lo denotamos $\bar{x}=\hat{\lambda}$ y decimos que $\hat{\lambda}$  es la **estimación** de $\lambda$ dada por $\bar{x}$. En términos de las variables aleatorias decimos que $\bar{X}$ es un **estimador** de $\lambda$, que además es insesgado por que
$$E(\bar{X})=E(X)=\lambda$$

Dibujemos la funcion poblacional $f(x;n)$ para $n=10$ segundos según la estimación dada por los primeros 10 segundos, o sea $\hat{\lambda}=0.5$. En R tenemos la función de probabilidad de Poisson dada por la instrucción *dpoiss(x, lambda)* de tal forma que $f(x; n=10, \lambda=\bar{x}$ es

```{r}
x <- 0:5
fx <- dpois(x, lambda=xbar)
names(fx) <- x

plot(x,fx, type="p", pch=16)
for(i in 1:11)
lines(c(x[i], x[i]), c(0, fx[i])) 
```
Esta es nuestro **modelo** para $f_X$ según los conteos de partículas radiactivas en los primeros diez segundos después de que el trabajador entra a la planta. Este es nuestro estado de conocimiento en los primeros diez segundos.   

###Modelización de la probabilidad de la media
Según nuestro estado de conocimiento, aunque limitado, queremos saber cómo se distribuye el promedio de detecciones $\bar{X}$ en 10 mediciones (una cada segundo). Queremos saber, según el modelo $f_{X}=Pois(x; n=10, \lambda=0.5)$, cuales son los valores de $\bar{x}$ que podemos esperar si pudiésemos hacer entrar al trabajador muchas veces, y cada vez calcular $\bar{x}$ sólo por 10 segundos. 

Para simular una de estas entradas en falso del trababjador, podemos generar $10$ valores aleatorios que siguen una Poisson usando la función *rpois(10, lambda=0.5)*

```{r}
conteos <-  rpois(n=10, lambda=0.5)
conteos
```

Cada vez que ejecutemos *rpois(10, lambda=0.5)* hacemos una nueva simulación. Hagamos el histograma de una de estas entradas en falso de $n=10$ de 10 mediciones del contador en 10 segundos.

```{r}
conteos <- rpois(10, lambda=0.5)
hist(conteos, freq=FALSE, breaks=seq(-0.5,5.5))
```

Este es un experimento con 10 mediciones con un promedio 
```{r}
mean(conteos)
```

Cada nuevo experimento con 10 mediciones tendrá su propia $\bar{x}$. 
Ahora estudiemos *teóricamente* cómo se comportaría $\bar{X}$ bajo este modelo de Poissson para la función de probabilidad poblacional. Más especificamente nos preguntamos ¿Cómo sería la distribución de valores de $\bar{x}$ cuando repetimos muchas veces la entrada del trabajador por 10 segundos? Hagamos muchas simulaciones, recojamos muchos valores de $\bar{x}$ y veamos su distribución.

Para esto hacemos una función general que computa $\bar{x}$ en un experimento de 10 mediciones. Creamos  la función **Xbar** tal que tome n=10 valores de una variable de Poisson con $\lambda=0.5$ y compute su media    

```{r}
Xbar <- function(n)
{
  conteos <- rpois(n, lambda=0.5)   
  mean(conteos)
}

Xbar(10)
```
Veamos ahora el histograma que resulta de muchos valores de **Xbar** ($\bar{X}$) para ver su distribución de valores. Creemos 600 valores para $\bar{X}$ recordando la función **sapply**  y dibujemos el histograma de estos 600 promedios  $(\bar{x_1}, \bar{x_2}...\bar{x}_{600})$?

```{r}
distXbar <- sapply(rep(10,600), Xbar)
head(distXbar)
hist(distXbar, freq=FALSE)
```

Ya tenemos una versión computacional de $f(\bar{X}; n=10, \lambda=0.5)$. Entonces, bajo el modelo *Pois(x; n=10, lambda=0.5)* y nuestros primeros 10 datos $\bar{x}=0.5$ no es improbable ${\bar X}<$ 0.4 
```{r}
mean(distXbar<0.4)
```
es decir que no podemos descartar que el trabajador se encuentre en zona segura, a pesar de $\bar{x}=0.5$. Sin embargo, si podríamos descartar que el trabajador esta en una zona de muy alto riesgo, digamos $\lambda>1$
```{r}
mean(distXbar>1)
```

###Teorema central del límite
Cuando n es grande ($>30$) sabemos que por el TCL

$$\bar{X}\sim N(\mu_{\bar{X}}, \sigma_{\bar{X}})$$
$$\mu_{\bar{X}}=E(X)=\mu=0.5$$
$$\sigma^2_{\bar{X}}=Var(X)/\sqrt{n}=\sigma^2_{X}/\sqrt{n}=0.5/\sqrt{n}$$


Recordemos que para una distribución de Poisson $\mu=\sigma^2=\lambda$ (=0.5 para nuestros datos).

Entonces:
Bajo el TCL podemos usar la distibución normal para calcular

$$P(\bar{X}<0.4)$$


Pero con n=10 no podemos usar este teorema: Veamos por qué.

Pinta la distribución normal; **dnrom(x,mu,sigma)** 
que le corresponde a la distribución de 
**Xbar(100)** en el intervalo: **xprom<-seq(0,1.5,0.01)}**

```{r}
sim <- sapply(rep(100,600), Xbar)
hist(sim, freq=FALSE)

xprom <- seq(0,1.5,0.01)
normvals <- dnorm(xprom, mean=0.5, sd=sqrt(0.5)/sqrt(100))
lines(xprom, normvals, col="red")
```

$\mu=0.5$, $\sigma=\frac{\sqrt{0.5}}{\sqrt{n}}$


Podemos confirmar que la aproximación no es buena para n=5

```{r}
sim <- sapply(rep(5,600), Xbar)
hist(sim, freq=FALSE)

xprom <- seq(0,1.5,0.01)
normvals <- dnorm(xprom, mean=0.5, sd=sqrt(0.5)/sqrt(5))
lines(xprom, normvals, col="red")
```

Tomemos más datos, ahora 50 mediciones (los primeros 50 segundos)


```{r}
detecciones50 <- detecciones[1:50]
xbar <- mean(detecciones50)
xbar
```

La situación cambia! ahora $xbar < 0.4$ y el trabajador estaría en zona segura. Pero con qué probabilidad $Pr(\bar{X}<0.4$)?


Ahora tenemos mas datos (50) y podemos calcular $Pr(\bar{X}<0.4)$ con el TCL

```{r}
pnorm(0.4, mean=0.36, sd=sqrt(0.36/50))
```

Según los 50 primeros datos el trabajador tiene una probabilidad de 0.68 de estar en zona segura.

Cuántos datos necesitamos para estar muy seguros?

El modelo ha cambiado ahora para estos 50 datos el estimador de $\hat{\lambda}=\bar{x}=0.36$ (le ponemos el gorro a $\lambda$ para remarcar que es un valor estimado)

```{r}
Xbar <- function(n)
{
  conteos <- rpois(n, lambda=0.36)   
  mean(conteos)
}
```
Hagamos los histogramas para muestras  de n=5, n=30 (blue), n=100(orange) 

```{r}
sim1 <- sapply(rep(5,600), Xbar)
hist(sim1, freq=FALSE, ylim=c(0,7))

sim2 <- sapply(rep(30,600), Xbar)
hist(sim2, freq=FALSE, add=TRUE, col="blue")

sim3 <- sapply(rep(100,600), Xbar)
hist(sim3, freq=FALSE, add=TRUE, col="orange")
```

A medida que las medidas aumentan (n) la varianza de $\bar{X}$, que llamamos $\sigma^2_{\bar{X}}$, es cada vez mas pequeña  ($\sqrt{0.5/n}$) y cada vez tenemos mas confianza de que nuestro promedio dado por los datos $\hat{\lambda}=\bar{x}$ está cerca del verdadero valor de $\lambda$: $\lambda \sim \hat{\lambda} = 0.36$


```{r}
mean(detecciones)
pnorm(0.4, mean=0.2899838, sd=sqrt(0.2899838/1238))
```

y muestran una probabilidad de 1 de que nuestro trabajador esta en zona segura.

También podemos comprobar que estos datos reales están muy bien descritos por una distribución de Poisson.

```{r}
hist(detecciones, freq=FALSE, breaks=seq(-0.5,5.5))

x <- 0:5
fx <- dpois(x, lambda= 0.2899838)

points(x,fx, type="p", pch=16)
for(i in 1:6)
lines(c(x[i], x[i]), c(0, fx[i]))
```{r}
