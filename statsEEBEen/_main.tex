% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\title{Stats theory (SDA)}
\author{Alejandro Caceres}
\date{2022-10-31}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Stats theory (SDA)},
  pdfauthor={Alejandro Caceres},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{about}{%
\chapter{About}\label{about}}

\begin{itemize}
\item
  This is the introduction to statistics course from EEBE (UPC).
\item
  Exam dates and additional study material can be found in ATENEA
\end{itemize}

\hypertarget{recommended-reading-list}{%
\section{Recommended reading list}\label{recommended-reading-list}}

\begin{itemize}
\tightlist
\item
  Douglas C. Montgomery and George C. Runger. ``Applied Statistics and Probability for Engineers'' 4th Edition. Wiley 2007.
\end{itemize}

\hypertarget{data-description}{%
\chapter{Data description}\label{data-description}}

\hypertarget{objective}{%
\section{Objective}\label{objective}}

\begin{itemize}
\tightlist
\item
  Data: discrete, continuous
\item
  Summarizing data in tables and figures
\end{itemize}

\hypertarget{statistics}{%
\section{Statistics}\label{statistics}}

\begin{itemize}
\item
  Solve problems in a systematic way (science, engineering and technology)
\item
  Modern humans use a general \textbf{method} historically developed for thousands of years! \ldots{} and still under development.
\item
  It has three main components: observation, logic, and generation of new knowledge
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{scientific-method}{%
\section{Scientific method}\label{scientific-method}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{outcome}{%
\section{Outcome}\label{outcome}}

\textbf{Observation} or \emph{Realization}

\begin{itemize}
\tightlist
\item
  an \textbf{observation} is the acquisition of a number or a characteristic from an experiment
\end{itemize}

\ldots{} 1 0 0 1 0 \textbf{1} 0 1 1 \ldots{} (the number in bold is an observation in a repetition of the experiment)

\textbf{Outcome}

\begin{itemize}
\tightlist
\item
  An \textbf{outcome} is a possible observation that is the result of an experiment.
\end{itemize}

\textbf{1} is an outcome, \textbf{0} is the other outcome

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{types-of-outcome}{%
\section{Types of outcome}\label{types-of-outcome}}

\begin{itemize}
\tightlist
\item
  \textbf{Categorical}: If the result of an experiment can only take discrete values (number of car pieces produced per hour, number of leukocytes in blood)
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Continuous}: If the result of an experiment can only take continuous values (battery state of charge, engine temperature).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{random-experiments}{%
\section{Random experiments}\label{random-experiments}}

\textbf{Definition:}

A \textbf{random experiment} is an experiment that gives different outcomes when repeated in the same manner.

\textbf{Examples:}

\begin{itemize}
\tightlist
\item
  on the same object (person): temperature, sugar levels.\\
\item
  on different objects but the same measurement: the weight of an animal.
\item
  on events: a number of emails received in an hour.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{absolute-frequencies}{%
\section{Absolute frequencies}\label{absolute-frequencies}}

When we repeat a random experiment, we record a list of outcomes.

We summarize the \textbf{categorical} observations by counting how many times we saw a particular outcome.

\textbf{Absolute frequency}:

\[n_i\]

is the number of times we observed the outcome \(i\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example}{%
\section{Example}\label{example}}

\textbf{Random experiment}: Extract a leukocyte from \textbf{one} donor and write down its type. Repeat experiment \(N=119\) times.

\begin{verbatim}
(T cell, Tcell, Neutrophil, ..., B cell)
\end{verbatim}

\begin{verbatim}
##      outcome ni
## 1     T Cell 34
## 2     B cell 50
## 3   basophil 20
## 4   Monocyte  5
## 5 Neutrophil 10
\end{verbatim}

\begin{itemize}
\tightlist
\item
  For instance: \(n_1=34\) is total number of T cells
\item
  \(N=\sum_i n_i=119\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{relative-frequencies}{%
\section{Relative frequencies}\label{relative-frequencies}}

We can also summarize the observations by computing the \textbf{proportion} of how many times we saw a particular outcome.

\[f_i=n_i/N\] where \(N\) is the total number of observations

In our example there are recorded \(n_1=34\) T cells, so we ask for the proportion of T cells from the total of \(119\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-1}{%
\section{Example}\label{example-1}}

\begin{verbatim}
##      outcome ni         fi
## 1     T Cell 34 0.28571429
## 2     B cell 50 0.42016807
## 3   basophil 20 0.16806723
## 4   Monocyte  5 0.04201681
## 5 Neutrophil 10 0.08403361
\end{verbatim}

We have

\(\sum_{i=1..M} n_i = N\)

\(\sum_{i=1..M} f_i = 1\)

where \(M\) is the number of outcomes.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bar-plot}{%
\section{Bar plot}\label{bar-plot}}

We can plot \(n_i\) Vs the outcomes, giving us a bar plot

\includegraphics{_main_files/figure-latex/unnamed-chunk-3-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{pie-chart}{%
\section{Pie chart}\label{pie-chart}}

We can visualize the relative frequencies with a pie chart

\begin{itemize}
\tightlist
\item
  Where the area of the circle represents 100\% of observations (proportion = 1) and the sections the relative frequencies of all the outcomes.
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-4-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{categorical-and-ordered-variables}{%
\section{Categorical and ordered variables}\label{categorical-and-ordered-variables}}

Cell types are not meaningfully ordered concerning the outcomes. However, sometimes \textbf{categorical} variables can be \textbf{ordered}.

Misophonia study:

\begin{itemize}
\item
  123 patients were examined for misophonia: anxiety/anger produced by certain sounds
\item
  They were categorized into 4 different groups according to severity.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-2}{%
\section{Example}\label{example-2}}

The results of the study are:

\begin{verbatim}
##   [1] 4 2 0 3 0 0 2 3 0 3 0 2 2 0 2 0 0 3 3 0 3 3 2 0 0 0 4 2 2 0 2 0 0 0 3 0 2
##  [38] 3 2 2 0 2 3 0 0 2 2 3 3 0 0 4 3 3 2 0 2 0 0 0 2 2 0 0 2 3 0 1 3 2 4 3 2 3
##  [75] 0 2 3 2 4 1 2 0 2 0 2 0 2 2 4 3 0 3 0 0 0 2 2 1 3 0 0 3 2 1 3 0 4 4 2 3 3
## [112] 3 0 3 2 1 2 3 3 4 2 3 2
\end{verbatim}

And its frequency table

\begin{verbatim}
##   outcome ni         fi
## 1       0 41 0.33333333
## 2       1  5 0.04065041
## 3       2 37 0.30081301
## 4       3 31 0.25203252
## 5       4  9 0.07317073
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{absolute-and-relative-cumulative-frequencies}{%
\section{Absolute and relative cumulative frequencies}\label{absolute-and-relative-cumulative-frequencies}}

Misophonia severity is \textbf{categorical} and \textbf{ordered}.

When outcomes can be ordered then it is useful to ask how many observations were obtained up to a given outcome we call this number the absolute cumulative frequency up to the outcome \(i\):
\[N_i=\sum_{k=1..i} n_k\]
It is also useful to compute the \textbf{proportion} of the observations that was obtained up to a given outcome

\[F_i=\sum_{k=1..i} f_k\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{frequency-table}{%
\section{Frequency table}\label{frequency-table}}

\begin{verbatim}
##   outcome ni         fi  Ni        Fi
## 0       0 41 0.33333333  41 0.3333333
## 1       1  5 0.04065041  46 0.3739837
## 2       2 37 0.30081301  83 0.6747967
## 3       3 31 0.25203252 114 0.9268293
## 4       4  9 0.07317073 123 1.0000000
\end{verbatim}

\begin{itemize}
\item
  \textbf{67\%} of patients had misophonia up to severity \textbf{2}
\item
  \textbf{37\%} of patients have severity less or equal than \textbf{1}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{cumulative-frequency-plot}{%
\section{Cumulative frequency plot}\label{cumulative-frequency-plot}}

We can also plot the cumulative frequency Vs the outcomes

\includegraphics{_main_files/figure-latex/unnamed-chunk-8-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-variables}{%
\section{Continuous variables}\label{continuous-variables}}

The result of a random experiment can also give continuous outcomes.

In the misophonia study, the researchers asked whether the convexity of the jaw would affect the misophonia severity (the scientific hypothesis is that the convexity angle of the jaw can influence the ear and its sensitivity). These are the results for the convexity of the jaw (degrees)

\begin{verbatim}
##   [1]  7.97 18.23 12.27  7.81  9.81 13.50 19.30  7.70 12.30  7.90 12.60 19.00
##  [13]  7.27 14.00  5.40  8.00 11.20  7.75  7.94 16.69  7.62  7.02  7.00 19.20
##  [25]  7.96 14.70  7.24  7.80  7.90  4.70  4.40 14.00 14.40 16.00  1.40  9.76
##  [37]  7.90  7.90  7.40  6.30  7.76  7.30  7.00 11.23 16.00  7.90  7.29  6.91
##  [49]  7.10 13.40 11.60 -1.00  6.00  7.82  4.80 11.00  9.00 11.50 16.00 15.00
##  [61]  1.40 16.80  7.70 16.14  7.12 -1.00 17.00  9.26 18.70  3.40 21.30  7.50
##  [73]  6.03  7.50 19.00 19.01  8.10  7.80  6.10 15.26  7.95 18.00  4.60 15.00
##  [85]  7.50  8.00 16.80  8.54  7.00 18.30  7.80 16.00 14.00 12.30 11.40  8.50
##  [97]  7.00  7.96 17.60 10.00  3.50  6.70 17.00 20.26  6.64  1.80  7.02  2.46
## [109] 19.00 17.86  6.10  6.64 12.00  6.60  8.70 14.05  7.20 19.70  7.70  6.02
## [121]  2.50 19.00  6.80
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bins}{%
\section{Bins}\label{bins}}

Continuous outcomes cannot be counted!

We transform them into ordered categorical variables

\begin{itemize}
\tightlist
\item
  We cover the range of the observations into regular intervals of the same size (bins)
\end{itemize}

\begin{verbatim}
## [1] "[-1.02,3.46]" "(3.46,7.92]"  "(7.92,12.4]"  "(12.4,16.8]"  "(16.8,21.3]"
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{create-a-categorical-variable-from-a-continuous-one}{%
\section{Create a categorical variable from a continuous one}\label{create-a-categorical-variable-from-a-continuous-one}}

\begin{itemize}
\tightlist
\item
  We map each observation to its interval: creating an \textbf{ordered categorical} variable; in this case with 5 possible outcomes
\end{itemize}

\begin{verbatim}
##   [1] "(7.92,12.4]"  "(16.8,21.3]"  "(7.92,12.4]"  "(3.46,7.92]"  "(7.92,12.4]" 
##   [6] "(12.4,16.8]"  "(16.8,21.3]"  "(3.46,7.92]"  "(7.92,12.4]"  "(3.46,7.92]" 
##  [11] "(12.4,16.8]"  "(16.8,21.3]"  "(3.46,7.92]"  "(12.4,16.8]"  "(3.46,7.92]" 
##  [16] "(7.92,12.4]"  "(7.92,12.4]"  "(3.46,7.92]"  "(7.92,12.4]"  "(12.4,16.8]" 
##  [21] "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(16.8,21.3]"  "(7.92,12.4]" 
##  [26] "(12.4,16.8]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]" 
##  [31] "(3.46,7.92]"  "(12.4,16.8]"  "(12.4,16.8]"  "(12.4,16.8]"  "[-1.02,3.46]"
##  [36] "(7.92,12.4]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]" 
##  [41] "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(7.92,12.4]"  "(12.4,16.8]" 
##  [46] "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(12.4,16.8]" 
##  [51] "(7.92,12.4]"  "[-1.02,3.46]" "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]" 
##  [56] "(7.92,12.4]"  "(7.92,12.4]"  "(7.92,12.4]"  "(12.4,16.8]"  "(12.4,16.8]" 
##  [61] "[-1.02,3.46]" "(12.4,16.8]"  "(3.46,7.92]"  "(12.4,16.8]"  "(3.46,7.92]" 
##  [66] "[-1.02,3.46]" "(16.8,21.3]"  "(7.92,12.4]"  "(16.8,21.3]"  "[-1.02,3.46]"
##  [71] "(16.8,21.3]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(16.8,21.3]" 
##  [76] "(16.8,21.3]"  "(7.92,12.4]"  "(3.46,7.92]"  "(3.46,7.92]"  "(12.4,16.8]" 
##  [81] "(7.92,12.4]"  "(16.8,21.3]"  "(3.46,7.92]"  "(12.4,16.8]"  "(3.46,7.92]" 
##  [86] "(7.92,12.4]"  "(12.4,16.8]"  "(7.92,12.4]"  "(3.46,7.92]"  "(16.8,21.3]" 
##  [91] "(3.46,7.92]"  "(12.4,16.8]"  "(12.4,16.8]"  "(7.92,12.4]"  "(7.92,12.4]" 
##  [96] "(7.92,12.4]"  "(3.46,7.92]"  "(7.92,12.4]"  "(16.8,21.3]"  "(7.92,12.4]" 
## [101] "(3.46,7.92]"  "(3.46,7.92]"  "(16.8,21.3]"  "(16.8,21.3]"  "(3.46,7.92]" 
## [106] "[-1.02,3.46]" "(3.46,7.92]"  "[-1.02,3.46]" "(16.8,21.3]"  "(16.8,21.3]" 
## [111] "(3.46,7.92]"  "(3.46,7.92]"  "(7.92,12.4]"  "(3.46,7.92]"  "(7.92,12.4]" 
## [116] "(12.4,16.8]"  "(3.46,7.92]"  "(16.8,21.3]"  "(3.46,7.92]"  "(3.46,7.92]" 
## [121] "[-1.02,3.46]" "(16.8,21.3]"  "(3.46,7.92]"
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{frequency-table-for-a-continuous-variable}{%
\section{Frequency table for a continuous variable}\label{frequency-table-for-a-continuous-variable}}

\begin{verbatim}
##        outcome ni         fi  Ni         Fi
## 1 [-1.02,3.46]  8 0.06504065   8 0.06504065
## 2  (3.46,7.92] 51 0.41463415  59 0.47967480
## 3  (7.92,12.4] 26 0.21138211  85 0.69105691
## 4  (12.4,16.8] 20 0.16260163 105 0.85365854
## 5  (16.8,21.3] 18 0.14634146 123 1.00000000
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{histogram}{%
\section{Histogram}\label{histogram}}

The histogram is the plot of \(n_i\) or \(f_i\) Vs the outcomes (bins). The histogram depends on the size of the bins

\includegraphics{_main_files/figure-latex/unnamed-chunk-13-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{histogram-1}{%
\section{Histogram}\label{histogram-1}}

The histogram is the plot of \(n_i\) or \(f_i\) Vs the outcomes (bins). The histogram depends on the size of the bins

\includegraphics{_main_files/figure-latex/unnamed-chunk-14-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{cumulative-frequency-plot-continous-variables}{%
\section{Cumulative frequency plot: Continous variables}\label{cumulative-frequency-plot-continous-variables}}

We can also plot the cumulative frequency Vs the outcomes

\includegraphics{_main_files/figure-latex/unnamed-chunk-15-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{summary-statistics}{%
\section{Summary statistics}\label{summary-statistics}}

The summary statistics are numbers computed from the data that tell us important features of numerical variables (categorical or continuous).

Limiting values:

\begin{itemize}
\tightlist
\item
  minimum: the minimum outcome observed
\item
  maximum: the maximum outcome observed
\end{itemize}

Central value for the outcomes

\begin{itemize}
\tightlist
\item
  The average is defined as
\end{itemize}

\[\bar{x}=\frac{1}{N} \sum_{j=1..N} x_j\]

where \(x_j\) is the \textbf{observation} \(j\) (convexity) from a total of \(N\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average}{%
\section{Average}\label{average}}

The average convexity can be computed directly from the \textbf{observations}

\(\bar{x}= \frac{1}{N}\sum_j x_j\)

\(= \frac{1}{N}(7.97 + 18.23 + 12.27... + 6.80) = 10.19894\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-categorical-ordered}{%
\section{Average (categorical ordered)}\label{average-categorical-ordered}}

For \textbf{categorical ordered} variables we can use the frequency table to compute the average

\begin{verbatim}
##   outcome ni         fi
## 1       0 41 0.33333333
## 2       1  5 0.04065041
## 3       2 37 0.30081301
## 4       3 31 0.25203252
## 5       4  9 0.07317073
\end{verbatim}

The average \textbf{severity} of misophonia in the study
can \textbf{also} be computed from the relative frequencies of the \textbf{outcomes}

\(\bar{x}=\frac{1}{N}\sum_{i=1...N} x_j=\frac{1}{N}\sum_{i=1...M} x_i*n_{i}=\sum_{i=1...M} x_i*f_{i}\)

\(=0*f_{0}+1*f_{1}+2*f_{2}+3*f_{3}+4*f_{4}=1.691057\)

(note the change from \(N\) to \(M\) in the second summation)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-categorical-ordered-1}{%
\section{Average (categorical ordered)}\label{average-categorical-ordered-1}}

In terms of the \textbf{outcomes} of categorical ordered variables, the \textbf{average} can be written as

\[\bar{x}= \sum_{i = 1...M} x_i f_i\]

from a total of \(M\) possible outcomes (number of severity levels).

\(\bar{x}\) is the \textbf{central value} or center of gravity of the outcomes. As if each outcome had a mass density given by \(f_i\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-1}{%
\section{Average}\label{average-1}}

\begin{itemize}
\item
  The average is not the result of one observation (random experiment).
\item
  It is the result of a series of observations (sample).
\item
  It describes the number where the observed values balance.
\end{itemize}

That is why we hear, for instance, that a patient with an infection can infect an average of 2.5 people.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-2}{%
\section{Average}\label{average-2}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-17-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{median}{%
\section{Median}\label{median}}

Another measure of centrality is the median. The median \(q_{0.5}\) is the value \(x_p\)

\[median(x)=q_{0.5}=x_p\]

below which we find half of the observations

\[\sum_{x\leq x_p} 1 = \frac{N}{2}\]

or in terms of the frequencies, is the value \(x_p\) that makes the cumulative frequency \(F_p\) equal to \(0.5\)

\[q_{0.5}=\sum_{x\leq x_p} f_x =F_p=0.5\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{median-vs-average}{%
\section{Median Vs Average}\label{median-vs-average}}

\begin{itemize}
\tightlist
\item
  Average: Center of mass (compensates distant values)
\item
  Median: Half of the mass
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-18-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{dispersion}{%
\section{Dispersion}\label{dispersion}}

An important measure of the outcomes is their \textbf{dispersion}. Many experiments can share their mean but differ on how dispersed the values are.

\includegraphics{_main_files/figure-latex/unnamed-chunk-19-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{dispersion-1}{%
\section{Dispersion}\label{dispersion-1}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-20-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-variance}{%
\section{Sample variance}\label{sample-variance}}

Dispersion about the mean is measured with the

\begin{itemize}
\tightlist
\item
  The sample variance:
\end{itemize}

\[s^2=\frac{1}{N-1} \sum_{j=1..N} (x_j-\bar{x})^2\]

It measures the average square distance of the \textbf{observations} to the average. The reason for \(N-1\) will be explained when we talk about inference.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-variance-1}{%
\section{Sample variance}\label{sample-variance-1}}

\begin{itemize}
\tightlist
\item
  In terms of the frequencies of \textbf{categorical and ordered} variables
\end{itemize}

\[s^2=\frac{N}{N-1} \sum_{x} (x-\bar{x})^2 f_x\]

\(s^2\) can be thought of as the moment of inertia of the observations.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-deviation}{%
\section{Standard deviation}\label{standard-deviation}}

The squared root of the sample variance is called the \textbf{standard deviation} \(s\).

The standard deviation of the convexity angle is

\(s= [\frac{1}{123-1}((7.97-10.19894)^2+ (18.23-10.19894)^2\)\\
\(+ (12.27-10.19894)^2 + ...)]^{1/2} = 5.086707\)

The jaw convexity deviates from its mean by \(5.086707\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{iqr}{%
\section{IQR}\label{iqr}}

\begin{itemize}
\item
  Dispersion of data can also be measured with respect to the median by the \textbf{interquartile range}
\item
  We define the \textbf{first} quartile as the value \(x_p\) that makes the cumulative frequency \(F_p\) equal to \(0.25\)
\end{itemize}

\[q_{0.25}=\sum_{x\leq x_p} f_x =F_p=0.25\]

\begin{itemize}
\tightlist
\item
  We also define the \textbf{third} quartile as the value \(x_p\) that makes the cumulative frequency \(F_p\) equal to \(0.75\)
\end{itemize}

\[q_{0.75}=\sum_{x\leq x_p} f_x =F_p=0.75\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{iqr-1}{%
\section{IQR}\label{iqr-1}}

The distance between the third quartile and the first quartile is called the \textbf{interquartile range} (IQR) and captures the central 50\% of the observations

\includegraphics{_main_files/figure-latex/unnamed-chunk-21-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{box-plot}{%
\section{Box plot}\label{box-plot}}

The interquartile range, the median, and the 5\% and 95\% of the data can be visualized in a \textbf{boxplot}, here the values of the outcomes are on the y-axis. The IQR is the box, the median is the line in the middle and the whiskers mark the 5\% and 95\% of the data.

\includegraphics{_main_files/figure-latex/unnamed-chunk-22-1.pdf}

\hypertarget{probability}{%
\chapter{Probability}\label{probability}}

\hypertarget{objective-1}{%
\section{Objective}\label{objective-1}}

\begin{itemize}
\tightlist
\item
  Definition of probability
\item
  Probability algebra
\item
  Joint probability
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{random-experiments-1}{%
\section{Random experiments}\label{random-experiments-1}}

\textbf{Observation}

\begin{itemize}
\tightlist
\item
  An \textbf{observation} is the acquisition of a number or a characteristic from an experiment
\end{itemize}

\textbf{Outcome}

\begin{itemize}
\tightlist
\item
  An \textbf{outcome} is a possible observation that is the result of an experiment.
\end{itemize}

\textbf{Random experiment}

\begin{itemize}
\tightlist
\item
  An experiment that gives \textbf{different} outcomes when repeated in the same manner.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-1}{%
\section{Probability}\label{probability-1}}

The \textbf{probability} of an outcome is a measure of how sure we are to observe that outcome when performing a random experiment.

\begin{itemize}
\item
  0: We are sure that the observation will \textbf{not} happen.
\item
  1: We are sure that the observation will happen.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-3}{%
\section{Example}\label{example-3}}

\begin{itemize}
\tightlist
\item
  Consider the following observations of a random experiment:
\end{itemize}

1 5 1 2 2 1 2 2

\begin{itemize}
\tightlist
\item
  How sure we are to obtain \(2\) in the following observation?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-4}{%
\section{Example}\label{example-4}}

The frequency table is

\begin{verbatim}
##   outcome ni    fi
## 1       1  3 0.375
## 2       2  4 0.500
## 3       5  1 0.125
\end{verbatim}

The \textbf{relative frequency} \(f_i\)

\begin{itemize}
\tightlist
\item
  is a number between \(0\) and \(1\).
\item
  measures the proportion of total observations that we observed a particular outcome.
\item
  seems a reasonable probability measure.
\end{itemize}

As \(f_2=0.5\) then we would be half certain to obtain a \(2\) in the next repetition of the experiment.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{relative-frequency}{%
\section{Relative frequency}\label{relative-frequency}}

As a measure of certainty is \(f_i\) enough?

Say we repeated the experiment 12 times more:

1 5 1 2 2 1 2 2 \textbf{3 1 1 3 3 1 6 3 5 6 4 4}

The frequency table is now

\begin{verbatim}
##   outcome ni  fi
## 1       1  6 0.3
## 2       2  4 0.2
## 3       3  4 0.2
## 4       4  2 0.1
## 5       5  2 0.1
## 6       6  2 0.1
\end{verbatim}

New outcomes appeared and \(f_2\) is now \(0.2\), we are now a fifth certain of obtaining \(2\) in the next experiment\ldots{} probability should not depend on \(N\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{at-infinity}{%
\section{At infinity}\label{at-infinity}}

Say we repeated the experiment 1000 times:

\begin{verbatim}
##   outcome  ni    fi
## 1       1 185 0.185
## 2       2 145 0.145
## 3       3 166 0.166
## 4       4 166 0.166
## 5       5 159 0.159
## 6       6 179 0.179
\end{verbatim}

We find that \(f_i\) is converging to a constant value

\[lim_{N\rightarrow \infty} f_i = P_i\]

\includegraphics{_main_files/figure-latex/unnamed-chunk-26-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{frequentist-probability}{%
\section{Frequentist probability}\label{frequentist-probability}}

We call \textbf{Probability} \(P_i\) to the limit when \(N \rightarrow \infty\) of the \textbf{relative frequency} of observing the outcome \(i\) in a random experiment.

Championed by \href{https://plato.stanford.edu/entries/probability-interpret/\#ClaPro}{Venn (1876)}

The frequentist interpretation of probabilities is derived from data/experience (empirical).

\begin{itemize}
\tightlist
\item
  We do not observe \(P_i\), we observe \(f_i\)
\item
  When we \textbf{estimate} \(P_i\) with \(f_i\) (typically when \(N\) is large), we write: \[\hat{P_i}=f_i\]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{classical-probability}{%
\section{Classical Probability}\label{classical-probability}}

Whenever a random experiment has \(M\) possible outcomes that are all \textbf{equally likely}, the probability of each outcome is \(\frac{1}{M}\).

Championed by \href{https://plato.stanford.edu/entries/probability-interpret/\#ClaPro}{Laplace (1814)}.

Since each outcome is \textbf{equally probable} we declare complete ignorance and the best we can do is to fairly distribute the same probability to each outcome.

What if I told you that our experiment was the throw of the dice? then

\(P_2=1/6=0.166666\).

\[P_i=lim_{N\rightarrow \infty} \frac{n_i}{N}=\frac{1}{M}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{classical-and-frequentist-probabilities}{%
\section{Classical and frequentist probabilities}\label{classical-and-frequentist-probabilities}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-2}{%
\section{Probability}\label{probability-2}}

Probability is a number between \(0\) and \(1\) that is assigned to each member \(E\) of a collection of \textbf{events} of a \textbf{sample space} (\(S\)) from a random experiment.

\[P(E) \in (0,1)\]

where \(E \in S\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-space}{%
\section{Sample space}\label{sample-space}}

We start by reasoning what are all the possible values (outcomes) that a random experiment could give.

Note that we do not have to observe them in a particular experiment: We are using \textbf{reason/logic} and not observation.

\textbf{Definition:}

\begin{itemize}
\item
  The set of all possible outcomes of a random experiment is called the \textbf{sample space}
  of the experiment.
\item
  The sample space is denoted as \(S\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{examples-of-sample-spaces}{%
\section{Examples of sample spaces}\label{examples-of-sample-spaces}}

\begin{itemize}
\tightlist
\item
  temperature 35 and 42 degrees Celcius
\item
  sugar levels: 70-80mg/dL
\item
  the size of one screw from a production line: 70mm-72mm
\item
  number of emails received in an hour: 0-100
\item
  a dice throw: 1, 2, 3, 4, 5, 6
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{discrete-and-continuous-sample-spaces}{%
\section{Discrete and continuous sample spaces}\label{discrete-and-continuous-sample-spaces}}

\begin{itemize}
\item
  A sample space is discrete if it consists of a finite or countable infinite set of outcomes.
\item
  A sample space is continuous if it contains an interval (either finite or infinite in length) of
  real numbers.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{event}{%
\section{Event}\label{event}}

\textbf{Definition:}

An \textbf{event} is a \textbf{subset} of the sample space of a random experiment. It is a \textbf{collection} of outcomes.

Examples of events:

\begin{itemize}
\tightlist
\item
  The event of a healthy temperature: temperature 37-38 degrees Celsius
\item
  The event of producing a screw with a size: of 71.5mm
\item
  The event of receiving more than 4 emails in an hour.
\item
  The event of obtaining a number less than 3 in the throw of a dice
\end{itemize}

One event refers to a possible set of \textbf{outcomes}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{event-operations}{%
\section{Event operations}\label{event-operations}}

For two events \(A\) and \(B\), we can construct the following derived events:

\begin{itemize}
\tightlist
\item
  Complement \(A'\): the event of \textbf{not} \(A\)
\item
  Union \(A \cup B\): the event of \(A\) \textbf{or} \(B\)\\
\item
  Intersection \(A \cap B\): the event of \(A\) \textbf{and} \(B\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{event-operations-example}{%
\section{Event operations example}\label{event-operations-example}}

Take

\begin{itemize}
\tightlist
\item
  Event \(A:\{1,2,3\}\) a number less or equal to three in the throw of a dice\\
\item
  Event \(B:\{2,4,6\}\) an even number in the throw of a dice
\end{itemize}

New events:

\begin{itemize}
\tightlist
\item
  Not less than three: \(A':\{4,5,6\}\)
\item
  Less or equal to three \textbf{or} even: \(A \cup B: \{1,2,3,4,6\}\)
\item
  Less or equal to three \textbf{and} even \(A \cap B: \{2\}\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{outcomes}{%
\section{Outcomes}\label{outcomes}}

Outcomes are events that are \textbf{mutually exclusive}

\textbf{Definition:}

Two events denoted as \(E_1\) and \(E_2\), such that

\[E_1\cap E_2=\emptyset\]
They cannot occur at the same time.

Example:

\begin{itemize}
\item
  The outcome of obtaining \(1\) \textbf{and} the outcome of obtaining \(5\) in the throw of one dice are mutually exclusive:
\item
  The event of obtaining \(1\) and \(5\) is empty:\[\{1\}\cap \{5\}=\emptyset\]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-definition}{%
\section{Probability definition}\label{probability-definition}}

A probability is a number that is assigned to each possible event (\(E\)) of a sample space (\(S\)) of a random experiment that satisfies the following properties:

\begin{itemize}
\tightlist
\item
  \(P(S)=1\)
\item
  \(0 \leq P(E) \leq 1\)
\item
  when \(E_1\cap E_2=\emptyset\) \[P(E_1\cup E_2) = P(E_1) + P(E_2)\]
\end{itemize}

Proposed by Kolmogorov's (1933)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-properties}{%
\section{Probability properties}\label{probability-properties}}

Kolmogorov says that we can build a probability table (likewise the relative frequency table)

\begin{longtable}[]{@{}cc@{}}
\toprule
outcome & Probability \\
\midrule
\endhead
\(1\) & 1/6 \\
\(2\) & 1/6 \\
\(3\) & 1/6 \\
\(4\) & 1/6 \\
\(5\) & 1/6 \\
\(6\) & 1/6 \\
\(P(1 \cup 2\cup ... \cup 6)\) & 1 \\
\bottomrule
\end{longtable}

As \(\{1,2,3,4,5,6\}\) are mutually exclusive then

\[P(S)=P(1\cup 2\cup ... \cup 6) = P(1)+P(2)+ ...+P(n)=1\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{addition-rule}{%
\section{Addition Rule}\label{addition-rule}}

When \(A\) and \(B\) are not mutually exclusive then:

\[P(A \cup B)=P(A) + P(B) - P(A\cap B)\]

Where \(P(A)\) and \(P(B)\) are called the \textbf{marginal probabilities}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-addition-rule}{%
\section{Example Addition Rule}\label{example-addition-rule}}

Take

\begin{itemize}
\tightlist
\item
  Event \(A:\{1,2,3\}\) a number less or equal to three in the throw of a dice\\
\item
  Event \(B:\{2,4,6\}\) an even number in the throw of a dice
\end{itemize}

then:

\begin{itemize}
\tightlist
\item
  \(P(A): P(1) + P(2) + P(3)=3/6\)
\item
  \(P(B): P(2) + P(4) + P(6)=3/6\)
\item
  \(P(A \cap B): P(2) = 1/6\)
\end{itemize}

\(P(A \cup B)=P(A) + P(B) - P(A\cap B)=3/6+3/6-1/6=5/6\)

Note: \(P(2)\) appears in \(P(A)\) and \(P(B)\) that's why we subtract it with the intersection

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{venn-diagram}{%
\section{Venn diagram}\label{venn-diagram}}

Note that can always break down the sample space in \textbf{mutually exclusive} sets involving the intersections:

\(S=\{A\cap B, A \cap B', A'\cap B, A'\cap B'\}\)

Marginals:

\begin{itemize}
\tightlist
\item
  \(P(A)=P(A\cap B') + P(A \cap B)=2/6+1/6=3/6\)
\item
  \(P(B)=P(A'\cap B) +P(A \cap B)=2/6+1/6=3/6\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-table}{%
\section{Probability table}\label{probability-table}}

Let's look at the probability table

\begin{longtable}[]{@{}cc@{}}
\toprule
outcome & Probability \\
\midrule
\endhead
\(A\cap B\) & \(P(A\cap B)\) \\
\(A\cap B'\) & \(P(A\cap B')\) \\
\(A'\cap B\) & \(P(A'\cap B)\) \\
\(A'\cap B'\) & \(P(A'\cap B')\) \\
sum & \(1\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-probability-table}{%
\section{Example probability table}\label{example-probability-table}}

We also write \(A \cap B\) as \((A,B)\) and call it the \textbf{joint probability} of \(A\) and \(B\)

In our example:

\begin{longtable}[]{@{}cc@{}}
\toprule
outcome & Probability \\
\midrule
\endhead
\((A, B)\) & \(P(A, B)=1/6\) \\
\((A, B')\) & \(P(A, B')=2/6\) \\
\((A', B)\) & \(P(A', B)=2/6\) \\
\((A', B')\) & \(P(A', B')=1/6\) \\
sum & \(1\) \\
\bottomrule
\end{longtable}

Note: each outcome has \(two\) values (one for the characteristic of type \(A\) and another for type \(B\))

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{contingency-table}{%
\section{Contingency table}\label{contingency-table}}

We can organize the probability of \textbf{joint outcomes} in a \textbf{contingency table}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& \(B\) & \(B'\) & sum \\
\midrule
\endhead
\(A\) & \(P(A, B )\) & \(P(A, B' )\) & \(P(A)\) \\
\(A'\) & \(P(A', B )\) & \(P(A', B' )\) & \(P(A')\) \\
sum & \(P(B)\) & \(P(B')\) & 1 \\
\bottomrule
\end{longtable}

Marginals:

\begin{itemize}
\tightlist
\item
  \(P(A)=P(A, B') + P(A, B)\)
\item
  \(P(B)=P(A', B) +P(A, B)\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-contingency-table}{%
\section{Example contingency table}\label{example-contingency-table}}

\begin{itemize}
\tightlist
\item
  Event \(A:\{1,2,3\}\) a number less or equal to three in the throw of a dice\\
\item
  Event \(B:\{2,4,6\}\) an even number in the throw of a dice
\end{itemize}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& \(B\) & \(B'\) & sum \\
\midrule
\endhead
\(A\) & \(1/6\) & \(2/6\) & \(3/6\) \\
\(A'\) & \(2/6\) & \(1/6\) & \(3/6\) \\
sum & \(3/6\) & \(3/6\) & 1 \\
\bottomrule
\end{longtable}

Three forms of the \textbf{addition rule}:

\(P(A \cup B)\)\[=P(A) + P(B) - P(A\cap B)\]
\[=P(A \cap B)+P(A\cap B')+P(A'\cap B)\]
\[=1-P(A'\cap B')\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{misophonia-study}{%
\section{Misophonia study}\label{misophonia-study}}

In the misophonia study, the patients were assessed for their misophonia severity \textbf{and} if they were depressed.

The outcome of one random experiment is to measure the misophonia severity \textbf{and} depression status of one patient. The repetition of the random experiment was to perform the same two measurements on another patient.

\begin{verbatim}
##     Misofonia.dic depresion.dic
## 1               4             1
## 2               2             0
## 3               0             0
## 4               3             0
## 5               0             0
## 6               0             0
## 7               2             0
## 8               3             0
## 9               0             1
## 10              3             0
## 11              0             0
## 12              2             0
## 13              2             1
## 14              0             0
## 15              2             0
## 16              0             0
## 17              0             0
## 18              3             0
## 19              3             0
## 20              0             0
## 21              3             0
## 22              3             0
## 23              2             0
## 24              0             0
## 25              0             0
## 26              0             0
## 27              4             1
## 28              2             0
## 29              2             0
## 30              0             0
## 31              2             0
## 32              0             0
## 33              0             0
## 34              0             0
## 35              3             0
## 36              0             0
## 37              2             0
## 38              3             1
## 39              2             0
## 40              2             0
## 41              0             0
## 42              2             0
## 43              3             0
## 44              0             0
## 45              0             0
## 46              2             0
## 47              2             0
## 48              3             0
## 49              3             0
## 50              0             0
## 51              0             0
## 52              4             1
## 53              3             0
## 54              3             1
## 55              2             1
## 56              0             1
## 57              2             0
## 58              0             0
## 59              0             0
## 60              0             0
## 61              2             0
## 62              2             0
## 63              0             0
## 64              0             0
## 65              2             0
## 66              3             1
## 67              0             0
## 68              1             0
## 69              3             0
## 70              2             0
## 71              4             1
## 72              3             0
## 73              2             1
## 74              3             0
## 75              0             1
## 76              2             0
## 77              3             0
## 78              2             0
## 79              4             1
## 80              1             0
## 81              2             0
## 82              0             0
## 83              2             0
## 84              0             0
## 85              2             0
## 86              0             1
## 87              2             0
## 88              2             0
## 89              4             1
## 90              3             0
## 91              0             1
## 92              3             0
## 93              0             0
## 94              0             0
## 95              0             0
## 96              2             0
## 97              2             0
## 98              1             0
## 99              3             0
## 100             0             0
## 101             0             0
## 102             3             1
## 103             2             0
## 104             1             0
## 105             3             0
## 106             0             0
## 107             4             1
## 108             4             1
## 109             2             0
## 110             3             0
## 111             3             0
## 112             3             1
## 113             0             0
## 114             3             0
## 115             2             0
## 116             1             0
## 117             2             0
## 118             3             1
## 119             3             0
## 120             4             1
## 121             2             0
## 122             3             0
## 123             2             0
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{contingency-table-for-frequencies}{%
\section{Contingency table for frequencies}\label{contingency-table-for-frequencies}}

\begin{itemize}
\tightlist
\item
  For the number of observations \(n_{i,j}\) of each outcome \((x_i, y_i)\), misophonia: \(x\in \{0,1,2,3,4\}\) and depression \(y\in \{0,1\}\) (no:\(0\), yes:\(1\))
\end{itemize}

\begin{verbatim}
##               
##                Depression:0 Depression:1
##   Misophonia:4            0            9
##   Misophonia:3           25            6
##   Misophonia:2           34            3
##   Misophonia:1            5            0
##   Misophonia:0           36            5
\end{verbatim}

\begin{itemize}
\tightlist
\item
  For the relative frequencies \(f_{i,j}\)
\end{itemize}

\begin{verbatim}
##               
##                Depression:0 Depression:1
##   Misophonia:4   0.00000000   0.07317073
##   Misophonia:3   0.20325203   0.04878049
##   Misophonia:2   0.27642276   0.02439024
##   Misophonia:1   0.04065041   0.00000000
##   Misophonia:0   0.29268293   0.04065041
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{heat-map}{%
\section{Heat map}\label{heat-map}}

The contingency table can be plotted as a \textbf{heat map}

\includegraphics{_main_files/figure-latex/unnamed-chunk-30-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continous-variables}{%
\section{Continous variables}\label{continous-variables}}

In the misophonia study, the jaw protrusion was also measured as a possible cephalometric factor for de disease.

\begin{verbatim}
##     Angulo_convexidad protusion.mandibular
## 1                7.97                13.00
## 2               18.23                -5.00
## 3               12.27                11.50
## 4                7.81                16.80
## 5                9.81                33.00
## 6               13.50                 2.00
## 7               19.30                -3.90
## 8                7.70                16.80
## 9               12.30                 8.00
## 10               7.90                28.80
## 11              12.60                 3.00
## 12              19.00                -7.90
## 13               7.27                28.30
## 14              14.00                 4.00
## 15               5.40                22.20
## 16               8.00                 0.00
## 17              11.20                15.00
## 18               7.75                17.00
## 19               7.94                49.00
## 20              16.69                 5.00
## 21               7.62                42.00
## 22               7.02                28.00
## 23               7.00                 9.40
## 24              19.20               -13.20
## 25               7.96                23.00
## 26              14.70                 2.30
## 27               7.24                25.00
## 28               7.80                 4.90
## 29               7.90                92.00
## 30               4.70                 6.00
## 31               4.40                17.00
## 32              14.00                 3.30
## 33              14.40                10.30
## 34              16.00                 6.30
## 35               1.40                19.50
## 36               9.76                22.00
## 37               7.90                 5.00
## 38               7.90                78.00
## 39               7.40                 9.30
## 40               6.30                50.60
## 41               7.76                18.00
## 42               7.30                18.00
## 43               7.00                10.00
## 44              11.23                 4.00
## 45              16.00                13.30
## 46               7.90                48.00
## 47               7.29                23.50
## 48               6.91                37.60
## 49               7.10                15.00
## 50              13.40                 5.10
## 51              11.60                -2.20
## 52              -1.00                32.00
## 53               6.00                25.00
## 54               7.82                24.00
## 55               4.80                33.60
## 56              11.00                 3.30
## 57               9.00                31.50
## 58              11.50                12.80
## 59              16.00                 3.00
## 60              15.00                 6.00
## 61               1.40                21.40
## 62              16.80               -10.00
## 63               7.70                19.00
## 64              16.14                32.00
## 65               7.12                15.00
## 66              -1.00                10.00
## 67              17.00               -16.90
## 68               9.26                 2.00
## 69              18.70               -10.10
## 70               3.40                12.20
## 71              21.30               -11.00
## 72               7.50                 5.20
## 73               6.03                16.00
## 74               7.50                 5.80
## 75              19.00                 5.20
## 76              19.01                13.00
## 77               8.10                13.60
## 78               7.80                16.10
## 79               6.10                33.20
## 80              15.26                 4.00
## 81               7.95                12.00
## 82              18.00                -1.50
## 83               4.60                18.30
## 84              15.00                 3.00
## 85               7.50                15.80
## 86               8.00                27.10
## 87              16.80               -10.00
## 88               8.54                25.00
## 89               7.00                27.10
## 90              18.30                -8.00
## 91               7.80                12.00
## 92              16.00                -8.00
## 93              14.00                23.00
## 94              12.30                 5.00
## 95              11.40                 1.00
## 96               8.50                18.90
## 97               7.00                15.00
## 98               7.96                22.00
## 99              17.60                -3.50
## 100             10.00                20.00
## 101              3.50                12.20
## 102              6.70                14.70
## 103             17.00                -5.00
## 104             20.26                -4.15
## 105              6.64                11.00
## 106              1.80                -4.00
## 107              7.02                25.00
## 108              2.46                35.00
## 109             19.00                -5.00
## 110             17.86               -30.00
## 111              6.10                12.20
## 112              6.64                19.00
## 113             12.00                 1.60
## 114              6.60                20.00
## 115              8.70                17.10
## 116             14.05                24.00
## 117              7.20                 7.10
## 118             19.70               -11.00
## 119              7.70                21.30
## 120              6.02                 5.00
## 121              2.50                12.90
## 122             19.00                 5.90
## 123              6.80                 5.80
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{heat-map-for-continuous-variables}{%
\section{Heat map for continuous variables}\label{heat-map-for-continuous-variables}}

\begin{itemize}
\item
  Two dimensional \textbf{histogram}.
\item
  It illustrates the ``continuous contingency'' table for continuous variables
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-32-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{scatter-plot}{%
\section{Scatter plot}\label{scatter-plot}}

\begin{itemize}
\item
  The \textbf{histogram} depends on the size of the bin (pixel).
\item
  If the pixel is small enough to contain a single observation then the heat map results in a \textbf{scatter plot}
\end{itemize}

The scatter plot is the illustration of a ``contingency table'' for continuous variables when the bin (pixel) is small enough to contain one single observation (consisting of a pair of values).

\includegraphics{_main_files/figure-latex/unnamed-chunk-33-1.pdf}

\hypertarget{conditional-probability}{%
\chapter{Conditional Probability}\label{conditional-probability}}

\hypertarget{objective-2}{%
\section{Objective}\label{objective-2}}

\begin{itemize}
\tightlist
\item
  Conditional probability
\item
  Independence
\item
  Bayes' theorem
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{joint-probability}{%
\section{Joint Probability}\label{joint-probability}}

The joint probability of two events \(A\) and \(B\) is
\[P(A,B)=P(A \cap B)\]

Let's imagine a random experiment that measures two different types of outcomes.

\begin{itemize}
\item
  height and weight of an individual: \((h, w)\)
\item
  time and place of an electric charge: \((p, t)\)
\item
  a throw of two dice: (\(n_1\),\(n_2\))
\item
  cross two traffic lights in green: (\(\bar{R_1}\), \(\bar{R_2}\))
\end{itemize}

In many cases, we are interested in finding out whether the values of one outcome \textbf{condition} the values of the other.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{diagnostics}{%
\section{Diagnostics}\label{diagnostics}}

Let's consider a \textbf{diagnostic tool}

We want to find the state of a system (s):

\begin{itemize}
\tightlist
\item
  inadequate (yes)
\item
  adequate (no)
\end{itemize}

with a test (t):

\begin{itemize}
\tightlist
\item
  positive
\item
  negative
\end{itemize}

We test a battery to find how long it can live. We stress a cable to find if it resists carrying a certain load. We perform a PCR to see if someone is infected.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{diagnostics-test}{%
\section{Diagnostics Test}\label{diagnostics-test}}

Let's consider diagnosing infection with a new test.

Infection status:

\begin{itemize}
\tightlist
\item
  yes (infected)
\item
  no (not infected)
\end{itemize}

Test:

\begin{itemize}
\tightlist
\item
  positive
\item
  negative
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{observations}{%
\section{Observations}\label{observations}}

Each individual is a random experiment with two measurements: (Infection, Test)

\begin{longtable}[]{@{}ccc@{}}
\toprule
Subject & Infection & Test \\
\midrule
\endhead
\(s_1\) & yes & positive \\
\(s_2\) & no & negative \\
\(s_3\) & yes & positive \\
\ldots{} & \ldots{} & \ldots{} \\
\(s_i\) & no & positive* \\
\ldots{} & \ldots{} & \ldots{} \\
\ldots{} & \ldots{} & \ldots{} \\
\(s_n\) & yes & negative* \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{contingency-tables}{%
\section{Contingency tables}\label{contingency-tables}}

\begin{itemize}
\tightlist
\item
  For the number of observations of each outcome
\end{itemize}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Infection: yes & Infection: no & sum \\
\midrule
\endhead
Test: positive & 18 & 12 & 30 \\
Test: negative & 30 & 300 & 330 \\
sum & 48 & 312 & 360 \\
\bottomrule
\end{longtable}

\begin{itemize}
\tightlist
\item
  For the relative frequencies, if \(N>>0\) we will take \(f_{i,j}=\hat{P}(x_i, y_j)\)
\end{itemize}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Infection: yes & Infection: no & sum \\
\midrule
\endhead
Test: positive & 0.05 & 0.0333 & 0.0833 \\
Test: negative & 0.0833 & 0.833 & 0.9166 \\
sum & 0.133 & 0.866 & 1 \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conditional-probability-1}{%
\section{Conditional probability}\label{conditional-probability-1}}

Let's think first in terms of those who are \textbf{infected}

Within those who are infected (\textbf{yes}), what is the probability of those who tested positive?

\begin{itemize}
\tightlist
\item
  Sensitivity (true positive rate)
\end{itemize}

\[\hat{P}(positive|yes)=\frac{n_{positive,yes}}{n_{yes}}\]

\[=\frac{\frac{n_{positive,yes}}{N}}{\frac{n_{yes}}{N}}=\frac{f_{positive,yes}}{f_{yes}}\]

Therefore, in the limit, we expect to have a probability of the type

\[P(positive|yes)=\frac{P(positive, yes)}{P(yes)}=\frac{P(positive \cap yes)}{P(yes)}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conditional-probability-2}{%
\section{Conditional probability}\label{conditional-probability-2}}

\textbf{Definition:}
The conditional probability of an event B given an event A, denoted as \(P(A|B)\), is
\[P(A|B) = \frac{P(A\cap B)}{P(B)}\]

\begin{itemize}
\tightlist
\item
  you can prove that the conditional probability satisfies the axioms of probability.
\item
  it is the probability with the sampling space given by \(B\): \(S_B\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conditional-contingency-table}{%
\section{Conditional contingency table}\label{conditional-contingency-table}}

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Infection: Yes & Infection: No \\
\midrule
\endhead
Test: positive & P(positive {\textbar{}} yes) & P(positive {\textbar{}} no) \\
Test: negative & P(negative {\textbar{}} yes) & P(negative {\textbar{}} no) \\
sum & 1 & 1 \\
\bottomrule
\end{longtable}

\begin{itemize}
\item
  True positive rate (Sensitivity): The probability of testing positive \textbf{if} having the disease \(P(positive|yes)\)
\item
  True negative rate (Specificity): The probability of testing negative \textbf{if} not having the disease \(P(negative|no)\)
\item
  False-positive rate: The probability of testing positive \textbf{if} not having the disease \(P(positive|no)\)
\item
  False-negative rate: The probability of testing negative \textbf{if} having the disease \(P(negative|yes)\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-conditional-contingency-table}{%
\section{Example conditional contingency table}\label{example-conditional-contingency-table}}

Taking the frequencies as estimates of the probabilities then

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Infection: Yes & Infection: No \\
\midrule
\endhead
Test: positive & 18/48 = 0.375 & 12/312 = 0.038 \\
Test: negative & 30/48 = 0.625 & 300/312 =0.962 \\
sum & 1 & 1 \\
\bottomrule
\end{longtable}

Our diagnostic tool has low sensitivity (0.375) but high
specificity (0.962).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multiplication-rule}{%
\section{Multiplication rule}\label{multiplication-rule}}

Now let's imagine the real situation where we want to compute \textbf{joint} probabilities from conditional \textbf{probabilities}

\begin{itemize}
\item
  PCRs for coronavirus were (performed){[}\url{https://www.nejm.org/doi/full/10.1056/NEJMp2015897}{]} in people in the hospital who we are sure to be infected. They have a sensitivity of 70\%. They have also been tested in the lab in conditions of no infection with 96\% specificity
\item
  A prevalence study in Spain showed that \(P(yes)=0.05\), \(P(no)=0.95\) before summer.
\end{itemize}

With this data, what was the probability that a randomly selected person in the population tested positive \textbf{and} was infected: \(P(yes \cap positive)=P(yes, positive)\)?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{diagnostic-performance}{%
\section{Diagnostic performance}\label{diagnostic-performance}}

To study the performance of a new diagnostic test:

\begin{itemize}
\item
  you select specimens that are inadequate (disease: \textbf{yes}) and apply the test, trying to find its sensitivity: \(P(positive|yes)\) (0.70 for PCRs)
\item
  you select specimens that are adequate (disease: \textbf{no}) and apply the test, trying to find its specificity: \(P(negative|no)\) (0.96 for PCRs)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Infection: Yes & Infection: No \\
\midrule
\endhead
Test: positive & P(positive{\textbar{}}yes)=0.7 & P(positive{\textbar{}}no)=0.06 \\
Test: negative & P(negative{\textbar{}}yes)=0.3 & P(negative{\textbar{}}no)=0.94 \\
sum & 1 & 1 \\
\bottomrule
\end{longtable}

From this matrix, can we obtain \(P(yes, positive)\)?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multiplication-rule-1}{%
\section{Multiplication rule}\label{multiplication-rule-1}}

How do you recover the joint probability from the conditional probability?

For two events \(A\) and \(B\) we have the multiplication rule

\[P(A, B) =  P(A|B) P(B)\]

that follows from the definition of the conditional probability.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{contingency-table-in-terms-of-conditional-probabilities}{%
\section{Contingency table in terms of conditional probabilities}\label{contingency-table-in-terms-of-conditional-probabilities}}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Infection: Yes & Infection: No & sum \\
\midrule
\endhead
Test: positive & P(positive {\textbar{}} yes)P(yes) & P(positive {\textbar{}} no)P(no) & P(positive) \\
Test: negative & P(negative {\textbar{}} yes)P(yes) & P(negative {\textbar{}} no) P(no) & P(negative) \\
sum & P(yes) & P(no) & 1 \\
\bottomrule
\end{longtable}

For instance the probability of testing \(positive\) and being infected \(yes\):

\begin{itemize}
\tightlist
\item
  \(P(positive, yes)=P(positive \cap yes) = P(positive|yes) P(yes)\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conditional-tree}{%
\section{Conditional tree}\label{conditional-tree}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{contingency-table-in-terms-of-conditional-probabilities-1}{%
\section{Contingency table in terms of conditional probabilities}\label{contingency-table-in-terms-of-conditional-probabilities-1}}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Infection: yes & Infection: no & sum \\
\midrule
\endhead
Test: positive & 0.035 & 0.057 & 0.092 \\
Test: negative & 0.015 & 0.893 & 0.908 \\
sum & 0.05 & 0.95 & 1 \\
\bottomrule
\end{longtable}

\begin{itemize}
\tightlist
\item
  \(P(positive,yes)= 0.035\)
\end{itemize}

But we also found the marginal of being positive:

\begin{itemize}
\tightlist
\item
  \(P(positive)=0.092\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{total-probability-rule}{%
\section{Total probability rule}\label{total-probability-rule}}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Infection: Yes & Infection: No & sum \\
\midrule
\endhead
Test: positive & P(positive {\textbar{}} yes)P(yes) & P(positive {\textbar{}} no)P(no) & P(positive) \\
Test: negative & P(negative {\textbar{}} yes)P(yes) & P(negative {\textbar{}} no) P(no) & P(negative) \\
sum & P(yes) & P(no) & 1 \\
\bottomrule
\end{longtable}

When we write the unknown marginals in terms of their conditional probabilities we call it the \textbf{total probability rule}

\begin{itemize}
\tightlist
\item
  \(P(positive)=P(positive|yes)P(yes)+P(positive|no)P(no)\)
\item
  \(P(negative)=P(negative|yes)P(yes)+P(negative|no)P(no)\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conditional-tree-1}{%
\section{Conditional tree}\label{conditional-tree-1}}

\textbf{Total probability rule} for the marginal of \(B\): In how many ways I can obtain the outcome \(B\)?

\(P(B)=P(B|A)P(A)+P(B|A')P(A')\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{finding-reverse-probabilities}{%
\section{Finding reverse probabilities}\label{finding-reverse-probabilities}}

From the conditional contingency table

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Infection: Yes & Infection: No \\
\midrule
\endhead
Test: positive & P(positive {\textbar{}} yes) & P(positive {\textbar{}} no) \\
Test: negative & P(negative {\textbar{}} yes) & P(negative {\textbar{}} no) \\
sum & 1 & 1 \\
\bottomrule
\end{longtable}

How can we calculate the probability of being infected if tested positive: \(P(yes|positive)\)?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{recover-joint-probabilities}{%
\section{Recover joint probabilities}\label{recover-joint-probabilities}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We recover the contingency table for joint probabilities
\end{enumerate}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Infection: Yes & Infection: No & sum \\
\midrule
\endhead
Test: positive & P(positive {\textbar{}} yes)P(yes) & P(positive {\textbar{}} no)P(no) & P(positive) \\
Test: negative & P(negative {\textbar{}} yes)P(yes) & P(negative {\textbar{}} no) P(no) & P(negative) \\
sum & P(yes) & P(no) & 1 \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{reverse-conditionals}{%
\section{Reverse conditionals}\label{reverse-conditionals}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We compute the conditional probabilities for the test:
\end{enumerate}

\[P(infection|test)=\frac{P(test|infection)P(infection)}{P(test)}\]

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Infection: Yes & Infection: No & sum \\
\midrule
\endhead
Test: positive & P(yes{\textbar{}}positive) & P(no{\textbar{}}positive) & 1 \\
Test: negative & P(yes{\textbar{}}negative) & P(no{\textbar{}}negative) & 1 \\
\bottomrule
\end{longtable}

For instance:
\[P(yes|positive)=\frac{P(positive|yes)P(yes)}{P(positive)}\]
since we usually don't have \(P(positive)\) we use the \textbf{total probability} rule in the denominator

\[P(yes|positive)=\frac{P(positive|yes)P(yes)}{P(positive|yes)P(yes)+P(positive|no)P(no)}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bayes-theorem}{%
\section{Baye's theorem}\label{bayes-theorem}}

The expression:

\[P(yes|positive)=\frac{P(positive|yes)P(yes)}{P(positive|yes)P(yes)+P(positive|no)P(no)}\]

is called the \textbf{Bayes theorem}

\textbf{Theorem}

If \(E1, E2, ..., Ek\) are \(k\) mutually exclusive and exhaustive events and \(B\) is any event,

\[P(Ei|B)=\frac{P(B|Ei)P(Ei)}{P(B|E1)P(E1) +...+ P(B|Ek)P(Ek)}\]

It allows to reverse the conditionals:

\[P(B|A) \rightarrow P(A|B)\]

Or \textbf{design} a test \(B\) in controlled condition \(A\) and then use it to \textbf{infer} the probability of the condition when the test is positive.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-bayes-theorem}{%
\section{Example: Bayes' theorem}\label{example-bayes-theorem}}

Baye's theorem:

\[P(yes|positive)  = \frac{P(positive|yes) P(yes)}{P(possitive|yes)P(yes)+P(positive|no)P(no)}\]

we know:

\begin{itemize}
\item
  \(P(positive|yes)=0.70\)
\item
  \(P(positive|no)=1- P(negative|no)=0.06\)
\item
  the probability of infection and not infection in the population: \(P(yes)=0.05\) and \(P(no)=1-P(yes)=0.95\).
\end{itemize}

Therefore:

\[P(yes|positive)=0.47\]

Tests are not so good to \textbf{confirm} infections.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-bayes-theorem-1}{%
\section{Example: Bayes' theorem}\label{example-bayes-theorem-1}}

Let's now apply it to the probability of not being infected if the test is negative

\[P(no|negative)  = \frac{P(negative|no)  P(no)}{P(negative|no) P(no)+P(negative|yes)P(yes)}\]

Substitution of all the values gives

\[P(no|negative)=0.98\]

Tests are good to \textbf{rule out} infections.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistical-independence}{%
\section{Statistical independence}\label{statistical-independence}}

In many applications, we want to know if the knowledge of one event conditions the outcome of another event.

\begin{itemize}
\tightlist
\item
  there are cases where we want to know if the events are not conditioned
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistical-independence-1}{%
\section{Statistical independence}\label{statistical-independence-1}}

Consider conductors for which we measure their surface flaws and if their conduction capacity is defective

The estimated \textbf{joint probabilities} are

\begin{longtable}[]{@{}cccc@{}}
\toprule
& flaws (F) & no flaws (F') & sum \\
\midrule
\endhead
defective (D) & \(0.005\) & \(0.045\) & \(0.05\) \\
no defective (D') & \(0.095\) & \(0.855\) & \(0.95\) \\
sum & \(0.1\) & \(0.9\) & 1 \\
\bottomrule
\end{longtable}

where, for instance, the joint probability of \(F\) and \(D\) is

\begin{itemize}
\tightlist
\item
  \(P(D,F)=0.005\)
\end{itemize}

The marginal probabilities are

\begin{itemize}
\tightlist
\item
  \(P(D)=P(D, F) + P(D, F')=0.05\)
\item
  \(P(F)=P(D, F) + P(D', F)= 0.1\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistical-independence-2}{%
\section{Statistical independence}\label{statistical-independence-2}}

What is the \textbf{conditional probability} of observing a defective conductor if they have a flaw?

\begin{longtable}[]{@{}ccc@{}}
\toprule
& F & F' \\
\midrule
\endhead
D & P(D{\textbar{}}F) = 0.05 & P(D{\textbar{}}F')=0.05 \\
D' & P(D'{\textbar{}}F)=0.95 & P(D'{\textbar{}}F')=0.95 \\
sum & 1 & 1 \\
\bottomrule
\end{longtable}

The marginals and the conditional probabilities are the same!

\begin{itemize}
\tightlist
\item
  \(P(D|F)=P(D|F')=P(D)\)
\item
  \(P(D'|F)=P(D'|F')=P(D')\)
\end{itemize}

The probability of observing a defective conductor \textbf{does not} depend on having observed or not a flaw.

\[P(D) = P(D|F)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistical-independence-3}{%
\section{Statistical independence}\label{statistical-independence-3}}

Two events \(A\) and \(B\) are statistically independent if

\begin{itemize}
\tightlist
\item
  \(P(A|B)=P(A)\); \(A\) is independent of \(B\)
\item
  \(P(B|A)=P(B)\); \(B\) is independent of \(A\)
\end{itemize}

and by the multiplication rule, their joint probability is

\begin{itemize}
\tightlist
\item
  \(P(A\cap B)=P(A|B)P(B)=P(A)P(B)\)
\end{itemize}

the multiplication of their marginal probabilities.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{products-of-marginals-products}{%
\section{Products of marginals products}\label{products-of-marginals-products}}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& F & F' & sum \\
\midrule
\endhead
D & \(0.005\) & \(0.045\) & \(0.05\) \\
D' & \(0.095\) & \(0.855\) & \(0.95\) \\
sum & \(0.1\) & \(0.9\) & 1 \\
\bottomrule
\end{longtable}

Confirm that all the entries of the matrix are the product of the marginals.

For example:

\begin{itemize}
\tightlist
\item
  \(P(F)P(D)= P(D \cap F)\)
\item
  \(P(D')P(F')=P(D' \cap F')\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-5}{%
\section{Example}\label{example-5}}

Outcomes of throwing two coins: \(S={(H,H), (H,T), (T,H), (T,T)}\)

\begin{longtable}[]{@{}cccc@{}}
\toprule
& H & T & sum \\
\midrule
\endhead
H & \(1/4\) & \(1/4\) & \(1/2\) \\
T & \(1/4\) & \(1/4\) & \(1/2\) \\
sum & \(1/2\) & \(1/2\) & 1 \\
\bottomrule
\end{longtable}

\begin{itemize}
\tightlist
\item
  Obtaining a head in the first coin does not condition obtaining a tail in the result of the second coin \(P(T|H)=P(T)=1/2\)
\item
  the probability of obtaining a head and then a tail is the product of each independent outcome \(P(H, T)=P(H)*P(T)=1/4\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{discrete-random-variables}{%
\chapter{Discrete Random Variables}\label{discrete-random-variables}}

\hypertarget{objective-3}{%
\section{Objective}\label{objective-3}}

\begin{itemize}
\tightlist
\item
  Random variables
\item
  Probability mass function
\item
  Mean and variance
\item
  Probability distribution
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{how-do-we-assign-probability-values-to-outcomes}{%
\section{How do we assign probability values to outcomes?}\label{how-do-we-assign-probability-values-to-outcomes}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{random-variable}{%
\section{Random variable}\label{random-variable}}

\textbf{Definition:}

A \textbf{random variable} is a function that assigns a real \textbf{number} to each \textbf{outcome} in the sample space of a random experiment.

\begin{itemize}
\tightlist
\item
  Most commonly a random variable is the value of the \textbf{measurement} of interest that is made in a random experiment.
\end{itemize}

A random variable can be:

\begin{itemize}
\tightlist
\item
  Discrete (nominal, ordinal)
\item
  Continuous (interval, ratio)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{random-variable-1}{%
\section{Random variable}\label{random-variable-1}}

A \textbf{value} (or \textbf{outcome}) of a random variable is one of the possible numbers that the variable can take in a random experiment.

We write the random variable in \textbf{capitals}.

Example:

If \(X \in \{0,1\}\), we then say \(X\) is a random variable that can take the values \(0\) or \(1\).

\textbf{Observation} of a random variable

\begin{itemize}
\tightlist
\item
  An observation is the \textbf{acquisition} of the value of a random variable in a random experiment
\end{itemize}

Example:

1 0 0 1 0 \textbf{1} 0 1 1

The number in bold is an observation of \(X\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{events-of-observing-a-random-variable}{%
\section{Events of observing a random variable}\label{events-of-observing-a-random-variable}}

\begin{itemize}
\tightlist
\item
  \(X=1\) is the \textbf{event} of observing the random variable \(X\) with value \(1\)
\item
  \(X=2\) is the \textbf{event} of observing the random variable \(X\) with value \(2\)
\end{itemize}

\ldots{}

\textbf{In general:}

\begin{itemize}
\item
  \(X=x\) is the \textbf{event} of observing the random variable \(X\) with value \(x\) (little \(x\))
\item
  Any two values of a random variable define two \textbf{mutually exclusive} events.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-of-random-variables}{%
\section{Probability of random variables}\label{probability-of-random-variables}}

We are interested in assigning probabilities to the values of a random variable.

We have already done this for the dice: \(X \in \{1,2,3,4,5,6\}\) (classical interpretation of pribability)

\begin{longtable}[]{@{}cc@{}}
\toprule
\(X\) & Probability \\
\midrule
\endhead
\(1\) & \(P(X=1)=1/6\) \\
\(2\) & \(P(X=2)=1/6\) \\
\(3\) & \(P(X=3)=1/6\) \\
\(4\) & \(P(X=4)=1/6\) \\
\(5\) & \(P(X=5)=1/6\) \\
\(6\) & \(P(X=6)=1/6\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-functions}{%
\section{Probability functions}\label{probability-functions}}

\begin{itemize}
\tightlist
\item
  We can write the probability table
\item
  plot it
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-34-1.pdf}

\begin{itemize}
\tightlist
\item
  or write as the function
\end{itemize}

\[f(x)=P(X=x)=1/6\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-functions-1}{%
\section{Probability functions}\label{probability-functions-1}}

We can \textbf{create} any type of probability function if we respect the probability rules:

\includegraphics{_main_files/figure-latex/unnamed-chunk-35-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-functions-2}{%
\section{Probability functions}\label{probability-functions-2}}

For a discrete random variable \(X \in \{x_1 , x_2 , .. , x_M\}\) , a \textbf{probability mass function}

is always positive

\begin{itemize}
\tightlist
\item
  \(f(x_i)\geq 0\)
\end{itemize}

is used to compute probabilities

\begin{itemize}
\tightlist
\item
  \(f(x_i)=P(X=x_i)\)
\end{itemize}

and its sum over all the values of the variable is \(1\):

\begin{itemize}
\tightlist
\item
  \(\sum_{i=1}^M f(x_i)=1\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-functions-3}{%
\section{Probability functions}\label{probability-functions-3}}

\begin{itemize}
\item
  Note that the definition of \(X\) and its probability mass function is general \textbf{without reference} to any experiment. The functions live in the model (abstract) space.
\item
  \(X\) and \(f(x)\) are abstract objects that may or may not map to an experiment
\item
  We have the freedom to construct them as we want as long as we respect their definition.
\item
  They have some \textbf{properties} that are derived exclusively from their definition.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-probability-mass-function}{%
\section{Example: Probability mass function}\label{example-probability-mass-function}}

Consider the following random variable \(X\) over the outcomes

\begin{longtable}[]{@{}cc@{}}
\toprule
outcome & \(X\) \\
\midrule
\endhead
\(a\) & 0 \\
\(b\) & 0 \\
\(c\) & 1.5 \\
\(d\) & 1.5 \\
\(e\) & 2 \\
\(f\) & 3 \\
\bottomrule
\end{longtable}

If each outcome is equally probable then what is the probability mass function of \(x\)?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-table-for-equally-likely-outcomes}{%
\section{Probability table for equally likely outcomes}\label{probability-table-for-equally-likely-outcomes}}

\begin{longtable}[]{@{}cc@{}}
\toprule
outcome & Probability(outcome) \\
\midrule
\endhead
\(a\) & \(1/6\) \\
\(b\) & \(1/6\) \\
\(c\) & \(1/6\) \\
\(d\) & \(1/6\) \\
\(e\) & \(1/6\) \\
\(f\) & \(1/6\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-table-for-x}{%
\section{\texorpdfstring{Probability table for \(X\)}{Probability table for X}}\label{probability-table-for-x}}

\begin{longtable}[]{@{}cc@{}}
\toprule
\(X\) & \(f(x)=P(X=x)\) \\
\midrule
\endhead
\(0\) & \(P(X=0)=2/6\) \\
\(1.5\) & \(P(X=1.5)=2/6\) \\
\(2\) & \(P(X=2)=1/6\) \\
\(3\) & \(P(X=3)=1/6\) \\
\bottomrule
\end{longtable}

We can compute, for instance, the following probabilities for events on the values of \(X\)

\begin{itemize}
\tightlist
\item
  \(P(X>3)\)
\item
  \(P(X=0\, \cup \, X=2 )\)
\item
  \(P(X \leq 2)\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-6}{%
\section{Example}\label{example-6}}

\textbf{Probability model:}

Consider the following experiment: In one urn put \(8\) balls and:

\begin{itemize}
\tightlist
\item
  mark \(1\) ball with \(-2\)
\item
  mark \(2\) balls with \(-1\)
\item
  mark \(2\) balls with \(0\)
\item
  mark \(2\) balls with \(1\)
\item
  mark \(1\) ball with \(2\)
\end{itemize}

\textbf{experiment:} Take one ball and read the number.

\begin{longtable}[]{@{}cc@{}}
\toprule
\(X\) & \(P(X=x)\) \\
\midrule
\endhead
\(-2\) & \(1/8=0.125\) \\
\(-1\) & \(2/8=0.25\) \\
\(0\) & \(2/8=0.25\) \\
\(1\) & \(2/8=0.25\) \\
\(2\) & \(1/8=0.125\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-7}{%
\section{Example}\label{example-7}}

Consider another experiment where we do not know what is in the previous urn. We draw a ball \(30\) times, write its nuber and put it back in the urn.

\begin{itemize}
\item
  we do not know what the primary events with equal probabilities are.
\item
  we then \textbf{estimate} the probability mass function from the relative frequencies observed for a random variable
\end{itemize}

\begin{longtable}[]{@{}cc@{}}
\toprule
\(X\) & \(f_i\) \\
\midrule
\endhead
\(-2\) & \(0.132\) \\
\(-1\) & \(0.262\) \\
\(0\) & \(0.240\) \\
\(1\) & \(0.248\) \\
\(2\) & \(0.118\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probabilities-and-frequencies}{%
\section{Probabilities and frequencies}\label{probabilities-and-frequencies}}

For computing the relative frequencies \(f_i\) you have to

\begin{itemize}
\tightlist
\item
  \textbf{repeat} the experiment \(N\) times (you have to put the ball back in the urn each time) and at the end compute
\end{itemize}

\[f_i=n_i/N\]

We are assuming that:

\[lim_{N \rightarrow \infty} f_i = f(x_i)=P(X=x_i)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probabilities-and-relative-frequencies}{%
\section{Probabilities and relative frequencies}\label{probabilities-and-relative-frequencies}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-36-1.pdf}

\begin{itemize}
\tightlist
\item
  In this example we \textbf{know} the probability \textbf{model} \(f(x)=P(X=x)\) by design.
\item
  We never observe \(f(x)\)
\item
  We can use relative frequencies to estimate the probabilities
  \[f_i = \hat{f}(x_i)=\hat{P}(X=x_i)\] (\(f_i\) depends on \(N\))
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-and-variance}{%
\section{Mean and Variance}\label{mean-and-variance}}

The probability mass functions \(f(x)\) have two main properties

\begin{itemize}
\tightlist
\item
  its center
\item
  its spread
\end{itemize}

We can ask,

\begin{itemize}
\item
  around which values of \(X\) the probability concentrated?
\item
  How dispersed are the values of \(X\) in relation to their probabilities?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-and-variance-1}{%
\section{Mean and Variance}\label{mean-and-variance-1}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean}{%
\section{Mean}\label{mean}}

Remember that the \textbf{average} in terms of the relative frequencies of the values of \(x_i\) (categorical ordered outcomes) can be written as

\[\bar{x}= \sum_{i=1}^M x_i \frac{n_i}{N}=\sum_{i=1}^M x_i f_i\]

\textbf{Definition}

The \textbf{mean} (\(\mu\)) or expected value of a discrete random variable \(X\), \(E(X)\), with mass function \(f(x)\) is given by

\[ \mu = E(X)= \sum_{i=1}^M x_i f(x_i) \]

It is the center of gravity of the \textbf{probabilities}: The point where probability loadings on a road are balanced

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-mean}{%
\section{Example: Mean}\label{example-mean}}

What is the mean of \(X\) if its probability mass function \(f(x)\) is given by

\(P(X=0)=1/16\)
\(P(X=1)=4/16\)
\(P(X=2)=6/16\)
\(P(X=3)=4/16\)
\(P(X=4)=1/16\)

\includegraphics{_main_files/figure-latex/unnamed-chunk-37-1.pdf}

\[ \mu =E(X)=\sum_{i=1}^m x_i f(x_i) \]

\(E(X)=\)\textbf{0} * 1/16 + \textbf{1} * 4/16 + \textbf{2} * 6/16 + \textbf{3} * 4/16 + \textbf{4} * 1/16 =2

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-and-average}{%
\section{Mean and Average}\label{mean-and-average}}

\begin{itemize}
\tightlist
\item
  The mean \(\mu\) is the centre of grevity of the probability mass function \textbf{it does not change}
\end{itemize}

For instance for

\begin{longtable}[]{@{}cc@{}}
\toprule
\(X\) & \(P(X=x)\) \\
\midrule
\endhead
\(-2\) & \(1/8=0.125\) \\
\(-1\) & \(2/8=0.25\) \\
\(0\) & \(2/8=0.25\) \\
\(1\) & \(2/8=0.25\) \\
\(2\) & \(1/8=0.125\) \\
\bottomrule
\end{longtable}

\begin{itemize}
\tightlist
\item
  The average \(\bar{x}\) is the centre of gravity of the observations (relative frequencies) it \textbf{changes} with different data
\end{itemize}

For instance for

\begin{longtable}[]{@{}cc@{}}
\toprule
\(X\) & \(f_i\) \\
\midrule
\endhead
\(-2\) & \(0.132\) \\
\(-1\) & \(0.262\) \\
\(0\) & \(0.240\) \\
\(1\) & \(0.248\) \\
\(2\) & \(0.118\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{variance}{%
\section{Variance}\label{variance}}

In similar terms we define the mean squared distance from the mean:

\textbf{Definition}

The variance, written as \(\sigma^2\) or \(V(X)\), of a discrete random variable \(X\) with mass function \(f(x)\) is given by

\[\sigma^2 = V(X)= \sum_{i=1}^M (x_i-\mu)^2 f(x_i)\]

\begin{itemize}
\item
  \(\sigma=\sqrt{V(X)}\) is called the \textbf{standard deviation} of the random variable
\item
  Think of it as the moment of inertia of probabilities about the mean.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-variance}{%
\section{Example: Variance}\label{example-variance}}

What is the variance of \(X\) if its probability mass function \(f(x)\) is given by

\(P(X=0)=1/16\)
\(P(X=1)=4/16\)
\(P(X=2)=6/16\)
\(P(X=3)=4/16\)
\(P(X=4)=1/16\)

\[\sigma^2 =V(X)=\sum_{i=1}^m (x_i-\mu)^2 f(x_i)\]

\(V(X)=\)\textbf{(0-2)}\(^2\)* 1/16 + \textbf{(1-2)}\(^2\)* 4/16 + \textbf{(2-2)}\(^2\)* 6/16 + \textbf{(3-2)}\(^2\)* 4/16 + \textbf{(4-2)}\(^2\)* 1/16 =1

\[V(X)=\sigma^2=1\]
\[\sigma=1\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{functions-of-x}{%
\section{\texorpdfstring{Functions of \(X\)}{Functions of X}}\label{functions-of-x}}

\textbf{Definition}

For any function \(h\) of a random variable \(X\), with mass function \(f(x)\), its expected value is given by

\[ E[h(X)]= \sum_{i=1}^M h(x_i) f(x_i) \]

This is an important definition that allows us to prove three important properties of the median and variance:

\begin{itemize}
\item
  The mean of a linear function is the linear function fo the mean: \[E(a\times X +b)= a\times E(X) +b\] for \(a\) and \(b\) scalars (numbers).
\item
  The variance of a linear function of \(X\) is:\[V(a\times X +b)= a^2\times V(X)\]
\item
  The variance \textbf{about the origin} is the variance \textbf{about the mean} plus the mean squared: \[E(X^2)=V(X)+E(X)^2\]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-variance-about-the-origin}{%
\section{Example: Variance about the origin}\label{example-variance-about-the-origin}}

What is the variance \(X\) about the origin, \(E(X^2)\), if its probability mass function \(f(x)\) is given by

\(P(X=0)=1/16\)
\(P(X=1)=4/16\)
\(P(X=2)=6/16\)
\(P(X=3)=4/16\)
\(P(X=4)=1/16\)

\[E(X^2) =\sum_{i=1}^m x_i^2 f(x_i)\]

\(E(X^2)=\)\textbf{(0)}\(^2\)* 1/16 + \textbf{(1)}\(^2\)* 4/16 + \textbf{(2)}\(^2\)* 6/16 + \textbf{(3)}\(^2\)* 4/16 + \textbf{(4)}\(^2\)* 1/16 =5

We can also verify:

\[E(X^2)=V(X)+E(X)^2\]

\(5=1+2^2\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-distribution}{%
\section{Probability distribution}\label{probability-distribution}}

\textbf{Definition:}

The \textbf{probability distribution} function is defined as

\[F(x)=P(X\leq x)=\sum_{x_i\leq x} f(x_i) \]

That is the accumulated probability up to a given value \(x\)

\(F(x)\) satisfies:

\begin{itemize}
\tightlist
\item
  \(0\leq F(x) \leq 1\)
\item
  If \(x \leq y\), then \(F(x) \leq F(y)\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-probability-distribution}{%
\section{Example: Probability distribution}\label{example-probability-distribution}}

For the probability mass function:

\(f(0)=P(X=0)=1/16\)
\(f(1)=P(X=1)=4/16\)
\(f(2)=P(X=2)=6/16\)
\(f(3)=P(X=3)=4/16\)
\(f(4)=P(X=4)=1/16\)

The probability distribution is:

\[
    F(x)=
\begin{cases}
    1/16,& \text{if } x < 1\\
    5/16,& 1\leq x < 2\\
    11/16,& 2\leq x < 3\\
    15/16,& 3\leq x < 4\\
    16/16,&  x \leq 5\\
\end{cases}
\]

For\(X \in \mathbb{Z}\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-distribution-1}{%
\section{Probability distribution}\label{probability-distribution-1}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-38-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-function-and-probability-distribution}{%
\section{Probability function and Probability distribution}\label{probability-function-and-probability-distribution}}

Compute the mass probability function of the following probability distribution:

\(F(0)=1/16\), \(F(1)=5/16\), \(F(2)=11/16\), \(F(3)=15/16\), \(F(4)=16/16\),

Let's work backward.

\(f(0)=F(0)=1/16\)
\(f(1)=F(1)-f(0)=5/32-1/32=4/16\)
\(f(2)=F(2)-f(1)-f(0)=F(2)-F(1)=6/16\)
\(f(3)=F(3)-f(2)-f(1)-f(0)=F(3)-F(2)=4/16\)
\(f(4)=F(4)-F(3)=1/16\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-function-and-probability-distribution-1}{%
\section{Probability function and Probability distribution}\label{probability-function-and-probability-distribution-1}}

The Probability distribution is another way to specify the probability of a random variable

\[f(x_i)=F(x_i)-F(x_{i-1})\]

with

\[f(x_1)=F(x_1)\]

for \(X\) taking values in \(x_1 \leq x_2 \leq ... \leq x_n\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{quantiles}{%
\section{Quantiles}\label{quantiles}}

We define the \textbf{q-quantile} as the value \(x_{p}\) \textbf{under} which we have accumulated q*100\% of the probability

\[q=\sum_{i=1}^p f(x_i) = F (x_p)\]

\begin{itemize}
\tightlist
\item
  The \textbf{median} is value \(x_m\) such that \(q=0.5\)
\end{itemize}

\[F(x_{m})=0.5\]

\begin{itemize}
\tightlist
\item
  The \(0.05\)-quantile is the value \(x_{r}\) such that \(q=0.05\)
\end{itemize}

\[F(x_{r})=0.05\]

\begin{itemize}
\tightlist
\item
  The \(0.25\)-quantile is \textbf{first quartile} the value \(x_{s}\) such that \(q=0.25\)
\end{itemize}

\[F(x_{s})=0.25\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.43}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.30}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.26}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\centering
quantity names
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
model (unobserved)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
data (observed)
\end{minipage} \\
\midrule
\endhead
probability mass function // relative frequency & \(f(x_i)=P(X=x_i)\) & \(f_i=\frac{n_i}{N}\) \\
probability distribution // cumulative relative frequency & \(F(x_i)=P(X \leq x_i)\) & \(F_i=\sum_{k\leq i} f_k\) \\
mean // average & \(\mu=E(X)=\sum_{i=1}^M x_i f(x_i)\) & \(\bar{x}=\sum_{j=1}^N x_j/N\) \\
variance // sample variance & \(\sigma^2=V(X)=\sum_{i=1}^M (x_i-\mu)^2 f(x_i)\) & \(s^2=\sum_{j=1}^N (x_j-\bar{x})^2/(N-1)\) \\
standard deviation // sample sd & \(\sigma=\sqrt{V(X)}\) & \(s\) \\
variance about the origin // 2nd sample moment & \(E(X^2)=\sum_{i=1}^M x_i^2 f(x_i)\) & \(m_2= \sum_{j=1}^N x_j^2/n\) \\
\bottomrule
\end{longtable}

Note that:

\begin{itemize}
\tightlist
\item
  \(i=1...M\) is an \textbf{outcome} of the random variable \(X\).
\item
  \(j=1...N\) is an \textbf{observation} of the random variable \(X\).
\end{itemize}

Properties:

\begin{itemize}
\tightlist
\item
  \(\sum_{i=1...N} f(x_i)=1\)
\item
  \(f(x_i)=F(x_i)-F(x_{i-1})\)
\item
  \(E(a\times X +b)= a\times E(X) +b\); for \(a\) and \(b\) scalars.
\item
  \(V(a\times X +b)= a^2\times V(X)\)
\item
  \(E(X^2)=V(X)+E(X)^2\)
\end{itemize}

\hypertarget{continous-random-variables}{%
\chapter{Continous Random Variables}\label{continous-random-variables}}

\hypertarget{objective-4}{%
\section{Objective}\label{objective-4}}

\begin{itemize}
\tightlist
\item
  Probability density function
\item
  Mean and variance
\item
  Probability distribution
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-random-variable}{%
\section{Continuous random variable}\label{continuous-random-variable}}

What happens with continuous random variables?

Let's reconsider the convexity angle of misophonia patients (Section 2.21).

\begin{itemize}
\tightlist
\item
  We redefined the outcomes as little regular intervals (bins) and computed the relative frequency for each of them as we did in the discrete case.
\end{itemize}

\begin{verbatim}
##        outcome ni         fi
## 1 [-1.02,3.46]  8 0.06504065
## 2  (3.46,7.92] 51 0.41463415
## 3  (7.92,12.4] 26 0.21138211
## 4  (12.4,16.8] 20 0.16260163
## 5  (16.8,21.3] 18 0.14634146
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-random-variable-1}{%
\section{Continuous random variable}\label{continuous-random-variable-1}}

Let's consider again that their relative frequencies are the probabilities when \(N \rightarrow \infty\)

\[f_i=\frac{n_i}{N} \rightarrow f(x_i)=P(X=x_i)\]

The probability depends now on the length of the bins \(\Delta x\). If we make the bins smaller and smaller then the frequencies get smaller and therefore

\(P(X=x_i) \rightarrow 0\) when \(\Delta x \rightarrow 0\), because \(n_i \rightarrow 0\)

\begin{verbatim}
##          outcome ni         fi
## 1  [-1.02,0.115]  2 0.01626016
## 2   (0.115,1.23]  0 0.00000000
## 3    (1.23,2.34]  3 0.02439024
## 4    (2.34,3.46]  3 0.02439024
## 5    (3.46,4.58]  2 0.01626016
## 6    (4.58,5.69]  4 0.03252033
## 7     (5.69,6.8] 11 0.08943089
## 8     (6.8,7.92] 34 0.27642276
## 9    (7.92,9.04] 12 0.09756098
## 10   (9.04,10.2]  4 0.03252033
## 11   (10.2,11.3]  3 0.02439024
## 12   (11.3,12.4]  7 0.05691057
## 13   (12.4,13.5]  2 0.01626016
## 14   (13.5,14.6]  6 0.04878049
## 15   (14.6,15.7]  4 0.03252033
## 16   (15.7,16.8]  8 0.06504065
## 17     (16.8,18]  4 0.03252033
## 18     (18,19.1]  9 0.07317073
## 19   (19.1,20.2]  3 0.02439024
## 20   (20.2,21.3]  2 0.01626016
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-random-variable-2}{%
\section{Continuous random variable}\label{continuous-random-variable-2}}

We define a quantity at a point \(x\) that is the amount of probability per unit distance that we would find in an \textbf{infinitesimal} bin \(dx\) at \(x\)

\[f(x)= \frac{P(x\leq X \leq x+dx)}{dx}\]

\(f(x)\) is called the probability \textbf{density} function.

Therefore, the probability of observing \(x\) between \(x\) and \(x+dx\)
is given by

\[P(x\leq X \leq x+dx)= f(x) dx\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-random-variable-3}{%
\section{Continuous random variable}\label{continuous-random-variable-3}}

\textbf{Definition}

For a continuous random variable \(X\), a \textbf{probability density} function is such that

The function is positive:

\begin{itemize}
\tightlist
\item
  \(f(x) \geq 0\)
\end{itemize}

The probability of observing a value within an interval is the \textbf{area under the curve}:

\begin{itemize}
\tightlist
\item
  \(P(a\leq X \leq b)=\int_{a}^{b} f(x) dx\)
\end{itemize}

The probability of observing \textbf{any} value is 1:

\begin{itemize}
\tightlist
\item
  \(\int_{-\infty}^{\infty} f(x) dx = 1\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-random-variable-4}{%
\section{Continuous random variable}\label{continuous-random-variable-4}}

\begin{itemize}
\item
  The probability density function is a step forward in the abstraction of probabilities: we add the continuous limit (\(dx \rightarrow 0\)).
\item
  All the properties of probabilities are translated in terms of densities (\(\sum \rightarrow \int\)).
\item
  Assignment of probabilities to a random variable can be done with equiprobability (classical) arguments.
\item
  Densities are mathematical quantities some will map to experiments some will not. \emph{Which density will map best to my experiment?}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{total-area-under-the-curve}{%
\section{Total area under the curve}\label{total-area-under-the-curve}}

Example: take the \textbf{probability density} that may describe the random variable that measures where a raindrop falls in a rain gutter of length \(100cm\).

\[
    f(x)= 
\begin{cases}
    \frac{1}{100},& \text{if } x\in (0,100)\\
    0,& otherwise 
\end{cases}
\]

Then the probability of \textbf{any} observation is the total \textbf{area under the curve}

\(P(-\infty\leq X \leq \infty)= \int_{-\infty}^{\infty} f(x) dx = 100*0.01= 1\)

\includegraphics{_main_files/figure-latex/unnamed-chunk-41-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{area-under-the-curve}{%
\section{Area under the curve}\label{area-under-the-curve}}

The probability of observing \(x\) in an interval is the \textbf{area under the curve} within the interval

\begin{itemize}
\tightlist
\item
  \(P(20 \leq X \leq 60) = \int_{20}^{60} f(x) dx = (60-20)*0.01=0.4\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-42-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{area-under-the-curve-1}{%
\section{Area under the curve}\label{area-under-the-curve-1}}

In general \(f(x)\) should satisfy:

\begin{itemize}
\tightlist
\item
  \(0 \leq P(a \leq X \leq b) = \int_{a}^{b} f(x) dx \leq 1\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-43-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-distribution-2}{%
\section{Probability distribution}\label{probability-distribution-2}}

The probability accumulated up to \(b\) is defined by the probability distribution \(F\)

\begin{itemize}
\tightlist
\item
  \(F(b) = P(X \leq b)=\int_{-\infty}^bf(x)dx\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-44-1.pdf}

The probability accumulated up to \(a\) is

\begin{itemize}
\tightlist
\item
  \(F(a) = P(X \leq a)\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-45-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-distribution-3}{%
\section{Probability distribution}\label{probability-distribution-3}}

The probability between \(a\) and \(b\) is defined by the probability distribution \(F\)

\begin{itemize}
\tightlist
\item
  \(P(a\leq X \leq b) = \int_a^b f(x)dx=F(b)-F(a)\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-46-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-distribution-4}{%
\section{Probability distribution}\label{probability-distribution-4}}

The probability distribution of a continuous random variable is defined as\\
\(F(a)=P(X\leq a) =\int_{-\infty} ^a f(x)dx\)

with the properties that:

It is between \(0\) and \(1\):

\begin{itemize}
\tightlist
\item
  \(F(-\infty)= 0\) and \(F(\infty)=1\)
\end{itemize}

It always increases:

\begin{itemize}
\tightlist
\item
  if \(a\leq b\) then \(F(a)\leq F(b)\)
\end{itemize}

It can be used to compute probabilities:

\begin{itemize}
\tightlist
\item
  \(P(a \leq X \leq b)=F(b)-F(a)\)
\end{itemize}

It recovers the probability density:

\begin{itemize}
\tightlist
\item
  \(f(x)=\frac{dF(x)}{dx}\)
\end{itemize}

We use \textbf{probability distributions} to \textbf{compute probabilities} of a random variable with intervals

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-distribution-5}{%
\section{Probability distribution}\label{probability-distribution-5}}

For the uniform density function:

\[
    f(x)= 
\begin{cases}
    \frac{1}{100},& \text{if } x\in (0,100)\\
    0,& otherwise 
\end{cases}
\]

The probability distribution is

\[
    F(a)= 
\begin{cases}
    0,& a \leq 0 \\
    \frac{a}{100},& \text{if } a\in (0,100)\\
    1, & 10 \leq a \\
    \\
\end{cases}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-graphics}{%
\section{Probability graphics}\label{probability-graphics}}

The probability \(P(20<X<60)\) is the \emph{area} under the \textbf{density} curve

\includegraphics{_main_files/figure-latex/unnamed-chunk-47-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-graphics-1}{%
\section{Probability graphics}\label{probability-graphics-1}}

The probability \(P(20<X<60)\) is the \emph{difference} in \textbf{distribution} values

\includegraphics{_main_files/figure-latex/unnamed-chunk-48-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-1}{%
\section{Mean}\label{mean-1}}

As in the discrete case, the \textbf{mean} measures the center of the distribution

\textbf{Definition}

Suppose \(X\) is a continuous random variable with probability \textbf{density} function \(f(x)\). The mean or expected value of \(X\), denoted as \(\mu\) or \(E(X)\), is

\[\mu=E(X)=\int_{-\infty}^\infty x f(x) dx\]

It is the continuous version of the center of mass.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-2}{%
\section{Mean}\label{mean-2}}

\[
    f(x)= 
\begin{cases}
    \frac{1}{100},& \text{if } x\in (0,100)\\
    0,& otherwise 
\end{cases}
\]

\(E(X)=50\)

\includegraphics{_main_files/figure-latex/unnamed-chunk-49-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{variance-1}{%
\section{Variance}\label{variance-1}}

As in the discrete case, the variance measures the dispersion about the mean

\textbf{Definition}

Suppose \(X\) is a continuous random variable with probability density function \(f(x)\). The variance of \(X\), denoted as \(\sigma^2\) or \(V(X)\), is

\[\sigma^2=V(X)=\int_{-\infty}^\infty (x-\mu)^2 f(x) dx\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{functions-of-x-1}{%
\section{\texorpdfstring{Functions of \(X\)}{Functions of X}}\label{functions-of-x-1}}

\textbf{Definition}

For any function \(h\) of a random variable \(X\), with mass function \(f(x)\), its expected value is given by

\[E[h(X)]= \int_{-\infty}^{\infty} h(x) f(x)dx\]

And we have the same properties as in the discrete case

\begin{itemize}
\item
  The mean of a linear function is the linear function fo the mean: \[E(a\times X +b)= a\times E(X) +b\] for \(a\) and \(b\) scalars.
\item
  The variance of a linear function of \(X\) is:\[V(a\times X +b)= a^2\times V(X)\]
\item
  The variance about the origin is the variance about the mean plus the mean squared: \[E(X^2)=V(X)+E(X)^2\]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-8}{%
\section{Example}\label{example-8}}

\begin{itemize}
\tightlist
\item
  for the probability density
\end{itemize}

\[
    f(x)= 
\begin{cases}
    \frac{1}{100},& \text{if } x\in (0,100)\\
    0,& otherwise 
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  compute the mean
\item
  compute variance using \(E(X^2)=V(X)+E(X)^2\)
\item
  compute \(P(\mu-\sigma\leq X \leq \mu+\sigma)\)
\item
  What are the first and third quartiles?
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-50-1.pdf}

\hypertarget{discrete-probability-models}{%
\chapter{Discrete Probability Models}\label{discrete-probability-models}}

\hypertarget{objective-5}{%
\section{Objective}\label{objective-5}}

Discrete probability models:

\begin{itemize}
\tightlist
\item
  Uniform and Bernoulli probability functions
\item
  Binomial and negative binomial probability functions
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-mass-function}{%
\section{Probability mass function}\label{probability-mass-function}}

A probability mass function of a \textbf{discrete random variable} \(X\) with possible values \(x_1 , x_2 , .. , x_M\) is \textbf{any function} such that

Positive:

\begin{itemize}
\tightlist
\item
  \(f(x_i)\geq 0\)
\end{itemize}

\emph{Allow us to compute probabilities:}

\begin{itemize}
\tightlist
\item
  \(f(x_i)=P(X=x_i)\)
\end{itemize}

The probability of any outcome is \(1\)

\begin{itemize}
\tightlist
\item
  \(\sum_{i=1}^M f(x_i)=1\)
\end{itemize}

\textbf{Properties:}

Central tendency:

\begin{itemize}
\tightlist
\item
  \(E(X)= \sum_{i=1}^M x_i f(x_i)\)
\end{itemize}

Dispersion:

\begin{itemize}
\tightlist
\item
  \(V(X)= \sum_{i=1}^M (x_i-\mu)^2 f(x_i)\)
\end{itemize}

They are abstract objects with general properties that may or may not \textbf{describe} a natural or engineered process.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-model}{%
\section{Probability model}\label{probability-model}}

A \textbf{probability model} is a probability mass function that may represent the probabilities of a random experiment.

\textbf{Examples:}

\begin{itemize}
\item
  \(f(x)=P(X=x)=1/6\) represents the probability of the outcomes of \textbf{one} throw of a dice.
\item
  The probability mass function
\end{itemize}

\begin{longtable}[]{@{}cc@{}}
\toprule
\(X\) & \(f(x)\) \\
\midrule
\endhead
\(-2\) & \(1/8\) \\
\(-1\) & \(2/8\) \\
\(0\) & \(2/8\) \\
\(1\) & \(2/8\) \\
\(2\) & \(1/8\) \\
\bottomrule
\end{longtable}

Represents the probability of drawing \textbf{one} ball from an urn where there are two balls per label: \(-1, 0, 1\) and one ball per label: \(-2, 2\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{parametric-models}{%
\section{Parametric models}\label{parametric-models}}

When we perform a random experiment and \textbf{do not} know the probabilities of the outcomes:

\begin{itemize}
\tightlist
\item
  We can always formulate the model given by the relative frequencies: \(\hat{P}(X=x_i)=f_i\) ( where \(i=1...M\)).
\end{itemize}

We need to find \(M\) numbers each depending on \(N\).

In many cases:

\begin{itemize}
\tightlist
\item
  We can formulate probability functions \(f(x)\) that depend on \textbf{very few} numbers only.
\end{itemize}

\textbf{Example:}

A random experiment with \(M\) equally likely outcomes has a probability mass function:
\[f(x)=P(X=x)=1/M\]

We only need to know \(M\).

The numbers we \textbf{need to know} to fully determine a probability function are called \textbf{parameters}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{uniform-distribution-one-parameter}{%
\section{Uniform distribution (one parameter)}\label{uniform-distribution-one-parameter}}

\textbf{Definition}
A random variable \(X\) with outcomes \(\{1,...M\}\) has a discrete \textbf{uniform distribution} if all its \(M\) outcomes have the same probability

\[f(x)=\frac{1}{M}\]

With mean and variance:

\(E(X)= \frac{M+1}{2}\)

\(V(X)= \frac{M^2-1}{12}\)

Note: \(E(X)\) and \(V(X)\) are also \textbf{parameters}. If we know any of them then we can fully determine the distribution.

\[f(x)=\frac{1}{2E(X)-1}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{uniform-distribution}{%
\section{Uniform distribution}\label{uniform-distribution}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-51-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{uniform-distribution-two-parameters}{%
\section{Uniform distribution (two parameters)}\label{uniform-distribution-two-parameters}}

Let's introduce a new uniform probability model with \textbf{two parameters}: The minimum and maximum outcomes.

If the random variable takes values in \(\{a, a+1, ...b\}\), where \(a\) and \(b\) are integers and all the outcomes are equally probable then

\[f(x)=\frac{1}{b-a+1}\]

as \(M=b-a+1\).

\begin{itemize}
\tightlist
\item
  We then say that \(X\) distributes uniformly between \(a\) and \(b\) and write
\end{itemize}

\[X \rightarrow Unif(a,b)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{uniform-distribution-two-parameters-1}{%
\section{Uniform distribution (two parameters)}\label{uniform-distribution-two-parameters-1}}

\textbf{Example:}

What is the probability of observing a child of a particular age in a primary school (if all classes have the same amount of children)?

From the experiment we know: \(a=6\) and \(b=11\) then

\[X \rightarrow Unif(a=6, b=11)\] that is

\[f(x)=\frac{1}{6}\] for \(x\in \{6,7,8,9,10,11\}\), and \(0\) otherwise

\includegraphics{_main_files/figure-latex/unnamed-chunk-52-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{uniform-distribution-1}{%
\section{Uniform distribution}\label{uniform-distribution-1}}

The probability model of a random variable \(X\)

\[f(x)=\frac{1}{b-a+1}\]

for \(x \in \{a, a+1, ...b\}\)

has mean and variance:

\begin{itemize}
\item
  \(E(X)= \frac{b+a}{2}\)
\item
  \(V(X)= \frac{(b-a+1)^2-1}{12}\)
\end{itemize}

(Change variables \(X=Y+a-1\), \(y \in \{1,...M\}\))

We can either specify \(a\) and \(b\) or \(E(X)\) and \(V(X)\).

In our example:

\begin{itemize}
\tightlist
\item
  \(E(X)=(11+6)/2=8.5\)
\item
  \(V(X)=(6^2-1)/12=2.916667\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{uniform-distribution-two-parameter}{%
\section{Uniform distribution (two-parameter)}\label{uniform-distribution-two-parameter}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-53-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{parameters-and-models}{%
\section{Parameters and Models}\label{parameters-and-models}}

\begin{itemize}
\item
  A \textbf{model} is a particular function \(f(x)\) that \textbf{describes} our experiment
\item
  If the model is a \textbf{known} function that depends on a few parameters then changing the value of the parameters we produce a \textbf{family of models}
\item
  Knowledge of \(f(x)\) is reduced to the knowledge of the value of the parameters
\item
  Ideally, the model and the parameters are \textbf{interpretable}
\end{itemize}

\emph{Example:}

\textbf{Model}: The data of our experiment is produced by a random process in which each age has the \textbf{same probability} of being observed.

\textbf{Parameters}: \(a\) is the minimum age, \(E(X)\) is the expected age \ldots{} they are \textbf{physical properties} of the experiment.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{parameters-and-models-1}{%
\section{Parameters and Models}\label{parameters-and-models-1}}

\textbf{Example:}

A \textbf{family} of models obtained from two-parameter uniform distributions changing the \textbf{variances} and keeping a constant mean (\(E(X)=8.5\)). It results on \textbf{changing} both \textbf{minimum} and \textbf{maximum} outcomes.

\begin{itemize}
\tightlist
\item
  Note: Only one model makes sense for our experiment (only one model can represent the ages of children in a school).
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-54-1.pdf}

\begin{itemize}
\tightlist
\item
  We can think of \textbf{families} that change only the \textbf{mean}, only the \textbf{minimum}, or only the \textbf{maximum}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bernoulli-trial}{%
\section{Bernoulli trial}\label{bernoulli-trial}}

Let's try to advance from the equal probability case and suppose a model with two outcomes (\(A\) and \(B\)) that have \textbf{unequal} probabilities

\textbf{Examples:}

\begin{itemize}
\item
  Writing down the sex of a patient who goes into an emergency room of a hospital (\(A:male\) and \(B:female\)).
\item
  Recording whether a manufactured machine is defective or not (\(A:defective\) and \(B:fine\)).
\item
  Hitting a target (\(A:success\) and \(B:failure\)).
\item
  Transmitting one pixel correctly (\(A:yes\) and \(B:no\)).
\end{itemize}

In these examples, the probability of outcome \(A\) is usually \textbf{unknown}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bernoulli-trial-1}{%
\section{Bernoulli trial}\label{bernoulli-trial-1}}

We will introduce the probability of an outcome (\(A\)) as the \textbf{parameter} of the model:

\begin{itemize}
\tightlist
\item
  outcome A (success): has probability \(p\) (parameter)
\item
  outcome B (failure): has a probability \(1-p\)
\end{itemize}

Or write, the probability mass function of \(K\) taking values \(\{0, 1\}\) for \(A\) and \(B\)

\[
    f(k)= 
\begin{cases}
    1-p,&  k=0\, (event\, B)\\
    p,& k=1\, (event\, A) 
\end{cases}
\]

or more shortly

\[f(k; p)=p^k(1-p)^{1-k} \]

for \(k=(0,1)\)

We only need to know \(p\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bernoulli-trial-2}{%
\section{Bernoulli trial}\label{bernoulli-trial-2}}

A Bernoulli variable \(K\) with outcomes \(\{0, 1\}\) has a probability mass function

\[f(k; p)=p^k(1-p)^{1-k} \]
With mean and variance:

\begin{itemize}
\item
  \(E(K)=p\)
\item
  \(V(K)=(1-p)p\)
\end{itemize}

Note:

\begin{itemize}
\item
  The probability of the outcome \(A\) is the parameter \(p\)
  which is the same as \(f(0)=P(X=0)\).
\item
  As \(p\) is usually \textbf{unknown} we typically estimated it by the relative frequency (more on this in the inference sections): \(\hat{p}=f_A=\frac{n_A}{N}\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bernoulli-trial-3}{%
\section{Bernoulli trial}\label{bernoulli-trial-3}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-55-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution}{%
\section{Binomial distribution}\label{binomial-distribution}}

When we are interested in learning about a particular Bernoulli trial

\begin{itemize}
\item
  We repeat the Bernoulli trial \(N\) times and count how many times we obtained \(A\) (\(n_A\)).
\item
  We define a random variable \(X=n_A\) taking values \(x \in {0,1,...N}\)
\end{itemize}

We now ask for the probability of observing \(x\) events of type \(A\) in the repetition of \(n\) independent Bernoulli trials, when the probability of observing \(A\) is \(p\).

\[P(X=x)=f(x)=?\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{examples-binomial-distribution}{%
\section{Examples: Binomial distribution}\label{examples-binomial-distribution}}

\begin{itemize}
\item
  Writing down the sex of \(n=10\) patients who go into an emergency room of a hospital. What is the probability that \(x=6\) patients are men when \(p=0.9\)?
\item
  Trying \(n=5\) times to hit a target (\(A:success\) and \(B:failure\)). What is the probability that I hit the target \(x=5\) times when I usually hit it \(25\%\) of the times (\(p=0.25\))?
\item
  Transmitting \(n=100\) pixels correctly (\(A:yes\) and \(B:no\)). What is the probability that \(x=2\) pixels are errors, when the probability of error is \(p=0.1\)?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution-1}{%
\section{Binomial distribution}\label{binomial-distribution-1}}

What is the probability of observing \(X=4\) errors when transmitting \(4\) pixels, if the probability of an error is \(p\)?

Consider \(4\) random variables: \(K_1\), \(K_2\), \(K_3\) and \(K_4\) that record whether an error has been made in the \(1^{st}\), \(2^{nd}\), \(3^{rd}\) and \(4^{th}\) pixel.

Then

\begin{itemize}
\item
  \(k_i\) takes values \(\{correct:0; error:1\}\)
\item
  \(X=\sum_{i=1}^4 K_i\) takes values \(\{0,1,2,3,4\}\)
\end{itemize}

Then the probability of observing \(4\) errors is:

\begin{itemize}
\tightlist
\item
  \(P(X=4)=P(1,1,1,1)=p*p*p*p=p^4\) because \(K_i\) are independent.
\end{itemize}

The probability of \(0\) errors is:

\begin{itemize}
\tightlist
\item
  \(P(X=0)=P(0,0,0,0)=(1-p)(1-p)(1-p)(1-p)=(1-p)^4\)
\end{itemize}

The probability of \(3\) errors is:

\(P(X=3)=P(0,1,1,1)+P(1,0,1,1)+P(1,1,0,1)+P(1,1,1,0)\)
\(=4p^3(1-p)^1\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution-2}{%
\section{Binomial distribution}\label{binomial-distribution-2}}

Therefore the probability of \(x\) errors is

\[
    f(x)= 
\begin{cases}
    1*p^0(1-p)^4,&  x=0 \\
    4*p^1(1-p)^3,&  x=1 \\
    6*p^2(1-p)^2,&  x=2 \\
    4*p^3(1-p)^1,&  x=3 \\
    1*p^4(1-p)^0,&  x=4 \\
\end{cases}
\]

or more shortly

\[f(x)=\binom 4 x p^x(1-p)^{4-x}\]
for \(x=0,1,2,3,4\)

where \(\binom 4 x\) is the number of possible outcomes (transmissions of \(4\) pixels) with \(x\) errors.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution-definition}{%
\section{Binomial distribution: Definition}\label{binomial-distribution-definition}}

The binomial probability function is the probability mass function of observing \(x\) outcomes of type \(A\) in \(n\) independent Bernoulli trials, where \(A\) has the same probability \(p\) in each trial.

The function is given by

\(f(x)=\binom n x p^x(1-p)^{n-x}\), \(x=0,1,...n\)

\(\binom n x= \frac{n!}{x!(n-x)!}\) is called \textbf{the binomial coefficient} and gives the number of ways one can obtain \(x\) events of type \(A\) in a set of \(n\).

When a variable \(X\) has a binomial probability function we say it distributes binomially and write

\[X\rightarrow Bin(n,p)\]

where \(n\) and \(p\) are parameters.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution-mean-and-variance}{%
\section{Binomial distribution: Mean and Variance}\label{binomial-distribution-mean-and-variance}}

The mean and variance of \(X\hookrightarrow Bin(n,p)\) are

\begin{itemize}
\item
  \(E(X)=np\)
\item
  \(V(X)=np(1-p)\)
\item
  Since \(X\) is the sum of \(n\) independent Bernoulli variables
\end{itemize}

\(E(X)=E(\sum_{i=1}^n K_i)=np\)

and

\(V(X)=V(\sum_{i=1}^n K_i)=n(1-p)p\)

Example:

\begin{itemize}
\item
  The expected value for the number of errors in the transmission of 4 pixels is \(np=4*0.1=0.4\) when the probability of an error is \(0.1\).
\item
  The variance is \(n(1-p)p=0.36\)
\end{itemize}

\textbf{Remember}: We can specify either the parameters \(n\) and \(p\), or the parameters \(E(X)\) and \(V(X)\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-1-1}{%
\section{Example 1}\label{example-1-1}}

Now let's answer:

\begin{itemize}
\tightlist
\item
  What is the probability of observing \(4\) errors when transmitting \(4\) pixels, if the probability of an error is \(0.1\)?
\end{itemize}

Since we are repeating a Bernoulli trial \(n=4\) times and counting the number of events of type \(A\) (errors), when \(P(A)=p=0.1\) then

\[X \rightarrow Bin(n=4, p=0.1)\]
That is \[f(x)=\binom 4 x 0.1^x(1-0.1)^{4-x}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-1-2}{%
\section{Example 1}\label{example-1-2}}

\begin{itemize}
\tightlist
\item
  We want to compute:
\end{itemize}

\(P(X=4)=f(4)=\binom 4 4 0.1^4 0.9^{0}=10^{-4}\)

In R dbinom(4,4,0.1)

\begin{itemize}
\tightlist
\item
  We can also compute:
\end{itemize}

\(P(X=2)=\binom 4 2 0.1^2 0.9^2=0.0486\)

In R dbinom(2,4,0.1)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-2-1}{%
\section{Example 2}\label{example-2-1}}

\begin{itemize}
\tightlist
\item
  What is the probability of observing at most \(8\) voters of the ruling party in an election poll of size \(10\), if the probability of a positive vote is \(0.9\)
\end{itemize}

For this case

\[X \rightarrow Bin(n=10, p=0.9)\]

That is \[f(x)=\binom {10} x 0.9^x(0.1)^{4-x}\]

We want to compute:
\(P(X\le 8)=F(8)= \sum_{i=1..8} f(x_i)=0.2639011\)

in R pbinom(8,10, 0.9)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution-3}{%
\section{Binomial distribution}\label{binomial-distribution-3}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-56-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{negative-binomial-distribution}{%
\section{Negative binomial distribution}\label{negative-binomial-distribution}}

Now let us imagine that we are interested in counting the well-transmitted pixels before a \textbf{given number} of errors occur. Say we can \textbf{tolerate} \(r\) errors in transmission.

\begin{itemize}
\item
  Experiment: Suppose performing Bernoulli trials until we observe the outcome \(A\) appears \(r\) times.
\item
  Random variable: We count the number of events \(B\)
\item
  Example: What is the probability of observing \(y\) well-transmitted (\(B\)) pixels before \(r\) errors (\(A\))?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{negative-binomial-distribution-1}{%
\section{Negative binomial distribution}\label{negative-binomial-distribution-1}}

Let's first find the probability of one particular transmission with \(y\) number of correct pixels (\(B\)) and \(r\) number of errors (\(A\)).

\((0,0,1,., 0,1,...0,1)\) (there are \(y\) zeros, and \(r\) ones)

We observe \(y\) correct pixels in a total of \(y + r\) trials.

Then

\begin{itemize}
\tightlist
\item
  \(P(0,0,1,., 0,1,...0,1)=p^r(1-p)^y\) (Remember: \(p\) is the probability of error)
\end{itemize}

How many transmissions can have \(y\) correct pixels before \(r\) errors?

Note:

\begin{itemize}
\item
  The last bit is fixed (marks the end of transmission)
\item
  The total number of transmissions with \(y\) number of correct pixels (\(B\)) that we can obtain in \(y + r-1\) trials is: \(\binom {y + r-1} y\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{negative-binomial-distribution-2}{%
\section{Negative binomial distribution}\label{negative-binomial-distribution-2}}

Therefore, the probability of observing \(y\) events of type \(B\) before \(r\) events of type \(A\) (with probability \(p\)) is

\[P(Y=y)=f(y)=\binom {y+r-1} y p^r(1-p)^y\]

for \(y=0,1,...\)

We then say that \(Y\) follows a negative binomial distribution and we write

\[Y\rightarrow NB(r,p)\]

where \(r\) and \(p\) are parameters representing the tolerance and the probability of a single error.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-and-variance-2}{%
\section{Mean and Variance}\label{mean-and-variance-2}}

A random variable with \(Y\rightarrow NB(r,p)\) has

\begin{itemize}
\item
  mean: \(E(Y)= r\frac{1-p}{p}\)
\item
  variance: \(V(Y)= r\frac{1-p}{p^2}\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{geometric-distribution}{%
\section{Geometric distribution}\label{geometric-distribution}}

We call \textbf{geometric distribution} to the negative binomial distribution with \(r=1\)

The probability of observing \(B\) events before observing the \textbf{first} event of type \(A\) is

\[P(Y=y)=f(y)= p(1-p)^y\]

\[Y\rightarrow Geom(p)\]
with mean

\begin{itemize}
\item
  mean: \(E(Y)= \frac{1-p}{p}\)
\item
  variance: \(V(Y)= \frac{1-p}{p^2}\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-9}{%
\section{Example}\label{example-9}}

\begin{itemize}
\item
  A website has three servers.
\item
  One server operates at a time and only when a request fails another server is used.
\item
  If the probability of failure for a request is known to be \(p=0.0005\) then
\item
  what is the expected number of successful requests before the three computers fail?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-10}{%
\section{Example}\label{example-10}}

Since we are repeating a Bernoulli trial until \(r=3\) events of type \(A\) (failure) are observed (each with \(P(A)=p=0.0005\)) and are counting the number of events of type \(B\) (successful requests) then

\[Y \rightarrow NB(r=3, p=0.0005)\]

Therefore, the expected number of requests before the system fails is:

\(E(Y)=r\frac{1-p}{p}=3\frac{1-0.0005}{0.0005}=5997\)

\begin{itemize}
\tightlist
\item
  Note that there are actually \(6000\) trials
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-11}{%
\section{Example}\label{example-11}}

What is the probability of dealing with at most \(5\) successful requests before the system fails?

Recall the cumulative function distribution \(F(y)=P(Y\leq 5)\)

\(F(5)=P(Y\leq 5)=\Sigma_{y=0}^5 f(y)\)

\(=\sum_{y=0}^5\binom {y+2} y 0.0005^r0.9995^y\)

\(=\binom {2} 0 0.0005^3 0.9995^0 +\binom {3} 1 0.0005^3 0.9995^1\)

\(+\binom {4} 2 0.0005^3 0.9995^2 +\binom {5} 3 0.0005^3 0.9995^3\)

\(+\binom {6} 4 0.0005^3 0.9995^4 +\binom {7} 5 0.0005^3 0.9995^5\)

\(= 6.9\times 10^{-9}\)

In R pnbinom(5,3,0.0005)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{examples}{%
\section{Examples}\label{examples}}

With the negative binomial probability function:

\[f(y)=\binom {y+r-1} y p^r (1-p)^y\]

We can now answer questions like:

\begin{itemize}
\tightlist
\item
  What is the probability of observing \(10\) correct pixels before \(2\) errors, if the probability of an error is \(0.1\)?
\end{itemize}

\(f(10; r=2, p=0.1)=0.03835463\)

in R dnbinom(10, 2, 0.1)

\begin{itemize}
\tightlist
\item
  What is the probability that \(2\) girls enter the class before \(4\) boys if the probability that a girl enters is \(0.5\)?
\end{itemize}

\(f(2; r=4, p=0.5)=0.15625\)

in R dnbinom(2, 4, 0.5)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{negative-binomial-distribution-3}{%
\section{Negative binomial distribution}\label{negative-binomial-distribution-3}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-57-1.pdf}

\hypertarget{poisson-and-exponential-models}{%
\chapter{Poisson and Exponential Models}\label{poisson-and-exponential-models}}

\hypertarget{objective-6}{%
\section{Objective}\label{objective-6}}

Discrete probability model:

\begin{itemize}
\tightlist
\item
  Poisson
\end{itemize}

Continuous probability model:

\begin{itemize}
\tightlist
\item
  Exponential
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{discrete-probability-models-1}{%
\section{Discrete probability models}\label{discrete-probability-models-1}}

We are building up more complex models from simple ones:

\textbf{Uniform}: Classical interpretation of probability
\(\downarrow\)
\textbf{Bernoulli}: Introduction of a \textbf{parameter} \(p\) (family of models)
\(\downarrow\)
\textbf{Binomial}: \textbf{Repetition} of a random experiment (\(n\)-times Bernoulli trials)
\(\downarrow\)
\textbf{Poisson}: Repetition of random experiment within a continuous interval, having \textbf{no control} on when/where the Bernouilli trial occurs.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{counting-events}{%
\section{Counting events}\label{counting-events}}

Imagine that we are observing events that \textbf{depend} on time or distance \textbf{intervals}.

\begin{itemize}
\tightlist
\item
  cars arriving at a traffic light
\item
  getting messages on your mobile phone
\item
  impurities occurring at random in a copper wire
\end{itemize}

Suppose that the events are outcomes of \textbf{independent} Bernoulli trials each appearing randomly on a continuous interval, and we want to \textbf{count} them.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{counting-events-1}{%
\section{Counting events}\label{counting-events-1}}

What is the probability of observing \(X\) events in an interval's unit (time or distance)?

Imagine that some impurities in a copper wire deposit randomly along a wire

\begin{itemize}
\tightlist
\item
  at each centimeter, you would count an average of \(\lambda=10/cm\).
\item
  divide the centimeter into micrometers (\(0.0001cm\))
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution}{%
\section{Poisson distribution}\label{poisson-distribution}}

micrometers are small enough so

\begin{itemize}
\tightlist
\item
  either there is or there is not an impurity in each micrometer
\item
  each micrometer can be considered a \textbf{Bernoulli trial}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution-1}{%
\section{Poisson distribution}\label{poisson-distribution-1}}

The probability of observing \(X\) impurities in \(n=10,000\mu\) (1cm) approximately follows a binomial distribution

\(P(X=x)=\binom n x p^x(1-p)^{n-x}\)

where \(p\) is the probability of finding an impurity in a micrometer.

Remember that
\(E(X)=np\)
so for \(\lambda=np\) (average number of impurities per 1cm), we can write

\[P(X=x)=\binom n x \big(\frac{\lambda}{n}\big)^x(1-\frac{\lambda}{n})^{n-x}\]

\begin{itemize}
\tightlist
\item
  There \textbf{could} still be two impurities in a micrometer so we need to increase the partition of the wire and \(n \rightarrow \infty\).
\end{itemize}

Then in the limit:

\[P(X=x)= \frac{e^{-\lambda}\lambda^x}{x!}\]

Where \(\lambda\) is constant because it is the density of impurities per centimeter, a \textbf{physical property} of the system.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution-derivation-details}{%
\section{Poisson distribution: Derivation details}\label{poisson-distribution-derivation-details}}

For \(P(X=x)=\binom n x \big(\frac{\lambda}{n}\big)^x(1-\frac{\lambda}{n})^{n-x}\)

in the limit (\(n \rightarrow \infty\))

\begin{itemize}
\tightlist
\item
  \(\frac{1}{n^x}\binom n x =\frac{1}{n^x}\frac{n!}{x! (n-x)!}=\frac{(n-x)!(n-x+1)...(n-1)n}{n^x x! (n-x)!}=\frac{n(n-1)..(n-x+1)}{n^x x!} \rightarrow \frac{1}{x!}\)
\item
  \((1-\frac{\lambda}{n})^{n} \rightarrow e^{-\lambda}\) (definition of exponential)
\item
  \((1-\frac{\lambda}{n})^{-x} \rightarrow 1\)
\end{itemize}

Therefore
\(P(X=x)= \frac{e^{-\lambda}\lambda^x}{x!}\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution-2}{%
\section{Poisson distribution}\label{poisson-distribution-2}}

\textbf{Definition}

Given

\begin{itemize}
\tightlist
\item
  an interval in the real numbers
\item
  counts occur at random in the interval
\item
  the average number of counts on the interval is known (\(\lambda\))
\item
  if one can find a small regular partition of the interval such that each of them can be considered Bernoulli trials
\end{itemize}

Then\ldots{}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution-3}{%
\section{Poisson distribution}\label{poisson-distribution-3}}

\textbf{Definition}

The random variable \(X\) that counts events across the interval is a \textbf{Poisson} variable with probability mass function\\
\[f(x)= \frac{e^{-\lambda}\lambda^x}{x!}, \lambda>0\]

\textbf{Properties:}

\begin{itemize}
\tightlist
\item
  mean \(E(X)= \lambda\)
\item
  variance \(V(X)= \lambda\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution-4}{%
\section{Poisson distribution}\label{poisson-distribution-4}}

With the Poisson probability function:

\[f(x)= \frac{e^{-\lambda}\lambda^x}{x!}\] for \(x \in \{0, 1, ...\}\)

We can now answer questions like:

\begin{itemize}
\tightlist
\item
  What is the probability of receiving 4 emails in an hour, when the average number of emails in \textbf{one} hour is \(1\)?
\end{itemize}

\(f(4; \lambda=1)=0.18\)

in R dpois(2,1)

\begin{itemize}
\tightlist
\item
  What is the probability of counting at least \(10\) cars arriving at a road toll in a minute, when the average number of cars that arrive at the toll in a minute is \(5\);
  \(P(X \leq 10)=F(10; \lambda=5)=0.98\)?
\end{itemize}

in R ppois(10,5)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution-5}{%
\section{Poisson distribution}\label{poisson-distribution-5}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-58-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-probability-models}{%
\section{Continuous probability models}\label{continuous-probability-models}}

Continuous probability models are probability density functions \(f(x)\) of a continuous random variables that we \textbf{believe} describe real random experiments.

Definition:

Positive:

\begin{itemize}
\tightlist
\item
  \(f(x) \geq 0\)
\end{itemize}

\emph{Allows us to compute probabilities using the area under the curve:}

\begin{itemize}
\tightlist
\item
  \(P(a\leq X \leq b)=\int_{a}^{b} f(x) dx\)
\end{itemize}

The probability of any value is \(1\):

\begin{itemize}
\tightlist
\item
  \(\int_{-\infty}^{\infty} f(x) dx = 1\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-density}{%
\section{Exponential density}\label{exponential-density}}

Let's go back to the Poisson probability for the number of events (\(k\)) in an interval

\[f(k)=\frac{e^{-\lambda}\lambda^k}{k!}, \lambda>0\]

\begin{itemize}
\item
  Let's now consider only the first event
\item
  the distance/time we have to wait until the fisrt event is a \textbf{continuous} random variable.
\end{itemize}

We can ask for the probability that the first event is at distance \(X\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-density-1}{%
\section{Exponential density}\label{exponential-density-1}}

The probability of observing \(0\) events \textbf{if} an interval has unit \(x\) is

\[f(0|x)=\frac{e^{-x\lambda}x\lambda^0}{0!}\]

or

\[f(0|x)=e^{-x\lambda}\]

We can treat this as the conditional probability of \(0\) events in a distance \(x\): \(f(K=0|X=x)\) and apply the Bayes theorem to reverse it:

\[f(x|0)=C f(0|x)=C e^{-x\lambda}\]

So we can calculate the \textbf{probability of observing a distance} \(x\) with \(0\) events (this is the distance until the first event or the distance between any two events).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-density-2}{%
\section{Exponential density}\label{exponential-density-2}}

In a Poisson process with parameter \(\lambda\) the probability of waiting a distance/time \(X\) until the first event has a \textbf{probability density}

\[f(x)= C e^{-x\lambda}\]

\begin{itemize}
\item
  \(C\) is a constant that ensures: \(\int_{-\infty}^{\infty} f(x) dx =1\)
\item
  by integration \(C=\lambda\)
\end{itemize}

Therefore

\[f(x)=\lambda e^{-\lambda x}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-density-3}{%
\section{Exponential density}\label{exponential-density-3}}

An exponential random variable \(X\) has a probability density

\[f(x)=\lambda e^{-\lambda x}, x\geq 0\]

\textbf{Properties:}

\begin{itemize}
\tightlist
\item
  Mean: \(E(X)=\frac{1}{\lambda}\)
\item
  Variance: \(V(Y)=\frac{1}{\lambda^2}\)
\end{itemize}

Where \(\lambda\) is its single parameter, known as a \textbf{decay rate}.

\textbf{Note:} The exponential model is a general model. It can describe the time/length until the first count in a Poisson process of the size of a whole made by a drill.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-density-4}{%
\section{Exponential density}\label{exponential-density-4}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-59-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-distribution}{%
\section{Exponential Distribution}\label{exponential-distribution}}

In a Poisson process: ¿What is the probability of observing distance \textbf{smaller} than \(a\) until the first event?

Remember that this probability \(F(a)=P(X \leq a)\) is the probability density

\[F(a)=\lambda \int_\infty^a e^{-x\lambda}dx=1-e^{-a\lambda}\]

\begin{itemize}
\tightlist
\item
  ¿What is the probability of observing a distance \textbf{larger} than \(a\) until the first event?
\end{itemize}

\[P(X > a)=1- P(X \leq a)= 1- F(a) = e^{-a\lambda}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-distribution-1}{%
\section{Exponential Distribution}\label{exponential-distribution-1}}

With the exponential density function:

\[f(x)=\lambda e^{-\lambda x}\]

We can answer questions like:

\begin{itemize}
\tightlist
\item
  What is the probability that we have to wait for a bus for more than \(1\) hour when on average there are two buses per hour?
\end{itemize}

\[P(X > 1)=1-P(X \le 1) = 1-F(1,\lambda=2)=0.1353\]
in R 1-pexp(1,2)

\begin{itemize}
\tightlist
\item
  What is the probability of having to wait less than \(2\) seconds to detect one particle when the radioactive decay rate is \(2\) particles each second; \(F(2,\lambda=2)\)
\end{itemize}

\[P(X \le 2)=F(2,\lambda=2)=0.981\]

in R pexp(2,2)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-distribution-2}{%
\section{Exponential Distribution}\label{exponential-distribution-2}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-60-1.pdf}

The median \(x_m\) is such that \(F(x_m)=0.5\). That is \(x_m=\frac{\log(2)}{\lambda}\)

\hypertarget{normal-distribution}{%
\chapter{Normal Distribution}\label{normal-distribution}}

\hypertarget{objective-7}{%
\section{Objective}\label{objective-7}}

Continuous probability model:

\begin{itemize}
\tightlist
\item
  Normal distribution
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-probability-models-1}{%
\section{Continuous probability models}\label{continuous-probability-models-1}}

Continuous probability models are probability density functions \(f(x)\) of a continuous random variables that we \textbf{believe} describe real random experiments.

Definition:

Positive:

\begin{itemize}
\tightlist
\item
  \(f(x) \geq 0\)
\end{itemize}

\emph{Allows us to compute probabilities using the area under the curve:}

\begin{itemize}
\tightlist
\item
  \(P(a\leq X \leq b)=\int_{a}^{b} f(x) dx\)
\end{itemize}

The probability of any value is \(1\):

\begin{itemize}
\tightlist
\item
  \(\int_{-\infty}^{\infty} f(x) dx = 1\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-density}{%
\section{Normal density}\label{normal-density}}

In 1801 Gauss analyzed the orbit of Ceres (large asteroid between Mars and Jupiter).

\begin{itemize}
\tightlist
\item
  People suspected it was a new planet.
\item
  The measurements had errors.
\item
  He was interested in finding how the observations were distributed so he could find the most probable orbit.
\item
  He wanted to predict where astronomers should point their telescopes to find it a few months after it had passed behind the Sun.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-density-1}{%
\section{Normal density}\label{normal-density-1}}

Errors due to measurement.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-density-2}{%
\section{Normal density}\label{normal-density-2}}

He assumed that

\begin{itemize}
\tightlist
\item
  small errors were more likely than large errors
\item
  error at a distance \(-\epsilon\) or \(\epsilon\) from the most likely measurement were equally likely
\item
  the most \textbf{likely} altitude of Ceres at a given time in the sky was the \textbf{average} of multiple altitude measurements at that latitude.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-density-3}{%
\section{Normal density}\label{normal-density-3}}

That was enough to show that the random deviations \(y\) \textbf{from the orbit} distributed like

\[f(y)=\frac{h}{\sqrt{\pi}}e^{-h^2y^2}\]

*The evolution of the Normal distribution, Saul Stahl, Mathematics Magazine, 2006.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-density-4}{%
\section{Normal density}\label{normal-density-4}}

Let's write the distribution of errors

\[f(y)=\frac{h}{\sqrt{\pi}}e^{-h^2y^2}\]

for the errors of measurements from the horizon \(X\) then \(y=x-x_0\)

\[f(x)=\frac{h}{\sqrt{\pi}}e^{-h^2(x-x_0)^2}\]

\begin{itemize}
\tightlist
\item
  The \textbf{mean} of this probability density is:
\end{itemize}

\(E(X)=\mu=x_0\), that represents the \textbf{true} position of Ceres from the horizon (property of the physical system).

\begin{itemize}
\tightlist
\item
  The \textbf{variance} is:
\end{itemize}

\(V(X)=\sigma^2=\frac{1}{2h^2}\), that represents the dispersion of the error in the observations (property of the measurement system).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{definition}{%
\section{Definition}\label{definition}}

A random variable \(X\) defined in the real numbers has a \textbf{Normal} density if it takes the form

\[f(x; \mu, \sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, x \in {\Bbb R}\]

with mean and variance:

\begin{itemize}
\tightlist
\item
  \(E (X) = \mu\)
\item
  \(V (X) = \sigma^2\)
\end{itemize}

\(\mu\) and \(\sigma\) are the \textbf{two parameters} that fully describe the normal density function and their \textbf{interpretation} depends on the random experiment.

When \(X\) follows a Normal density, i.e.~distributes normally, we write

\[X\rightarrow N(\mu,\sigma^2)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-probability-density-gaussian}{%
\section{Normal probability density (Gaussian)}\label{normal-probability-density-gaussian}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-61-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-1}{%
\section{Normal distribution}\label{normal-distribution-1}}

The probability distribution of the Normal density:

\[F_{normal}(a)=P(Z \leq a)\]

is the \textbf{error} function defined by the area under the curve from \(-\infty\) to \(a\)

\[F_{normal}(a)=\int_{-\infty}^{a}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx\]
The function is found in most computer programs.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-2}{%
\section{Normal distribution}\label{normal-distribution-2}}

When \[X \rightarrow N(\mu, \sigma^2)\]

We can ask questions like:

\begin{itemize}
\tightlist
\item
  What is the probability that a woman in the population is at most \(150cm\) tall if women have a mean height of \(165cm\) with standard deviation of \(8cm\)?
\end{itemize}

\(P(X\le 150)=F(150, \mu=165, \sigma=8)=0.03039636\)

in R pnorm(150, 165, 8)

\begin{itemize}
\tightlist
\item
  What is the probability that a woman's height in the population is between \(165cm\) and \(170cm\)?
\end{itemize}

\(P(165 \le X \le 170)=F(170, \mu=165, \sigma=8)-F(165, \mu=165, \sigma=8)=0.2340145\)

in R pnorm(170, 165, 8)-pnorm(165, 165, 8)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-3}{%
\section{Normal distribution}\label{normal-distribution-3}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-62-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-4}{%
\section{Normal distribution}\label{normal-distribution-4}}

\begin{itemize}
\tightlist
\item
  the mean \(\mu\) is also the median as it splits the measurements in two
\item
  \(x\) values that fall farther than 2\(\sigma\) are considered \textbf{rare} \(5\%\)
\item
  \(x\) values that fall farther than 3\(\sigma\) are considered \textbf{extremely rare} \(0.2\%\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-5}{%
\section{Normal distribution}\label{normal-distribution-5}}

We can define the limits of \textbf{common observations} for the distribution of women's height in the population.

\begin{itemize}
\tightlist
\item
  \(P(165-8 \leq X \leq 165+8)=P(157 \leq X \leq 173)=0.68\)
\item
  \(P(165-2 \times 8 \leq X \leq 165+2\times 8)=P(149 \leq X \leq 181)=0.95\)
\item
  \(P(165-3 \times 8 \leq X \leq 165+3\times 8)=P(141 \leq X \leq 189)=0.997\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-63-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-normal-density}{%
\section{Standard normal density}\label{standard-normal-density}}

Let's change variables to a \textbf{standardized variable}

\[Z=\frac{X-\mu}{\sigma}\]

in the density

\[f(x; \mu, \sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, x \in {\Bbb R}\]

replacing \(x=\sigma z+\mu\) and \(dx=\sigma dz\) in the probability expression we have

\(P(x\leq X \leq x +dx)=P(z\leq Z \leq z +dz)\)
\[=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx\] \[=\frac{1}{ \sqrt{2\pi}}e^{-\frac{z^2}{2}} dz\]

we obtain the \textbf{standardized} form of the normal density.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-normal-density-1}{%
\section{Standard normal density}\label{standard-normal-density-1}}

\textbf{Definition}

A random variable \(Z\) defined in the real numbers has a \textbf{standard} density if it takes the form

\[f(z)=\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}} dz,z \in {\Bbb R}\]

with mean and variance

\begin{itemize}
\item
  \(E (X) = 0\)
\item
  \(V (X) =1\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-normal-density-2}{%
\section{Standard normal density}\label{standard-normal-density-2}}

The standard density:

\[f(z)=\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}} dz,z \in {\Bbb R}\]

\begin{itemize}
\item
  is the normal density \(N(\mu=0,\sigma^2=1)\)
\item
  any normally distributed variable \(X\) can be transformed to a variable \(Z\)
\end{itemize}

\[Z=\frac{x-\mu}{\sigma}\]

that follows a standard distribution:

\[Z \rightarrow N(0,1)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-6}{%
\section{Normal distribution}\label{normal-distribution-6}}

All normal densities can be obtained from the standard density with the values of \(\mu\) and \(\sigma\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-distribution}{%
\section{Standard distribution}\label{standard-distribution}}

The probability distribution of the standard density:

\[\phi(a)=F_{standard}(a)=P(Z \leq a)\]

is the \textbf{error} function defined by

\[\phi(a)=\int_{-\infty}^{a} \frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}} dz\]

You can find it in most computer programs

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-normal-density-3}{%
\section{Standard normal density}\label{standard-normal-density-3}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-normal-density-4}{%
\section{Standard normal density}\label{standard-normal-density-4}}

We define the limits of the \textbf{most common observations} for the standard variable

\begin{itemize}
\tightlist
\item
  \(P(-0.67 \leq X \leq 0.67)=0.50\)
\item
  \(P(-1.96 \leq X \leq 1.96)=0.95\)
\item
  \(P(-2.58 \leq X \leq 2.58)=0.99\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-64-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-and-standard-distributions}{%
\section{Normal and standard distributions}\label{normal-and-standard-distributions}}

For any normally distributed variable \(X\), such that

\[X\rightarrow N(\mu, \sigma^2)\]

its distribution \(F(a)=P(X \leq a)\) can be computed from

\[F(a)= \phi \big(\frac{a-\mu}{\sigma}\big)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-7}{%
\section{Normal distribution}\label{normal-distribution-7}}

For computing \(P(a\leq X \leq b)\), we use the property of the probability distributions

\[F(b)-F(a)=P(X\leq b)-P(X\leq a)\]

Let's standardize

\(=P(\frac{X-\mu}{\sigma}\leq \frac{a-\mu}{\sigma})-P(\frac{X-\mu}{\sigma}\leq \frac{b-\mu}{\sigma})\)

\(=P(Z \leq \frac{b-\mu}{\sigma})-P(Z \leq \frac{a-\mu}{\sigma}\big)\)

\(=\phi \big(\frac{b-\mu}{\sigma}\big)-\phi \big(\frac{a-\mu}{\sigma}\big)\)

Then

\[F(b)-F(a)=\phi \big(\frac{b-\mu}{\sigma}\big)-\phi \big(\frac{a-\mu}{\sigma}\big)\]

The probabilities of \textbf{any normal variable} can be obtained from the \textbf{standard distribution}, after standardization (subtract the mean and divide by the standard deviation).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{summary-of-probability-models}{%
\section{Summary of probability models}\label{summary-of-probability-models}}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.17}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.19}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.19}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.17}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.14}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\centering
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
X
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
range of x
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
f(x)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
E(X)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
V(X)
\end{minipage} \\
\midrule
\endhead
Uniform & integer or real number & \([a, b]\) & \(\frac{1}{n}\) & \(\frac{b+a}{2}\) & \(\frac{(b-a+1)^2-1}{12}\) \\
Bernoulli & event A & 0,1 & \((1-p)^{1-x}p^x\) & \(p\) & \(p(1-p)\) \\
Binomial & \# of A events in \(n\) repetitions of Bernoulli trials & 0,1,\ldots{} & \(\binom n x (1-p)^{n-x}p^x\) & \(np\) & \(np(1-p)\) \\
Negative Binomial for events & \# of B events in Bernoulli repetitions before \(r\) As are observed & 0,1,.. & \(\binom {x+r-1} x (1-p)^xp^r\) & \(\frac{r(1-p)}{p}\) & \(\frac{r(1-p)}{p^2}\) \\
Hypergeometric & \# A events in a sample \(n\) from population \(N\) with \(K\) As & \(\max(0, n+K-N)\), \ldots{} \(\min(K, n)\) & \(\frac{1}{\binom N n}\binom K x \binom {N-K} {n-x}\) & \(n*\frac{N}{K}\) & \(n \frac{N}{K} (1-\frac{N}{K})\frac{N-n}{N-1}\) \\
Poisson & \# of events A in an interval & 0,1, .. & \(\frac{e^{-\lambda}\lambda^x}{x!}\) & \(\lambda\) & \(\lambda\) \\
Exponential & Interval between two events A & \([0,\infty)\) & \(\lambda e^{-\lambda x}\) & \(\frac{1}{\lambda}\) & \(\frac{1}{\lambda^2}\) \\
Normal & measurement with symmetric errors whose most likely value is the average & \((-\infty, \infty)\) & \(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\) & \(\mu\) & \(\sigma^2\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{r-functions-of-probability-models}{%
\section{R functions of probability models}\label{r-functions-of-probability-models}}

\begin{longtable}[]{@{}lc@{}}
\toprule
Model & R \\
\midrule
\endhead
Uniform (continuous) & dunif(x, a, b) \\
Binomial & dbimon(x,n,p) \\
Negative Binomial for events & dnbinom(x,r,p) \\
Hypergeometric & dhyper(x, K, N-K, n) \\
Poisson & dpois(x, lambda) \\
Exponential & dexp(x, lambda) \\
Normal & dnomr(x, mu, sigma) \\
\bottomrule
\end{longtable}

\hypertarget{sampling-distributions}{%
\chapter{Sampling Distributions}\label{sampling-distributions}}

\hypertarget{objective-8}{%
\section{Objective}\label{objective-8}}

Distributions for

\begin{itemize}
\tightlist
\item
  Sample mean (average)
\item
  sample sum
\item
  Sample variance
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-8}{%
\section{Normal distribution}\label{normal-distribution-8}}

When we have a normal random variable

\[X \rightarrow N(x; \mu, \sigma^2)\]

How do we estimate \(\mu\) and \(\sigma^2\)?

\begin{itemize}
\item
  we need to take a \textbf{random sample}
\item
  we need to \textbf{estimate} each parameter
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-when-we-do-not-know-the-parameters}{%
\section{Example: When we do not know the parameters}\label{example-when-we-do-not-know-the-parameters}}

Imagine a client asking your metallurgical company to sell them \(8\) cables that can carry up to \(96\) Tons; that is \(12\) Tons each.

\begin{itemize}
\tightlist
\item
  You have in \textbf{stock} a set of cables that could do the job.
\end{itemize}

Can you use the cables in stock or do you need to produce new ones?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-12}{%
\section{Example}\label{example-12}}

you take a sample of \(8\) random experiments, each of which consists of loading a cable until it breaks and recording the breaking load.

These are the results: The observation of a \textbf{sample} of size \(8\)

\begin{verbatim}
## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747
\end{verbatim}

\begin{itemize}
\item
  None of them broke at \(12\) Tons.
\item
  There was one that broke at \(12.62747\) Tons.
\end{itemize}

Do you take the risk and sell a random sample of \(8\) cables from your stock?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{_main_files/figure-latex/unnamed-chunk-66-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{random-sample}{%
\section{Random sample}\label{random-sample}}

A \textbf{random sample} of size \(n\) is the \textbf{repetition} of a random experiment \(n\) \textbf{independent} times.

\begin{itemize}
\tightlist
\item
  A random sample is a \(n\)-dimensional \textbf{random variable}
\end{itemize}

\[(X_1, X_2, ... X_n)\]

where \(X_i\) is the \emph{i-th} repetition of the random experiment with comon distribution \(f(x; \theta)\) for any \(i\)

\begin{itemize}
\tightlist
\item
  \textbf{One observation} of a random sample is the set of \(n\) values obtained from the experiments
\end{itemize}

\[(x_1, x_2, ... x_n)\]
Our \textbf{observation} of the sample of \(8\) cables was

\begin{verbatim}
## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-13}{%
\section{Example}\label{example-13}}

We would like to compute \(P(X \leq 12)\).

We are going to \textbf{assume} that the braking point is \textbf{normally distributed}.

\[X \rightarrow N(x; \mu, \sigma^2)\]

\begin{itemize}
\item
  For computing \(P(X \leq 12)\) we need the parameters \(\mu\) and \(\sigma^2\).
\item
  How do we estimate the parameters from the observed sample?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-or-sample-mean}{%
\section{Average or sample mean}\label{average-or-sample-mean}}

\textbf{Definition}

The sample mean (or average) of a \textbf{random sample} of size \(n\) is defined as

\[\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i\]

The average is a \textbf{random variable} that in our \(8\)-size sample took the value

\[\bar{x}_{stock}=13.21\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{the-average-as-an-estimator}{%
\section{The average as an estimator}\label{the-average-as-an-estimator}}

This number can be used to \textbf{estimate} the unknown parameter \(\mu\) because:

\begin{itemize}
\tightlist
\item
  \(E(\bar{X})=E(X)=\mu\)
\item
  \(V(\bar{X})=\frac{V(X)}{n}=\frac{\sigma^2}{n}\)
\end{itemize}

(since each random experiment in the sample is independent)

as

\begin{itemize}
\tightlist
\item
  \(n \rightarrow \infty\), \(V(\bar{X}) \rightarrow 0\)
\end{itemize}

then

\begin{itemize}
\tightlist
\item
  \(\bar{x}\) \textbf{concentrates closer and closer} to \(\mu\) as \(n\) increases.
\end{itemize}

We can take one value of \(\bar{x}\) as estimation for \(\mu\) or

\[\bar{x}=\hat{\mu}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-variance-2}{%
\section{Sample variance}\label{sample-variance-2}}

\textbf{Definition}

The \textbf{sample variance} \(S^2\) of a random sample of size \(n\)

\[S^2= \frac{1}{n-1}\sum_{i=1}^n (X_i-\bar{X})^2\]

is the dispersion of the measurements about \(\bar{X}\). In our \(8\)-size sample \(S^2\) took the value

\[s_{stock}^2=\frac{1}{n-1}\sum_{i=1}^n (x_i-\bar{x})^2=0.1275608\]

The expected value of \(S^2\) is

\begin{itemize}
\tightlist
\item
  \(E(S^2)=V(X)=\sigma^2\) (unbiased)
\end{itemize}

and therefore \(S^2\) is

\begin{itemize}
\tightlist
\item
  an estimator of \(V(X)\)
\item
  it also concentrates around \(\sigma^2\) because as \(n \rightarrow \infty\), \(V(\bar{S^2}) \rightarrow 0\) (consistent)
\end{itemize}

We can take one value of \(s^2\) as estimation for \(\sigma^2\) or

\[s^2=\hat{\sigma}^2\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-variance-3}{%
\section{Sample variance}\label{sample-variance-3}}

\(S^2\) aims to estimate the dispersion of the outcomes about \(\mu\) (the variance)

If we use \(\bar{X}\) as an estimator of \(\mu\) we need to correct for its dispersion (i.e.~mean squared error of \(\bar{X}\)).

The correction is achieved by dividing by \(n-1\) and not \(n\) in the definition of \(S^2\)

For:

\(S_n^2=\frac{1}{n}\sum_{i=1}^n (X_i-\bar{X})^2\)

\(E(S_n^2) = \sigma^2-\frac{\sigma^2}{n} \neq \sigma^2\) (we say that \(S_n^2\) is \textbf{biased} estimator of \(\sigma\))

\includegraphics{_main_files/figure-latex/unnamed-chunk-68-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{fitting-a-model}{%
\section{Fitting a model}\label{fitting-a-model}}

We \textbf{fit a model} when we

\begin{itemize}
\tightlist
\item
  \textbf{estimate} the parameters of the model
\end{itemize}

We also say we \textbf{train} a model (machine learning)

\textbf{Assuming} that

\[X \rightarrow N(x; \mu, \sigma^2)\]

Since we do not know the parameters, we \textbf{plugin} the estimates \(\bar{x}\) and \(s^2\) as the values of \(\mu\)
and \(\sigma^2\)

\[X \rightarrow N(x; \mu=13.21, \sigma^2=0.3571565^2)\]

\includegraphics{_main_files/figure-latex/unnamed-chunk-69-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{prediction}{%
\section{Prediction}\label{prediction}}

We \textbf{predict} the value of an \textbf{outcome} when we compute its \textbf{probability}

What is the probability that the cable breaks at \(12\) Tons?

If we assumed the random variable

\[X \rightarrow N(x; \mu, \sigma^2)\]
We plug in the estimates \(\bar{x}\) and \(s^2\) into the probability distribution

\[P(X \leq 12)= F_{normal}(12; \mu=13.21, \sigma^2=0.1275608)\]

In R pnorm(12,13.21, 0.3571565)\(=0.000352188\)

Given the \textbf{observed} sample, there is an estimated probability of \(0.03\%\) that a single cable will break at \(12\) Tons.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference}{%
\section{Inference}\label{inference}}

When we have a normal random variable

\begin{verbatim}
$$X \rightarrow N(x; \mu, \sigma^2)$$
\end{verbatim}

and we know \(\mu\) and \(\sigma^2\).

We can make inferences about \(\bar{X}\), that is, calculate probabilities of the random variable \(\bar{X}\).

When we make \textbf{inferences}, we usually ask the question:

How \textbf{sure} are we that the value of the estimator \textbf{is close} to the \textbf{true parameter}?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-when-we-do-know-the-parameters}{%
\section{Example: When we do know the parameters}\label{example-when-we-do-know-the-parameters}}

Let us imagine that our cables are certified to break with an average load of \(\mu = 13\) Tons with variance \(\sigma^2=0.35^2\).

We take a random sample of \(8\) cables

\begin{verbatim}
## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747
\end{verbatim}

Can we say that we actually produced stronger cables because we got \(\bar{x}=13.21\) from this sample of \(8\) cables?

We need to calculate probabilities of \(\bar{X}\).

\begin{itemize}
\tightlist
\item
  What is the probability that the distance between \(\bar{X}\) and \(\mu\) is less than \(\bar{x}_{stock}-\mu=0.21\)?
\end{itemize}

\[P(- 0.21\leq \bar{X}-\mu \leq 0.21)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{density-for-x-and-barx}{%
\section{\texorpdfstring{Density for \(X\) and \(\bar{X}\)}{Density for X and \textbackslash bar\{X\}}}\label{density-for-x-and-barx}}

If we \textbf{know} that the \textbf{true} parameters are \(\mu=13\) and \(\sigma=0.35\) this is what we would see

\includegraphics{_main_files/figure-latex/unnamed-chunk-71-1.pdf} \includegraphics{_main_files/figure-latex/unnamed-chunk-71-2.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-mean-distribution}{%
\section{Sample mean distribution}\label{sample-mean-distribution}}

\textbf{Theorem:} When \(X\) follows a normal distribution \(X \rightarrow N(\mu, \sigma^2)\)

\(\bar{X}\) is normal:

\[\bar{X} \rightarrow N(\mu, \frac{\sigma^2}{n})\]
Then, if we \textbf{know} \(\mu\) and \(\sigma\) we can compute the true \textbf{probabilities of \(\bar{X}\)} using the normal distribution.

The mean and variance of \(\bar{X}\) are

\begin{itemize}
\tightlist
\item
  \(E(\bar{X})=\mu\)
\item
  \(V(\bar{X})=\frac{\sigma^2}{n}\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference-on-the-average}{%
\section{Inference on the average}\label{inference-on-the-average}}

\textbf{Example:}

If we \textbf{know} that the breaking load of our cables truly distribute as \[X \rightarrow N(\mu=13, \sigma^2=0.35^2)\] then

\[\bar{X} \rightarrow N(13, \frac{0.35^2}{8})\]

\begin{itemize}
\tightlist
\item
  \(E(\bar{X})=13\)
\item
  \(V(\bar{X})=\frac{0.35^2}{8}=0.01530169\)
\end{itemize}

Our \textbf{observed error} in the estimation of the mean is the difference

\[\bar{x}_{stock}-\mu=13.21-13=0.21\]

We ask: Is this a \textbf{typical} error?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{density-for-barx}{%
\section{\texorpdfstring{Density for \(\bar{X}\)}{Density for \textbackslash bar\{X\}}}\label{density-for-barx}}

If we \textbf{knew} that the \textbf{true} parameters were \(\mu=13\) and \(\sigma=0.35\) this is the error we would see

\includegraphics{_main_files/figure-latex/unnamed-chunk-72-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probabilities-of-barx}{%
\subsection{\texorpdfstring{Probabilities of \(\bar{X}\)}{Probabilities of \textbackslash bar\{X\}}}\label{probabilities-of-barx}}

\textbf{If} we \textbf{know} that the braking load of our cables \textbf{truly} distribute as \[\bar{X} \rightarrow N(\mu=13, \frac{\sigma^2}{n}=0.1237^2)\]

What is the probability of observing an \textbf{error in estimation} of \(\mu\) (distance between \(\bar{X}\) and \(\mu\)) smaller than \(0.21\)?

We want to compute \[P(-0.21 \leq \bar{X} - 13\leq 0.21)=P(12.79 \leq \bar{X} \leq 13.21)\]

\(=F_{normal}(13.21; \mu, se^2)-F_{normal}(12.79; \mu, se^2)\)

In R we can compute it as:

pnorm(13.21, 13, 0.1237)-pnorm(12.79, 13, 0.1237)=0.9104.

\(91.0\%\) of the errors are less than \(0.21\), therefore the \textbf{observed} error does not seem to be too typical (only \(9\%\) of the errors are higher).

Maybe we have stronger cables than we thought.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-sum}{%
\subsection{Sample sum}\label{sample-sum}}

If we are interested in using all the \(8\) cables at the same time to carry a total of \(96\) Tons, then we should consider adding their individual contributions.

The \textbf{sample sum} is the \textbf{statistic}:

\[Y=n \bar{X}=\sum_{i=1}^n X_i\]

\textbf{Theorem:} if \(X \rightarrow N(\mu, \sigma^2)\) then \[Y \rightarrow N(n\mu, n\sigma^2)\]

With mean and variance:

\begin{itemize}
\tightlist
\item
  \(E(Y)=n\mu\)
\item
  \(V(Y)=n\sigma^2\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference-on-the-sample-sum}{%
\subsection{Inference on the sample sum}\label{inference-on-the-sample-sum}}

If we \textbf{know} that for our cables \[X \rightarrow N(\mu=13, \sigma^2=0.35^2)\] then

\[Y \rightarrow N(n\mu=104, n\sigma^2=8\times 0.35^2)\]

\begin{itemize}
\tightlist
\item
  \(E(Y)=104\)
\item
  \(V(Y)=8\times 0.35^2=0.98\)
\end{itemize}

For our \(8\)-sample, we observed

\begin{itemize}
\tightlist
\item
  \(y_{stock}=105.7014\)
\end{itemize}

and, therefore, the \textbf{observed error} in the estimation of the mean of the \textbf{true} total braking load (\(n\mu\)) of \(8\) cables was

\begin{itemize}
\tightlist
\item
  \(y_{stock}-n\mu= 1.7014\)
\end{itemize}

Is this a \textbf{typical} error?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probabilities-of-the-sample-sum-propagation-of-error}{%
\subsection{Probabilities of the sample sum: Propagation of error}\label{probabilities-of-the-sample-sum-propagation-of-error}}

What is the probability of observing a difference \(Y-E(Y)\) smaller than \(1.7014\)?

We want to compute the probability

\[P(-1.7014 \leq \bar{Y} - 104 \leq 1.7014)=P(102.2986 \leq Y \leq 105.7014)\]

\(=F_{normal}(105.7014; n\mu, n\sigma^2)-F_{normal}(102.2986; n\mu, n\sigma^2)\)

In R we can compute it as:

pnorm(105.7014, 104, sqrt(0.98)) - pnorm(102.2986, 104, sqrt(0.98))=0.914.

\(91.4\%\) of the times we obtain sample sums that are smaller than \(1.7014\)

This proportion is higher than the proportion for individual cables.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference-in-the-sample-variance}{%
\section{Inference in the sample variance}\label{inference-in-the-sample-variance}}

Consider a quality control process that requires that the cables are produced close to the specified value \(\mu\).

If a sample of \(8\) cables is too dispersed (\(S^2>0.3\)), we stop production: the process is out of control.

What is the probability that the sample variance of a sample of \(8\) cables is greater than the required \(0.3\)?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probabilities-of-the-sample-variance}{%
\section{Probabilities of the sample variance}\label{probabilities-of-the-sample-variance}}

\textbf{Theorem:} When \(X\) follows a normal distribution
\[X \rightarrow N(\mu, \sigma^2)\]

The \textbf{statistic}:

\[W=\frac{(n-1)S^2}{\sigma^2} \rightarrow \chi^2(n-1)\]

has a \(\chi^2\) (chi-squared) distribution with \(df=n-1\) degrees of freedom given by

\[f(w)=C_n  w^{\frac{n-3}{2}} e^{-\frac{w}{2}}\]

where:

\begin{itemize}
\item
  \(C_n=\frac{1}{2^{(n-1)/2\sqrt{\pi(n-1)}}}\) ensures \(\int_{-\infty}^{\infty} f(t)dt=1\)
\item
  \(\Gamma(x)\) is Euler's factorial for real numbers
\item
  If we \textbf{know} the true values of \(\mu\) and \(\sigma\) we can compute probabilities of \(S^2\) using the \(\chi^2\) distribution for \(W\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{chi2-statistic}{%
\section{\texorpdfstring{\(\chi^2\)-statistic}{\textbackslash chi\^{}2-statistic}}\label{chi2-statistic}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-73-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{chi2-statistic-1}{%
\section{\texorpdfstring{\(\chi^2\)-statistic}{\textbackslash chi\^{}2-statistic}}\label{chi2-statistic-1}}

If we \textbf{know} that our cables trully distribute as \[X \rightarrow N(\mu=13, \sigma^2=0.35^2)\]

then we can compute \[P(S^2 > 0.2)=P(\frac{(n-1)S^2}{\sigma^2} > \frac{(n-1)0.3}{\sigma^2})\]
\(=P(W > \frac{(n-1)0.3}{\sigma^2})\)

\(=1-P(W \leq \frac{(n-1)0.3}{\sigma^2})=1-P(W\leq \frac{(8-1)0.3}{0.1225})\)

\(= 1- F_{\chi^2,df=7}(17.14286)=0.016\)

In R
1-pchisq(17.14286, df=7)=0.016

There is only a probability of \(1\%\) of obtaining a value greater than \(s^2=0.3\).

\begin{itemize}
\item
  \(s^2>0.3\) seems to be a good criterion to stop production and revise the process.
\item
  our observed value was \(s^2_{stock}=0.1275608\)
\item
  the sample is not too dispersed and we believe that the production is under control.
\end{itemize}

\hypertarget{exercises}{%
\chapter{Exercises}\label{exercises}}

\hypertarget{data-description-1}{%
\section{Data description}\label{data-description-1}}

\hypertarget{exercise-1}{%
\subsubsection{Exercise 1}\label{exercise-1}}

We have performed an experiment 8 times with the following results

\begin{verbatim}
## [1]  3  3 10  2  6 11  5  4
\end{verbatim}

Answer the following questions:

\begin{itemize}
\tightlist
\item
  Compute the relative frequencies of each outcome.
\item
  Compute the cumulative frequencies of each outcome.
\item
  What is the average of the observations?
\item
  What is the median?
\item
  What is the third quartile?
\item
  What is the first quartile?
\end{itemize}

\hypertarget{exercise-2}{%
\subsubsection{Exercise 2}\label{exercise-2}}

We have performed an experiment 10 times with the following results

\begin{verbatim}
##  [1] 2.875775 7.883051 4.089769 8.830174 9.404673 0.455565 5.281055 8.924190
##  [9] 5.514350 4.566147
\end{verbatim}

Consider 10 bins of size 1: {[}0,1{]}, (1,2{]}\ldots(9,10{]}.

Answer the following questions:

\begin{itemize}
\item
  Compute the relative frequencies of each outcome and draw the histogram
\item
  Compute the cumulative frequencies of each outcome and sketch the cumulative plot.
\item
  Sketch a boxplot.
\end{itemize}

\hypertarget{probability-3}{%
\section{Probability}\label{probability-3}}

\hypertarget{exercise-1-1}{%
\subsubsection{Exercise 1}\label{exercise-1-1}}

The outcome of one random experiment is to measure the misophonia severity \textbf{and} depression status of one patient.

\begin{itemize}
\tightlist
\item
  Misophonia severity: \(x\in \{0,1,2,3,4\}\)
\item
  Depression: \(y\in \{0,1\}\) (no:\(0\), yes:\(1\))
\end{itemize}

\begin{verbatim}
##   Misofonia.dic depresion.dic
## 1             4             1
## 2             2             0
## 3             0             0
## 4             3             0
## 5             0             0
## 6             0             0
\end{verbatim}

A large study on 123 patients showed the frequencies \(n_{x,y}\) given in the contingency table:

\begin{verbatim}
##               
##                Depression:0 Depression:1
##   Misophonia:4            0            9
##   Misophonia:3           25            6
##   Misophonia:2           34            3
##   Misophonia:1            5            0
##   Misophonia:0           36            5
\end{verbatim}

Let's assume that \(N>>0\) and that the frequencies \textbf{estimate} the probabilities \(f_{x,y}=\hat{P}(X, Y)\)

\begin{verbatim}
##               
##                Depression:0 Depression:1
##   Misophonia:4   0.00000000   0.07317073
##   Misophonia:3   0.20325203   0.04878049
##   Misophonia:2   0.27642276   0.02439024
##   Misophonia:1   0.04065041   0.00000000
##   Misophonia:0   0.29268293   0.04065041
\end{verbatim}

\begin{itemize}
\tightlist
\item
  What is the marginal probability of misophonia severity 3? (R/0.3)
\item
  What is the probability of not being misophonic \textbf{and} not depressed? (R/0.293)
\item
  What is the probability of being misophonic \textbf{or} depressed? (R/0.293)
\item
  What is the probability of being misophonic \textbf{and} depressed? (R/0.707)
\item
  Describe in English the outcomes with probability 0.
\end{itemize}

\hypertarget{exercise-2-1}{%
\subsubsection{Exercise 2}\label{exercise-2-1}}

We have performed an experiment 10 times with the following results

\begin{verbatim}
##         A     B
## 1    male  dead
## 2    male  dead
## 3    male  dead
## 4  female alive
## 5    male  dead
## 6  female alive
## 7  female  dead
## 8  female alive
## 9    male alive
## 10   male alive
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Create the contingency table for the number (\(n_{i,j}\)) of observations of each outcome (\(A,B\))
\item
  Create the contingency table for the relative frequency (\(f_{i,j}\)) of the outcomes
\item
  What is the marginal frequency of being male? (R/0.6)
\item
  What is the marginal frequency of being alive? (R/0.5)
\item
  What is the frequency of being alive \textbf{or} female? (R/0.6)
\end{itemize}

\hypertarget{conditional-probability-3}{%
\section{Conditional Probability}\label{conditional-probability-3}}

\hypertarget{exercise-1-2}{%
\subsubsection{Exercise 1}\label{exercise-1-2}}

A machine is tested for its performance to produce high-quality turning rods. These are the results of the testing

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Rounded: Yes & Rounded: No \\
\midrule
\endhead
smooth surface: yes & 200 & 1 \\
smooth surface: no & 4 & 2 \\
\bottomrule
\end{longtable}

\begin{itemize}
\item
  What is the estimated probability that the machine produces a rod that does not satisfy any quality control? (R: 2/207)
\item
  What is the estimated probability that the machine produces a rod that does not satisfy at least one quality control?(R: 7/207)
\item
  What is the estimated probability that the machine produces rounded and smoothed surfaced rods? (R: 200/207)
\item
  what is the estimated probability that the rod is rounded if the rod is smooth? (R: 201/201)
\item
  what is the estimated probability that the rod is smooth if it is rounded? (R: 201/204)
\item
  what is the estimated probability that the rod is neither smooth nor rounded if it does not satisfy at least one quality control? (R: 2/7)
\item
  Are smoothness and roundness independent events? (no)
\end{itemize}

\hypertarget{exercise-2-2}{%
\subsubsection{Exercise 2}\label{exercise-2-2}}

We develop a test to detect the presence of bacteria in a lake. We find that if the lake contains the bacteria the test is positive 70\% of the time. If there are no bacteria then the test is negative 60\% of the time. We deploy the test in a region where we know that 20\% of the lakes have bacteria.

\begin{itemize}
\tightlist
\item
  What is the probability that one lake that tests positive is contaminated with bacteria? (R: 0.30)
\end{itemize}

\hypertarget{exercise-3}{%
\subsubsection{Exercise 3}\label{exercise-3}}

Two machines are tested for their performance to produce high-quality turning rods. These are the results of the testing

\textbf{Machine 1}

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Rounded: Yes & Rounded: No \\
\midrule
\endhead
smooth surface: yes & 200 & 1 \\
smooth surface: no & 4 & 2 \\
\bottomrule
\end{longtable}

\textbf{Machine 2}

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Rounded: Yes & Rounded: No \\
\midrule
\endhead
smooth surface: yes & 145 & 4 \\
smooth surface: no & 8 & 6 \\
\bottomrule
\end{longtable}

\begin{itemize}
\tightlist
\item
  what is the probability that the rod is rounded? (R: 357/370)
\item
  What is the probability that the rod has been produced by machine 1? (R: 207/370)
\item
  what is the probability that the rod is not smooth? (R: 20/370)
\item
  What is the probability that the rod is smooth or rounded or produced by machine 1? (R: 364/370)
\item
  What is the probability that the rod is rounded if it is smoothed and from machine 1? (R: 200/201)
\item
  What is the probability that the rod is not rounded if it is not smoothed and is from machine 2? (R: 6/8)
\item
  what is the probability that the rod has come from machine 1 if it it is smoothed and rounded? (R: 200/345)
\item
  what is the probability that the rod has come from machine 2 if it does not pass at least one of the quality controls? (R:0.72)
\end{itemize}

\hypertarget{exercise-4}{%
\subsubsection{Exercise 4}\label{exercise-4}}

We want to cross an avenue with two traffic lights. The probability of finding the first traffic light in red is 0.6. If we stopped at the first traffic light, the probability of stopping at the second one is 0.15. Whereas the probability of stopping on the second one if we do not stop on the first one is 0.25.

When we try to cross both traffic lights:

\begin{itemize}
\tightlist
\item
  what is the probability of having to stop at each traffic light? (R:0.09)
\item
  What is the probability of having to stop at at least one traffic light?(R:0.7)
\item
  What is the probability of having to stop at only one traffic light? (R:0.61)
\item
  If I stopped at the second traffic light, what is the probability that I had to stop at the first one? (R: 0.62)
\item
  If I had to stop at any traffic light, what is the probability that I had to do it twice? (R: 0.12)
\item
  Is stopping at the first traffic light an independent event from stopping at the second traffic light? (no)
\end{itemize}

Now, we want to cross an avenue with three traffic lights. The probability of finding a traffic light in red only depends on the previous one. In particular, the probability of finding one traffic light in red given that the previous one was in red is 0.15. Whereas, the probability of finding one traffic right in red given that the previous one was in green is 0.25. Also, the probability of finding the first traffic light in red is 0.6.

\begin{itemize}
\tightlist
\item
  What is the probability of having to stop at each traffic light? (R:0.013)
\item
  What is the probability of having to stop at at least one traffic light? (R:0.775)
\item
  What is the probability of having to stop at only one traffic light? (R:0.5425)
\end{itemize}

hints:

\begin{itemize}
\item
  If the probability that one traffic light is red depends only on the previous one then
  \(P(R_3|R_2,R_1)=P(R_3|R_2,\bar{R}_1)=P(R_3|R_2)\) and \(P(R_3|\bar{R}_2,R_1)=P(R_3|\bar{R}_2,\bar{R}_1)=P(R_3|\bar{R}_2)\)
\item
  The joint probability of finding three traffic lights in red can be written as:
  \(P(R_1,R_2,R_3)=P(R_3|R_2)P(R_2|R_1)P(R_1)\)
\end{itemize}

\hypertarget{exercise-5}{%
\subsubsection{Exercise 5}\label{exercise-5}}

A quality test on a random brick is defined by the events:

\begin{itemize}
\tightlist
\item
  Pass quality test: \(E\), do no pass quality test: \(\bar{E}\)
\item
  Defective: \(D\), non-defective: \(\bar{D}\)
\end{itemize}

If the diagnostic test has sensitivity \(P(E|\bar{D})=0.99\) and specificity \(P(\bar{E}|D)=0.98\), and the probability of passing a test is \(P(E)=0.893\) then

\begin{itemize}
\item
  what is the probability that a brick chosen at random is defective \(P(D)\)? (R:0.1)
\item
  What is the probability that a brick that has passed the test is really defective? (R:0.022)
\item
  The probability that a brick is not defective \textbf{and} that it does not pass the test (R:0.009)
\item
  Are \(D\) and \(\bar{E}\) statistical independent? (no)
\end{itemize}

\hypertarget{random-variables}{%
\section{Random variables}\label{random-variables}}

\hypertarget{exercise-1-3}{%
\subsubsection{Exercise 1}\label{exercise-1-3}}

Given the probability mass function

\begin{longtable}[]{@{}cc@{}}
\toprule
\(x\) & \(f(x)=P(X=x)\) \\
\midrule
\endhead
10 & 0.1 \\
12 & 0.3 \\
14 & 0.25 \\
15 & 0.15 \\
17 & ? \\
20 & 0.15 \\
\bottomrule
\end{longtable}

\begin{itemize}
\tightlist
\item
  what is its expected value and standard deviation? (R: 14.2; 2.95)
\end{itemize}

\hypertarget{exercise-2-3}{%
\subsubsection{Exercise 2}\label{exercise-2-3}}

Given the probability distribution for a discrete variable \(X\)

\[
    F(x)= 
\begin{cases}
0, & x < -1 \\
0.2,& x \in [-1,0)\\
0.35,& x \in [0,1)\\
0.45,& x \in [1,2)\\
1,& x \geq 2\\
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  find \(f(x)\)
\item
  find \(E(X)\) and \(V(X)\) (R:1; 1.5)
\item
  what is the expected value and variance of \(Y=2X+3\) (R: 6)
\item
  what is the median and the first and third quartiles of \(X\)? (R:2,0,2)
\end{itemize}

\hypertarget{exercise-3-1}{%
\subsubsection{Exercise 3}\label{exercise-3-1}}

We are testing a system to transmit digital pictures. We first consider the experiment of sending \(3\) pixels and having as \textbf{possible} outcomes events such like \((0,1,1)\). This is the event of receiving the first pixel with no error, the second with error and third with error.

\begin{itemize}
\item
  List in one column the sample space of the random experiment.
\item
  In the a second column assign the random variable that counts the number of errors transmitted for each outcome
\end{itemize}

Consider that we have a totally noisy channel, that is any outcome of three pixels is equally likely.

\begin{itemize}
\item
  What is the probability of receiving \(0\), \(1\), \(2\), or \(3\) errors in the transmission of \(3\) pixels? (R: 1/8; 3/8; 3/8; 1/8)
\item
  Sketch the probability mass function for the number of errors
\item
  What is the expected value for the number of errors? (R:1.5)
\item
  What is its variance? (R: 0.75)
\item
  Sketch the probability distribution
\item
  What is the probability of transmitting at least 1 error? (R:7/8)
\end{itemize}

\hypertarget{exercise-4-1}{%
\subsubsection{Exercise 4}\label{exercise-4-1}}

\begin{itemize}
\tightlist
\item
  for the probability density
\end{itemize}

\[
    f(x)= 
\begin{cases}
    \frac{1}{100},& \text{if } x\in (0,100)\\
    0,& otherwise 
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  compute the mean (R:50)
\item
  compute variance using \(E(X^2)=V(X)+E(X)^2\) (R:100\^{}2/12)
\item
  compute \(P(\mu-\sigma\leq X \leq \mu+\sigma)\) (R: 0.57)
\item
  What are the first and third quartiles? (R: 25; 75)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-80-1.pdf}

\hypertarget{exercise-5-1}{%
\subsubsection{Exercise 5}\label{exercise-5-1}}

Given
\[
    f(x)= 
\begin{cases}
0, & x < 0 \\
ax, & x \in [0,3] \\
b, & x \in (3,5) \\
\frac{b}{3}(8-x),& x \in [5,8]\\
0, & x > 8 \\
\end{cases}
\]

\begin{itemize}
\item
  What are the values of \(a\) and \(b\) such that \(f(x)\) is a continous probability density function? (R: 1/15; 1/5)
\item
  what is the mean of \(X\)? (R:4)
\end{itemize}

\hypertarget{exercise-6}{%
\subsubsection{Exercise 6}\label{exercise-6}}

For the probability density

\[
    f(x)= 
\begin{cases}
    \lambda e^{-\lambda x},& \text{if } x \geq 0\\
    0,& otherwise 
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  Confirm that this is a probability density
\item
  Compute the mean (R: 1/\(\lambda\))
\item
  Compute the expected value of \(X^2\) (R: 2/\(\lambda^2\))
\item
  Compute variance (R: 1/\(\lambda^2\))
\item
  Find the probability distribution \(F(a)\) (R: \(1-exp(-\lambda a)\))
\item
  Find the median (R: \(\log{2}\)/\(\lambda\))
\end{itemize}

\hypertarget{exercise-7}{%
\subsubsection{Exercise 7}\label{exercise-7}}

Given the cumulative distribution for a random variable X

\[
    F(x)= 
\begin{cases}
0, & x  < -1 \\
\frac{1}{80}(17+16x-x^2),& x \in [-1,7)\\
1,& x \geq 7\\
\end{cases}
\]

compute:

\begin{itemize}
\tightlist
\item
  \(P(X>0)\) (R:63/80)
\item
  \(E(X)\) (R:1.93)
\item
  \(P(X>0|X<2)\) (R:28/45)
\end{itemize}

\hypertarget{probability-models}{%
\section{Probability Models}\label{probability-models}}

\hypertarget{exercise-1-4}{%
\subsubsection{Exercise 1}\label{exercise-1-4}}

In a population, the probability that a baby boy is born is \(p=0.51\). Consider a family of 4 children

\begin{itemize}
\tightlist
\item
  What is the probability that a family has only one boy?(R: 0.240)
\item
  What is the probability that a family has only one girl?(R: 0.259)
\item
  What is the probability that a family has only one boy or only one girl?(R: 0.4999)
\item
  What is the probability that the family has at least two boys?(R: 0.7023)
\item
  What is the number of children that a family should have such that the probability of having at least one girl is more than \(0.75\)?(R:\(n=3>\log(0.25)/\log(0.51)\))
\end{itemize}

\hypertarget{exercise-2-4}{%
\subsubsection{Exercise 2}\label{exercise-2-4}}

A search engine fails to retrieve information with a probability \(0.1\)

\begin{itemize}
\item
  If we system receives \(50\) search requests, what is the probability that the system fails to answer three of them?(R:0.1385651)
\item
  What is the probability that the engine successfully completes \(15\) searches before the first failure?(R:0.020)
\item
  We consider that a search engine works sufficiently well when it is able to find information for moe than \(10\) requests for every \(2\) failures. What is the probability that in a reliability trial our search engine is satisfactory?(R: 0.697)
\end{itemize}

\hypertarget{exercise-3-2}{%
\subsubsection{Exercise 3}\label{exercise-3-2}}

The average number of radioactive particles hitting a Geiger counter in a nuclear energy plant under control is \(2.3\) per minute.

\begin{itemize}
\item
  What is the probability of counting exactly \(2\) particles in a minute? (R:0.265)
\item
  What is the probability of detecting exactly \(10\) particles in \(5\) minute? (R:0.112)
\item
  What is the probability of at least one count in two minutes? (R:0.9899)
\item
  What is the probability of having to wait less than \(1\) second to detect a radioactive particle, after we switch on the detector? (R:0.037)
\item
  We suspect that a nuclear plant has a radioactive leak if we wait less than \(1\) second to detect a radioactive particle, after we switch on the detector. What is the probability that when we visit in \(5\) plants that are under control, we suspect that at least one has a leak? (R:0.1744).
\end{itemize}

\hypertarget{exercise-4-2}{%
\subsubsection{Exercise 4}\label{exercise-4-2}}

\begin{itemize}
\item
  What is the probability that a man's height is at least
  \(165\)cm if the population mean is \(175\)cm y the standard deviation is \(10\)cm? (R:0.841)
\item
  What is the probability that a man's height is between
  \(165\)cm and \(185\)cm? (R:0.682)
\item
  What is the height that defines the \(5\%\) of the smallest men? (R:158.55)
\end{itemize}

\hypertarget{sampling-and-central-limit-theorem}{%
\section{Sampling and Central Limit Theorem}\label{sampling-and-central-limit-theorem}}

\hypertarget{exercise-1-5}{%
\subsubsection{Exercise 1}\label{exercise-1-5}}

A battery model charges up to \(75\%\) of its capacity within an hour with a standard deviation of \(15\%\).

\begin{itemize}
\item
  If we charge \(25\) batteries, what is the probability that the difference in charge between the sample average and the mean charge is at most \(5\%\)? (R:0.9044)
\item
  If we charge \(100\), what is that probability? (R:0.9991)
\item
  If, instead we only charge \(9\) batteries, which value \(c\) is surpassed by the sample mean with probability of \(0.015\)? (R:85.850)
\end{itemize}

\hypertarget{exercise-2-5}{%
\subsubsection{Exercise 2}\label{exercise-2-5}}

An electronic component is needed for the correct functioning of a telescope. It needs to be replaced immediately when it wears out.

The mean life of the component (\(\mu\)) is \(100\) hours and its standard deviation \(\sigma\) is \(30\) hours.

\begin{itemize}
\item
  what is the probability that the average of the mean life of \(50\) components is within \(1\) hour from the mean life of a single component?
\item
  How many components do we need such that the telescope is operational \(2750\) consecutive hours with \(0.95\) probability?
\end{itemize}

\hypertarget{exercise-3-3}{%
\subsubsection{Exercise 3}\label{exercise-3-3}}

An automated machine fills test tubes with biological samples with mean \(\mu=130\)mg and a standard deviation of \(\sigma=5\)mg.

\begin{itemize}
\item
  for a random sample of size \(50\). What is the probability that
  the sample mean (average) is between \(128\) and \(132\)gr?
\item
  what should be the size of the sample (\(n\)) such that the sample mean \(\bar{X}\) is higher than \(131\)gr with a probability less or equal than \(0.025\)?
\end{itemize}

\hypertarget{exercise-4-3}{%
\subsubsection{Exercise 4}\label{exercise-4-3}}

In the Caribbean, there appears to be an average of \(6\) hurricanes per year. Considering that hurricane formation is a Poisson process, meteorologists plan to estimate the mean time between the formation of two hurricanes. They plan to collect a sample of size \(36\) for the times between two hurricanes.

\begin{itemize}
\item
  What is the probability that their sample average is between \(45\) and \(60\) days?
\item
  Which should be the sample size such that they have a probability of \(0.025\) that the sample mean is greater than \(70\) days?
\end{itemize}

\hypertarget{exercise-5-2}{%
\subsubsection{Exercise 5}\label{exercise-5-2}}

The probability that a particular mutation is found in the population is \(0.4\). If we test \(2000\) people for the mutation:

\begin{itemize}
\tightlist
\item
  What is the probability that the total number of people with the mutation is between \(791\) and \(809\)?
\end{itemize}

hint: Use the CLT with a sample of \(2000\) Bernoulli trials. This is known as the normal approximation of the binomial distribution.

\hypertarget{point-estimators}{%
\section{Point Estimators}\label{point-estimators}}

\hypertarget{exercise-1-6}{%
\subsubsection{Exercise 1}\label{exercise-1-6}}

Consider the probability model

\[
    f(x)= 
\begin{cases}
    1/2-a,& \text{if } x=-1 \\ 
    1/2,& \text{if } x=0\\
    a,& 1 \text{if } x=1\\ 
\end{cases}
\]

where \(a\) is a parameter.

Compute the mean and variance of the statistic: \[T=\frac{\bar{X}}{2}+\frac{1}{4}\]

where \(\bar{X}=\frac{1}{N}\sum_{i=1}^N X_i\)

\begin{itemize}
\item
  is \(T\) a biased estimator of \(a\)?
\item
  is \(T\) consistent? i.e.~\(V(T) \rightarrow 0\) when \(N\rightarrow \infty\)
\end{itemize}

\hypertarget{exercise-2-6}{%
\subsubsection{Exercise 2}\label{exercise-2-6}}

\begin{itemize}
\tightlist
\item
  Is \(\bar{X}^2=(\frac{1}{N}\sum_{i=1}^N X_i)^2\) an unbiased estimator of \(E(X)^2\)?
\end{itemize}

\hypertarget{maximum-likelihood}{%
\section{Maximum likelihood}\label{maximum-likelihood}}

\hypertarget{exercise-1-7}{%
\subsubsection{Exercise 1}\label{exercise-1-7}}

For a random variable with a binomial probability function

\[f(x; p)=\binom n x p^x(1-p)^{n-x}\]

\begin{itemize}
\item
  What is the maximum-likelihood estimator of \(p\) for a sample of size \(1\) of this random variable?
\item
  In \textbf{one} exam of \(100\) students we observed \(x_1=68\) students that passed the exam. What is the estimate of the \(p\)?
\end{itemize}

\hypertarget{exercise-2-7}{%
\subsubsection{Exercise 2}\label{exercise-2-7}}

Take a random variable with the following probability density function

\[
f(x)=
\begin{cases}
    (1+\theta)x^\theta,& \text{if } x\in (0,1)\\
    0,&  x\notin (0,1)
\end{cases}
\]

\begin{itemize}
\item
  What is the maximum likelihood estimate for \(\theta\)?
\item
  If we take a \(5\)-sample with observations
  \(x_1 = 0.92; \qquad x_2 = 0.79; \qquad x_3 = 0.90; \qquad x_4 = 0.65; \qquad x_5 = 0.86\)
\end{itemize}

What is the estimated value of the parameter \(\theta\)?

\hypertarget{exercise-3-4}{%
\subsubsection{Exercise 3}\label{exercise-3-4}}

Take a random variable with the following probability density function

\[
    f(x)= 
\begin{cases}
    \lambda e^{-\lambda x},& \text{if } 0 \leq\\
    0,& otherwise 
\end{cases}
\]

\begin{itemize}
\item
  What is the maximum likelihood estimate for \(\lambda\)?
\item
  If we take a \(5\)-sample with observations
  \(x_1 = 0.223 \qquad x_2 = 0.681; \qquad x_3 = 0.117; \qquad x_4 = 0.150; \qquad x_5 = 0.520\)
\end{itemize}

What is the estimated value of the parameter \(\lambda\)?

\hypertarget{method-of-moments}{%
\section{Method of moments}\label{method-of-moments}}

\hypertarget{exercise-1-8}{%
\subsubsection{Exercise 1}\label{exercise-1-8}}

What are the estimators of the following parametric models given by the method of moments?

\begin{longtable}[]{@{}lll@{}}
\toprule
Model & f(x) & E(X) \\
\midrule
\endhead
Bernoulli & \(p^x(1-p)^{1-x}\) & \(p\) \\
Binomial & \(\binom n x p^x(1-p)^{n-x}\) & \(np\) \\
Shifted geometric & \(p(1-p)^{x-1}\) & \(\frac{1}{p}\) \\
Negative Binomial & \(\binom {x+r-1} x p^r(1-p)^x\) & \(r\frac{1-p}{p}\) \\
Poisson & \(\frac{e^{-\lambda}\lambda^x}{x!}\) & \(\lambda\) \\
Exponential & \(\lambda e^{-\lambda x}\) & \(\frac{1}{\lambda}\) \\
Normal & \(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\) & \(\mu\) \\
\bottomrule
\end{longtable}

\hypertarget{exercise-2-8}{%
\subsubsection{Exercise 2}\label{exercise-2-8}}

Take a random variable with the following probability density function

\[
f(x)=
\begin{cases}
    (1+\theta)x^\theta,& \text{if } x\in (0,1)\\
    0,& x\notin (0,1)
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  Compute \(E(X)\) as a function of \(\theta\)
\item
  What is the estimate for \(\theta\) using the method of moments?
\item
  If we take a \(5\)-sample with observations
  \(x_1 = 0.92; \qquad x_2 = 0.79; \qquad x_3 = 0.90; \qquad x_4 = 0.65; \qquad x_5 = 0.86\)
\end{itemize}

What is the estimated value of the parameter \(\theta\)?

\hypertarget{exercise-3-5}{%
\subsubsection{Exercise 3}\label{exercise-3-5}}

Consider a discrete random variable \(X\) that follows a negative binomial distribution with probability mass function:

\[f(x) = \binom{x+r-1}{x}p^r(1-p)^x\]

Given that

\begin{itemize}
\tightlist
\item
  \(E(X)=\dfrac{r(1-p)}{p}\)
\item
  \(V(X) =\dfrac{r(1-p)}{p^2}\)
\end{itemize}

compute:

\begin{itemize}
\item
  An estimate for the parameter \(r\) and an estimate for the parameter \(p\) obtained from a random sample of size \(n\) using the method of moments.
\item
  The values of the estimates of \(r\) y \(p\) for the folowing random sample:
\end{itemize}

\[x_1 = 27; \qquad x_2 = 8; \qquad  x_3 = 22; \qquad  x_4 = 29; \qquad  x_5 = 19; \qquad  x_5 = 32\]

\hypertarget{confidence-intervals}{%
\section{Confidence intervals}\label{confidence-intervals}}

\hypertarget{exercise-1-9}{%
\subsubsection{Exercise 1}\label{exercise-1-9}}

In a scientific paper the authors report a \(95\%\) confidence interval of \((228, 232)\) for the natural frequency (Hz) of metallic beam. They useda sample of size \(25\) and considered that the measurements distributed normally.

\begin{itemize}
\item
  What is the mean and the standard deviation of the measurements?
\item
  Compute the \(99\%\) confidence interval.
\end{itemize}

hints:

\begin{itemize}
\item
  in R \(t_{0.025, 24}=\) qt(0.975, 24)\(\sim 2\)
\item
  in R \(t_{0.005, 24}=\)qt(0.995, 24)\(\sim 2.8\)
\end{itemize}

\hypertarget{exercise-2-9}{%
\subsubsection{Exercise 2}\label{exercise-2-9}}

compute \(95\%\) CI the mean of a normal variable with known variance \(\sigma^2=9\) and \(\bar{x}=22\), using a sample of size \(36\).

\hypertarget{exercise-3-6}{%
\subsubsection{Exercise 3}\label{exercise-3-6}}

This year, \(17\) from \(1000\) of patients with influenza developed complications.

\begin{itemize}
\item
  Compute the \(99\%\) confidence interval for the proportion of complications.
\item
  The previous year \(2\%\) showed complications. Can we say with \(99\%\) confidence that this year there is a significant drop in influenza complications?
\end{itemize}

\hypertarget{hypothesis-testing}{%
\section{Hypothesis testing}\label{hypothesis-testing}}

\hypertarget{exercise-1-10}{%
\subsubsection{Exercise 1}\label{exercise-1-10}}

Imagine we take a random sample of size \(n = 41\) of a normal random variable \(X\), and find that the sample average is \(10\) and the sample variance is \(1.5\).

\begin{itemize}
\tightlist
\item
  What is then the confidence interval for the mean of \(X\) at \(95\%\) confidence level?
\end{itemize}

Consider that \(t_{0.025,40}=\) qt(0.975, 40) \(\sim 2\).

\begin{itemize}
\item
  Test the hypothesis that the mean of \(X\) is \textbf{different} than \(10.5\), using a \(5\%\) significance threshold.
\item
  Write the code to calculate the P-value to test the hypothesis that the mean of \(\mu\) is \textbf{lower} than \(10.5\), using a \(5\%\) significance threshold.
\end{itemize}

Consider that the code for the T probability distribution with \(n-1\) degrees of freedom is pt(tobs, n-1).

\hypertarget{exercise-2-10}{%
\subsubsection{Exercise 2}\label{exercise-2-10}}

\(10\) gas condensates showed the following concentrations of mercury (in \(ng/ml\)):

\(23.3\), \(22.5\), \(21.9\), \(21.5\), \(19.9\), \(21.3\), \(21.7\), \(23.8\), \(22.6\), \(24.7\)

Assuming that the mercury concentration is distributed normally a across gas condensates, test the hypothesis that a condensate does not surpass the toxicity limit established at \(24 ng/ml\).

\hypertarget{exercise-3-7}{%
\subsubsection{Exercise 3}\label{exercise-3-7}}

The manufacturer of gene expression microarrays guarantees that at least \(97\%\) of the microarrys they produce have high quality signals. A customer receives a batch of \(200\) pieces and finds that \(8\) unperformed.

Should the costumer return the lot due to poor quality?

  \bibliography{book.bib,packages.bib}

\end{document}
