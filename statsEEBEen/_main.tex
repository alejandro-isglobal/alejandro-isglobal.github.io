% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={EEBE stats},
  pdfauthor={Alejandro Caceres},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{EEBE stats}
\author{Alejandro Caceres}
\date{2023-11-01}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{objective}{%
\chapter{Objective}\label{objective}}

This is the introduction course to the statistics of the EEBE (UPC).

Statistics is a \textbf{language} that allows you to face new problems, on which we have no solution, and where the \textbf{randomness} plays a crucial role.

In this course we will discuss the \textbf{fundamental concepts} of statistics.

\begin{itemize}
\item
  3 hours of \textbf{Theory} per week: we will explain the concepts, we will exercise.
\item
  6 hours of \textbf{Individual study} per week: notes of course notes and resources in Athena.
\item
  2 hours of problem solving with \textbf{R}: face-to-face sessions (practices).
\end{itemize}

Exam dates and additional study material can be found in \textbf{ATENEA metacurso}:

\includegraphics{./figures/notas.JPG}

Evaluation objectives:

\textbf{Q1} (10\%): Test in computer Duration 2h on the indicated dates.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Basic command knowledge (practices)
\item
  Ability to calculate descriptive statistics and graphics, in specific situations (theory/practice)
\item
  Knowledge about linear regression (practices)
\end{enumerate}

\textbf{EP1} (25\%): Written test (2-3 problems)

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Capacity to interpret statements in probability formulas (theory).
\item
  Knowledge of the basic tools to solve problems of joint probability and conditional probability (theory).
\item
  Mathematical knowledge of probability functions to calculate its basic properties (theory).
\end{enumerate}

\textbf{Q2} (10\%): Test in computer Duration 2h on the indicated dates

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Capacity to identify probability models in concrete problems (theory/practice).
\item
  Use of R functions to calculate probabilities of probabilistic models (practice/theory)
\item
  Identification of a sampling statistic and its properties (theory/practice)
\item
  Knowledge of how to calculate the probability of sampling statistics (theory/practice)
\item
  Use of R commands to calculate probabilities and make random sampling simulations (practice)
\end{enumerate}

\textbf{EP2} (40\%): Written test (2-3 problems)

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Mathematical capacity to determine specific estimators of probability models.
\item
  Knowledge of the properties of specific estimators.
\item
  Knowledge of confidence intervals and their properties (theory).
\item
  Ability to identify the type of confidence interval in a specific problem (theory).
\item
  Knowledge
  of hypothesis types to be used in a specific problems (theory).
  f.Use of R commands to solve confidence intervals and hypothesis tests (practice).
\end{enumerate}

\textbf{CG} (5\%): Written test (2 questions about a text)

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Written expression capacity on a subject related to statistics.
\end{enumerate}

Coordinators:

\begin{itemize}
\tightlist
\item
  Luis Mujica (\href{mailto:Luis.eduardo.mujica@upc.edu}{\nolinkurl{Luis.eduardo.mujica@upc.edu}})
\item
  Pablo Buenestado (\href{mailto:Pablo.buenetado@upc.edu}{\nolinkurl{Pablo.buenetado@upc.edu}})
\end{itemize}

\hypertarget{recommended-reading}{%
\section{Recommended reading}\label{recommended-reading}}

\begin{itemize}
\item
  Class notes are our section will be accessible in Athena in PDF and HTML.
\item
  Douglas C. Montgomery and George C. Runger. ``Apply Statistics and Probability for Engineers'' 4th Edition. Wiley 2007.
\end{itemize}

\hypertarget{data-description}{%
\chapter{Data description}\label{data-description}}

In this chapter, we will introduce tools for describing data.

We will do so using tables, figures, and descriptive statistics of central tendency and dispersion.

We will also introduce key concepts in statistics such as randomized experiments, observations, outcomes, and absolute and relative frequencies.

\hypertarget{scientific-method}{%
\section{Scientific method}\label{scientific-method}}

One of the goals of the scientific method is to provide a framework for solving problems that arise in the study of natural phenomena or in the design of new technologies.

Modern humans have developed a \textbf{method} over thousands of years that is still in development.

The method has three main human activities:

\begin{itemize}
\tightlist
\item
  \emph{Observation} characterized by the acquisition of \textbf{data}
\item
  \emph{Reason} characterized by the development of mathematical \textbf{models}
\item
  \emph{Action} characterized by the development of new \textbf{experiments} (technology)
\end{itemize}

Their complex interaction and results are the basis of \emph{scientific activity}.

\includegraphics{./figures/stats.JPG}

\hypertarget{statistics}{%
\section{Statistics}\label{statistics}}

Statistics deals with the interaction between \emph{models} and \emph{data} (the bottom part of the figure).

The statistical questions are:

\begin{itemize}
\tightlist
\item
  What is the best model for my data (inference)?
\item
  What are the data that a certain model (prediction) would produce?
\end{itemize}

\hypertarget{data}{%
\section{Data}\label{data}}

The data is presented in the form of observations.

An \textbf{Observation} or \textbf{Realization} is the acquisition of a number or characteristic of an experiment.

For example, let's take the series of numbers produced by repeating an experiment (1: success, 0: failure).

\ldots{} 1 0 0 1 0 \textbf{1} 0 1 1 \ldots{}

The number in bold is \textbf{an observation} in a repeat of the experiment

An \textbf{outcome} is a \textbf{possible} observation that is the result of an experiment.

\textbf{1} is one result, \textbf{0} is the other result of the experiment.

Remember that the observation is \textbf{concrete} is the number you get one day in the laboratory. The \textbf{abstract} result is one of the characteristics of the type of experiment you are running.

\hypertarget{result-types}{%
\section{Result types}\label{result-types}}

In statistics we are mainly interested in two types of results.

\begin{itemize}
\item
  \textbf{Categorical}: If the result of an experiment is a quality. They can be nominal (binary: yes, no; multiple: colors) or ordinal when the qualities can be ranked (severity of a disease).
\item
  \textbf{Numeric}: If the result of an experiment is a number. The number can be discrete (number of emails received in an hour, number of leukocytes in the blood) or continuous (battery charge status, engine temperature).
\end{itemize}

\hypertarget{random-experiments}{%
\section{Random experiments}\label{random-experiments}}

It can be said that the subject of study of statistics is random experiments, the means by which we produce data.

\textbf{Definition:}

A \textbf{random experiment} is an experiment that gives different results when repeated in the same way.

Randomized experiments are of different types, depending on how they are conducted:

\begin{itemize}
\tightlist
\item
  on the same object (person): temperature, sugar levels.
  different objects but of the same size: the weight of an animal.
\item
  about events: the number of hurricanes per year.
\end{itemize}

\hypertarget{absolute-frequencies}{%
\section{Absolute frequencies}\label{absolute-frequencies}}

When we repeat a randomized experiment with \textbf{categorical} results, we record a list of results.

We summarize observations by counting how many times we saw a particular result.

\textbf{Absolute frecuency}:

\[ n_i \]

is the number of times we observe the result \(i\).

\newpage

\textbf{Example (leukocytes)}

Let's take a leukocyte from \textbf{a} donor and write down its type. Let's repeat the experiment \(N=119\) times.

\begin{verbatim}
(T cell, T cell, Neutrophil, ..., B cell)
\end{verbatim}

The second \textbf{T cell} in bold is the second observation. The last \textbf{B cell} is observation number 119.

We can list the \textbf{results} (categories) in a \textbf{frequency table}:

\begin{verbatim}
##      outcome ni
## 1     T Cell 34
## 2     B cell 50
## 3   basophil 20
## 4   Monocyte  5
## 5 Neutrophil 10
\end{verbatim}

From the table, we can say that, for example, \(n_1=34\) is the total number of T cells observed in the repeat experiment. We also note that the total number of repetitions \(N=\sum_i n_i = 119\).

\hypertarget{relative-frequencies}{%
\section{Relative frequencies}\label{relative-frequencies}}

We can also summarize observations by calculating the \textbf{proportion} of how many times we saw a particular result.

\[ f_i = n_i /N\] where \(N\) is the total number of observations

In our example, \(n_1=34\) T cells were recorded, so we asked about the proportion of T cells out of the total \(119\). We can add these proportions \(f_i\) in the frequency table.

\begin{verbatim}
##      outcome ni         fi
## 1     T Cell 34 0.28571429
## 2     B cell 50 0.42016807
## 3   basophil 20 0.16806723
## 4   Monocyte  5 0.04201681
## 5 Neutrophil 10 0.08403361
\end{verbatim}

Relative frequencies are \textbf{fundamental} in statistics. They give the proportion of one result in relation to the other results. Later we will understand them as the observations of probabilities.

For absolute and relative frequencies we have the properties

\begin{itemize}
\tightlist
\item
  \(\sum_{i=1..M} n_i = N\)
\item
  \(\sum_{i=1..M} f_i = 1\)
\end{itemize}

where \(M\) is the number of results.

\hypertarget{bar-chart}{%
\section{Bar chart}\label{bar-chart}}

When we have a lot of results and want to see which ones are most likely, we can use a bar chart that is a number of \(n_i\) Vs the results.

\includegraphics{_main_files/figure-latex/unnamed-chunk-3-1.pdf}

\hypertarget{pie-chart-pie}{%
\section{Pie chart (pie)}\label{pie-chart-pie}}

We can also visualize the relative frequencies with a pie chart.

The area of the circle represents 100\% of the observations (proportion = 1) and the sections the relative frequencies of each result.

\includegraphics{_main_files/figure-latex/unnamed-chunk-4-1.pdf}

\hypertarget{ordinal-categorical-variables}{%
\section{Ordinal categorical variables}\label{ordinal-categorical-variables}}

The leukocyte type in the above examples is a \textbf{categorical} nominal variable. Each observation belongs to a category (quality). The categories do not always have a certain order .

Sometimes \textbf{categorical} variables can be \textbf{sorted} when they meet a natural ranking. This allows you to compute \textbf{cumulative frequencies}.

\newpage

\textbf{Example (Misophonia)}

This is a clinical study on 123 patients who were examined for their degree of misophonia. Misophnia is uncontrolled anxiety/anger produced by certain sounds .

Each patient was evaluated with a questionnaire (AMISO) and they were classified into 4 different groups according to severity.

The results of the study are

\begin{verbatim}
##   [1] 4 2 0 3 0 0 2 3 0 3 0 2 2 0 2 0 0 3 3 0 3 3 2 0 0 0 4 2 2 0 2 0 0 0 3 0 2
##  [38] 3 2 2 0 2 3 0 0 2 2 3 3 0 0 4 3 3 2 0 2 0 0 0 2 2 0 0 2 3 0 1 3 2 4 3 2 3
##  [75] 0 2 3 2 4 1 2 0 2 0 2 0 2 2 4 3 0 3 0 0 0 2 2 1 3 0 0 3 2 1 3 0 4 4 2 3 3
## [112] 3 0 3 2 1 2 3 3 4 2 3 2
\end{verbatim}

Each observation is the result of a randomized experiment: measurement of the level of misophonia in a patient. This data series can be summarized in terms of the results in the frequency table

\begin{verbatim}
##   outcome ni         fi
## 1       0 41 0.33333333
## 2       1  5 0.04065041
## 3       2 37 0.30081301
## 4       3 31 0.25203252
## 5       4  9 0.07317073
\end{verbatim}

\hypertarget{accumulated-absolute-and-relative-frequencies}{%
\section{Accumulated absolute and relative frequencies}\label{accumulated-absolute-and-relative-frequencies}}

Misophonia severity is \textbf{categorical} \textbf{ordinal} because its results can be ordered relative to its degree.

When the results can be ordered, it is useful to ask how many observations were obtained up to a given result. We call this number the \textbf{absolute cumulative frequency} up to the result \(i\):
\[N_i =\sum_{k= 1..i } n_k\]
It is also useful for calculating the \textbf{proportion} of observations up to a given result.

\[F_i =\sum_{k= 1..i } f_k\]

We can add these frequencies in the \textbf{frequency table}

\begin{verbatim}
##   outcome ni         fi  Ni        Fi
## 0       0 41 0.33333333  41 0.3333333
## 1       1  5 0.04065041  46 0.3739837
## 2       2 37 0.30081301  83 0.6747967
## 3       3 31 0.25203252 114 0.9268293
## 4       4  9 0.07317073 123 1.0000000
\end{verbatim}

Therefore, \textbf{67\%} of patients had misophonia up to severity \textbf{2} and \textbf{37\%} of patients had severity less than or equal to \textbf{1}.

\hypertarget{cumulative-frequency-graph}{%
\section{Cumulative frequency graph}\label{cumulative-frequency-graph}}

\(F_i\) is an important quantity because it allows us to define the accumulation of probabilities down to intermediate levels.

The probability of an intermediate level \(x\) (\(i\leq x< i+1\)) is just the accumulation up to the lower level \(F_x = F_i\).

\(F_x\) is therefore a function on a \textbf{continuous} range of values. We can draw it with respect to the results.

\includegraphics{_main_files/figure-latex/unnamed-chunk-8-1.pdf}

Therefore, we can say that \textbf{67\%} of the patients had misophonia up to severity \(2.3\), although \(2.3\) is not an observed outcome.

\hypertarget{numeric-variables}{%
\section{Numeric variables}\label{numeric-variables}}

The result of a random experiment can produce a number. If the number is \textbf{discrete}, we can generate a frequency table, with absolute, relative, and cumulative frequencies, and illustrate them with bar, pie, and cumulative charts.

When the number is \textbf{continuous} the frequencies are not useful, we are most likely to observe or not observe a particular continuous number.

\textbf{Example (misophonia)}

The researchers wondered if the convexity of the jaw would affect the severity of misophonia. The scientific hypothesis is that the angle of convexity of the jaw can influence hearing and its sensitivity. These are the mandibular convexity results (degrees) for each patient:

\begin{verbatim}
##   [1]  7.97 18.23 12.27  7.81  9.81 13.50 19.30  7.70 12.30  7.90 12.60 19.00
##  [13]  7.27 14.00  5.40  8.00 11.20  7.75  7.94 16.69  7.62  7.02  7.00 19.20
##  [25]  7.96 14.70  7.24  7.80  7.90  4.70  4.40 14.00 14.40 16.00  1.40  9.76
##  [37]  7.90  7.90  7.40  6.30  7.76  7.30  7.00 11.23 16.00  7.90  7.29  6.91
##  [49]  7.10 13.40 11.60 -1.00  6.00  7.82  4.80 11.00  9.00 11.50 16.00 15.00
##  [61]  1.40 16.80  7.70 16.14  7.12 -1.00 17.00  9.26 18.70  3.40 21.30  7.50
##  [73]  6.03  7.50 19.00 19.01  8.10  7.80  6.10 15.26  7.95 18.00  4.60 15.00
##  [85]  7.50  8.00 16.80  8.54  7.00 18.30  7.80 16.00 14.00 12.30 11.40  8.50
##  [97]  7.00  7.96 17.60 10.00  3.50  6.70 17.00 20.26  6.64  1.80  7.02  2.46
## [109] 19.00 17.86  6.10  6.64 12.00  6.60  8.70 14.05  7.20 19.70  7.70  6.02
## [121]  2.50 19.00  6.80
\end{verbatim}

\hypertarget{transforming-continuous-data}{%
\section{Transforming continuous data}\label{transforming-continuous-data}}

Since continuous outcomes cannot be counted (informatively), we transform them into ordered categorical variables.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  First we cover the range of observations in regular intervals of the same size (bins)
\end{enumerate}

\begin{verbatim}
## [1] "[-1.02,3.46]" "(3.46,7.92]"  "(7.92,12.4]"  "(12.4,16.8]"  "(16.8,21.3]"
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Then we map each observation to its interval: creating a categorical variable \textbf{ordered}; in this case with 5 possible outcomes
\end{enumerate}

\begin{verbatim}
##   [1] "(7.92,12.4]"  "(16.8,21.3]"  "(7.92,12.4]"  "(3.46,7.92]"  "(7.92,12.4]" 
##   [6] "(12.4,16.8]"  "(16.8,21.3]"  "(3.46,7.92]"  "(7.92,12.4]"  "(3.46,7.92]" 
##  [11] "(12.4,16.8]"  "(16.8,21.3]"  "(3.46,7.92]"  "(12.4,16.8]"  "(3.46,7.92]" 
##  [16] "(7.92,12.4]"  "(7.92,12.4]"  "(3.46,7.92]"  "(7.92,12.4]"  "(12.4,16.8]" 
##  [21] "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(16.8,21.3]"  "(7.92,12.4]" 
##  [26] "(12.4,16.8]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]" 
##  [31] "(3.46,7.92]"  "(12.4,16.8]"  "(12.4,16.8]"  "(12.4,16.8]"  "[-1.02,3.46]"
##  [36] "(7.92,12.4]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]" 
##  [41] "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(7.92,12.4]"  "(12.4,16.8]" 
##  [46] "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(12.4,16.8]" 
##  [51] "(7.92,12.4]"  "[-1.02,3.46]" "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]" 
##  [56] "(7.92,12.4]"  "(7.92,12.4]"  "(7.92,12.4]"  "(12.4,16.8]"  "(12.4,16.8]" 
##  [61] "[-1.02,3.46]" "(12.4,16.8]"  "(3.46,7.92]"  "(12.4,16.8]"  "(3.46,7.92]" 
##  [66] "[-1.02,3.46]" "(16.8,21.3]"  "(7.92,12.4]"  "(16.8,21.3]"  "[-1.02,3.46]"
##  [71] "(16.8,21.3]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(16.8,21.3]" 
##  [76] "(16.8,21.3]"  "(7.92,12.4]"  "(3.46,7.92]"  "(3.46,7.92]"  "(12.4,16.8]" 
##  [81] "(7.92,12.4]"  "(16.8,21.3]"  "(3.46,7.92]"  "(12.4,16.8]"  "(3.46,7.92]" 
##  [86] "(7.92,12.4]"  "(12.4,16.8]"  "(7.92,12.4]"  "(3.46,7.92]"  "(16.8,21.3]" 
##  [91] "(3.46,7.92]"  "(12.4,16.8]"  "(12.4,16.8]"  "(7.92,12.4]"  "(7.92,12.4]" 
##  [96] "(7.92,12.4]"  "(3.46,7.92]"  "(7.92,12.4]"  "(16.8,21.3]"  "(7.92,12.4]" 
## [101] "(3.46,7.92]"  "(3.46,7.92]"  "(16.8,21.3]"  "(16.8,21.3]"  "(3.46,7.92]" 
## [106] "[-1.02,3.46]" "(3.46,7.92]"  "[-1.02,3.46]" "(16.8,21.3]"  "(16.8,21.3]" 
## [111] "(3.46,7.92]"  "(3.46,7.92]"  "(7.92,12.4]"  "(3.46,7.92]"  "(7.92,12.4]" 
## [116] "(12.4,16.8]"  "(3.46,7.92]"  "(16.8,21.3]"  "(3.46,7.92]"  "(3.46,7.92]" 
## [121] "[-1.02,3.46]" "(16.8,21.3]"  "(3.46,7.92]"
\end{verbatim}

Therefore, instead of saying that the first patient had an angle of convexity of \(7.97\), we say that his angle was between the interval (or \textbf{bin}) \((7.92,12.4]\).

No other patients had an angle of \(7.97\), but many had angles between \((7.92,12.4]\).

\hypertarget{frequency-table-for-a-continuous-variable}{%
\section{Frequency table for a continuous variable}\label{frequency-table-for-a-continuous-variable}}

For a given regular partition of the interval of results into intervals, we can produce a frequency table as before

\begin{verbatim}
##        outcome ni         fi  Ni         Fi
## 1 [-1.02,3.46]  8 0.06504065   8 0.06504065
## 2  (3.46,7.92] 51 0.41463415  59 0.47967480
## 3  (7.92,12.4] 26 0.21138211  85 0.69105691
## 4  (12.4,16.8] 20 0.16260163 105 0.85365854
## 5  (16.8,21.3] 18 0.14634146 123 1.00000000
\end{verbatim}

\hypertarget{histogram}{%
\section{Histogram}\label{histogram}}

The histogram is the graph of \(n_i\) or \(f_i\) Vs the results in intervals (bins). The histogram depends on the size of the bins .

\includegraphics{_main_files/figure-latex/unnamed-chunk-13-1.pdf}

\newpage

This is a histogram with 20 bins .

\includegraphics{_main_files/figure-latex/unnamed-chunk-14-1.pdf}

We see that most people have angles within \((7, 8]\)

\hypertarget{cumulative-frequency-graph-1}{%
\section{Cumulative frequency graph}\label{cumulative-frequency-graph-1}}

We can also plot \(F_x\) against the results. Since \(F_x\) is of continuous range, we can order the observations (\(x_1 <... x_j < x_{j+1} < x_n\)) and therefore

\[F_x = \frac{k}{ n}\]

for \(x_{k} \leq x < x_{k+ 1}\) .

\(F_x\) is known as the \textbf{distribution} of the data. \(F_x\) does not depend on the size of the bin. However, its \textbf{resolution} depends on the amount of data.

\includegraphics{_main_files/figure-latex/unnamed-chunk-15-1.pdf}

\hypertarget{summary-statistics}{%
\section{Summary Statistics}\label{summary-statistics}}

Summary statistics are numbers calculated from the data that tell us important characteristics of the numerical variables (discrete or continuous).

For example, we have statistics that describe extreme values:

\begin{itemize}
\tightlist
\item
  \textbf{minimum}: the minimum result observed
\item
  \textbf{maximum}: the maximum result observed
\end{itemize}

\hypertarget{average-sample-mean}{%
\section{Average (sample mean)}\label{average-sample-mean}}

An important statistic that describes the central value of the results (where to expect most observations) is the \textbf{average}

\[\bar{x}=\frac{1}{N} \sum_{j= 1..N } x_j \]

where \(x_j\) is the \textbf{observation} \(j\) out of a total of \(N\).

\textbf{Example (Misophonia)}

The average convexity can be calculated directly from the \textbf{observations} in the usual way

\(\bar{x}= \frac{1}{ N}\sum_j x_j\)

\(= \frac{1}{ N}( 7.97 + 18.23 + 12.27... + 6.80) = 10.19894\)

For \textbf{categorically ordered} variables, we can \textbf{also} use the relative frequencies to calculate the average

\(\bar{x}=\frac{1}{ N}\sum_{i=1...N} x_j =\frac{1}{N}\sum_{i=1...M} x_i n_ {i}=\)
\[\sum_{i=1...M} x_i f_{i}\]

where we went from adding \(N\) \textbf{observations} to adding \(M\) \textbf{results}.

The form \(\bar{x}= \sum_{i = 1...M} x_i f_i\) shows that the average is the \textbf{center of gravity} of the results. As if each result had a mass density given by \(f_i\).

\textbf{Example (Misophonia)}

The average \textbf{severity} of misophonia in the study can be calculated from the relative frequencies of the \textbf{outcomes}

\begin{verbatim}
##   outcome ni         fi
## 1       0 41 0.33333333
## 2       1  5 0.04065041
## 3       2 37 0.30081301
## 4       3 31 0.25203252
## 5       4  9 0.07317073
\end{verbatim}

\(\bar{x}=0*f_ {0}+ 1*f_{1}+2*f_{2}+3*f_{3}+4*f_{4}=1.691057\)

The average is also the center of gravity for continuous variables. That is the point where the relative frequencies balance.

\includegraphics{_main_files/figure-latex/unnamed-chunk-17-1.pdf}

\hypertarget{median}{%
\section{Median}\label{median}}

Another measure of centrality is the median. The median \(x_m\), or \(q_{0.5}\), is the value below which we find half of the observations. When we order the observations \(x_1 <... x_j < x_{j+1} < x_N\), we count them until we find half of them. Therefore, \(x_m\) is the observation such that \(m\) satisfies

\[\sum_{i\leq m} 1 = \frac{N}{2}\]
\textbf{Example (Misophonia)}

If we order the angles of convexity, we see that \(62\) observations (individuals) (\(N/2 \sim 123/2\)) are below \(7.96\). The \textbf{median convexity} is therefore \(q_{0.5}= x_{62}=7.96\)

\begin{verbatim}
##  [1] -1.00 -1.00  1.40  1.40  1.80  2.46  2.50  3.40  3.50  4.40  4.60  4.70
## [13]  4.80  5.40  6.00  6.02  6.03  6.10  6.10  6.30  6.60  6.64  6.64  6.70
## [25]  6.80  6.91  7.00  7.00  7.00  7.00  7.02  7.02  7.10  7.12  7.20  7.24
## [37]  7.27  7.29  7.30  7.40  7.50  7.50  7.50  7.62  7.70  7.70  7.70  7.75
## [49]  7.76  7.80  7.80  7.80  7.81  7.82  7.90  7.90  7.90  7.90  7.90  7.94
## [61]  7.95  7.96
\end{verbatim}

\begin{verbatim}
##  [1]  7.96  7.97  8.00  8.00  8.10  8.50  8.54  8.70  9.00  9.26  9.76  9.81
## [13] 10.00 11.00 11.20 11.23 11.40 11.50 11.60 12.00 12.27 12.30 12.30 12.60
## [25] 13.40 13.50 14.00 14.00 14.00 14.05 14.40 14.70 15.00 15.00 15.26 16.00
## [37] 16.00 16.00 16.00 16.14 16.69 16.80 16.80 17.00 17.00 17.60 17.86 18.00
## [49] 18.23 18.30 18.70 19.00 19.00 19.00 19.00 19.01 19.20 19.30 19.70 20.26
## [61] 21.30
\end{verbatim}

We cut the data at

\begin{verbatim}
## [1] 7.96
\end{verbatim}

to split them in half.

\newpage

In terms of frequencies, \(q_{0.5}\) makes the cumulative frequency \(F_x\) equal to \(0.5\)

\[\sum_{i = 0, ... m} f_i =F_{q_{0.5}}=0.5\]
that is

\[q_{ 0.5}= F^{-1}(0.5)\]

This last equation means that, in the distribution graph, the median \(q_{ 0.5}\) is the value of \(x\) at which we have climbed half of the total height of \(F\).

\includegraphics{_main_files/figure-latex/unnamed-chunk-21-1.pdf}

The mean and median are not always the same.

\includegraphics{_main_files/figure-latex/unnamed-chunk-22-1.pdf}

\hypertarget{dispersion}{%
\section{Dispersion}\label{dispersion}}

Other important summary statistics for observations are the \textbf{spread} statistics.

Many experiments may share their mean, but differ in how \textbf{sparse} the values are.

The dispersion of the observations is a measure of the \textbf{noise}.

\includegraphics{_main_files/figure-latex/unnamed-chunk-23-1.pdf} \includegraphics{_main_files/figure-latex/unnamed-chunk-23-2.pdf}

\hypertarget{sample-variance}{%
\section{Sample variance}\label{sample-variance}}

The dispersion about the mean is measured by the sample variance

\[s^2=\frac{ 1}{ N-1} \sum_{j=1..N} ( x_j -\bar{x})^2\]

This number measures the average squared distance of the \textbf{observations} from the average. The reason for \(N-1\) will be explained when we talk about inference, when we study the spread of \(\bar{x}\), as well as the spread of the observations.

In terms of the frequencies of the variables that are \textbf{categorical and ordered}, we can \textbf{also} calculate the sample variance as

\[s^2=\frac{N}{N-1} \sum_{i=1... M} (x_i -\bar{x})^2 f_i\]

\(s^2\) can be considered as the \textbf{moment of inertia} of the observations.

The square root of the sample variance, \(s\), is called \textbf{standard deviation} of the sample.

\textbf{Example (Misophonia)}

The standard deviation of the angle of convexity is

\(s= [\frac{ 1}{123-1}((7.97-10.19894)^2+ (18.23-10.19894)^2\)
\[+ (12.27-10.19894 )^ 2 + ...)]^{1/2} = 5.086707\]

The jaw convexity deviates from its mean by \(5.086707\).

\hypertarget{interquartile-range-iqr}{%
\section{Interquartile range (IQR)}\label{interquartile-range-iqr}}

The spread of the data can also be measured with respect to the median using the \textbf{interquartile range}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  We define the \textbf{first} quartile as the value \(x_m\) that makes the cumulative frequency \(F_{q_{0.25}}\) equal to \(0.25\) (or the value of \(x\) where we have accumulated a quarter of the observations, or the value that splits the first quarter of the observations)
\end{enumerate}

\[q_{0.25}=F^{-1}(0.25)\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  We define the \textbf{third} quartile as the value \(x_m\) that makes the cumulative frequency \(F_{q_{0.75}}\) equal to \(0.75\) (or the value of \(x\) where we have accumulated three quarters of observations)
\end{enumerate}

\[q_{0.75}=F^{-1}(0.75)\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  The \textbf{interquartile range} (IQR) is \[IQR=q_{0.75} - q_{0.25}\]
\end{enumerate}

This is the distance between the third and first quartiles and captures the central \(50\%\) of the observations

\includegraphics{_main_files/figure-latex/unnamed-chunk-24-1.pdf}

\hypertarget{boxplot}{%
\section{Boxplot}\label{boxplot}}

The interquartile range, median, and \(5\%\) and \(95\%\) of the data can be displayed in a \textbf{box plot}.

In the boxplot, the values of the results are on the y-axis. The IQR is the box, the median is the middle line, and the whiskers mark the \(5\%\) and \(95\%\) of the data.

\includegraphics{_main_files/figure-latex/unnamed-chunk-25-1.pdf}

\hypertarget{questions}{%
\section{Questions}\label{questions}}

\textbf{1)} In the following boxplot, the first quartile and second quartile of the data are:

\textbf{\(\qquad\)a:} \((-1.00, 21.30)\); \textbf{\(\qquad\)b:} \((-1.00, 7.02)\); \textbf{\(\qquad\)c:} \((7.02, 7.96)\); \textbf{\(\qquad\)d:} \((7.02, 14.22)\)

\textbf{2)} The main disadvantage of a histogram is that:

\textbf{\(\qquad\) a :} Depends on the size of the bin ; \textbf{\(\qquad\)b :} Cannot be used for categorical variables;

\textbf{\(\qquad\) c :} Cannot be used when the bin size is small;
\textbf{\(\qquad\) d :} Used only for relative frequencies;

\textbf{3)} If the relative cumulative frequencies of a random experiment with outcomes \(\{1,2,3,4\}\) are: \(F(1)=0.15, \qquad F(2)=0.60, \qquad F(3)=0.85, \qquad F(4)=1\).

Then the relative frequency for the outcome \(3\) is

\textbf{\(\qquad\)a:} \(0.15\); \textbf{\(\qquad\)b:} \(0.85\); \textbf{\(\qquad\)c:} \(0.45\); \textbf{\(\qquad\)d:} \(0.25\)

\textbf{4)} In a sample of size \(10\) from a random experiment we obtained the following data:

\(8, \qquad 3, \qquad 3, \qquad 7, \qquad 3, \qquad 6, \qquad 5, \qquad 10, \qquad 3, \qquad 8\).

The first quartile of the data is:

\textbf{\(\qquad\)a:} \(3.5\); \textbf{\(\qquad\)b:} \(4\); \textbf{\(\qquad\)c:} \(5\); \textbf{\(\qquad\)d:} \(3\)

\textbf{5)} Imagine that we collect data for two quantities that are not mutually exclusive, for example, the gender and nationality of passengers on a flight. If we want to make a single pie chart for the data, which of these statements is true?

\textbf{\(\qquad\)a :} We can \textbf{only} make a nationality pie chart because it has more than two possible outcomes;

\textbf{\(\qquad\)b :} We can make a pie graph for a new variable marking gender \textbf{and} nationality;

\textbf{\(\qquad\)c :} We can make a pie chart for the variable sex \textbf{or} the variable nationality;

\textbf{\(\qquad\)d :} We can only choose \textbf{whether} to make a pie chart for gender \textbf{or} a pie chart for nationality.

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

\hypertarget{exercise-1}{%
\subsubsection{Exercise 1}\label{exercise-1}}

We have performed an experiment 8 times with the following results

\begin{verbatim}
## [1]  3  3 10  2  6 11  5  4
\end{verbatim}

Answer the following questions:

\begin{itemize}
\tightlist
\item
  Calculate the relative frequencies of each result.
\item
  Calculate the cumulative frequencies of each result.
\item
  What is the average of the observations?
\item
  What is the median?
\item
  What is the third quartile?
\item
  What is the first quartile?
\end{itemize}

\hypertarget{exercise-2}{%
\subsubsection{Exercise 2}\label{exercise-2}}

We have performed an experiment 10 times with the following results

\begin{verbatim}
##  [1] 2.875775 7.883051 4.089769 8.830174 9.404673 0.455565 5.281055 8.924190
##  [9] 5.514350 4.566147
\end{verbatim}

Consider 10 bins of size 1: {[}0,1{]}, (1,2{]} \ldots( 9,10).

Answer the following questions:

\begin{itemize}
\item
  Calculate the relative frequencies of each result and draw the histogram
\item
  Calculate the cumulative frequencies of each result and draw the cumulative graph.
\item
  Draw a box plot .
\end{itemize}

\hypertarget{probability}{%
\chapter{Probability}\label{probability}}

In this chapter we will introduce the concept of probability from relative frequencies.

We will define the events as the elements on which the probability is applied. Composite events will be defined using set algebra.

Then we will discuss the concept of conditional probability derived from the joint probability of two events.

\hypertarget{random-experiments-1}{%
\section{Random experiments}\label{random-experiments-1}}

Let's remember the basic objective of statistics. Statistics deals with data that is presented in the form of observations.

\begin{itemize}
\tightlist
\item
  An \textbf{observation} is the acquisition of a number or characteristic from an experiment
\end{itemize}

Observations are realizations of \textbf{results}.

\begin{itemize}
\tightlist
\item
  An \textbf{outcome} is a possible observation that is the result of an experiment.
\end{itemize}

When conducting experiments, we often get different results. The description of the variability of the results is one of the objectives of statistics.

\begin{itemize}
\tightlist
\item
  A \textbf{random experiment} is an experiment that gives different results when repeated in the same way.
\end{itemize}

The philosophical question behind it is how can we know something if every time we look at it it changes?

\hypertarget{measurement-probability}{%
\section{Measurement probability}\label{measurement-probability}}

We would like to have a measure for the outcome of a randomized experiment that tells us \textbf{how sure} we are of observing the outcome when we perform a \textbf{future} randomized experiment.

We will call this measure the probability of the outcome and assign values to it:

\begin{itemize}
\item
  0, when we are sure that the observation will \textbf{not} occur.
\item
  1, when we are sure that the observation will happen.
\end{itemize}

\hypertarget{classical-probability}{%
\section{Classical probability}\label{classical-probability}}

\textbf{As long as} a random experiment has \(M\) possible outcomes that are all \textbf{equally likely}, the probability of each \(i\) outcome is \[P_i =\frac{1}{ M}\].

Classical probability was defended by Laplace (1814).

Since every outcome is \textbf{equally likely} in this type of experiment, we declare complete ignorance and the best we can do is equally distribute the same probability for each outcome.

\begin{itemize}
\tightlist
\item
  We do not observe \(P_i\)
\item
  We deduce \(P_i\) from our ratio and we don't need to carry out any experiment to know it.
\end{itemize}

\textbf{Example (dice):}

What is the probability that we will get \(2\) on the roll of a die?

\(P_2=1/6=0.166666\).

\hypertarget{relative-frequencies-1}{%
\section{Relative frequencies}\label{relative-frequencies-1}}

What about random experiments whose possible outcomes are \textbf{not} equally likely?

How then can we define the probabilities of the outcomes?

\textbf{Example (random experiment)}

Imagine that we repeat a random experiment \(8\) times and obtain the following observations

8 4 12 7 10 7 9 12

\begin{itemize}
\tightlist
\item
  How sure are we of obtaining the result \(12\) in the following observation?
\end{itemize}

The frequency table is

\begin{verbatim}
##   outcome ni    fi
## 1       4  1 0.125
## 2       7  2 0.250
## 3       8  1 0.125
## 4       9  1 0.125
## 5      10  1 0.125
## 6      12  2 0.250
\end{verbatim}

\includegraphics{_main_files/figure-latex/unnamed-chunk-29-1.pdf}

The \textbf{relative frequency} \(f_i =\frac{ n_ i }{ N}\) seems like a reasonable probability measure because

\begin{itemize}
\tightlist
\item
  is a number between \(0\) and \(1\).
\item
  measures the proportion of the total number of observations that we observe of a particular result.
\end{itemize}

Since \(f_{12}=0.25\) then we would be one quarter sure, one out of every 4 observations, of getting \(12\).

\textbf{Question}: How good is \(f_i\) as a measure of certainty of the result \(i\)?

\textbf{Example (random experiment with more repetitions)}

Let's say we repeat the experiment 100,000 more times:

The frequency table is now

\begin{verbatim}
##    outcome    ni      fi
## 1        2  2807 0.02807
## 2        3  5607 0.05607
## 3        4  8435 0.08435
## 4        5 11070 0.11070
## 5        6 13940 0.13940
## 6        7 16613 0.16613
## 7        8 13806 0.13806
## 8        9 10962 0.10962
## 9       10  8402 0.08402
## 10      11  5581 0.05581
## 11      12  2777 0.02777
\end{verbatim}

and the barplot is

\includegraphics{_main_files/figure-latex/unnamed-chunk-31-1.pdf}

New results came out and \(f_{12}\) is now only \(0.027\), and so we are only \(\sim 3\%\) sure to get \(12\) in the next experiment. The probabilities measured by \(f_i\) change with \(N\).

\hypertarget{relative-frequencies-at-infinity}{%
\section{Relative frequencies at infinity}\label{relative-frequencies-at-infinity}}

A crucial observation is that if we measure the probabilities of \(f_i\) in increasing values of \(N\) they \textbf{converge}!

In this graph each vertical section gives the relative frequency of each observation. We see that after \(N=1000\) (\(log10(N)=3\)) the proportions hardly change with more \(N\).

\includegraphics{_main_files/figure-latex/unnamed-chunk-32-1.pdf}

We find that each of the relative frequencies \(f_i\) converges to a constant value

\[lim_{N\rightarrow \infty} f_i = P_i\]

\hypertarget{frequentist-probability}{%
\section{Frequentist probability}\label{frequentist-probability}}

We call \textbf{Probability} \(P_i\) the limit as \(N \rightarrow \infty\) of the \textbf{relative frequency} of observing the outcome \(i\) in a random experiment.

Defended by Venn (1876), the frequentist definition of probability is derived from (empirical) data/experience.

\begin{itemize}
\tightlist
\item
  We do not observe \(P_i\), we observe \(f_i\)
\item
  \textbf{We estimate} \(P_i\) with \(f_i\) (usually when \(N\) is large), we write: \[\hat {P_ i}= f_i\]
\end{itemize}

Similar to the relationship between \textbf{observation} and \textbf{result}, we have the relationship between \textbf{relative frequency} and \textbf{probability} as a concrete value of an abstract quantity.

\hypertarget{classical-and-frequentist-probabilities}{%
\section{Classical and frequentist probabilities}\label{classical-and-frequentist-probabilities}}

We have situations where classical probability can be used to find the limit of relative frequencies:

\begin{itemize}
\tightlist
\item
  If the results are \textbf{equally probable}, the classical probability gives us the limit:
\end{itemize}

\[P_i=lim_{N\rightarrow \infty} \frac{n_i}{N}=\frac{1}{M}\]

\begin{itemize}
\tightlist
\item
  If the results in which we are interested can be derived from other \textbf{equally probable} results. We will see more about this when we study probability models.
\end{itemize}

\textbf{Example (sum of two dice)}

Our previous example is based on the \textbf{sum of two dice}.
Although we perform the experiment many times, write down the results, and calculate the \textbf{relative frequencies}, we can know the exact value of probability.

This probability \textbf{follows} from the fact that the outcome of each die is \textbf{equally likely}. From this assumption, we can find that (Exercise 1)

\[
    P_i =
\begin{cases}
\frac{i-1}{36},& i \in \{2,3,4,5,6, 7\} \\
\frac{13-i}{36},& i \in \{8,9,10,11,12\} \\
\end{cases}
\]

The motivation of the frequentist definition is \textbf{empirical} (data) while that of the classical definition is \textbf{rational} (models). We often combine both approaches (inference and deduction) to find out the probabilities of our random experiment.

\includegraphics{./figures/prob.JPG}

\hypertarget{definition-of-probability}{%
\section{Definition of probability}\label{definition-of-probability}}

A probability is a number that is assigned to each possible outcome of a random experiment and satisfies the following properties or \textbf{axioms}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  when the results \(E_1\) and \(E_2\) are mutually exclusive; that is, only one of them can occur, so the probability of observing \(E_1\) \textbf{or} \(E_1\), written as \(E_1\cup E_2\), is their sum:
  \[ P( E_1\cup E_2) = P(E_1) + P(E_2)\]
\item
  when \(S\) is the set of all possible outcomes, then its probability is \(1\) (at least something is observed): \[P(S)=1\]
\item
  The probability of any outcome is between 0 and 1 \[P(E) \in [0,1]\]
\end{enumerate}

Proposed by Kolmogorov's less than 100 years ago (1933)

\hypertarget{probabilities-table}{%
\section{Probabilities Table}\label{probabilities-table}}

Kolmogorov properties are the basic rules for building a \textbf{probability table}, similar to the relative frequency table.

\textbf{Example (dice)}

The probability table for the throw of a dice

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
result & probability \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(1\) & 1/6 \\
\(2\) & 1/6 \\
\(3\) & 1/6 \\
\(4\) & 1/6 \\
\(5\) & 1/6 \\
\(6\) & 1/6 \\
\(P( 1 \cup 2\cup ... \cup 6)\) & 1 \\
\end{longtable}

Let's verify the axioms:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Where \(1 \cup 2\) is, for example, the \textbf{event} of rolling a \(1\) \textbf{or} a \(2\). So \[ P( 1 \cup 2)=P(1)+P(2)=2/6\]
\item
  Since \(S= \{ 1,2,3,4,5,6\}\) is made up of \textbf{mutually exclusive} outcomes, then
\end{enumerate}

\[P(S)=P(1\cup 2\cup ... \cup 6) = P(1)+P(2)+ ...+P(n)=1\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  The probabilities of each outcome are between \(0\) and \(1\).
\end{enumerate}

\hypertarget{sample-space}{%
\section{Sample space}\label{sample-space}}

The set of all possible outcomes of a random experiment is called the \textbf{sample space} and is denoted \(S\).

The sample space can be made up of categorical or numerical outcomes.

\emph{For example:}

\begin{itemize}
\tightlist
\item
  human temperature: \(S = (36, 42)\) degrees Celsius.
\item
  sugar levels in humans: \(S =( 70-80) mg/ dL\)
\item
  the size of a production line screw: \(S =( 70-72) mm\)
\item
  number of emails received in an hour: \(S = \{ 1, ...\infty \}\)
\item
  the throw of a dice: \(S= \{ 1, 2, 3, 4, 5, 6\}\)
\end{itemize}

\hypertarget{events}{%
\section{Events}\label{events}}

An \textbf{event} \(A\) is a \textbf{subset} of the sample space. It is a \textbf{collection} of posible results.

\emph{Examples of events:}

\begin{itemize}
\tightlist
\item
  The event of a healthy temperature: \(A=37-38\) degrees Celsius
\item
  The event of producing a screw with a size: \(A=71.5mm\)
\item
  The event of receiving more than 4 emails in an hour: \(A= \{ 4, \infty \}\)
\item
  The event of obtaining a number less than or equal to 3 in the roll of a says: \(A= \{ 1,2,3\}\)
\end{itemize}

An event refers to a possible set of \textbf{outcomes}.

\hypertarget{algebra-of-events}{%
\section{Algebra of events}\label{algebra-of-events}}

For two events \(A\) and \(B\), we can construct the following derived events using the basic set operations:

\begin{itemize}
\tightlist
\item
  Complement \(A'\): the event of \textbf{not} \(A\)
\item
  Union \(A \cup B\): the event of \(A\) \textbf{or} \(B\)
\item
  Intersection \(A \ cap B\): the event of \(A\) \textbf{and} \(B\)
\end{itemize}

\textbf{Example (dice)}

Let's roll a die and look at the events (result set):

\begin{itemize}
\tightlist
\item
  a number less than or equal to three \(A:\{ 1,2,3\}\)
\item
  an even number \(B:\{ 2,4,6\}\)
\end{itemize}

Let's see how we can build new events with set operations:

\begin{itemize}
\tightlist
\item
  a number not less than three: \(A ':\{4,5,6\}\)
\item
  a number less than or equal to three \textbf{or} even: \(A \cup B: \{ 1,2,3,4,6\}\)
\item
  a number less than or equal to three \textbf{and} even \(A \cap B: \{ 2\}\)
\end{itemize}

\hypertarget{mutually-exclusive-results}{%
\section{Mutually exclusive results}\label{mutually-exclusive-results}}

Outcomes like rolling \(1\) and \(2\) on a die are events that cannot occur at the same time. We say that they are \textbf{mutually exclusive}.

In general, two events denoted as \(E_1\) and \(E_2\) are mutually exclusive when

\[E_1\cap E_2=\emptyset\]

\emph{Examples:}

\begin{itemize}
\item
  The result of having a misophonia severity of \(1\) and a severity of \(4\).
\item
  The results of obtaining \(12\) and \(5\) by adding the throw of two dice.
\end{itemize}

According to the Kolmogorov properties , only \textbf{mutually exclusive} outcomes can be arranged in \textbf{probability tables}, as in relative frequency tables.

\hypertarget{joint-probabilities}{%
\section{Joint probabilities}\label{joint-probabilities}}

The \textbf{joint probability} of \(A\) and \(B\) is the probability of \(A\) and \(B\). That's \[P( A \cap B)\] or \(P(A,B)\).

To write joint probabilities of non mutually exclusive events (\(A \cap B \neq \emptyset\)) into a probability table, we note that we can always decompose the sample space into \textbf{mutually exclusive} sets involving the intersections:

\(S=\{A\cap B, A \cap B', A'\cap B, A'\cap B'\}\)

\textbf{Let's consider the Ven diagram} for the example where \(A\) is the event that corresponds to drawing a number less than or equal to 3 and \(B\) corresponds to an even number:

\includegraphics{./figures/Venn.JPG}

The \textbf{marginals} of \(A\) and \(B\) are the probability of \(A\) and the probability of \(B\), respectively:

\begin{itemize}
\tightlist
\item
  \(P(A)=P(A\cap B') + P(A \cap B)=2/6+1/6=3/6\)
\item
  \(P(B)=P(A'\cap B) +P(A \cap B)=2/6+1/6=3/6\)
\end{itemize}

We can now write the \textbf{probability table} for the joint probabilities

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
Result & probability \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\((A\cap B)\) & \(P(A \cap B)=1/6\) \\
\((A\cap B')\) & \(P(A \cap B')=2/6\) \\
\((A'\cap B)\) & \(P(A' \cap B)=2/6\) \\
\((A'\cap B')\) & \(P(A' \cap B')=1/6\) \\
sum & \(1\) \\
\end{longtable}

Each result has \(two\) values (one for the feature of type \(A\) and one for type \(B\))

\hypertarget{contingency-table}{%
\section{Contingency table}\label{contingency-table}}

The joint probability table can also be written in a \textbf{contingency table}

\begin{longtable}[]{@{}cccc@{}}
\toprule\noalign{}
& \(B\) & \(B'\) & sum \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(A\) & \(P(A \cap B )\) & \(P(A\cap B' )\) & \(P(A)\) \\
\(A'\) & \(P(A'\cap B )\) & \(P(A'\cap B' )\) & \(P(A')\) \\
sum & \(P(B)\) & \(P(B')\) & 1 \\
\end{longtable}

Where the marginals are the sums in the margins of the table, for example:

\begin{itemize}
\tightlist
\item
  \(P(A)=P(A \cap B') + P(A \cap B)\)
\item
  \(P(B)=P(A' \cap B) +P(A \cap B)\)
\end{itemize}

In our example, the contingency table is

\begin{longtable}[]{@{}cccc@{}}
\toprule\noalign{}
& \(B\) & \(B'\) & sum \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(A\) & \(1/6\) & \(2/6\) & \(3/6\) \\
\(A'\) & \(2/6\) & \(1/6\) & \(3/6\) \\
sum & \(3/6\) & \(3/6\) & \(1\) \\
\end{longtable}

\hypertarget{the-addition-rule}{%
\section{The addition rule:}\label{the-addition-rule}}

The addition rule allows us to calculate the probability of \(A\) or \(B\), \(P( A \cup B)\), in terms of the probability of \(A\) and \(B\), \(P(A \cup B )\). We can do this in three equivalent ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Using the marginals and the joint probability
  \[P(A \cup B)=P(A) + P(B) - P(A\cap B)\]
\item
  Using only joint probabilities
  \[P( A \cup B)=P(A \cap B)+P(A\cap B')+P(A'\cap B)\]
\item
  Using the complement of joint probability
  \[P(A \cup B)=1-P(A'\cap B')\]
\end{enumerate}

\textbf{Example (dice)}

Take the events \(A:\{ 1,2,3\}\), rolling a number less than or equal to \(3\), and \(B:\{2,4,6\}\), rolling an even number on the roll of a dice.

Therefore:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \(P( A \cup B)=P(A) + P(B) - P(A\cap B)=3/6+3/6-1/6=5/6\)
\item
  \(P(A \cup B)=P(A \cap B)+P(A\cap B')+P(A'\cap B)=1/6+2/6+2/6=5/6\)
\item
  \(P(A \cup B)=1-P(A'\cap B')= 1-1/6=5/6\)
\end{enumerate}

In the contingency table \(P( A \cup B)\) corresponds to the cells in bold (method 2 above). That is all cells but 1/6 from the bottom right (method 3).

\begin{longtable}[]{@{}ccc@{}}
\toprule\noalign{}
& \(B\) & \(B'\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(A\) & \textbf{1/6} & \textbf{2/6} \\
\(A'\) & \textbf{2/6} & \emph{1/6} \\
\end{longtable}

\hypertarget{questions-1}{%
\section{Questions}\label{questions-1}}

We collect the age and category of 100 athletes in a competition

\begin{longtable}[]{@{}ccc@{}}
\toprule\noalign{}
& \(age:junior\) & \(age:senior\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(category:1st\) & \(14\) & \(12\) \\
\(category:2nd\) & \(21\) & \(18\) \\
\(category:3rd\) & \(22\) & \(13\) \\
\end{longtable}

\textbf{1)} What is the estimated probability that an athlete is 2nd category and senior?

\textbf{\(\qquad\)a:} \(18/100\); \textbf{\(\qquad\)b:} \(18/43\); \textbf{\(\qquad\)c:} \(18\); \textbf{\(\qquad\)d:} \(18/39\)

\textbf{2)} What is the estimated probability that the athlete is not in the third category and is senior?

\textbf{\(\qquad\)a:} \(35/100\); \textbf{\(\qquad\)b:} \(30/100\); \textbf{\(\qquad\)c:} \(22/100\); \textbf{\(\qquad\)d:} \(13/100\)

\textbf{3)} What is the marginal probability of the third category?

\textbf{\(\qquad\)a:} \(13/100\); \textbf{\(\qquad\)b:} \(35/100\); \textbf{\(\qquad\)c:} \(22/100\); \textbf{\(\qquad\)d:} \(13/22\)

\textbf{4)} What is the marginal probability of being senior?

\textbf{\(\qquad\)a:} \(13/100\); \textbf{\(\qquad\)b:} \(43/100\); \textbf{\(\qquad\)c:} \(43/57\); \textbf{\(\qquad\)d:} \(57/100\)

\textbf{5)} What is the probability of being senior or third category?

\textbf{\(\qquad\)a:} \(65/100\); \textbf{\(\qquad\)b:} \(86/100\); \textbf{\(\qquad\)c:} \(78/100\); \textbf{\(\qquad\)d:} \(13/100\)

\hypertarget{exercises-1}{%
\section{Exercises}\label{exercises-1}}

\hypertarget{classical-probability-exercise-1}{%
\subsubsection{Classical probability: Exercise 1}\label{classical-probability-exercise-1}}

\begin{itemize}
\item
  Write the table of \textbf{joint probability} for the \textbf{results} of rolling two dice; In the rows write the results of the first die and in the columns the results of the second die.
\item
  What is the probability of drawing \((3, 4)\) ? (R:1/36)
\item
  What is the probability of rolling \(3\) and \(4\) with any of the two dice? (R:2/36)
\item
  What is the probability of rolling \(3\) on the first die or \(4\) on the second? (To:11/36)
\item
  What is the probability of rolling \(3\) or \(4\) with any dice? (R:20/36)
\item
  Write the \textbf{probability table} for the result of the \textbf{add} of two dice. Assume that the outcome of each die is \textbf{equally likely}. Verify that it is:
\end{itemize}

\[
P_i=
\begin{cases}
\frac{i-1}{36},& i \in \{2,3,4,5,6, 7\} \\
\frac{13-i}{36},& i \in \{8,9,10,11,12\} \\
\end{cases}
\]

\hypertarget{frequentist-probability-exercise-2}{%
\subsubsection{Frequentist probability: Exercise 2}\label{frequentist-probability-exercise-2}}

The result of a randomized experiment is to measure the severity of misophonia \textbf{and} the state of depression of a patient.

Misophonia

\begin{itemize}
\tightlist
\item
  severity: \(S_M:\{M_ 0,M _1,M_2,M_3,M_4\}\)
\item
  Depression: \(S_ D:\{ D', D\}\))
\end{itemize}

Write the contingency table for the absolute frequencies (\(n_{ M,D }\)) for a study on a total of 123 patients in which it was observed

\begin{itemize}
\tightlist
\item
  100 individuals did not have depression.
\item
  No individual with misophonia 4 and without depression.
\item
  5 individuals with grade 1 misophonia and no depression.
\item
  The same number as the previous case for individuals with depression and without misophonia .
\item
  25 individuals without depression and grade 3 misophonia .
\item
  The number of misophonics without depression for grades 2 and 0 were distributed equally .
\item
  The number of individuals with depression and misophonia increased progressively
  in multiples of three, starting at 0 individuals for grade 1.
\end{itemize}

Answer the following questions:

\begin{itemize}
\tightlist
\item
  How many individuals had misophonia ? (A:83)
\item
  How many individuals had grade 3 misophonia ? (R:31)
\item
  How many individuals had grade 2 misophonia without depression? (R:35)
\end{itemize}

Write down the contingency table for relative frequencies \(f_{ M,D }\). Suppose \(N\) is large and the absolute frequencies \textbf{estimate} the probabilities \(f_{ M,D }=\hat {P}(M \cap D)\). Answer the following questions:

\begin{itemize}
\tightlist
\item
  What is the marginal probability of severity 2 misophonia ? (R: 0.3)
\item
  What is the probability of not being misophonic \textbf{and} not being depressed? (R:0.284)
\item
  What is the probability of being misophonic \textbf{or} depressed? (R: 0.715)
\item
  What is the probability of being misophonic \textbf{and} being depressed? (R: 0.146)
\item
  Describe in spoken language the results with probability 0.
\end{itemize}

\hypertarget{exercise-3}{%
\subsubsection{Exercise 3}\label{exercise-3}}

We have carried out a randomized experiment \(10\) times, which consists of recording the sex and vital status of patients with some type of cancer after 10 years of diagnosis. We got the following results

\begin{verbatim}
##           A     B
## 1     male   dead
## 2     male   dead
## 3     male   dead
## 4   female  alive
## 5     male   dead
## 6   female  alive
## 7   female   dead
## 8   female  alive
## 9     male  alive
## 10    male  alive
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Create the contingency table for the number (\(n_{ i,j }\)) of observations of each result (\(A,B\))
\item
  Create the contingency table for the relative frequency (\(f_{ i,j }\)) of the results
\item
  What is the marginal frequency of being a man? (R/0.6)
\item
  What is the marginal frequency of being alive? (R/0.5)
\item
  What is the frequency of being alive \textbf{or} being a woman? (R/0.6)
\end{itemize}

\hypertarget{theory-exercise-4}{%
\subsubsection{Theory: Exercise 4}\label{theory-exercise-4}}

\begin{itemize}
\item
  From the second form of the addition rule, obtain the first and the third form.
\item
  What is the third form addition rule for the probability of three events \(P(A \cup B \cup C)\)?
\end{itemize}

\hypertarget{conditional-probability}{%
\chapter{Conditional probability}\label{conditional-probability}}

In this chapter, we will introduce conditional probability.

We will use conditional probability to define statistical independence.

We will discuss Bayes' theorem and we will discuss one of its main applications, which is the predictive efficiency of a diagnostic tool.

\hypertarget{joint-probability}{%
\section{Joint probability}\label{joint-probability}}

Recall that the joint probability of two events \(A\) and \(B\) is defined as their intersection

\[P( A ,B )=P(A \cap B)\]

Now imagine randomized experiments that measure two different types of outcomes.

\begin{itemize}
\item
  height and weight of an individual: \((h, w)\)
\item
  time and position of an electric charge: \((p, t)\)
\item
  the throw of two dice: (\(n_1\), \(n_2\))
\item
  cross two green traffic lights: (\(\bar{ R_ 1}\) , \(\bar{R_2}\))
\end{itemize}

We are often interested in whether the values of one result \textbf{condition} the values of the other.

\hypertarget{statistical-independence}{%
\section{Statistical independence}\label{statistical-independence}}

In many cases, we are interested in whether two events often tend to occur together. We want to be able to discern between two cases.

\begin{itemize}
\item
  \textbf{Independence} between events. For example, rolling a 1 on one die does not make it more likely to roll another 1 on a second die.
\item
  \textbf{Correlation} between events. For example, if a man is tall, he is probably heavy.
\end{itemize}

\textbf{Example (conductor)}

We conducted an experiment to find out if observing structural flaws in a material affects its conductivity.

The data would look like

\begin{longtable}[]{@{}ccc@{}}
\toprule\noalign{}
Conductor & Structure & conductivity \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(c_1\) & flaws & low \\
\(c_2\) & no flaws & high \\
\(c_3\) & flaws & low \\
\ldots{} & \ldots{} & \ldots{} \\
\(c_i\) & no flaws & low* \\
\ldots{} & \ldots{} & \ldots{} \\
\ldots{} & \ldots{} & \ldots{} \\
\(c_n\) & flaws & high* \\
\end{longtable}

We can expect low conductivity to occur more often with flaws than without flaws if the flaws affect conductivity.

Let's imagine that from the data we obtain the following contingency table of \textbf{estimated joint probabilities}

\begin{longtable}[]{@{}cccc@{}}
\toprule\noalign{}
& with flaws (F) & no flaws (F') & sum \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
low (L) & \(0.005\) & \(0.045\) & \(0.05\) \\
high (L') & \(0.095\) & \(0.855\) & \(0.95\) \\
sum & \(0.1\) & \(0.9\) & 1 \\
\end{longtable}

where, for example, the joint probability of \(L\) and \(F\) is

\begin{itemize}
\tightlist
\item
  \(P(L,F )=0.005\)
\end{itemize}

and the marginal probabilities are

\begin{itemize}
\tightlist
\item
  \(P(L)=P(L, F) + P(L, F')=0.05\)
\item
  \(P(F)=P(L, F) + P(L', F)= 0.1\).
\end{itemize}

\hypertarget{the-conditional-probability}{%
\section{The conditional probability}\label{the-conditional-probability}}

Low conductivity is \textbf{independent} of having structural flaws if the probability of having low conductivity (\(L\)) is the same \textbf{whether} it has flaws (\(F\)) or not (\(F'\) ) .

Let us first consider only the materials that have flaws.

Among those materials that have flaws (\(F\)), what is the estimated probability that they have low conductivity?

\(\hat{P}(L| F)= \frac{n_{L,F}}{n_{F}}=\frac{n_{L,F}/n}{n_{F}/n}= \frac{f_{L,F}}{f_{F}}\)
\[= \frac{\hat{P}( L,F )}{\hat{P}(F)}\]
Therefore, in the limit when \(N \rightarrow \infty\), we have

\[P(L| F)= \frac{P(L,F)}{P(D)}=\frac{P(L\cap F)}{P(D)}\]
\textbf{Definition:}

The \emph{conditional probability} of an event \(B\) given an event \(A\), denoted \(P(A| B)\) , is

\[P(A|B) = \frac{P(A\cap B)}{P(B)}\]

We can prove that conditional probability satisfies the axioms of probability. The conditional probability can be understood as a probability with a sample space given by \(B\): \(S_B\). In our example, the materials with stuctural flaw.

\hypertarget{conditional-contingency-table}{%
\section{Conditional contingency table}\label{conditional-contingency-table}}

If we divide the columns of the joint probability table by the marginal probabilities of the conditioning effects (\(F\) and \(F'\)), we can write \textbf{a conditional contingency table}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3438}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3438}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3125}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
F
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
F'
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
L & P( L {\textbar{}} F) & P(L {\textbar{}} F') \\
L' & P(L' {\textbar{}} F) & P(L' {\textbar{}} F') \\
sum & 1 & 1 \\
\end{longtable}

where the column probabilities sum to one. The first column shows the probabilities of low conductivity or not only of the materials that have flaws (first condition: \(F\)). The second column shows the probabilities only for the materials that have no flaws (second condition: \(F'\)).

Conditional probabilities are the probabilities of the event within each condition. We read them as:

\begin{itemize}
\tightlist
\item
  \(P(L| F)\): Probability of having low conductivity \textbf{if} it has flaws
\item
  \(P(L'| F)\): Probability of not having low conductivity \textbf{if} it has flaws
\item
  \(P(L|F ')\): Probability of having low conductivity \textbf{if} it has no flaws
\item
  \(P(L'|F ')\): Probability of not having low conductivity \textbf{if} it has no flaws
\end{itemize}

\hypertarget{statistical-independence-1}{%
\section{Statistical independence}\label{statistical-independence-1}}

In our example, the conditional contingency table is

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3438}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3438}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3125}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
F
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
F'
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
L & P(L{\textbar{}}F) = 0.05 & P(L{\textbar{}}F')= 0.05 \\
L' & P(L'{\textbar{}}F)=0.95 & P(L'{\textbar{}}F')=0.95 \\
sum & 1 & 1 \\
\end{longtable}

We note that the marginal and conditional probabilities are the same!

\begin{itemize}
\tightlist
\item
  \(P(L| F)= P(L|F')=P(L)\)
\item
  \(P(L'| F)= P(L'|F')=P(L')\)
\end{itemize}

This means that the probability of observing a low conductivity is \textbf{not} dependent on having a structural flaw or not.

We conclude that low conductivity is not affected by having a structural flaw.

\textbf{Definition}

Two events \(A\) and \(B\) are statistically independent if either of the equivalent cases occurs.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \(P(A| B)= P(A)\); \(A\) is independent of \(B\)
\item
  \(P(B| A)= P(B)\); \(B\) is independent of \(A\)
\end{enumerate}

and by the definition of conditional probability

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  \(P(A\cap B)=P(A|B)P(B)=P(A)P(B)\)
\end{enumerate}

This third form is a statement about joint probabilities. It says that we can obtain joint probabilities by multiplying the marginal ones.

In our original joint probability table

\begin{longtable}[]{@{}cccc@{}}
\toprule\noalign{}
& F & F' & sum \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
L & \(0.005\) & \(0.045\) & \(0.05\) \\
L' & \(0.095\) & \(0.855\) & \(0.95\) \\
sum & \(0.1\) & \(0.9\) & 1 \\
\end{longtable}

we can confirm that all the entries of the matrix are the product of the marginal ones. For example: \(P(F)P(L)= P( L \cap F)\) and \(P(L')P(F')=P(L' \cap F')\). Therefore, low conductivity is independent of having a structural flaw.

\textbf{Example (Coins)}

We want to confirm that the results of tossing two coins are independent. We consider all outcomes to be equally likely:

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
result & probability \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(( H,T )\) & 1/4 \\
\(( H,H )\) & 1/4 \\
\(( T,T )\) & 1/4 \\
\(( T, H )\) & 1/4 \\
sum & 1 \\
\end{longtable}

where \(( H,T )\) is, for example, the event of heads on the first coin and tails on the second coin. The contingency table for the joint probabilities is:

\begin{longtable}[]{@{}cccc@{}}
\toprule\noalign{}
& H & T & sum \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
H & \(1/4\) & \(1/4\) & \(1/2\) \\
T & \(1/4\) & \(1/4\) & \(1/2\) \\
sum & \(1/2\) & \(1/2\) & 1 \\
\end{longtable}

From this table, we see that the probability of getting a head and then a tail is the product of the marginals \(P( H, T)=P(H)*P(T)=1/4\). Therefore, the events of heads in the first coin and tails in the second are independent.

If we build the conditional contingency table on the toss of the first coin, we will see that obtaining tails in the second coin is not conditioned by having obtained heads in the first coin: \(P(T| H)= P(T) =1 / 2\)

\hypertarget{statistical-dependency}{%
\section{Statistical dependency}\label{statistical-dependency}}

An important example of statistical dependency is found in the performance of \textbf{diagnostic tools}, where we want to determine the state of a system(s) with results

\begin{itemize}
\tightlist
\item
  satisfactory (yes)
\item
  unsatisfactory (not)
\end{itemize}

with a test (t) with results

\begin{itemize}
\tightlist
\item
  positive
\item
  negative
\end{itemize}

For example, we test a battery to see how long it can last. We load a cable to find out if it resists carrying a certain load. We run a PCR to see if someone is infected.

\hypertarget{diagnostic-test}{%
\section{Diagnostic test}\label{diagnostic-test}}

Let's consider diagnosing an infection with a new test. Infection status:

\begin{itemize}
\tightlist
\item
  yes (infected)
\item
  no (not infected)
\end{itemize}

Test:

\begin{itemize}
\tightlist
\item
  positive
\item
  negative
\end{itemize}

The \textbf{conditional contingency table} is what we get in a controlled environment (laboratory)

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3438}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3438}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3125}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Infection: yes
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Infection: No
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Test: positive & P(positive {\textbar{}} yes) & P(positive {\textbar{}} no) \\
Test: negative & P(negative {\textbar{}} yes) & P(negative {\textbar{}} no) \\
sum & 1 & 1 \\
\end{longtable}

Let's look at the table entries
1) Rate of true positives (Sensitivity): The probability of testing positive \textbf{if} you have the disease \(P(positive| yes)\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\item
  Rate of true negatives (Specificity): The probability of testing negative \textbf{if} you do not have the disease \(P(negative| no)\)
\item
  False positive rate: the probability of testing positive \textbf{if} you do not have the disease \(P(positive| no)\)
\item
  False negative rate: the probability of testing negative \textbf{if} you have the disease \(P(negative| yes)\)
\end{enumerate}

High correlation (statistical dependence) between test and infection means high values for probabilities 1 and 2 \textbf{and} low values for probabilities 3 and 4.

\textbf{Example (COVID)}

Now let's consider a real situation. In the early days of the coronavirus pandemic, there was no measure of the effectiveness of PCRs in detecting the virus. One of the first published studies (\url{https://www.nejm.org/doi/full/10.1056/NEJMp2015897}) found that

\begin{itemize}
\tightlist
\item
  The PCR had a sensitivity of 70\%, in infection condition.
\item
  The PCR had a specificity of 94\%, in non-infected condition.
\end{itemize}

The conditional contingency table is

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3438}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3438}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3125}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Infection: yes
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Infection: No
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Test: positive & P(positive{\textbar{}} yes)= 0.7 & P(positive{\textbar{}}no)=0.06 \\
Test: negative & P(negative{\textbar{}} yes)= 0.3 & P(negative{\textbar{}}no)=0.94 \\
sum & 1 & 1 \\
\end{longtable}

Therefore, the errors in the diagnostic tests were:

\begin{itemize}
\tightlist
\item
  The false positive rate is \(P(positive| no)= 0.06\)
\item
  The false negative rate is \(P(negative| yes)= 0.3\)
\end{itemize}

\hypertarget{inverse-probabilities}{%
\section{Inverse probabilities}\label{inverse-probabilities}}

We are interested in finding the probability of being infected if the test is positive: \[P(si| positive)\]

For that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We recover the contingency table for joint probabilities, multiplying by the marginal \(P(yes)\) and \(P(no)\) that we need to know
\end{enumerate}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2619}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2619}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2381}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2381}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Infection: yes
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Infection: No
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
sum
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Test: positive & P( positive {\textbar{}} yes)P(yes) & P(positive {\textbar{}} no)P(no) & P(positive) \\
Test: negative & P( negative {\textbar{}} yes)P(yes) & P(negative {\textbar{}} no) P(no) & P(negative) \\
sum & P(yes) & P(no) & 1 \\
\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We use the definition of conditional probabilities for rows instead of columns (we divide by the marginal of the test results)
\end{enumerate}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2619}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2619}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2381}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2381}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Infection: yes
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Infection: No
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
sum
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Test: positive & P(yes{\textbar{}}positive) & P(no{\textbar{}}positive) & 1 \\
Test: negative & P(yes{\textbar{}}negative) & P(no{\textbar{}}negative) & 1 \\
\end{longtable}

For example:

\[P(yes| positive)= \frac{P(positive|yes)P(yes)}{P(positive)}\]

To apply this formula we need the marginals \(P(yes)\) (prevalence) and \(P(positive)\).

\begin{itemize}
\item
  The prevalence \(P(yes)\) needs to be given from another study. The first prevalence study in Spain showed that during confinement \(P(yes)=0.05\), \(P(no)=0.95\), before the summer of 2020.
\item
  To find the marginal of positives \(P(positive)\), we can then use the definition of marginal and conditional probability:
\end{itemize}

\(P(positive)= P(positive \cap yes) + P(positive \cap no)\)
\[= P(positive| yes)P (yes)+P(positive|no)P(no)\]
This last relation of the marginals is called \textbf{rule of total probability}.

\hypertarget{bayes-theorem}{%
\section{Bayes' Theorem}\label{bayes-theorem}}

After substituting the total probability rule into \(P(yes| positive)\) , we have

\[P(yes| positive)= \frac{P(positive|yes)P(yes)}{P(positive|yes)P(yes)+P(positive|no)P(no)}\]
This expression is known as \textbf{Bayes' theorem}. It allows us to reverse the conditionals:

\[P(positive|yes) \rightarrow P(yes| positive)\]
Or \textbf{assess} a test in a controlled condition (infection) and then use it to \textbf{infer} the probability of the condition when the test is positive.

\textbf{Example (COVID)}:

The test performance was:

\begin{itemize}
\item
  Sensitivity: \(P(positive| yes)= 0.70\)
\item
  False positive rate: \(P(positive| no)= 1- P(negative|no)=0.06\)
\end{itemize}

The study in the Spanish population gave:

\begin{itemize}
\tightlist
\item
  \(P(yes)=0.05\)
\item
  \(P(no)=1-P(yes)=0.95\).
\end{itemize}

Therefore, the probability of being infected in case of testing positive was:

\[P(yes| positive)= 0.38\]

We concluded that at that time PCR was not very good at \textbf{confirming} infections.

However, let us now apply Bayes' theorem to the probability of not being infected if the test was negative.

\[P(no|negative) = \frac{P(negative|no) P(no )}{ P(negative|no) P(no)+P(negative|yes)P(yes)}\]

Substituting all values gives

\[P(no| negative)= 0.98\]

So the tests were good for \textbf{ruling out} infections and a fair requirement for travel.

\textbf{Bayes's theorem}

In general, we can have more than two conditioning events. Therefore, Baye's theorem says:

If \(E1, E2, ..., Ek\) are \(k\) mutually exclusive and exhaustive events and \(B\) is any event, then the probability inverse \(P(Ei| B)\) is

\[P(Ei| B)= \frac{P(B|Ei)P(Ei)}{P(B|E1)P(E1) +...+ P(B|Ek)P(Ek)}\]
The denominator is the total probability rule for the marginal \(P(B)\), in terms of the marginals \(P(E1), P(E2), ... P(Ek)\).

\[P(B)=P(B|E 1)P (E1) +...+ P(B|Ek)P(Ek)\]

\textbf{Conditional tree}

The total probability rule can also be illustrated using a \textbf{conditional} tree.

\includegraphics{./figures/treetot.PNG}

\textbf{Rule of total probability} for the marginal of \(B\): In how many ways can I get the result \(B\)?

\(P(B)=P(B|A)P(A)+P(B|A')P(A')\)

\hypertarget{questions-2}{%
\section{Questions}\label{questions-2}}

We collect the age and category of 100 athletes in a competition

\begin{longtable}[]{@{}ccc@{}}
\toprule\noalign{}
& \(junior\) & \(senior\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(1st\) & \(14\) & \(12\) \\
\(2nd\) & \(21\) & \(18\) \\
\(3rd\) & \(22\) & \(13\) \\
\end{longtable}

\textbf{1)} What is the estimated probability that the athlete is in the third category if the athlete is a junior?

\textbf{\(\qquad\)a:} \(22\); \textbf{\(\qquad\)b:} \(22/100\); \textbf{\(\qquad\)c:} \(22/57\); \textbf{\(\qquad\)d:} \(22/35\);

\textbf{2)} What is the estimated probability that the athlete is a junior and is in the 1st category if the athlete is not in the 3rd category?

\textbf{\(\qquad\)a:} \(14/35\); \textbf{\(\qquad\)b:} \(14/65\); \textbf{\(\qquad\)c:} \(14/100\); \textbf{\(\qquad\)d:} \(14/26\)

\textbf{3)} A diagnostic test has a probability of \(8/9\) of detecting a disease if the patients are sick and a probability of \(3/9\) of detecting the disease if the patients are healthy. If the probability of being sick is \(1/9\). What is the probability that a patient is sick if a test detects the disease?

\textbf{\(\qquad\)a:} \(\frac{8/9}{8/9+3/9}*1/9\);
\textbf{\(\qquad\)b:} \(\frac{3/9}{8/9+3/9}*1/9\);
\textbf{\(\qquad\)c:} \(\frac{3/9*8/9}{8/9*1/9+3/9*8/9}\); \textbf{\(\qquad\)d:} \(\frac{8/9*1/9}{8/9*1/9+3/9*8/9}\);

\textbf{4)} As discussed in the notes, a PCR test for coronavirus had a sensitivity of 70\% and a specificity of 94\% and in Spain during confinement there was an incidence of 5\%. With these data, what was the probability of testing positive in Spain (\(P(positive)\))

\textbf{\(\qquad\)a:} \(0.035\); \textbf{\(\qquad\)b:} \(0.092\); \textbf{\(\qquad\)c:} \(0.908\); \textbf{\(\qquad\)d:} \(0.95\)

\textbf{5)} With the same data as in question 4, testing positive in the PCR and being infected are not independent events because:

\textbf{\(\qquad\) a:} Sensitivity is 70\%;
\textbf{\(\qquad\)b:} Sensitivity and false positive rate are different;
\textbf{\(\qquad\)c:} The false positive rate is 0.06\%; \textbf{\(\qquad\)d:} the specificity is 96\%

\hypertarget{exercises-2}{%
\section{Exercises}\label{exercises-2}}

\hypertarget{exercise-1-1}{%
\subsubsection{Exercise 1}\label{exercise-1-1}}

A machine is tested for its performance in producing high-quality turning rods. These are the test results

\begin{longtable}[]{@{}ccc@{}}
\toprule\noalign{}
& Rounded: yes & Rounded: No \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
smooth surface: yes & 200 & 1 \\
smooth surface: no & 4 & 2 \\
\end{longtable}

\begin{itemize}
\item
  What is the estimated probability that the machine will produce a rod that does not satisfy any quality control? (A: 2/207)
\item
  What is the estimated probability that the machine will produce a rod that fails at least one quality check? (A: 7/207)
\item
  What is the estimated probability that the machine will produce rods with a rounded and smooth surface? (A: 200/207)
\item
  What is the estimated probability that the bar is rounded if the bar is smooth? (A: 200/201)
\item
  What is the estimated probability that the rod is smooth if it is rounded? (A: 200/204)
\item
  What is the estimated probability that the rod is neither smooth nor rounded if it does not satisfy at least one quality check? (A: 2/7)
\item
  Are smoothness and roundness independent events? (No)
\end{itemize}

\hypertarget{exercise-2-1}{%
\subsubsection{Exercise 2}\label{exercise-2-1}}

We developed a test to detect the presence of bacteria in a lake. We found that if the lake contains the bacteria, the test is positive 70\% of the time. If there are no bacteria, the test is negative 60\% of the time. We implemented the test in a region where we know that 20\% of the lakes have bacteria.

\begin{itemize}
\tightlist
\item
  What is the probability that a lake that tests positive is contaminated with bacteria? (R: 0.30)
\end{itemize}

\hypertarget{exercise-3-1}{%
\subsubsection{Exercise 3}\label{exercise-3-1}}

Two machines are tested for their performance in producing high-quality turning rods. These are the test results

\textbf{Machine 1}

\begin{longtable}[]{@{}ccc@{}}
\toprule\noalign{}
& Rounded: yes & Rounded: No \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
smooth surface: yes & 200 & 1 \\
smooth surface: no & 4 & 2 \\
\end{longtable}

\textbf{Machine 2}

\begin{longtable}[]{@{}ccc@{}}
\toprule\noalign{}
& Rounded: yes & Rounded: No \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
smooth surface: yes & 145 & 4 \\
smooth surface: no & 8 & 6 \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  What is the probability that the bar is rounded? (A: 357/370)
\item
  What is the probability that the rod was produced by machine 1? (A: 207/370)
\item
  What is the probability that the rod is not smooth? (R: 20/370)
\item
  What is the probability that the rod is smooth or rounded or produced by machine 1? (A: 364/370)
\item
  What is the probability that the rod will be rounded if it is smoothed and from machine 1? (A: 200/201)
\item
  What is the probability that the rod is not rounded if it is not smooth and it is from machine 2? (A: 6/14)
\item
  What is the probability that the rod has come out of machine 1 if it is smooth and rounded? (R: 200/345)
\item
  What is the probability that the rod came from machine 2 if it fails at least one of the quality controls? (R:0.72)
\end{itemize}

\hypertarget{exercise-4}{%
\subsubsection{Exercise 4}\label{exercise-4}}

We want to cross an avenue with two traffic lights. The probability of finding the first red light is 0.6. If we stop at the first traffic light, the probability of stopping at the second is 0.15. While the probability of stopping at the second if we don't stop at the first is 0.25.

When we try to cross both traffic lights:

\begin{itemize}
\tightlist
\item
  What is the probability of having to stop at each traffic light? (R:0.09)
\item
  What is the probability of having to stop at at least one traffic light? (R:0.7)
\item
  What is the probability of having to stop at a single traffic light? (R:0.61)
\item
  If I stopped at the second traffic light, what is the probability that I would have to stop at the first? (R: 0.47)
\item
  If you were to stop at any traffic light, what is the probability that you would have to stop twice? (R: 0.12)
\item
  Is stopping at the first traffic light an independent event from stopping at the second traffic light? (No)
\end{itemize}

Now, we want to cross an avenue with three traffic lights. The probability of encountering the first red light is 0.6, and the probability of encountering a red light at the second signal depends solely on the probability of the first light. Similarly, the probability of encountering a red light at the third signal depends only on the probabilities of the second light. As previously mentioned, the probability of stopping at a traffic light is 0.15 if we stopped at the previous light. If we didn't stop at the previous one, the probability of stopping at a traffic light is 0.25.

\begin{itemize}
\item
  What is the probability of having to stop at each traffic light? (R:0.013)
\item
  What is the probability of having to stop at at least one traffic light? (R:0.775)
\item
  What is the probability of having to stop at a single traffic light? (R:0.5425)
\end{itemize}

tips:

\begin{itemize}
\tightlist
\item
  If the probability that a traffic light is red depends only on the previous one, then
\end{itemize}

\(P(R_3|R_2,R_1)=P(R_3|R_2,\bar{R}_1)=P(R_3|R_2)\) and \(P(R_3|\bar{R}_2,R_1)=P(R_3|\bar{R}_2,\bar{R}_1)=P(R_3|\bar{R}_2)\)

\begin{itemize}
\tightlist
\item
  The joint probability of finding three red lights can be written as:
\end{itemize}

\(P(R_ 1,R _2,R_3)=P(R_3|R_2)P(R_2|R_1)P(R_1)\)

\hypertarget{exercise-5}{%
\subsubsection{Exercise 5}\label{exercise-5}}

A quality test on a random brick is defined by the events:

\begin{itemize}
\tightlist
\item
  Pass the quality test: \(E\), fail the quality test: \(\bar{E}\)
\item
  Defective: \(D\), non-defective: \(\bar{D}\)
\end{itemize}

If the diagnostic test has sensitivity \(P(E|\bar{D })= 0.99\) and specificity \(P(\bar{E}|D)=0.98\), and the probability of passing the test is \(P(E) =0.893\) then

\begin{itemize}
\item
  What is the probability that a randomly chosen brick is defective \(P(D)\)? (R:0.1)
\item
  What is the probability that a brick that has passed the test is actually defective? (R:0.022)
\item
  The probability that a brick is not defective \textbf{and} that it fails the test (R:0.009)
\item
  Are \(D\) and \(\bar{E}\) statistically independent? (No)
\end{itemize}

\hypertarget{discrete-random-variables}{%
\chapter{Discrete Random Variables}\label{discrete-random-variables}}

\hypertarget{objective-1}{%
\section{Objective}\label{objective-1}}

In this chapter we will define a random variables and study discrete random variables.

We will define the probability mass function and its main properties of mean and variance. Following the abstraction process of the relative frequencies into probabilities we also define the probability distribution as the limiting case of the relative cumulative frequency.

\hypertarget{relative-frequencies-2}{%
\section{Relative frequencies}\label{relative-frequencies-2}}

Relative frequencies of the outcomes of a random experiment are a measure of their propensity. We can used them as estimators of their probabilities, when the we repeat the random experiment a lot of times (\(n \rightarrow \infty\)).

We defined central tendency (average), dispersion (sample variance) and the frequency distribution the data (\(F_i\)).

In terms of probabilities, how are this quantities defined?

\includegraphics{./figures/randomvar.JPG}

\hypertarget{random-variable}{%
\section{Random variable}\label{random-variable}}

We defined the relative frequencies on the \textbf{observations} of the experiments. We now define the equivalent quantities for probabilities in terms of the \textbf{outcomes} of the experiments. We will deal with numerical outcomes only.

A \textbf{random variable} is a symbol that represents a \textbf{numerical outcome} of a random experiment. We write the random variable in \textbf{capitals} (i.e.~\(X\)).

\textbf{Definition:}

A \textbf{random variable} is a function that assigns a real \textbf{number} to an \textbf{event} from the sample space of a random experiment.

Remember than an event can be an outcome or a collection of outcomes.

When the random variable takes a \textbf{value}, it indicates the realization of an \textbf{event} of a random experiment.

\emph{Example:}

If \(X \in \{0,1\}\), we then say \(X\) is a random variable that can take the values \(0\) or \(1\).

\hypertarget{events-of-observing-a-random-variable}{%
\section{Events of observing a random variable}\label{events-of-observing-a-random-variable}}

We make the distinction between variables in the model space with capital letters, as abstract entities, and the realization of a particular event or outcome. For instance:

\begin{itemize}
\tightlist
\item
  \(X=1\) is the \textbf{event} of observing the random variable \(X\) with value \(1\)
\item
  \(X=2\) is the \textbf{event} of observing the random variable \(X\) with value \(2\)
\end{itemize}

\ldots{}

\textbf{In general:}

\begin{itemize}
\tightlist
\item
  \(X=x\) is the \textbf{event} of observing the random variable \(X\) (big \(X\)) with value \(x\) (little \(x\)).
\end{itemize}

\hypertarget{probability-of-random-variables}{%
\section{Probability of random variables}\label{probability-of-random-variables}}

We are interested in assigning probabilities to the events of observing a particular value of a random variable.

For instance for the dice we will write the probability table as

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
\(X\) & Probability \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(1\) & \(P(X=1)=1/6\) \\
\(2\) & \(P(X=2)=1/6\) \\
\(3\) & \(P(X=3)=1/6\) \\
\(4\) & \(P(X=4)=1/6\) \\
\(5\) & \(P(X=5)=1/6\) \\
\(6\) & \(P(X=6)=1/6\) \\
\end{longtable}

where we make explicit the events that the variable takes a given outcome \(X=x\).

\hypertarget{probability-functions}{%
\section{Probability functions}\label{probability-functions}}

Because (little) \(x\) is a numerical variable, the probabilities of the random variable can be plotted

\includegraphics{_main_files/figure-latex/unnamed-chunk-36-1.pdf}

or written as the mathematical function

\[f(x)=P(X=x)=1/6\]

\hypertarget{probability-functions-1}{%
\section{Probability functions}\label{probability-functions-1}}

We can \textbf{create} any type of probability function if we satisfy Kolmogorov's probability rules:

For a discrete random variable \(X \in \{x_1 , x_2 , .. , x_M\}\), a \textbf{probability mass function} that is used to compute probabilities

\begin{itemize}
\tightlist
\item
  \(f(x_i)=P(X=x_i)\)
\end{itemize}

is always positive

\begin{itemize}
\tightlist
\item
  \(f(x_i)\geq 0\)
\end{itemize}

and its sum over all the values of the variable is \(1\):

\begin{itemize}
\tightlist
\item
  \(\sum_{i=1}^M f(x_i)=1\)
\end{itemize}

Where \(M\) is the number of possible outcomes.

Note that the definition of \(X\) and its probability mass function is general \textbf{without reference} to any experiment. The functions live in the model (abstract) space.

Here is an example

\includegraphics{_main_files/figure-latex/unnamed-chunk-37-1.pdf}

\(X\) and \(f(x)\) are abstract objects that may or may not map to an experiment. We have the freedom to construct them as we want as long as we respect their definition.

Probability mass functions have some \textbf{properties} that are derived exclusively from their definition.

\hypertarget{probabilities-and-relative-frequencies}{%
\section{Probabilities and relative frequencies}\label{probabilities-and-relative-frequencies}}

\textbf{Consider the example}

Make following set-up: In one urn put \(8\) balls and:

\begin{itemize}
\tightlist
\item
  mark \(1\) ball with \(-2\)
\item
  mark \(2\) balls with \(-1\)
\item
  mark \(2\) balls with \(0\)
\item
  mark \(2\) balls with \(1\)
\item
  mark \(1\) ball with \(2\)
\end{itemize}

And consider performing the following random \textbf{experiment:} Take one ball and read the number.

From the classical probability, we can write the probability table, for which we do not need to run any experiment

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
\(X\) & \(P(X=x)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(-2\) & \(1/8=0.125\) \\
\(-1\) & \(2/8=0.25\) \\
\(0\) & \(2/8=0.25\) \\
\(1\) & \(2/8=0.25\) \\
\(2\) & \(1/8=0.125\) \\
\end{longtable}

Now, let's perform the experiment \(30\) times and write the frequency table

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
\(X\) & \(f_i\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(-2\) & \(0.132\) \\
\(-1\) & \(0.262\) \\
\(0\) & \(0.240\) \\
\(1\) & \(0.248\) \\
\(2\) & \(0.118\) \\
\end{longtable}

The frequentist probability tells us
\[lim_{N \rightarrow \infty} f_i = f(x_i)=P(X=x_i)\]
Then, if we did not know the set up of the experiment (black box), the best we can do is to \textbf{estimate} the probabilities with the frequencies, obtained from \(N\) repetitions of the random experiment:

\[f_i = \hat{P}_i\]

\includegraphics{_main_files/figure-latex/unnamed-chunk-38-1.pdf}

Everytime we estimate the probabilities, our estimates \(\hat{P}_i=f_i\) change. But \(P_i\) is an abstract quantity that never changes. As \(N\) increases we get closer to it.

\hypertarget{mean-or-expected-value}{%
\section{Mean or expected value}\label{mean-or-expected-value}}

When we discussed summary statistics of data, we defined the centre of the observations as a value around which the outcome frequencies are concentrated.

We used the \textbf{average} to measure the centre of gravity of the \textbf{data}. In terms of the relative frequencies of the values of discrete outcomes, we wrote the average as

\(\bar{x}= \sum_{i=1}^M x_i \frac{n_i}{N}=\) \[\sum_{i=1}^M x_i f_i\]

\textbf{Definition}

The \textbf{mean} (\(\mu\)) or expected value of a discrete random variable \(X\), \(E(X)\), with mass function \(f(x)\) is given by

\[ \mu = E(X)= \sum_{i=1}^M x_i f(x_i) \]

\includegraphics{./figures/mu.png}

It is the center of gravity of the \textbf{probabilities}: The point where probability loading on a road are balanced.

From the definition we have

\[\bar{x} \rightarrow \mu\] in the \textbf{limit} when
\(N \rightarrow \infty\) as the frequency tends to the probability mass fucntion \(f_i \rightarrow f(x_i)\).

\textbf{Example}

What is the mean of \(X\) if its probability mass function \(f(x)\) is given by

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
\(X\) & \(f(x)=P(X=x)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(0\) & \(1/16\) \\
\(1\) & \(4/16\) \\
\(2\) & \(6/16\) \\
\(3\) & \(4/16\) \\
\(4\) & \(1/16\) \\
\end{longtable}

\includegraphics{_main_files/figure-latex/unnamed-chunk-39-1.pdf}

\[ \mu =E(X)=\sum_{i=1}^m x_i f(x_i) \]

\(E(X)=\)\textbf{0} * 1/16 + \textbf{1} * 4/16 + \textbf{2} * 6/16 + \textbf{3} * 4/16 + \textbf{4} * 1/16 =2

The mean \(\mu\) is the centre of gravity of the probability mass function \textbf{it does not change}. However, the average \(\bar{x}\) is the centre of gravity of the observations (relative frequencies) it \textbf{changes} with different data.

\hypertarget{variance}{%
\section{Variance}\label{variance}}

When we discussed summary statistics of data, we also defined the spread of the observations as an average distance from the data average.

\textbf{Definition}

The variance, written as \(\sigma^2\) or \(V(X)\), of a discrete random variable \(X\) with mass function \(f(x)\) is given by

\[\sigma^2 = V(X)= \sum_{i=1}^M (x_i-\mu)^2 f(x_i)\]
\(\sigma=\sqrt{V(X)}\) is called the \textbf{standard deviation} of the random variable.

Teh variance is the spread of the \textbf{probabilities} about the mean: The moment of inertia of probabilities about the mean.

\textbf{Example}

What is the variance of \(X\) if its probability mass function \(f(x)\) is given by

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
\(X\) & \(f(x)=P(X=x)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(0\) & \(1/16\) \\
\(1\) & \(4/16\) \\
\(2\) & \(6/16\) \\
\(3\) & \(4/16\) \\
\(4\) & \(1/16\) \\
\end{longtable}

\[\sigma^2 =V(X)=\sum_{i=1}^m (x_i-\mu)^2 f(x_i)\]

\(V(X)=\)\textbf{(0-2)}\(^2\)* 1/16 + \textbf{(1-2)}\(^2\)* 4/16 + \textbf{(2-2)}\(^2\)* 6/16 + \textbf{(3-2)}\(^2\)* 4/16 + \textbf{(4-2)}\(^2\)* 1/16 =1

\[V(X)=\sigma^2=1\]
\[\sigma=1\]

\hypertarget{probability-functions-for-functions-of-x}{%
\section{\texorpdfstring{Probability functions for functions of \(X\)}{Probability functions for functions of X}}\label{probability-functions-for-functions-of-x}}

In many occasions, we will be interested in outcomes that are function of the random variables. Perhaps, we are interested in the square of the number of flu infections, or on the square root of the number of emails in an hour.

\textbf{Definition}

For any function \(h\) of a random variable \(X\), with mass function \(f(x)\), its expected value is given by

\[ E[h(X)]= \sum_{i=1}^M h(x_i) f(x_i) \]

This is an important definition that allows us to prove three frequently used properties of the mean and variance:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  The mean of a linear function is the linear function fo the mean: \[E(a\times X +b)= a\times E(X) +b\] for \(a\) and \(b\) scalars (numbers).
\item
  The variance of a linear function of \(X\) is:\[V(a\times X +b)= a^2\times V(X)\]
\item
  The variance \textbf{about the origin} is the variance \textbf{about the mean} plus the mean squared: \[E(X^2)=V(X)+E(X)^2\]
\end{enumerate}

\textbf{Example}

What is the variance \(X\) about the origin, \(E(X^2)\), if its probability mass function \(f(x)\) is given by

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
\(X\) & \(f(x)=P(X=x)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(0\) & \(1/16\) \\
\(1\) & \(4/16\) \\
\(2\) & \(6/16\) \\
\(3\) & \(4/16\) \\
\(4\) & \(1/16\) \\
\end{longtable}

\[E(X^2) =\sum_{i=1}^m x_i^2 f(x_i)\]

\(E(X^2)=\)\textbf{(0)}\(^2\)* 1/16 + \textbf{(1)}\(^2\)* 4/16 + \textbf{(2)}\(^2\)* 6/16 + \textbf{(3)}\(^2\)* 4/16 + \textbf{(4)}\(^2\)* 1/16 =5

We can also verify:

\[E(X^2)=V(X)+E(X)^2\]

\(5=1+2^2\)

\hypertarget{probability-distribution}{%
\section{Probability distribution}\label{probability-distribution}}

When we discussed summary statistics of data, we also defined the frequency distribution (or the relative cumulative frequency) \(F_i\). \(F_i\) is an important quantity because it is a continuous function \(F_x\) is therefore a \textbf{continuous} function, even if the outcomes are discrete.

\textbf{Definition:}

The \textbf{probability distribution} function is defined as

\[F(x)=P(X\leq x)=\sum_{x_i\leq x} f(x_i) \]

That is the accumulated probability up to a given value \(x\)

\(F(x)\) satisfies therefore satisifies:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \(0\leq F(x) \leq 1\)
\item
  If \(x \leq y\), then \(F(x) \leq F(y)\)
\end{enumerate}

For the probability mass function:

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
\(X\) & \(f(x)=P(X=x)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(0\) & \(1/16\) \\
\(1\) & \(4/16\) \\
\(2\) & \(6/16\) \\
\(3\) & \(4/16\) \\
\(4\) & \(1/16\) \\
\end{longtable}

The probability distribution is:

\[
    F(x)=
\begin{cases}
    1/16,& \text{if } 0 \leq x < 1\\
    5/16,& 1\leq x < 2\\
    11/16,& 2\leq x < 3\\
    15/16,& 4\leq x < 5\\
    16/16,&  x \leq 5\\
\end{cases}
\]

For\(X \in \mathbb{Z}\)

\includegraphics{_main_files/figure-latex/unnamed-chunk-40-1.pdf}

\hypertarget{probability-function-and-probability-distribution}{%
\section{Probability function and probability distribution}\label{probability-function-and-probability-distribution}}

The probability function and distribution are equivalent. We can get one from the other and vice-versa

\[f(x_i)=F(x_i)-F(x_{i-1})\]

with

\[f(x_1)=F(x_1)\]

for \(X\) taking values in \(x_1 \leq x_2 \leq ... \leq x_n\)

\textbf{Example}

From probability distribution:

\[
    F(x)=
\begin{cases}
    1/16,& \text{if } 0 \leq x < 1\\
    5/16,& 1\leq x < 2\\
    11/16,& 2\leq x < 3\\
    15/16,& 4\leq x < 5\\
    16/16,&  x \leq 5\\
\end{cases}
\]

We can obtain the probability mas function.

\(f(0)=F(0)=1/16\)
\(f(1)=F(1)-f(0)=5/32-1/32=4/16\)
\(f(2)=F(2)-f(1)-f(0)=F(2)-F(1)=6/16\)
\(f(3)=F(3)-f(2)-f(1)-f(0)=F(3)-F(2)=4/16\)
\(f(4)=F(4)-F(3)=1/16\)

\hypertarget{quantiles}{%
\section{Quantiles}\label{quantiles}}

Finally, we can use the probability distribution \(F(x)\) to define the median and the quartiles of the radom variable \(X\).

In general, we define the \textbf{q-quantile} as the value \(x_{p}\) \textbf{under} which we have accumulated q*100\% of the probability

\[q=\sum_{i=1}^p f(x_i) = F (x_p)\]

\begin{itemize}
\tightlist
\item
  The \textbf{median} is value \(x_m\) such that \(q=0.5\)
\end{itemize}

\[F(x_{m})=0.5\]

\begin{itemize}
\tightlist
\item
  The \(0.05\)-quantile is the value \(x_{r}\) such that \(q=0.05\)
\end{itemize}

\[F(x_{r})=0.05\]

\begin{itemize}
\tightlist
\item
  The \(0.25\)-quantile is \textbf{first quartile} the value \(x_{s}\) such that \(q=0.25\)
\end{itemize}

\[F(x_{s})=0.25\]

\hypertarget{summary}{%
\section{Summary}\label{summary}}

\includegraphics{./figures/randomvarsum.JPG}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4348}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3043}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2609}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
quantity names
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
model (unobserved)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
data (observed)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
probability mass function // relative frequency & \(f(x_i)=P(X=x_i)\) & \(f_i=\frac{n_i}{N}\) \\
probability distribution // cumulative relative frequency & \(F(x_i)=P(X \leq x_i)\) & \(F_i=\sum_{k\leq i} f_k\) \\
mean // average & \(\mu=E(X)=\sum_{i=1}^M x_i f(x_i)\) & \(\bar{x}=\sum_{j=1}^N x_j/N\) \\
variance // sample variance & \(\sigma^2=V(X)=\sum_{i=1}^M (x_i-\mu)^2 f(x_i)\) & \(s^2=\sum_{j=1}^N (x_j-\bar{x})^2/(N-1)\) \\
standard deviation // sample sd & \(\sigma=\sqrt{V(X)}\) & \(s\) \\
variance about the origin // 2nd sample moment & \(E(X^2)=\sum_{i=1}^M x_i^2 f(x_i)\) & \(m_2= \sum_{j=1}^N x_j^2/n\) \\
\end{longtable}

Note that:

\begin{itemize}
\tightlist
\item
  \(i=1...M\) is an \textbf{outcome} of the random variable \(X\).
\item
  \(j=1...N\) is an \textbf{observation} of the random variable \(X\).
\end{itemize}

Properties:

\begin{itemize}
\tightlist
\item
  \(\sum_{i=1...N} f(x_i)=1\)
\item
  \(f(x_i)=F(x_i)-F(x_{i-1})\)
\item
  \(E(a\times X +b)= a\times E(X) +b\); for \(a\) and \(b\) scalars.
\item
  \(V(a\times X +b)= a^2\times V(X)\)
\item
  \(E(X^2)=V(X)+E(X)^2\)
\end{itemize}

\hypertarget{questions-3}{%
\section{Questions}\label{questions-3}}

\textbf{1)} For a probability mass function is not true that

\textbf{\(\qquad\)a:} the addition of their image values is 1; \textbf{\(\qquad\)b:} its values can be interpreted as probabilities of events;
\textbf{\(\qquad\)c:} it is always positive;
\textbf{\(\qquad\)d:} cannot take value 1;

\textbf{2)} A value of a random variable is

\textbf{\(\qquad\)a:} an observation of a random experiment; \textbf{\(\qquad\)b:} the frequency of an outcome of a random experiment;
\textbf{\(\qquad\)c:} an outcome of a random experiment;
\textbf{\(\qquad\)d:} a probability of an outcome;

\textbf{3)} The estimated value of a probability \(\hat{P_i}\) is equal to the probability \(P_i\) when the number of repetitions of the random experiment is

\textbf{\(\qquad\)a:} large; \textbf{\(\qquad\)b:} infinite;
\textbf{\(\qquad\)c:} small
\textbf{\(\qquad\)d:} zero;

\textbf{4)} If a probability mass function is symmetric around \(x=0\)

\textbf{\(\qquad\)a:} The mean is lower than the median; \textbf{\(\qquad\)b:} The mean is greater than the median;
\textbf{\(\qquad\)c:} The mean and the median are equal;
\textbf{\(\qquad\)d:} The mean and the median are different from 0;

\textbf{5)} The mean and variance

\textbf{\(\qquad\)a:} are inversely proportional; \textbf{\(\qquad\)b:} are expected values of functions of \(X\);
\textbf{\(\qquad\)c:} of a linear function are the linear function of the mean and the linear function of the variance;
\textbf{\(\qquad\)d:} change when we repeat the random experiment;

\hypertarget{exercises-3}{%
\section{Exercises}\label{exercises-3}}

\hypertarget{exercise-1-2}{%
\subsubsection{Exercise 1}\label{exercise-1-2}}

We place ballots with letters from a to f in an urn. Consider the drawing that gives \(0\) euros to the first two letters of the alphabet, \(1.5\) euros to the next two, and \(2\) and \(3\) euros to the following ones.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  What are the probability mass function and the probability distribution for the money prizes in the game?
\item
  What is the expected value of the prize? (R: 1.3)
\item
  What is the variance of the prize? (R: 1.13)
\item
  What is the probability of winning 2 or more euros? (R: 2/6)
\end{enumerate}

\hypertarget{exercise-2-2}{%
\subsubsection{Exercise 2}\label{exercise-2-2}}

Given the probability mass function

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
\(x\) & \(f(x)=P(X=x)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
10 & 0.1 \\
12 & 0.3 \\
14 & 0.25 \\
15 & 0.15 \\
17 & ? \\
20 & 0.15 \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  what is its expected value and standard deviation? (R: 14.2; 2.95)
\end{itemize}

\hypertarget{exercise-3-2}{%
\subsubsection{Exercise 3}\label{exercise-3-2}}

Given the probability distribution for a discrete variable \(X\)

\[
    F(x)= 
\begin{cases}
0, & x < -1 \\
0.2,& x \in [-1,0)\\
0.35,& x \in [0,1)\\
0.45,& x \in [1,2)\\
1,& x \geq 2\\
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  find \(f(x)\)
\item
  find \(E(X)\) and \(V(X)\) (R:1; 1.5)
\item
  what is the expected value and variance of \(Y=2X+3\) (R: 6)
\item
  what is the median and the first and third quartiles of \(X\)? (R:2,0,2)
\end{itemize}

\hypertarget{exercise-4-1}{%
\subsubsection{Exercise 4}\label{exercise-4-1}}

We are testing a system to transmit digital pictures. We first consider the experiment of sending \(3\) pixels and having as \textbf{possible} outcomes events such like \((0,1,1)\). This is the event of receiving the first pixel with no error, the second with error and third with error.

\begin{itemize}
\item
  List in one column the sample space of the random experiment.
\item
  In the a second column assign the random variable that counts the number of errors transmitted for each outcome
\end{itemize}

Consider that we have a totally noisy channel, that is any outcome of three pixels is equally likely.

\begin{itemize}
\item
  What is the probability of receiving \(0\), \(1\), \(2\), or \(3\) errors in the transmission of \(3\) pixels? (R: 1/8; 3/8; 3/8; 1/8)
\item
  Sketch the probability mass function for the number of errors
\item
  What is the expected value for the number of errors? (R:1.5)
\item
  What is its variance? (R: 0.75)
\item
  Sketch the probability distribution
\item
  What is the probability of transmitting at least 1 error? (R:7/8)
\end{itemize}

\hypertarget{continous-random-variables}{%
\chapter{Continous Random Variables}\label{continous-random-variables}}

\hypertarget{objective-2}{%
\section{Objective}\label{objective-2}}

In this chapter we will study continous random variables.

We will define the probability density function, its mean and variance and, similar to discrete random variables, we will define the probability distribution function.

\hypertarget{continuous-random-variables}{%
\section{Continuous random variables}\label{continuous-random-variables}}

In the last chapter we used the probabilities of discrete random variables to define the probability mass function \[f(x)=P(X= x)\]

Where the probability that the random variable takes the value \(x\) is understood as the value of its relative frequency, when the number of repetitions of the random experiment tends to infinity.

When we talked about continuous data, we saw that we had to transform them into discrete variables (bins) to produce relative frequency tables or histograms. Let's see how to define the probabilities of continuous variables taking these partitions into account.

\textbf{Example (misophonia)}

Let us reconsider the angle of convexity of patients with misophonia (Section 2.21). The angle of convexity of 123 patients was measured. We understood each measurement as the result of a random experiment that we repeated 123 times and that we could describe in a frequency table or in a histogram.

To do this, we redefine the results as small regular intervals (bins) and calculate the relative frequency of each interval.

\begin{verbatim}
##        outcome ni         fi
## 1 [-1.02,3.46]  8 0.06504065
## 2  (3.46,7.92] 51 0.41463415
## 3  (7.92,12.4] 26 0.21138211
## 4  (12.4,16.8] 20 0.16260163
## 5  (16.8,21.3] 18 0.14634146
\end{verbatim}

\hypertarget{relative-frequencies-3}{%
\section{relative frequencies}\label{relative-frequencies-3}}

Relative frequencies for the intervals are the probabilities when \(N \rightarrow \infty\)

\[f_i=\frac{n_i}{N} \rightarrow P(x_i \leq X  \leq x_i + \Delta x)\]

The probability depends now on the length of the bins \(\Delta x\). If we make the bins smaller and smaller then the frequencies get smaller and therefore

\[P(x_i \leq X  \leq x_i + \Delta x) \rightarrow 0\] when \(\Delta x \rightarrow 0\), because \(n_i \rightarrow 0\)

Let's see how the frequencies get smaller when we divide the range of \(X\) into \(20\) bins

\begin{verbatim}
##          outcome ni         fi
## 1  [-1.02,0.115]  2 0.01626016
## 2   (0.115,1.23]  0 0.00000000
## 3    (1.23,2.34]  3 0.02439024
## 4    (2.34,3.46]  3 0.02439024
## 5    (3.46,4.58]  2 0.01626016
## 6    (4.58,5.69]  4 0.03252033
## 7     (5.69,6.8] 11 0.08943089
## 8     (6.8,7.92] 34 0.27642276
## 9    (7.92,9.04] 12 0.09756098
## 10   (9.04,10.2]  4 0.03252033
## 11   (10.2,11.3]  3 0.02439024
## 12   (11.3,12.4]  7 0.05691057
## 13   (12.4,13.5]  2 0.01626016
## 14   (13.5,14.6]  6 0.04878049
## 15   (14.6,15.7]  4 0.03252033
## 16   (15.7,16.8]  8 0.06504065
## 17     (16.8,18]  4 0.03252033
## 18     (18,19.1]  9 0.07317073
## 19   (19.1,20.2]  3 0.02439024
## 20   (20.2,21.3]  2 0.01626016
\end{verbatim}

\hypertarget{probability-density-function}{%
\section{probability density function}\label{probability-density-function}}

We define a quantity at a point \(x\) that is the amount of probability per unit distance that we would find in an \textbf{infinitesimal} bin \(dx\) at \(x\)

\[f(x)= \frac{P(x\leq X \leq x+dx)}{dx}\]

\(f(x)\) is called the probability \textbf{density} function.

Therefore, the probability of observing \(x\) between \(x\) and \(x+dx\)
is given by

\[P(x\leq X \leq x+dx)= f(x) dx\]

\textbf{Definition}

For a continuous random variable \(X\), a \textbf{probability density} function is such that

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  The function is positive:
\end{enumerate}

\[f(x) \geq 0\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  The probability of observing \textbf{any} value is 1:
\end{enumerate}

\[\int_{-\infty}^{\infty} f(x) dx = 1\]
3) The probability of observing a value within an interval is the \textbf{area under the curve}:

\[P(a\leq X \leq b)=\int_{a}^{b} f(x) dx\]

The properties make sure that \(f(x)dx\) satisfy Kolmogorov's properties of a probability measure.

The probability density function is a step forward in the abstraction of probabilities: we add the continuous limit

\[dx \rightarrow 0\]

All the properties of probabilities are translated in terms of densities

\[\sum \rightarrow \int\]

Probability densities are mathematical quantities that do not necessarily represent random experiments.

A fundamental interest in statistics is to describe the densities that describe our particular random experiment.

\hypertarget{total-area-under-the-curve}{%
\section{Total area under the curve}\label{total-area-under-the-curve}}

\textbf{Example (raindrop fall)}

take the \textbf{probability density} that may describe the random variable that measures where a raindrop falls in a rain gutter of length \(100cm\).

\[
    f(x)= 
\begin{cases}
    \frac{1}{100},& \text{if } x\in (0,100)\\
    0,& otherwise 
\end{cases}
\]

Let us verify that the function satisfies the three properties of a probability density.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  it is evident from the definition that \(f(x) \geq 0\)
\item
  The probability of observing \textbf{anything} is the total \textbf{area under the curve}
\end{enumerate}

\(P(-\infty\leq X \leq \infty)= \int_{-\infty}^{\infty} f(x) dx = 100*0.01= 1\)

\includegraphics{_main_files/figure-latex/unnamed-chunk-43-1.pdf}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  The probability of observing \(x\) in an interval is the \textbf{area under the curve} within the interval
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(P(20 \leq X \leq 60) = \int_{20}^{60} f(x) dx = (60-20)*0.01=0.4\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-44-1.pdf}

\hypertarget{probabilities-of-continous-variables}{%
\section{Probabilities of continous variables}\label{probabilities-of-continous-variables}}

For continuous variables, we compute the probability that the variable is in a given interval. That is

\[P(a \leq X \leq b)\]
We saw that for continuous variables, the probability that the experiment gives us a particular real number is zero: \(P(X=a)=0\)

The \(P(a \leq X \leq b)\) is the area under the curve of \(f(x)\) between \(a\) and \(b\)

\begin{itemize}
\tightlist
\item
  \(P(a \leq X \leq b) = \int_{a}^{b} f(x) dx\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-45-1.pdf}

\hypertarget{probability-distribution-1}{%
\section{Probability distribution}\label{probability-distribution-1}}

The \textbf{probability distribution} \(F(c)\) defined as the accumulation of probability up to the outcome \(C\)

\(F(c) = P(X \leq c)\)

can be used to compute the probability \(P(a \leq X \leq b)\).

Consider:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  that the probability accumulated up to \(b\) is given by
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(F(b) = P(X \leq b)=\int_{-\infty}^bf(x)dx\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-46-1.pdf}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  that the probability accumulated up to \(a\) is
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(F(a) = P(X \leq a)\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-47-1.pdf}

Then the probability between \(a\) and \(b\) is given by the difference in the value of the probability distribution

\begin{itemize}
\tightlist
\item
  \(P(a\leq X \leq b) = \int_a^b f(x)dx=F(b)-F(a)\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-48-1.pdf}

\textbf{Definition}

The \textbf{probability distribution} of a continuous random variable is defined as

\[F(a)=P(X\leq a) =\int_{-\infty} ^a f(x)dx\]

and have the properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  It is between \(0\) and \(1\):
\end{enumerate}

\[F(-\infty)= 0 \,\, and \,\,F(\infty)=1\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  It always increases:
\end{enumerate}

\[F(a)\leq F(b)\]

if \(a\leq b\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  It can be used to compute probabilities:
\end{enumerate}

\[P(a \leq X \leq b)=F(b)-F(a)\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  It recovers the probability density:
\end{enumerate}

\[f(x)=\frac{dF(x)}{dx}\]

We use \textbf{probability distributions} to \textbf{compute probabilities} of a random variable within intervals, and its derivative is the probability density function.

\textbf{Example (raindrop fall)}

For the uniform density function:

\[
    f(x)= 
\begin{cases}
    \frac{1}{100},& \text{if } x\in (0,100)\\
    0,& otherwise 
\end{cases}
\]

We find that the probability distribution is

\[
    F(a)= 
\begin{cases}
    0,& a \leq 0 \\
    \frac{a}{100},& \text{if } a\in (0,100)\\
    1, & 100 \leq a \\
    \\
\end{cases}
\]

\newpage

\hypertarget{probability-plots}{%
\section{Probability plots}\label{probability-plots}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  We can plot the the probability of a random variable in an interval as the \emph{area} under the \textbf{density} curve. For instance
\end{enumerate}

\[P(20<X<60)\]

\includegraphics{_main_files/figure-latex/unnamed-chunk-49-1.pdf}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Equivalenty, we can plot the probability \(P(20<X<60)\) as the \emph{difference} in \textbf{distribution} values
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-50-1.pdf}

\hypertarget{mean}{%
\section{Mean}\label{mean}}

As in the discrete case, the \textbf{mean} measures the center of mass of probabilities

\textbf{Definition}

Suppose \(X\) is a continuous random variable with probability \textbf{density} function \(f(x)\). The mean or expected value of \(X\), denoted as \(\mu\) or \(E(X)\), is

\[\mu=E(X)=\int_{-\infty}^\infty x f(x) dx\]

It is the continuous version of the center of mass.

\textbf{Example (raindrop fall)}

The random variable with probability density

\[
    f(x)= 
\begin{cases}
    \frac{1}{100},& \text{if } x\in (0,100)\\
    0,& otherwise 
\end{cases}
\]

Has an expected value at

\[E(X)=50\]

\includegraphics{_main_files/figure-latex/unnamed-chunk-51-1.pdf}

\hypertarget{variance-1}{%
\section{Variance}\label{variance-1}}

As in the discrete case, the variance measures the dispersion of probabilities about the mean

\textbf{Definition}

Suppose \(X\) is a continuous random variable with probability density function \(f(x)\). The variance of \(X\), denoted as \(\sigma^2\) or \(V(X)\), is

\[\sigma^2=V(X)=\int_{-\infty}^\infty (x-\mu)^2 f(x) dx\]

It is the continuous version of the moment of inertia.

\hypertarget{functions-of-x}{%
\section{\texorpdfstring{Functions of \(X\)}{Functions of X}}\label{functions-of-x}}

In many occasions, we will be interested in outcomes that are function of the random variables. Perhaps, we are interested in the square of the elongation of a spring, or on the square root of the temperature of an engine.

\textbf{Definition}

For any function \(h\) of a random variable \(X\), with mass function \(f(x)\), its expected value is given by

\[E[h(X)]= \int_{-\infty}^{\infty} h(x) f(x)dx\]

From this definition we recover the same properties as in the discrete case

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  The mean of a linear function is the linear function fo the mean: \[E(a\times X +b)= a\times E(X) +b\] for \(a\) and \(b\) scalars.
\item
  The variance of a linear function of \(X\) is:\[V(a\times X +b)= a^2\times V(X)\]
\item
  The variance about the origin is the variance about the mean plus the mean squared: \[E(X^2)=V(X)+E(X)^2\]
\end{enumerate}

\hypertarget{exercises-4}{%
\section{Exercises}\label{exercises-4}}

\hypertarget{exercise-1-3}{%
\subsubsection{Exercise 1}\label{exercise-1-3}}

For the probability density

\[
    f(x)= 
\begin{cases}
    \frac{1}{100},& \text{if } x\in (0,100)\\
    0,& otherwise 
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  compute the mean (R:50)
\item
  compute variance using \(E(X^2)=V(X)+E(X)^2\) (R:100\^{}2/12)
\item
  compute \(P(\mu-\sigma\leq X \leq \mu+\sigma)\) (R: 0.57)
\item
  What are the first and third quartiles? (R: 25; 75)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-52-1.pdf}

\hypertarget{exercise-2-3}{%
\subsubsection{Exercise 2}\label{exercise-2-3}}

Given
\[
    f(x)= 
\begin{cases}
0, & x < 0 \\
ax, & x \in [0,3] \\
b, & x \in (3,5) \\
\frac{b}{3}(8-x),& x \in [5,8]\\
0, & x > 8 \\
\end{cases}
\]

\begin{itemize}
\item
  What are the values of \(a\) and \(b\) such that \(f(x)\) is a continous probability density function? (R: 1/15; 1/5)
\item
  what is the mean of \(X\)? (R:4)
\end{itemize}

\hypertarget{exercise-3-3}{%
\subsubsection{Exercise 3}\label{exercise-3-3}}

For the probability density

\[
    f(x)= 
\begin{cases}
    \lambda e^{-\lambda x},& \text{if } x \geq 0\\
    0,& otherwise 
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  Confirm that this is a probability density
\item
  Compute the mean (R: 1/\(\lambda\))
\item
  Compute the expected value of \(X^2\) (R: 2/\(\lambda^2\))
\item
  Compute variance (R: 1/\(\lambda^2\))
\item
  Find the probability distribution \(F(a)\) (R: \(1-exp(-\lambda a)\))
\item
  Find the median (R: \(\log{2}\)/\(\lambda\))
\end{itemize}

\hypertarget{exercise-4-2}{%
\subsubsection{Exercise 4}\label{exercise-4-2}}

Given the cumulative distribution for a random variable X

\[
    F(x)= 
\begin{cases}
0, & x  < -1 \\
\frac{1}{80}(17+16x-x^2),& x \in [-1,7)\\
1,& x \geq 7\\
\end{cases}
\]

compute:

\begin{itemize}
\tightlist
\item
  \(P(X>0)\) (R:63/80)
\item
  \(E(X)\) (R:1.93)
\item
  \(P(X>0|X<2)\) (R:28/45)
\end{itemize}

\hypertarget{discrete-probability-models}{%
\chapter{Discrete Probability Models}\label{discrete-probability-models}}

\hypertarget{objective-3}{%
\section{Objective}\label{objective-3}}

In this chapter we will see some probability mass functions that are used to describe common random experiments.

We will introduce the concept of parameter and thus parametric models.

In particular, we will discuss the uniform and Bernoulli probability functions and how they are used to derive the binomial and negative binomial probability functions.

\hypertarget{probability-mass-function}{%
\section{Probability mass function}\label{probability-mass-function}}

Let us remenber that a probability mass function of a \textbf{discrete random variable} \(X\) with possible values \(x_1 , x_2 , .. , x_M\) is \textbf{any function} such that

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  It Allows us to compute probabilities for all outcomes
\end{enumerate}

\[f(x_i)=P(X=x_i)\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  It is always positive:
\end{enumerate}

\[f(x_i)\geq 0\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  The probability of of obtaining anything in the random experiment is \(1\)
\end{enumerate}

\[\sum_{i=1}^M f(x_i)=1\]

We studied two important \textbf{properties:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  The mean as a measure of central tendency:
\end{enumerate}

\[E(X)= \sum_{i=1}^M x_i f(x_i)\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  The variance as a measure of dispersion:
\end{enumerate}

\[V(X)= \sum_{i=1}^M (x_i-\mu)^2 f(x_i)\]

\hypertarget{probability-model}{%
\section{Probability model}\label{probability-model}}

A \textbf{probability model} is a probability mass function that may represent the probabilities of a random experiment.

\textbf{Examples:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  The probability mass function defined by
\end{enumerate}

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
\(X\) & \(f(x)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(-2\) & \(1/8\) \\
\(-1\) & \(2/8\) \\
\(0\) & \(2/8\) \\
\(1\) & \(2/8\) \\
\(2\) & \(1/8\) \\
\end{longtable}

Represents the probability of drawing \textbf{one} ball from an urn where there are two balls with labels: \(-1, 0, 1\) and one ball with labels: \(-2, 2\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  \(f(x)=P(X=x)=1/6\) represents the probability of the outcomes of \textbf{one} throw of a dice.
\end{enumerate}

\hypertarget{parametric-models}{%
\section{Parametric models}\label{parametric-models}}

When we have a random experiment with \(M\) possible outcomes, we need to find \(M\) numbers to determine the probability mass function. As in the previous example 1, we needed \(5\) values in the column \(f(x)\) of the probability table.

However, \textbf{in many cases}, we can formulate probability functions \(f(x)\) that depend on \textbf{very few} numbers only. As in the previous example 2, we only needed to know how many possible outcomes the throw of a dice has.

\textbf{Example (classical probability):}

A random experiment with \(M\) equally likely outcomes has a probability mass function:
\[f(x)=P(X=x)=1/M\]

We only need to know \(M\).

The numbers we \textbf{need to know} to fully determine a probability function are called \textbf{parameters}.

\hypertarget{uniform-distribution-one-parameter}{%
\section{Uniform distribution (one parameter)}\label{uniform-distribution-one-parameter}}

The previous example is the classical interpretation of probability, and defines our first parametric model.

\textbf{Definition}

A random variable \(X\) with outcomes \(\{1,...M\}\) has a discrete \textbf{uniform distribution} if all its \(M\) outcomes have the same probability

\[f(x)=\frac{1}{M}\]

\(M\) is the natural parameter of the model. Once we define \(M\) for an experiment, we choose a particular probability mass function. The function above is really a family of functions that depend on \(M\): \(f(x; M)\).

The mean and variance of a variable that follows a uniform distribution are:

\[E(X)= \frac{M+1}{2}\]

and

\[V(X)= \frac{M^2-1}{12}\]

Note: \(E(X)\) and \(V(X)\) are also \textbf{parameters}. If we know any of them then we can fully determine the distribution. For instance:

\[f(x)=\frac{1}{2E(X)-1}\]
Let's look at some probability mass functions in the family of uniform parametric models:

\includegraphics{_main_files/figure-latex/unnamed-chunk-53-1.pdf}

\hypertarget{uniform-distribution-two-parameters}{%
\section{Uniform distribution (two parameters)}\label{uniform-distribution-two-parameters}}

Let's introduce a new \textbf{uniform} probability model with \textbf{two parameters}: The minimum and maximum outcomes.

If the random variable takes values in \(\{a, a+1, ...b\}\), where \(a\) and \(b\) are integers and all the outcomes are equally probable then

\[f(x)=\frac{1}{b-a+1}\]

as \(M=b-a+1\).

We then say that \(X\) distributes uniformly between \(a\) and \(b\) and write

\[X \rightarrow Unif(a,b)\]

\textbf{Properties:}

If \(X\) distributes uniformly between \(a\) and \(b\)

\[X \rightarrow Unif(a,b)\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Its mean is
\end{enumerate}

\[E(X)= \frac{b+a}{2}\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Its variance is
\end{enumerate}

\[V(X)= \frac{(b-a+1)^2-1}{12}\]

To prove this change variables \(X=Y+a-1\), \(y \in \{1,...M\}\).

\textbf{Probability mass functions}

Let's look at some probability mass functions in the family of uniform parametric models:

\includegraphics{_main_files/figure-latex/unnamed-chunk-54-1.pdf}

\textbf{Example (school classes):}

What is the probability of observing a child of a particular age in a primary school (if all classes have the same amount of children)?

From the set up of the experiment we know: \(a=6\) and \(b=11\) then

\[X \rightarrow Unif(a=6, b=11)\] that is

\[f(x)=\frac{1}{6}\] for \(x\in \{6,7,8,9,10,11\}\), and \(0\) otherwise.

The mean and variance for this probability mass function is:

\begin{itemize}
\tightlist
\item
  \(E(X)=8.5\)
\item
  \(V(X)=2.916667\)
\end{itemize}

Remember that

\begin{itemize}
\item
  The expected value is the \textbf{mean} \(\mu=8.5\)
\item
  The \textbf{standard deviation} \(\sigma=1.707825\) is the average distance from the mean and is computed from the square root of the variance.
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-55-1.pdf}

\textbf{Parameters and Models:}

A \textbf{model} is a particular function \(f(x)\) that \textbf{describes} our experiment.

If the model is a \textbf{known} function that depends on a few parameters then changing the value of the parameters we produce a \textbf{family of models}: \(f(x; a,b)\).

Knowledge of \(f(x)\) is reduced to the knowledge of the value of the parameters \(a\), \(b\).

Ideally, the model and the parameters are \textbf{interpretable}.

In our example, \(a\) represents the the minimum age at school and \(b\) the maximum age. They can be considered as the \textbf{physical properties} of the experiment.

\hypertarget{bernoulli-trial}{%
\section{Bernoulli trial}\label{bernoulli-trial}}

Let's try to advance from the equal probability case and suppose a model with only two possible outcomes (\(A\) and \(B\)) that have \textbf{unequal} probabilities

\textbf{Examples:}

\begin{itemize}
\item
  Writing down the sex of a patient who goes into an emergency room of a hospital (\(A:male\) and \(B:female\)).
\item
  Recording whether a manufactured machine is defective or not (\(A:defective\) and \(B:not \,\, defective\)).
\item
  Hitting a target (\(A:success\) and \(B:failure\)).
\item
  Transmitting one pixel correctly (\(A:yes\) and \(B:no\)).
\end{itemize}

In these examples, the probability of outcome \(A\) is usually \textbf{unknown}.

\textbf{Probability model:}

We will introduce the probability of an outcome (\(A\)) as the \textbf{parameter} of the model. The model can be written in different forms

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  As a probability table:
\end{enumerate}

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
\(Outcome\) & \(P_i\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(A\) & \(p\) \\
\(B\) & \(1-p\) \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  \(i \in \{A,B\}\)
\item
  outcome A (success): has probability \(p\) (parameter)
\item
  outcome B (failure): has a probability \(1-p\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  As a probability mass function of the random variable \(K\) taking values \(\{0, 1\}\) for \(B\) and \(A\), respectively.
\end{enumerate}

\[
    f(k)= 
\begin{cases}
    1-p,&  k=0\, (event\, B)\\
    p,& k=1\, (event\, A) 
\end{cases}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  As a concise probability mass function
\end{enumerate}

\[f(k; p)=p^k(1-p)^{1-k} \]

for \(k=(0,1)\).

We then say that \(X\) follows a Bernoulli distribution with parameter \(p\)
\[X \rightarrow Bernoulli(p)\]

\textbf{Properties:}

If \(X\) follows a Bernoulli distribution then

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  its mean is
\end{enumerate}

\[E(K)=p\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  its variance is
\end{enumerate}

\[V(K)=(1-p)p\]

Note that the probability of the outcome \(A\) is the parameter \(p\)
which is the same as its value at \(1\): \(f(1)=P(X=1)\). The parameter fully determines the probability mass function, including its mean and variance.

Let's look at some probability mass functions in the family of uniform parametric models:

\includegraphics{_main_files/figure-latex/unnamed-chunk-56-1.pdf}

\hypertarget{binomial-experiment}{%
\section{Binomial experiment}\label{binomial-experiment}}

When we are interested in predicting \textbf{absolute frequencies} when we know the parameter \(p\) of particular Bernoulli trial, we

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \textbf{repeat} the Bernoulli trial \(n\) times and count how many times we obtained \(A\) using the absolute frequency of \(A\): \(N_A\).
\item
  define a \textbf{random variable} \(X=N_A\) taking values \(x \in {0,1,...n}\)
\end{enumerate}

When we repeat \(n\) times a Bernoulli trial, we obtain one value for \(n_A\). If we perform other \(n\) Bernoulli trials then \(n_A\) changes its value. Therefore, \(X=N_A\) is a random variable and \(x=n_A\) is its observation.

\textbf{Examples (Some binomial experiments):}

\begin{itemize}
\item
  Writing down the sex of \(n=10\) patients who go into an emergency room of a hospital. What is the probability that \(X=9\) patients are men when \(p=0.8\)?
\item
  Trying \(n=5\) times to hit a target (\(A:success\) and \(B:failure\)). What is the probability that I hit the target \(X=5\) times when I usually hit it \(25\%\) of the times (\(p=0.25\))?
\item
  Transmitting \(n=100\) pixels correctly (\(A:yes\) and \(B:no\)). What is the probability that \(X=2\) pixels are errors, when the probability of error is \(p=0.1\)?
\end{itemize}

\hypertarget{binomial-probability-function}{%
\section{Binomial probability function}\label{binomial-probability-function}}

Let us suppose that \textbf{we know} the real value of the parameter of the Bernoulli trial \(p\).

When we repeat a Bernoulli trial and stop at \(n\), is the value \(x\) that we obtain common or rare? what is its probability mass function \(P(X=x)=f(x)\)?

\textbf{Example (transmission of pixels):}

What is the probability of observing \(X=x\) errors when transmitting \(n=4\) pixels, if the probability of an error is \(p\)?

Let us consider that

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  A random variable of the \textbf{transmission experiment} is the vector \[(K_1, K_2, K_3, K_4)\] where one observation may be \((K_1=0, K_2=1, K_3=0, K_4=1)\) or \((0, 1, 0, 1)\).
\item
  Each \[K_i \rightarrow Bernoulli(p)\] \(k_i \in \{0, 1\}\)
\item
  \(X=N_A\) can be computed as the sum \[X=\sum_{i=1}^4 K_i\] \(x\in \{0,1,2,3,4\}\). For example \(X=2\) for the outcome \((0, 1, 0, 1)\).
\end{enumerate}

Now let's see the probabilities of some \textbf{error events} and then we will generalize them.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  What is the probability of observing \(4\) \textbf{errors}?
\end{enumerate}

The probability of observing \(4\) errors is the probability of observing an error in the \(1^{st}\) \textbf{and} \(2^{nd}\) \textbf{and} \(3^{rd}\) \textbf{and} \(4^{th}\) pixel:

\[P(X=4)=P(1,1,1,1)=p*p*p*p=p^4\]

because \(K_i\) are \textbf{independent}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  What is the probability of observing \(0\) \textbf{errors}?
\end{enumerate}

The probability of \(0\) errors is the joint probability of observing \textbf{no error} in \textbf{any} transmission:

\[P(X=0)=P(0,0,0,0)=(1-p)(1-p)(1-p)(1-p)=(1-p)^4\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  What is the probability of observing \(3\) \textbf{errors}?
\end{enumerate}

The probability of \(3\) errors is the \textbf{addition} of probability of observing \(3\) errors in \textbf{different events}:

\[P(X=3)=P(0,1,1,1)+P(1,0,1,1)+P(1,1,0,1)+P(1,1,1,0)=4p^3(1-p)^1\]
because all off these events are \textbf{mutually exclusive}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  Therefore the probability of \(x\) \textbf{errors} is
\end{enumerate}

\[
    f(x)= 
\begin{cases}
    1*p^0(1-p)^4,&  x=0 \\
    4*p^1(1-p)^3,&  x=1 \\
    6*p^2(1-p)^2,&  x=2 \\
    4*p^3(1-p)^1,&  x=3 \\
    1*p^4(1-p)^0,&  x=4 \\
\end{cases}
\]

or more shortly

\[f(x)=\binom 4 x p^x(1-p)^{4-x}\]
for \(x=0,1,2,3,4\)

where \(\binom 4 x\) is the number of \textbf{possible outcomes} (transmissions of \(4\) pixels) with \(x\) errors.

\textbf{Definition:}

The \textbf{binomial probability} function is the probability mass function of observing \(x\) outcomes of type \(A\) in \(n\) independent Bernoulli trials, where \(A\) has the same probability \(p\) in each trial.

The function is given by

\(f(x)=\binom n x p^x(1-p)^{n-x}\), \(x=0,1,...n\)

\(\binom n x= \frac{n!}{x!(n-x)!}\) is called \textbf{the binomial coefficient} and gives the number of ways one can obtain \(x\) events of type \(A\) in a set of \(n\).

When a variable \(X\) has a binomial probability function we say it distributes binomially and write

\[X\rightarrow Bin(n,p)\]

where \(n\) and \(p\) are parameters.

Let's look at some probability mass functions in the family of binomial parametric models:

\includegraphics{_main_files/figure-latex/unnamed-chunk-57-1.pdf}

\textbf{Properties:}

If a random variable \(X\rightarrow Bin(n,p)\) then

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  its mean is
\end{enumerate}

\[E(X)=np\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  its variance is
\end{enumerate}

\[V(X)=np(1-p)\]

These properties can be demonstrated by the fact that \(X\) is the sum of \(n\) independent Bernoulli variables. Therefore,

\(E(X)=E(\sum_{i=1}^n K_i)=np\)

and

\(V(X)=V(\sum_{i=1}^n K_i)=n(1-p)p\)

\textbf{Example (transmission of pixels):}

\begin{itemize}
\item
  The expected value for the number of errors in the transmission of \(4\) pixels is \(np=4*0.1=0.4\) when the probability of an error is \(0.1\).
\item
  The variance is \(n(1-p)p=0.36\)
\item
  What is the probability of observing \(4\) errors?
\end{itemize}

Since we are repeating a Bernoulli trial \(n=4\) times and counting the number of events of type \(A\) (errors), when \(P(A)=p=0.1\) then

\[X \rightarrow Bin(n=4, p=0.1)\]
That is \[f(x)=\binom 4 x 0.1^x(1-0.1)^{4-x}\]

\(P(X=4)=f(4)=\binom 4 4 0.1^4 0.9^{0}=0.1^4=10^{-4}\)

In R dbinom(4,4,0.1)

\begin{itemize}
\tightlist
\item
  What is the probability of observing \(2\) errors?
\end{itemize}

\(P(X=2)=\binom 4 2 0.1^2 0.9^2=0.0486\)

In R dbinom(2,4,0.1)

\textbf{Example (opinion polls):}

\begin{itemize}
\tightlist
\item
  What is the probability of observing \textbf{at most} \(8\) voters of the ruling party in an election poll of size \(10\), if the probability of a positive vote for the party is \(0.9\)
\end{itemize}

For this case

\[X \rightarrow Bin(n=10, p=0.9)\]

That is \[f(x)=\binom {10} x 0.9^x(0.1)^{4-x}\]

We want to compute:
\(P(X\le 8)=F(8)= \sum_{i=1..8} f(x_i)=0.2639011\)

in R pbinom(8,10, 0.9)

\hypertarget{negative-binomial-probability-function}{%
\section{Negative binomial probability function}\label{negative-binomial-probability-function}}

Now let us imagine that we are interested in counting the well-transmitted pixels before a \textbf{given number} of errors occur. Say we can \textbf{tolerate} \(r\) errors in transmission.

Our random experiment is now: Repeat Bernoulli trials until we observe the outcome \(A\) appears \(r\) times.

The outcome of the experiment is the number of events \(n_B=y\)

We are interested in finding the probability of observing a particular number of events \(B\), \(P(Y=y)\), where \(Y=N_B\) is the random variable.

\textbf{Example (transmission of pixels):}

What is the probability of observing \(y\) well-transmitted (\(B\)) pixels before \(r\) errors (\(A\))?

Let's first find the probability of \textbf{one particular} \textbf{transmission event} with \(y\) number of correct pixels (\(B\)) and \(r\) number of errors (\(A\)).

\[(0,0,1,., 0,1,...0,1)\]

where we consider that there are \(y\) zeros and \(r\) ones. Therefore, we observe \(y\) correct pixels in a total of \(y + r\) pixels.

The probability of this event is:

\[P(0,0,1,., 0,1,...0,1)=p^r(1-p)^y\]

Remember that \(p\) is the probability of error (\(A\)).

How many \textbf{transmissions events} can have \(y\) correct pixels (0's) before \(r\) errors (1's)?

Note that

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  The last bit is fixed (marks the end of transmission)
\item
  The total number of ways in which \(y\) number of zeros can be allocated in \(y + r-1\) pixels (the last pixel is fixed with value 1) is: \(\binom {y + r-1} y\)
\end{enumerate}

Therefore, the probability of observing \(y\) 1's before \(r\) 0's (each 1 with probability \(p\)) is

\[P(Y=y)=f(y)=\binom {y+r-1} y p^r(1-p)^y\]

for \(y=0,1,...\)

We then say that \(Y\) follows a negative binomial distribution and we write

\[Y\rightarrow NB(r,p)\]

where \(r\) and \(p\) are parameters representing the tolerance and the probability of a single error (event \(A\)).

\textbf{Properties:}

A random variable \(Y\rightarrow NB(r,p)\) has

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  mean \[E(Y)= r\frac{1-p}{p}\]
\item
  and variance \[V(Y)= r\frac{1-p}{p^2}\]
\end{enumerate}

Let's look at some probability mass functions in the family of negative binomial parametric models:

\includegraphics{_main_files/figure-latex/unnamed-chunk-58-1.pdf}

\textbf{Example (website)}

A website has three servers. One server operates at a time and only when a request fails another server is used.

If the probability of failure for a request is known to be \(p=0.0005\) then

\begin{itemize}
\tightlist
\item
  what is the expected number of successful requests before the three computers fail?
\end{itemize}

Since we are repeating a Bernoulli trial until \(r=3\) events of type \(A\) (failure) are observed (each with \(P(A)=p=0.0005\)) and are counting the number of events of type \(B\) (successful requests) then

\[Y \rightarrow NB(r=3, p=0.0005)\]

Therefore, the expected number of requests before the system fails is:

\(E(Y)=r\frac{1-p}{p}=3\frac{1-0.0005}{0.0005}=5997\)

Note that there are actually \(6000\) trials.

\begin{itemize}
\tightlist
\item
  What is the probability of dealing with at most \(5\) successful requests before the system fails?
\end{itemize}

We therefore want to compute the probability distribution at \(5\):

\(F(5)=P(Y\leq 5)=\Sigma_{y=0}^5 f(y)\)

\(=\sum_{y=0}^5\binom {y+2} y 0.0005^r0.9995^y\)

\(=\binom {2} 0 0.0005^3 0.9995^0 +\binom {3} 1 0.0005^3 0.9995^1\)

\(+\binom {4} 2 0.0005^3 0.9995^2 +\binom {5} 3 0.0005^3 0.9995^3\)

\(+\binom {6} 4 0.0005^3 0.9995^4 +\binom {7} 5 0.0005^3 0.9995^5\)

\(= 6.9\times 10^{-9}\)

In R this is computed with pnbinom(5,3,0.0005)

\textbf{Examples}

\begin{itemize}
\tightlist
\item
  What is the probability of observing \(10\) correct pixels before \(2\) errors, if the probability of an error is \(0.1\)?
\end{itemize}

\(f(10; r=2, p=0.1)=0.03835463\)

in R dnbinom(10, 2, 0.1)

\begin{itemize}
\tightlist
\item
  What is the probability that \(2\) girls enter the class before \(4\) boys if the probability that a girl enters is \(0.5\)?
\end{itemize}

\(f(2; r=4, p=0.5)=0.15625\)

in R dnbinom(2, 4, 0.5)

\hypertarget{geometric-distribution}{%
\section{Geometric distribution}\label{geometric-distribution}}

We call \textbf{geometric distribution} to the \textbf{negative binomial} distribution with \(r=1\)

The probability of observing \(B\) events before observing the \textbf{first} event of type \(A\) is

\[P(Y=y)=f(y)= p(1-p)^y\]

\[Y\rightarrow Geom(p)\]
which has

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  mean \[E(Y)= \frac{1-p}{p}\]
\item
  and variance \[V(Y)= \frac{1-p}{p^2}\]
\end{enumerate}

\hypertarget{hypergeometric-model}{%
\section{Hypergeometric model}\label{hypergeometric-model}}

The \textbf{hypergeometric model} arises when we want to count the number of events of type \(A\) that are drawn from a finite population.

The general model is to consider \(N\) total balls in a urn. Mark \(K\) with label \(A\) and \(N-K\) with label \(B\). Take out \(n\) balls one by one with no replacement in the urn, and then count how many \(A\)'s you obtained.

The \textbf{Binomial} model can be derived from the \textbf{Hypergeometric} model when we consider that either \(N\) is infinite, or that every time we draw a ball we replace it back in the urn.

\textbf{Example (chickenpox):}

A school of \(N=600\) children has an epidemic of chickenpox. We tested \(n=200\) children and observed that \(x=17\) were positive. If we knew that a total of \(K=64\) were really infected in the school, what is the probability of our observation?

\textbf{Definition:}

The probability of obtaining \(x\) cases (type \(A\)) in a sample of \(n\) drawn from a population of \(N\) where \(K\) are cases (type \(A\)).

\(P(X=x)=P(one\,sample) \times (Number\, of\, ways\, of\, obtaining\, x)\)

\[=\frac{1}{\binom N n}\binom K x \binom {N-K} {n-x}\]

where \(k \in \{\max(0, n+K-N), ... \min(K, n) \}\)

We then say that \(X\) follows a hypergeometric distribution and write

\[X \rightarrow Hypergeometric(N,K,n)\]
The hypergeometric model has three parameters.

\textbf{Properties:}

If \(X \rightarrow Hypergeometric(N,K,n)\) then it has

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  mean \[E(X) =  n  \frac{K}{N} = np\]
\item
  and variance \[V(X) = np(1-p)\frac{N-n}{N-1}\]
\end{enumerate}

when \(p=\frac{K}{N}\) is the proportion of hepatitis C in a population of size \(N\). Note that when \(N \rightarrow \infty\) then we recover the binomial properties.

Let's look at some probability mass functions in the family of hypergeometric parametric models:

\includegraphics{_main_files/figure-latex/unnamed-chunk-59-1.pdf}

\includegraphics{_main_files/figure-latex/unnamed-chunk-60-1.pdf}

\textbf{Example (chickenpox):}

\begin{itemize}
\tightlist
\item
  what is the probability of infections less or equal than \(17\) in a sample of \(200\), drawn from a population of \(600\) where \(64\) are infected?
\end{itemize}

The probability that we need to compute is
\(P(X \leq 17)=F(17)\)

where \(X \rightarrow Hypergeometric(N=600,K=64,n=200)\)

in R phyper(17, 64, 600-64, 200)=0.140565

The solution is the addition of the blue needles in the plot.

\includegraphics{_main_files/figure-latex/unnamed-chunk-61-1.pdf}

\hypertarget{questions-4}{%
\section{Questions}\label{questions-4}}

\textbf{1)} What is the expected value and the variance of the number of failures in \(100\) prototypes, when the probability of a failure is \(0.25\)

\textbf{\(\qquad\)a:} \(0.25\), \(0.1875\);
\textbf{\(\qquad\)b:} \(25\), \(0.1875\);
\textbf{\(\qquad\)c:} \(0.25\), \(18.75\);\\
\textbf{\(\qquad\)d:} \(25\), \(18.75\)

\textbf{2)} The number of available tables at lunch time in a restaurant is better described by which probability model

\textbf{\(\qquad\)a:} Binomial;
\textbf{\(\qquad\)b:} Uniform;
\textbf{\(\qquad\)c:} Negative Binomial;\\
\textbf{\(\qquad\)d:} Hypergeometric

\textbf{3)} The expected value of a Binomial distribution is not

\textbf{\(\qquad\)a:} \(n\) times the expected value of a Bernoulli;
\textbf{\(\qquad\)b:} the expected value of a Hypergeometric, when the population is very big;
\textbf{\(\qquad\)c:} \(np\);\\
\textbf{\(\qquad\)d:} the limit of the relative frequency when the number of repetitions is large

\textbf{4)} Opinion polls for the USA 2022 election give a probability of \(0.55\) that a voter favors the republican party. If we conduct our own poll and ask 100 random people on the street, How would you compute the probability that in our poll democrats win the election?

\textbf{\(\qquad\)a:}pbinom(x=49, n=100, p=0.55)=0.13;
\textbf{\(\qquad\)b:}1-pbinom(x=49, n=100, p=0.55)=0.86;
\textbf{\(\qquad\)c:}pbinom(x=51, n=100, p=0.45)=0.90; \textbf{\(\qquad\)d:}1-pbinom(x=51, n=100, p=0.45)=0.095

\textbf{5)} In an exam a student chooses at random one of the four answers that he does not know. If he doesn't know \(10\) questions what is the probability that more than \(5\) (\(>5\)) questions are correct?

\textbf{\(\qquad\)a:}dbinom(x=5, n=10, p=0.25)\textasciitilde{} 0.05; \textbf{\(\qquad\)b:}pbinom(x=5, n=10, p=0.75)\textasciitilde{} 0.07; \textbf{\(\qquad\)c:}dbinom(x=5, n=10, p=0.75)\textasciitilde{} 0.05; \textbf{\(\qquad\)d:}1-pbinom(x=5, n=10, p=0.25)\textasciitilde{} 0.02

\hypertarget{exercises-5}{%
\section{Exercises}\label{exercises-5}}

\hypertarget{exercise-1-4}{%
\subsubsection{Exercise 1}\label{exercise-1-4}}

If the 25\% of screws produced by a machine are defective, determine the probability that, out of 5 randomly chosen screws:

\begin{itemize}
\tightlist
\item
  none of the screws is defective (R: 0.2373)
\item
  1 screw is defective (R: 0.3955)
\item
  2 screws are defective (R: 0.2636)
\item
  at most 2 screws are defective (R: 0.8964)
\end{itemize}

\hypertarget{exercise-2-4}{%
\subsubsection{Exercise 2}\label{exercise-2-4}}

In a population, the probability that a baby boy is born is \(p=0.51\). Consider a family of 4 children

\begin{itemize}
\tightlist
\item
  What is the probability that a family has only one boy?(R: 0.240)
\item
  What is the probability that a family has only one girl?(R: 0.259)
\item
  What is the probability that a family has only one boy or only one girl?(R: 0.4999)
\item
  What is the probability that the family has at least two boys?(R: 0.7023)
\item
  What is the number of children that a family should have such that the probability of having at least one girl is more than \(0.75\)?(R:\(n=3>\log(0.25)/\log(0.51)\))
\end{itemize}

\hypertarget{exercise-3-4}{%
\subsubsection{Exercise 3}\label{exercise-3-4}}

A search engine fails to retrieve information with a probability \(0.1\)

\begin{itemize}
\item
  If we system receives \(50\) search requests, what is the probability that the system fails to answer three of them?(R:0.1385651)
\item
  What is the probability that the engine successfully completes \(15\) searches before the first failure?(R:0.020)
\item
  We consider that a search engine works sufficiently well when it is able to find information for moe than \(10\) requests for every \(2\) failures. What is the probability that in a reliability trial our search engine is satisfactory?(R: 0.697)
\end{itemize}

\hypertarget{poisson-and-exponential-models}{%
\chapter{Poisson and Exponential Models}\label{poisson-and-exponential-models}}

\hypertarget{objective-4}{%
\section{Objective}\label{objective-4}}

In this chapter we will see two tightly related probability models: the \textbf{Poisson} and the \textbf{exponential} models.

The Poisson model is for discrete random variables while the exponential function is \textbf{continuous} random variables

\hypertarget{discrete-probability-models-1}{%
\section{Discrete probability models}\label{discrete-probability-models-1}}

In the previous chapter we built complex models from simple ones. At each stage, we introduced some novel concept:

\textbf{Uniform}: Classical interpretation of probability
\(\downarrow\)
\textbf{Bernoulli}: Introduction of a \textbf{parameter} \(p\) (family of models)
\(\downarrow\)
\textbf{Binomial}: Introductio of the \textbf{Repetition} of a random experiment (\(n\)-times Bernoulli trials)
\(\downarrow\)
\textbf{Poisson}: Repetition of random experiment within a continuous interval, having \textbf{no control} on when/where the Bernoulli trial occurs.

The last stage is the Poisson process that describes a the repetition of a random experiment with additional randomness at the time of repetition.

\hypertarget{poissson-experiment}{%
\section{Poissson experiment}\label{poissson-experiment}}

Imagine that we are observing events that \textbf{depend} on time or distance \textbf{intervals}.

for example:

\begin{itemize}
\tightlist
\item
  cars arriving at a traffic light
\item
  getting messages on your mobile phone
\item
  impurities occurring at random in a copper wire
\end{itemize}

Suppose that the events are outcomes of \textbf{independent} Bernoulli trials each appearing randomly on a continuous interval, and we want to \textbf{count} them.

What is the probability of observing \(X\) events in an interval's unit (time or distance)?

\textbf{Example (Impurities in a wire):}

Imagine that some impurities deposit randomly along a copper wire. You want to count the number of impurities in one given centimeter of the wire (\(X\)).

Consider that you know that on average there are \(10\) impurities per centimeter \(\lambda=10/cm\).

What is the probability of observing \(X=5\) impurities in one specimen of one centimeter in particular?

\hypertarget{poisson-probability-mass-function}{%
\section{Poisson probability mass function}\label{poisson-probability-mass-function}}

To calculate the probability mass function \(f(x)=P(X=x)\) of the previous example we divide the centimeter into micrometers (\(0.0001cm\)).

Micrometers are small enough so

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  either there is or there is not an impurity in each micrometer
\item
  each micrometer can be then be considered a \textbf{Bernoulli trial}
\end{enumerate}

\includegraphics{./figures/Pois.JPG}

\textbf{From the Binomial to the Poisson probability function}

The probability of observing \(X\) impurities in \(n=10,000\mu\) (1cm) approximately follows a binomial distribution

\[f(x) \sim \binom n x p^x(1-p)^{n-x}\]

where \(p\) is the probability of finding an impurity in a micrometer.

Since the expected value of a Binomial variable is:
\(E(X)=np\). This is the average number of impurities per 1cm or \(\lambda=np\). Therefore, substitute \(p=\lambda/n\)

\[f(x) \sim \binom n x \big(\frac{\lambda}{n}\big)^x(1-\frac{\lambda}{n})^{n-x}\]

Since, there \textbf{could} still be two impurities in a micrometer, we need to increase the partition of the wire and \(n \rightarrow \infty\).

Then in the limit:

\[f(x)= \frac{e^{-\lambda}\lambda^x}{x!}\]

Where \(\lambda\) is constant because it is the density of impurities per centimeter, a \textbf{physical property} of the system. \(\lambda\) is therefore the \textbf{parameter} of the probability model.

\textbf{Derivation details:}

For \(f(x) \sim \binom n x \big(\frac{\lambda}{n}\big)^x(1-\frac{\lambda}{n})^{n-x}\)

in the limit (\(n \rightarrow \infty\))

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \(\frac{1}{n^x}\binom n x =\frac{1}{n^x}\frac{n!}{x! (n-x)!}=\frac{(n-x)!(n-x+1)...(n-1)n}{n^x x! (n-x)!}=\frac{n(n-1)..(n-x+1)}{n^x x!} \rightarrow \frac{1}{x!}\)
\item
  \((1-\frac{\lambda}{n})^{n} \rightarrow e^{-\lambda}\) (definition of exponential)
\item
  \((1-\frac{\lambda}{n})^{-x} \rightarrow 1\)
\end{enumerate}

Putting everything together then:

\(f(x)= \frac{e^{-\lambda}\lambda^x}{x!}\)

\textbf{Definition}

Given

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  an interval in the real numbers
\item
  counts occur at random in the interval
\item
  the average number of counts on the interval is known (\(\lambda\))
\item
  if one can find a small regular partition of the interval such that each of them can be considered Bernoulli trials.
\end{enumerate}

Then the random variable \(X\) that counts events across the interval is a \textbf{Poisson} variable with probability mass function\\
\[f(x)= \frac{e^{-\lambda}\lambda^x}{x!}, \lambda>0\]

\textbf{Properties:}
When \(X \rightarrow Poiss(\lambda)\) it has

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  mean \[E(X)= \lambda\]
\item
  and variance \[V(X)= \lambda\]
\end{enumerate}

\textbf{Examples}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  What is the probability of receiving 4 emails in an hour, when the average number of emails in an hour is \(1\)?
\end{enumerate}

We have that the variable is Poisson: \(X \rightarrow Poiss(\lambda)\) with \(\lambda=1\) and its probability mass function is:

\[f(x)= \frac{e^{-1}1^x}{x!}\]
Therefore the probability that the variable takes value 4 is \(P(X=4)\):

\(f(4; \lambda=1)= \frac{e^{-1}1^4}{4!}=0.01532831\)

in R dpois(4,1)

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  What is the probability of receiving 4 emails in \textbf{three hours}, when the average number of emails in an hour is \(1\)?
\end{enumerate}

The unit in which we do the counts has changed from 1 hour to 2 hours, so we have to \textbf{re-scale} \(\lambda\). If before the average number of emails in one hour was \(\lambda=1\), the average number of emails in three hours is now 3: \(\lambda_{3h}=3\lambda_{1h}=3*1=3\)

We have that the variable is Poisson: \(X \rightarrow Poiss(\lambda_{3h})\) with \(\lambda_{3h}=3\) and its probability mass function is:

\[f(x)= \frac{e^{-3}3^x}{x!}\]
Therefore the probability that the variable takes value 4 is \(P(X=4)\):

\(f(4; \lambda=3)= \frac{e^{-3}3^4}{4!}=0.1680314\)

in R dpois(4,3)

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  What is the probability of counting \textbf{at least} \(10\) cars arriving at a toll booth in one minute, when the average number of cars arriving at a toll booth in one minute is \(5\);
\end{enumerate}

We have that the variable is Poisson: \(X \rightarrow Poiss(\lambda)\) with \(\lambda=5\) and its probability mass function is:

\[f(x)= \frac{e^{-5}5^x}{x!}\]

\(P(X\geq 10)=1-P(X < 10)=1-P(X \leq 9)=1-F(9; \lambda=5)=1-\sum_{x=0, ...10}f(x; \lambda=5)=0.03182806\)

in R 1-ppois(9,5)

Let's look at some probability mass functions in the family of parametric Poisson models:

\includegraphics{_main_files/figure-latex/unnamed-chunk-62-1.pdf}

\hypertarget{continuous-probability-models}{%
\section{Continuous probability models}\label{continuous-probability-models}}

Continuous probability models are probability density functions \(f(x)\) of a continuous random variables that we \textbf{believe} describe real random experiments.

Probability density function \(f(x)\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  is positive
\end{enumerate}

\[f(x) \geq 0\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  allows us to compute probabilities using the area under the curve:
\end{enumerate}

\[P(a\leq X \leq b)=\int_{a}^{b} f(x) dx\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  is such that the probability of anything is \(1\):
\end{enumerate}

\[\int_{-\infty}^{\infty} f(x) dx = 1\]

\hypertarget{exponential-process}{%
\section{Exponential process}\label{exponential-process}}

Let's go back to a \textbf{Poisson process} defined by probability

\[f(k)=\frac{e^{-\lambda}\lambda^k}{k!}, \lambda>0\]

for the number of events (\(k\)) in an interval.

Let us now consider that we are interested in the duration/time we should wait for the \textbf{first} count to occur.

We can ask about the probability that the first event occurs after duration/time \(X\).

Therefore, since \(X\) is a \textbf{continuous} random variable, we look for its probaility density function \(f(x)\).

\hypertarget{exponential-probability-density}{%
\section{Exponential probability density}\label{exponential-probability-density}}

The probability of \(0\) counts \textbf{if} an interval has unit \(x\) is

\[f(0|x)=\frac{e^{-x\lambda}x\lambda^0}{0!}\]

or

\[f(0|x)=e^{-x\lambda}\]

We can treat this as the conditional probability of \(0\) events in a distance \(x\): \(f(K=0|X=x)\) and apply the Bayes theorem to reverse it:

\[f(x|0)=C f(0|x)=C e^{-x\lambda}\]

So we can calculate the \textbf{probability of observing a distance} a distance \(x\) with \(0\) counts. This is the distance between any two events or the distance until the first event.

\textbf{Definition}

In a Poisson process with parameter \(\lambda\) the probability of waiting a distance/time \(X\) between two counts is given by the \textbf{probability density}

\[f(x)= C e^{-x\lambda}\]

\begin{itemize}
\item
  \(C\) is a constant that ensures: \(\int_{-\infty}^{\infty} f(x) dx =1\)
\item
  by integration \(C=\lambda\)
\end{itemize}

Therefore:

\[f(x)=\lambda e^{-\lambda x}, x\geq 0\]

\(\lambda\) is the paramenter of the model also known as a \textbf{decay rate}.

\textbf{Properties:}

When \(X \rightarrow Exp(\lambda)\) then

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  It has mean
\end{enumerate}

\[E(X)=\frac{1}{\lambda}\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  and variance
  \[V(Y)=\frac{1}{\lambda^2}\]
\end{enumerate}

Let's look at a couple of the probability densities in the exponential family

\includegraphics{_main_files/figure-latex/unnamed-chunk-63-1.pdf}

\hypertarget{exponential-distribution}{%
\section{Exponential Distribution}\label{exponential-distribution}}

Consider the following questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  In a Poisson process ¿What is the probability of observing an interval \textbf{smaller} than size \(a\) until the first count?
\end{enumerate}

Remember that this probability \(F(a)=P(X \leq a)\) is the probability density

\[F(a)=\lambda \int_\infty^a e^{-x\lambda}dx=1-e^{-a\lambda}\]
2) In a Poisson process ¿What is the probability of observing an interval \textbf{larger} than size \(a\) until the first event?

\[P(X > a)=1- P(X \leq a)= 1- F(a) = e^{-a\lambda}\]

Let's look at a couple of exponential distributions from the exponential family

\includegraphics{_main_files/figure-latex/unnamed-chunk-64-1.pdf}

The median \(x_m\) is such that \(F(x_m)=0.5\). That is \(x_m=\frac{\log(2)}{\lambda}\)

\textbf{Examples}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  What is the probability that we have to wait for a bus for more than \(1\) hour when on average there are two buses per hour?
\end{enumerate}

\[P(X > 1)=1-P(X \le 1) = 1-F(1,\lambda=2)=0.1353353\]
in R 1-pexp(1,2)

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  What is the probability of having to wait less than \(2\) seconds to detect one particle when the radioactive decay rate is \(2\) particles each second; \(F(2,\lambda=2)\)
\end{enumerate}

\[P(X \le 2)=F(2,\lambda=2)=0.9816844\]

in R pexp(2,2)

\hypertarget{questions-5}{%
\section{Questions}\label{questions-5}}

\textbf{1)} During WWII in London, the expected number of bombs that hit an area of \(3km^2\) was \(0.92\). The probability that, in one day, one area received two bombs was

\textbf{\(\qquad\)a:}1-ppois(x=2, lambda=0.92);
\textbf{\(\qquad\)b:}ppois(x=2, lambda=0.92); \textbf{\(\qquad\)c:}1-dpois(x=2, lambda=0.92); \textbf{\(\qquad\)d:}dpois(x=2, lambda=0.92)

\textbf{2)} The probability that a passenger has to wait less than 20 minutes until the next bus arrives at her stop is better described by

\textbf{\(\qquad\)a:} A poisson model on the number of buses per 20 minutes;\\
\textbf{\(\qquad\)b:} An exponential distribution at 20 minutes with a given expectation of buses per minute;
\textbf{\(\qquad\)c:} A binomial model that counts the number of buses per 20 minutes
\textbf{\(\qquad\)d:} A normal distribution with an average of buses per 20 minutes and a given standard deviation;

\textbf{3)} From the exponential probability distribution in the figure below, what is the most likely value of the median?

\textbf{\(\qquad\)a:} \(2\); \textbf{\(\qquad\)b:} \(3\); \textbf{\(\qquad\)c:} \(4\); \textbf{\(\qquad\)d:} \(5\)

\includegraphics{./figures/exp.png}

\hypertarget{exercises-6}{%
\section{Exercises}\label{exercises-6}}

\hypertarget{exercise-1-5}{%
\subsubsection{Exercise 1}\label{exercise-1-5}}

The average number of phone calls per hour coming into the switchboard of a company is \(150\). Find the probability that during one particular minute there will be

\begin{itemize}
\tightlist
\item
  0 phone calls (R:0.082)
\item
  1 phone call (R:0.205)
\item
  4 or fewer calls (R:0.891)
\item
  more than 6 phone calls (R:0.0141)
\end{itemize}

\hypertarget{exercise-2-5}{%
\subsubsection{Exercise 2}\label{exercise-2-5}}

The average number of radioactive particles hitting a Geiger counter in a nuclear energy plant under control is \(2.3\) per minute.

\begin{itemize}
\item
  What is the probability of counting exactly \(2\) particles in a minute? (R:0.265)
\item
  What is the probability of detecting exactly \(10\) particles in \(5\) minute? (R:0.112)
\item
  What is the probability of at least one count in two minutes? (R:0.9899)
\item
  What is the probability of having to wait less than \(1\) second to detect a radioactive particle, after we switch on the detector? (R:0.037)
\item
  We suspect that a nuclear plant has a radioactive leak if we wait less than \(1\) second to detect a radioactive particle, after we switch on the detector. What is the probability that when we visit in \(5\) plants that are under control, we suspect that at least one has a leak? (R:0.1744).
\end{itemize}

\hypertarget{normal-distribution}{%
\chapter{Normal Distribution}\label{normal-distribution}}

\hypertarget{objective-5}{%
\section{Objective}\label{objective-5}}

In this chapter we will introduce the normal probability distribution.

We will discuss its origin and its main properties.

\hypertarget{history}{%
\section{History}\label{history}}

In 1801 Gauss analyzed data obtained for the position of the Ceres, a large asteroid between Mars and Jupiter.

At the time people suspected it was a new planet, as it moved day to day against the fixed stars. In January, it could be seen at the horizon just before dawn. However, as days passed Ceres will rise latter and latter until it could not longer be seen because of the Sun rise.

Gauss understood that the measurements for the position of Ceres had errors.

He was therefore interested in finding how the observations were \textbf{distributed} so he could find the most \textbf{likely} orbit. With the orbit, we could derive the mass of the object and then decide whether it was a planet or a big asteroid.

Data was available only for the month of January. After which Ceres would disappear. He wanted to \textbf{predict} where astronomers should point their telescopes to find it six months after at dusk, once it had passed behind the Sun.

Gauss had to account for the errors in the position of ceres at a given day due to measurement

\includegraphics{./figures/ceres.JPG}

Gauss supposed that

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  small errors were more likely than large errors
\item
  the error at a distance \(-\epsilon\) from the value at the position of Ceres was equally likely as a distance \(\epsilon\)
\item
  the most \textbf{likely} (that we believe) of Ceres at any given time in the sky was the \textbf{average} of multiple altitude measurements at that latitude.
\end{enumerate}

That was enough to show that the deviations of the observations \(y\) \textbf{from the orbit} satisfied the equation

\[\frac{df(y)}{dy}=-Cyf(y)\]
with \(C\) a positive constant. The solution of this differential equation is:

\[f(y)=\frac{\sqrt{C}}{\sqrt{2\pi}}e^{-\frac{Cy^2}{2}}\]

*The evolution of the normal distribution, Saul Stahl, Mathemetics Magazine, 2006.

\hypertarget{normal-density}{%
\section{normal density}\label{normal-density}}

Gaussian probability density gives the distribution of measurement errors from the \textbf{actual} but \textbf{unknown} position of Ceres in the sky. Let's make a couple of changes to the function.

1- Let's write the error density from the horizon using the random variable \(X\), that is, \(y=x-\mu\). \(\mu\) is the \textbf{actual} but \textbf{unknown} position of Ceres from the horizon. After a change of variable we find the probability density function:

\[f(x)=\frac{\sqrt{C}}{\sqrt{2\pi}}e^{-C(x-\mu)^2}\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Let's rename the variable \(C\) to \(\frac{1}{\sigma^2}\)
\end{enumerate}

So, we arrive at the following definition.

\hypertarget{definition}{%
\section{Definition}\label{definition}}

A random variable \(X\) defined on the real numbers has a density \textbf{Normal} if it takes the form

\[f(x; \mu, \sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, x \in {\mathbb R}\]

The variable has

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  mean
\end{enumerate}

\[E(X) = \mu\]

which for Gauss represented the actual position of Ceres.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  and variance
  \[V(X) = \sigma^2\]
\end{enumerate}

which represented the dispersion of the error in the observations, which depends on the quality of the telescope.

\(\mu\) and \(\sigma\) are the \textbf{two parameters} that completely describe the normal density function and their \textbf{interpretation} depends on the random experiment.

When \(X\) follows a Normal density, that is, it is normally distributed, we write

\[X\rightarrow N(\mu,\sigma^2)\]

Let's look at some probability densities in the normal parametric model

\includegraphics{_main_files/figure-latex/unnamed-chunk-65-1.pdf}

\hypertarget{probability-distribution-2}{%
\section{Probability distribution}\label{probability-distribution-2}}

The probability distribution of the Normal density:

\[F_{normal}(a)=P(Z \leq a)\]

is the \textbf{error} function defined by the area under the curve from \(-\infty\) to \(a\)

\[F_{normal}(a)=\int_{-\infty}^{a}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx\]
The function is found in most computer programs, and it does not have a closed form of known functions.

\textbf{Example (women's height)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  What is the probability that a woman in the population is at most \(150cm\) tall if women have a mean height of \(165cm\) with standard deviation of \(8cm\)?
\end{enumerate}

\(P(X\le 150)=F(150, \mu=165, \sigma=8)=0.03039636\)

in R pnorm(150, 165, 8)

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  What is the probability that a woman's height in the population is between \(165cm\) and \(170cm\)?
\end{enumerate}

\(P(165 \le X \le 170)=F(170, \mu=165, \sigma=8)-F(165, \mu=165, \sigma=8)=0.2340145\)

in R pnorm(170, 165, 8)-pnorm(165, 165, 8)

Let's look at the probability distribution function

\includegraphics{_main_files/figure-latex/unnamed-chunk-66-1.pdf}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  What is the first quartile for height in women?
\end{enumerate}

The first quartile is defined as:

\(F(x_{0.25}, \mu=165, \sigma=8)=0.25\)

or

\(x_{0.25}=F^{-1}(0.25, µ=165, \sigma=8)=159.6041\)

in R qnorm(0.25, 165, 8)

\textbf{Properties of the Normal distribution}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  the mean \(\mu\) is also the median as it splits the measurements in two
\item
  \(x\) values that fall farther than 2\(\sigma\) are considered \textbf{rare} \(5\%\)
\item
  \(x\) values that fall farther than 3\(\sigma\) are considered \textbf{extremely rare} \(0.2\%\)
\end{enumerate}

\includegraphics{./figures/probs.png}

\textbf{Example (women's height)}

We can define the limits of \textbf{common observations} for the distribution of women's height in the population.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  at a distance of one standard deviation from the mean, we find \(68\%\) of the population
\end{enumerate}

\[P(165-8 \leq X \leq 165+8)=P(157 \leq X \leq 173)=F(173)-F(157)=0.68\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  at a distance of two standard deviations from the mean, we find \(95\%\) of the population
\end{enumerate}

\[P(165-2 \times 8 \leq X \leq 165+2\times 8)=F(181)-F(149)=0.95\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  at a distance of three standard deviations from the mean, we find \(99.7\%\) of the population
\end{enumerate}

\[P(165-3 \times 8 \leq X \leq 165+3\times 8)=F(189)-F(141)=0.997\]

\includegraphics{_main_files/figure-latex/unnamed-chunk-67-1.pdf}

\hypertarget{standard-normal-density}{%
\section{Standard normal density}\label{standard-normal-density}}

The standard normal density is the particular density from the normal family

\[f(x; \mu, \sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, x \in {\mathbb R}\]

It is therefore the density with

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  mean
\end{enumerate}

\[E (X)= \mu = 0\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  and variance
\end{enumerate}

\[V (X)=  \sigma^2 =1\]

When a random variable follows a normal probability density, we say that it distributes normally and write

\[X \rightarrow N(0,1)\]

\hypertarget{standard-distribution}{%
\section{Standard distribution}\label{standard-distribution}}

The probability distribution of the standard density:

\[\phi(a)=F_{N(0,1)}(a)=P(Z \leq a)\]

is the \textbf{error} function defined by

\[\phi(a)=\int_{-\infty}^{a} \frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}} dz\]

Because the standard distribution is special and will appear often then we use the letter \(\phi\) for it.

\includegraphics{./figures/st.png}

You can find it in most computer programs. in R is pnorm(x) with the default parameters, 0 and 1.

We usually define the limits of the \textbf{most common observations} for the standard variable

\includegraphics{./figures/phi.JPG}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  The interquartile range \[P(-0.67 \leq X \leq 0.67)=0.50\]
\end{enumerate}

in R: c(qnorm(0.25), qnorm(0.75))

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  \(95\%\) range \[P(-1.96 \leq X \leq 1.96)=0.95\]
\end{enumerate}

in R: c(qnorm(0.025), qnorm(0.975))

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  \(99\%\) range \[P(-2.58 \leq X \leq 2.58)=0.99\]
\end{enumerate}

in R: c(qnorm(0.005), qnorm(0.995))

\includegraphics{_main_files/figure-latex/unnamed-chunk-68-1.pdf}

\hypertarget{standardization}{%
\section{Standardization}\label{standardization}}

All normal variables can be \textbf{standardized}. This means that if \(X \rightarrow N(\mu, \sigma^2)\), then we can transform the variable to
a \textbf{standardized variable}

\[Z=\frac{X-\mu}{\sigma}\]

which will have density:

\[f(z)=\frac{1}{ \sqrt{2\pi}}e^{-\frac{z^2}{2}} \]
Therefore, for any \(X \rightarrow N(\mu, \sigma^2)\)

\[Z=\frac{X-\mu}{\sigma} \rightarrow N(0, 1) \]

\includegraphics{./figures/stand.png}

You can demonstrate this by replacing \(x=\sigma z+\mu\) and \(dx=\sigma dz\) in the probability expression we have

\(P(x\leq X \leq x +dx)=P(z\leq Z \leq z +dz)\)
\[=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx\] \[=\frac{1}{ \sqrt{2\pi}}e^{-\frac{z^2}{2}} dz\]

The probability of \textbf{any normal variable} \(X\rightarrow N(\mu, \sigma^2)\) can be computed using the standard distribution

\(F(a)=P(X<a)=P(\frac{X-\mu}{\sigma}<\frac{a-\mu}{\sigma})\)
\[=P(Z < \frac{a-\mu}{\sigma})= \phi \big(\frac{a-\mu}{\sigma}\big)\]

For computing \(P(a\leq X \leq b)\), we can use the property of the probability distributions

\(F(b)-F(a)=P(X\leq b)-P(X\leq a)\)

\[=\phi \big(\frac{b-\mu}{\sigma}\big)-\phi \big(\frac{a-\mu}{\sigma}\big)\]

\hypertarget{summary-of-probability-models}{%
\section{Summary of probability models}\label{summary-of-probability-models}}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1905}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1905}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1429}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1429}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
X
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
range of x
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
f(x)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
E(X)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
V(X)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Uniform & integer or real number & \([a, b]\) & \(\frac{1}{n}\) & \(\frac{b+a}{2}\) & \(\frac{(b-a+1)^2-1}{12}\) \\
Bernoulli & event A & 0,1 & \((1-p)^{1-x}p^x\) & \(p\) & \(p(1-p)\) \\
Binomial & \# of A events in \(n\) repetitions of Bernoulli trials & 0,1,\ldots{} & \(\binom n x (1-p)^{n-x}p^x\) & \(np\) & \(np(1-p)\) \\
Negative Binomial for events & \# of B events in Bernoulli repetitions before \(r\) As are observed & 0,1,.. & \(\binom {x+r-1} x (1-p)^xp^r\) & \(\frac{r(1-p)}{p}\) & \(\frac{r(1-p)}{p^2}\) \\
Hypergeometric & \# A events in a sample \(n\) from population \(N\) with \(K\) As & \(\max(0, n+K-N)\), \ldots{} \(\min(K, n)\) & \(\frac{1}{\binom N n}\binom K x \binom {N-K} {n-x}\) & \(n*\frac{N}{K}\) & \(n \frac{N}{K} (1-\frac{N}{K})\frac{N-n}{N-1}\) \\
Poisson & \# of events A in an interval & 0,1, .. & \(\frac{e^{-\lambda}\lambda^x}{x!}\) & \(\lambda\) & \(\lambda\) \\
Exponential & Interval between two events A & \([0,\infty)\) & \(\lambda e^{-\lambda x}\) & \(\frac{1}{\lambda}\) & \(\frac{1}{\lambda^2}\) \\
Normal & measurement with symmetric errors whose most likely value is the average & \((-\infty, \infty)\) & \(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\) & \(\mu\) & \(\sigma^2\) \\
\end{longtable}

\hypertarget{r-functions-of-probability-models}{%
\section{R functions of probability models}\label{r-functions-of-probability-models}}

\begin{longtable}[]{@{}lc@{}}
\toprule\noalign{}
Model & R \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Uniform (continuous) & dunif(x, a, b) \\
Binomial & dbinom(x,n,p) \\
Negative Binomial for events & dnbinom(x,r,p) \\
Hypergeometric & dhyper(x, K, N-K, n) \\
Poisson & dpois(x, lambda) \\
Exponential & dexp(x, lambda) \\
Normal & dnorm(x, mu, sigma) \\
\end{longtable}

\hypertarget{questions-6}{%
\section{Questions}\label{questions-6}}

\textbf{1)} It is not true that for a normally distributed variable

\textbf{\(\qquad\)a:} its mean and median are the same; \textbf{\(\qquad\)b:} the standard probability distribution can be used to compute its probabilities; \textbf{\(\qquad\)c:} its interquartile range is twice its standard deviation; \textbf{\(\qquad\)d:} \(5\%\) of its observations are a distance greater than twice its standard deviation

\textbf{2)} For a normal standard variable

\textbf{\(\qquad\)a:} \(50\%\) of its observations are between \((-0.67,0.67)\);
\textbf{\(\qquad\)b:} \(2\%\) of its observations are lower than \(-2.58\);
\textbf{\(\qquad\)c:} \(5\%\) of its observations are greater than \(1.96\);
\textbf{\(\qquad\)d:} \(25\%\) of its observations are between \((-1.96,-0.67)\)

\textbf{3)} if we know that \(\phi(-0.8416212)=0.2\) then what is \(\phi(0.8416212)\)

\textbf{\(\qquad\)a:} \(0.1\);
\textbf{\(\qquad\)b:} \(0.2\);
\textbf{\(\qquad\)c:} \(0.8\);
\textbf{\(\qquad\)d:} \(0.9\)

\textbf{4)} the third quartile of a normal variable with mean \(10\) and standard deviation \(2\) is

\textbf{\(\qquad\)a:} qnorm(1/3, 10, 2)=9.138545;
\textbf{\(\qquad\)b:} qnorm(1-0.75, 10, 2)=8.65102 ;
\textbf{\(\qquad\)c:} qnorm(1-1/3, 10, 2)=10.86145 ;
\textbf{\(\qquad\)d:} qnorm(0.75, 10, 2)= 11.34898

\textbf{5)} probability that a normal variable with mean \(10\) and standard deviation \(2\) is in \((-\infty,10)\) is

\textbf{\(\qquad\)a:} 0.25;
\textbf{\(\qquad\)b:} 0.5;
\textbf{\(\qquad\)c:} 0.75;
\textbf{\(\qquad\)d:} 1:

\hypertarget{exercises-7}{%
\section{Exercises}\label{exercises-7}}

\hypertarget{exercise-1-6}{%
\subsubsection{Exercise 1}\label{exercise-1-6}}

Find the area under the standard normal curve in the following cases:

\begin{itemize}
\tightlist
\item
  Between \(z=0.81\) and \(z=1.94\) (R:0.182)
\item
  To the right of \(z=-1.28\) (R:0.899)
\item
  To the right of \(z=2.05\) or to the left of \(z=-1.44\) (R:0.0951)
\end{itemize}

\hypertarget{exercise-2-6}{%
\subsubsection{Exercise 2}\label{exercise-2-6}}

\begin{itemize}
\item
  What is the probability that a man's height is at least
  \(165\)cm if the population mean is \(175\)cm y the standard deviation is \(10\)cm? (R:0.841)
\item
  What is the probability that a man's height is between
  \(165\)cm and \(185\)cm? (R:0.682)
\item
  What is the height that defines the \(5\%\) of the smallest men? (R:158.55)
\end{itemize}

\hypertarget{sampling-distributions}{%
\chapter{Sampling distributions}\label{sampling-distributions}}

\hypertarget{objective-6}{%
\section{Objective}\label{objective-6}}

In this chapter, we are going to study estimates of the mean and variance of normal distributions using \textbf{random samples}.

We will introduce the \textbf{sample mean} and the \textbf{sample variance} as random variables that estimate the parameters of the normal distribution.

The sample mean and the sample variance have probability density functions, these are called \textbf{sample density functions}.

\hypertarget{aleatory-sample}{%
\section{Aleatory sample}\label{aleatory-sample}}

\textbf{Example (Cables)}

Imagine that a client asks your metallurgical company to sell cables for \(8\) that can transport up to \(96\) Tons; that's \(12\) Tons each. You must guarantee that none of them will break with this weight.

You \textbf{stock} a set of cables that might work, but you're not sure. So you take \(8\) Cables at random, and charge them until they break.

We say that you take a \textbf{random sample} of size \(8\), which means that you repeat the random experiment \(8\) times. Here are the results

\begin{verbatim}
## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747
\end{verbatim}

\includegraphics{_main_files/figure-latex/unnamed-chunk-70-1.pdf}

\textbf{Definition:}

A \textbf{random sample} of size \(n\) is the \textbf{replication} of a random experiment \(n\) \textbf{independent} times.

\begin{itemize}
\tightlist
\item
  A random sample is an \textbf{random variable} \(n\)-dimensional
\end{itemize}

\[(X_1, X_2, ... X_n)\]

where \(X_i\) is the \emph{i-th} iteration of the random experiment with common distribution \(f(x; \theta)\) for any \(i\)

\begin{itemize}
\tightlist
\item
  \textbf{An observation} of a random sample is the set of \(n\) values obtained from the experiments
\end{itemize}

\[(x_1, x_2, ... x_n)\]
Our \textbf{observation} of the sample of size \(8\) cables was

\begin{verbatim}
## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747
\end{verbatim}

\textbf{Example (Cables)}

In the observed sample of the breaking load of the cables it was observed that

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  None of them broke \(12\) Tons.
\item
  There was one that broke at \(12.62747\) Tons.
\end{enumerate}

Do you take a chance and sell a random sample of \(8\) cables from your stock? What happens if your company is responsible for a cable break and has to pay a large fine?

To assure the customer that the cables will not break at \(12\) Tons, we would like to see that \(P(X \leq 12)\) is reasonably low.

\hypertarget{calculation-of-probabilities}{%
\section{Calculation of probabilities}\label{calculation-of-probabilities}}

To calculate probabilities we need:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A probability model (probability function)
\item
  The parameters of the model (the values of the probability function)
\end{enumerate}

Let's \textbf{assume} that the breaking load of the cables follows a \textbf{normal} probability density function.

\[X \rightarrow N(x; \mu, \sigma^2)\]

To compute \(P(X \leq 12)\), we need the parameters \(\mu\) and \(\sigma^2\). How can we estimate the parameters of the observed sample?

\hypertarget{parameter-estimation}{%
\section{Parameter estimation}\label{parameter-estimation}}

To find likely values for the parameters we use data. Therefore, we take a \textbf{random sample}. That is, we repeat the experiment \(n\) times, collect data, and use it to estimate the parameters.

\textbf{Estimate of the mean and variance}

Recall that for a discrete random variable, we define the mean as

\[\mu=\sum_{i}^m x_if(x_i)\]

which is the center of gravity of the \textbf{probabilities}, where \(f(x_i)\) is the probability function. This definition was motivated by the center of gravity of the \textbf{observations}

\[\bar{x}= \frac{1}{n} \sum_{i}^n x_i = \sum_{i}^m x_if_i\]

which we define as the \textbf{average}, and where \(f_i\) are the relative frequencies. Remember that \(n\) is the number of observations (it can be as large as we want) and \(m\) is the number of possible outcomes (usually fixed by the sample space). We have argued that when \(n \rightarrow \infty\) then

\[\hat{P}(X=x)=f_i\]
This means that the probabilities can be \textbf{estimated} (by putting on a \textbf{hat}) by the relative frequencies when \(n\) is large, because \(lim_{n\rightarrow \infty}f_i=f(x_i)\). Therefore, we should also have that the \textbf{mean} \(\mu\) can be estimated by the \textbf{mean} \(\bar{x}\)

\[\hat{\mu}=\bar{x}= \sum_{i}^m x_i\hat{P}(X=x)\]

Thus, we may take the center of the probability function as the center of gravity of the data. Doing this we will make an error that we can assume, as we will discuss later on.

With the variance

\[\sigma^2=\sum_{i}^m(x_i-\mu)^2f(x_i)\]

we have a similar situation. In the limit when \(n \rightarrow \infty\)

\[\hat{\sigma}^2=s^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2\]

and we assume that the moment of inertia of the data is close to the moment of inertia of the probabilities.

\textbf{Example (Cables)}

Assuming that the breaking load of our cable is a normal random variable

\[X \rightarrow N(x; \mu, \sigma^2)\]

\textbf{we use} the estimates \(\bar{x}_{stock}=13.21\) (mean(x)) and \(s^2=0.3571565^2\) (sd ( x)\^{}2) as the values of \(\mu\) and \(\sigma^2\). So the \textbf{fitted} model is

\[X \rightarrow N(x; \mu=13.21, \sigma^2=0.3571565^2)\]
In this problem we \textbf{didn't know} \(\mu\) or \(\sigma\) and therefore we are guessing their values and the underlying model

\includegraphics{_main_files/figure-latex/unnamed-chunk-72-1.pdf}

What is the probability that the cable will break at \(12\) Tons?

Since \[X \rightarrow N(x; \mu=13.21, \sigma^2=0.3571565^2)\]

so

\[P(X \leq 12)= F(12; \mu=13.21, \sigma^2=0.1275608)\]

In R pnorm(12,13.21, 0.3571565)\(=0.000352188\)

Given the \textbf{observed} sample, there is an estimated probability of \(0.03\%\) that a single cable breaks in \(12\) Tons. We have a probabilistic argument for selling the cables.

\hypertarget{margin-of-error-of-estimates}{%
\section{Margin of error of estimates}\label{margin-of-error-of-estimates}}

When we estimate parameters using data, such as by taking the value of \[\hat{\mu}=\bar{x}\]

for the value of \(\mu\); and the value of

\[\hat{\sigma}^2=s^2\]
by the value of \(\sigma^2\), we know that we are \textbf{making a mistake}. We know that if we take another sample of size \(8\) cables \textbf{the estimate will change}, because the average \(\bar{x}\) will change.

Can we get an idea of how big the error is in our estimate?

The first thing to realize is that the numerical value we get for \[\bar{x}\] is the observation of a \textbf{random variable} \[\bar{X}\]

\textbf{Definition}

The \textbf{sample mean} (or average) of a random sample of size \(n\) is defined as

\[\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i\]

The average is a \textbf{random variable} that in our sample of size \(8\) took the value

\[\bar{x}_{stock}=13.21\]

If we take another sample, this number will change.

\textbf{Mean as estimator}

The number \(\bar{x}\) can be used to \textbf{estimate} the unknown parameter \(\mu\) because the random variable \(\bar{X}\) satisfies these two important properties

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  is \textbf{unbiased}: \[E(\bar{X})=\mu\]
\item
  is \textbf{consistent}: \[lim_{n \rightarrow \infty} V(\bar{X}) = 0\]
\end{enumerate}

The first property holds because
\[E(\bar{X})=E\big(\frac{1}{n}\sum_{i=1}^n X_i\big)=E(X)=\mu\]

The second property holds because
\[V(\bar{X})=V\big(\frac{1}{n}\sum_{i=1}^n X_i\big)=\frac{V(\sum_{i=1}^ nX_i)}{n^2}=\frac{V(X)}{n}=\frac{\sigma^2}{n}\]

Which uses the fact that each random experiment in the sample is independent and therefore \(V(\sum_{i=1}^n X_i)=nV(X)\).

\emph{Estimate of \(\mu\)}

As a consequence of properties 1 and 2, we understand that the value \(\bar{x}\) \textbf{concentrates closer and closer} to \(\mu\) as \(n\) increases. This means that the error we make when we take a value of \(\bar{x}\) as the estimate of \(\mu\)

\[\bar{x}=\hat{\mu}\]

gets smaller and smaller as the sample gets larger and larger because the variance of \(\bar{x}\) gets smaller with large \(n\).

\hypertarget{inference}{%
\section{Inference}\label{inference}}

We know that when we take large samples, our error is small. However, for a given value of \(n\) we want to have a \textbf{error measure}. Therefore, we ask about the \textbf{probability of making an error} of a given size when estimating \(\mu\) with \(\bar{x}\).

When we calculate probabilities in an estimator, we say that we are making an \textbf{inference}. Inference problems often arise when we are interested in calculating the probability of making an error in estimating \(\mu\) with \(\bar{x}\).

To calculate probabilities we need

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A probability model (probability function)
\item
  The parameters of the model (the values of the probability function)
\end{enumerate}

What are the probability functions of \(\bar{X}\) and \(S^2\) so that we can calculate their probabilities?

These probability functions are called \textbf{sampling probability functions}, because they are derived from a sampling experiment.

\textbf{Example (Cables)}

Let's ask an inference question. Imagine our cables are \textbf{certified} to break with an average load of \(\mu = 13\) Tons with variance \(\sigma^2=0.35^2\).

If we take a random sample of \(8\) cables, what is the probability that the sample mean \(\bar{X}\) will be within a \textbf{margin of error} of \(0.25\) Tons of the mean \(\mu\)?

\[P(- 0.25\leq \bar{X}-\mu \leq 0.25)\]

To calculate this probability, we need to know the probability function of \(\bar{X}\).

\hypertarget{sample-mean-distribution}{%
\section{Sample mean distribution}\label{sample-mean-distribution}}

\textbf{Theorem:} If \(X\) follows a normal distribution \[X \rightarrow N(\mu, \sigma^2)\]

then \(\bar{X}\) is normal

\[\bar{X} \rightarrow N(\mu, \frac{\sigma^2}{n})\]
and \(\bar{X}\) has

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  mean
  \[E(\bar{X})=\mu\]
  We say that \(\bar{X}\) is unbiased because its expected value is exaclty \(\mu\).
\item
  variance
  \[V(\bar{X})=\frac{\sigma^2}{n}\]
  We say that \(\bar{X}\) is consistent because is gets smaller with large \(n\).
\end{enumerate}

We call \(se=\sqrt{V(\bar{X})}\) the \textbf{standard error} of the sample mean. The standard error is also written as \(\sigma_{\bar{x}}\). Note that this is the error we expect when using \(\bar{x}\) as the value of \(\mu\), and it is the bias we needed to correct for \(S_n^2\).

So, if we \textbf{know} \(\mu\) and \(\sigma\), we can calculate the \textbf{probabilities of} \(\bar{X}\) using the normal distribution.

Remember that we have \textbf{two probability functions}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The probability function of \(X\) is also known as the \textbf{population} probability function
\item
  The probability function of \(\bar{X}\) is a probability function of the \textbf{sample}.
\end{enumerate}

\textbf{Example (Cables)}

\emph{Probability densities for \(X\) and \(\bar{X}\)}

In our new problem, we now \textbf{know} \(\mu\) and \(\sigma\) and the probability function of the \textbf{population}

\[X \rightarrow N(\mu=13, \sigma^2=0.35^2)\]

\includegraphics{_main_files/figure-latex/unnamed-chunk-73-1.pdf}

Since \(X\) is normal, then \(\bar{X}\) is normal, and therefore we also know the probability function of the sample mean \(\bar{X}\)

\[\bar{X} \rightarrow N(13, \frac{0.35^2}{8})\]

which has mean and variance

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \(E(\bar{X})=\mu=13\)
\item
  \(V(\bar{X})=\frac{\sigma^2}{n}=\frac{0.35^2}{8}=0.01530169\)
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-74-1.pdf}

Finally we want to calculate \textbf{the probability} that our estimate has a margin of error of \(0.25\). That is a distance of \(0.25\) from the mean. That's \[P(-0.25 \leq \bar{X} - 13\leq 0.25)=P(12.75 \leq \bar{X} \leq 13.25)\]

\(=F(13.25; \mu, \sigma^2/n)-F(12.75; \mu, \sigma^2/n)\)

In R we can calculate it as:

pnorm(13.25, 13, 0.1237)-pnorm(12.75, 13, 0.1237)=0.956.

Remember: \(se=\sigma_{\bar{x}}=\sqrt{0.01530169}=0.1237\)

Therefore \(95.6\%\) of the means \(\bar{X}\) of random samples of size \(8\) are at a distance of \(0.25\) from the mean \(\mu=13\).

If we sell our process for building the cables, we can tell new manufacturers that when they follow our instructions, they can test the process by taking a sample of size \(8\) cables. In that case, they can expect the sample mean to fall between \((12.75, 13.25)\) about \(95\%\) of the times.

\includegraphics{_main_files/figure-latex/unnamed-chunk-75-1.pdf}

When we perform random sampling we observe:

\begin{verbatim}
## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747
\end{verbatim}

Assuming \(\mu=13\) then our \textbf{observed error} in estimating the mean is the difference

\[\bar{x}_{stock}-\mu=13.21-13=0.21\]

This is within the margin of error of \(95.6\%\) and therefore, we must consider that the manufacturing process is working as expected.

\includegraphics{_main_files/figure-latex/unnamed-chunk-77-1.pdf}

\hypertarget{sample-sum}{%
\subsection{Sample sum}\label{sample-sum}}

If we are interested in using all \(8\) cables at the same time to carry a total of \(96\) Tons, then we should consider \textbf{adding} their individual contributions.

The \textbf{sample sum} is the \textbf{statistic}

\[Y=n \bar{X}=\sum_{i=1}^n X_i\]

A \textbf{statistic} is any function from the random sample \((X_1, ... X_n)\).

\textbf{Theorem:} if \(X\) follows a normal distribution
\[X \rightarrow N(\mu, \sigma^2)\]

so \(Y\) is normal

\[Y \rightarrow N(n\mu, n\sigma^2)\]

\(Y\) has

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  mean \[E(Y)=n\mu\]
\item
  variance \[V(Y)=n\sigma^2\]
\end{enumerate}

\textbf{Example (sum of cables)}

What is the probability that when we put all the cables together, they can carry a total weight between \(102=8(13 - 0.25)\) and \(106=8(13+ 0.25)\) Tons?

\textbf{We know} that for our Cables \[X \rightarrow N(\mu=13, \sigma^2=0.35^2)\] then

\[Y \rightarrow N(n\mu=104, n\sigma^2=8\times 0.35^2)\]

with mean and variance

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \(E(Y)=n\mu=104\)
\item
  \(V(Y)=n\sigma^2=8\times 0.35^2=0.98\); \(\sqrt{V(Y)}=0.9899495\)
\end{enumerate}

We want to calculate \[P(102 \leq Y \leq 106)\]

\(=F(102; n\mu, n\sigma^2)-F(106; n\mu, n\sigma^2)\)

In R we can calculate it as:

pnorm(106, 104, 0.9899495)-pnorm(102, 104, 0.9899495)=0.956.

Therefore \(95.6\%\) of the total weight that \(8\) cables can carry is between \(102\) and \(106\) Tons, or a distance of \(8*0.25=2\) Tons from the total mean \(n\mu=104\).

\hypertarget{sample-variance-1}{%
\section{Sample variance}\label{sample-variance-1}}

By estimating the variance

\[s^2=\hat{\sigma}\]

We also make a mistake. How can we estimate the error we make?

\textbf{Definition}

The \textbf{sample variance} \(S^2\) of a random sample of size \(n\)

\[S^2= \frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2\]

is the dispersion of the measurements around \(\bar{X}\). In our sample of size \(8\), \(S^2\) took the value

\[s_{stock}^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2=0.1275608\]

\(S^2\) is

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  unbiased: \(E(S^2)=V(X)=\sigma^2\)
\item
  consistent: \(n \rightarrow \infty\), \(V(\bar{S^2}) \rightarrow 0\)
\end{enumerate}

and therefore \(S^2\) consistently estimates \(\sigma^2\)

We can take a value of \(s^2\) as an estimate for \(\sigma^2\) or

\[s^2=\hat{\sigma}^2\]
Similar to \(\hat{\mu}\), the error of this estimate gets smaller and smaller as \(n\) gets bigger and bigger.

\textbf{The unbiased sample variance (why do we divide by n-1?)}

We could propose to estimate \(\sigma^2\) by dividing the squared differences of \(\bar{X}\) by \(n\)

\[S_n^2=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2\]

\(S_n^2\) is therefore

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \textbf{biased}: \(E(S_n^2) = \sigma^2-\frac{\sigma^2}{n} \neq \sigma^2\)
\item
  but consistent \(V(S_n^2) \rightarrow 0\) when \(n\rightarrow \infty\)
\end{enumerate}

The bias term \(\frac{\sigma^2}{n}\) arises because \(S_n^2\) measures the spread around \(\bar{X}\) and not around \(\mu\). Remember that the error we make when we substitute \(\bar{x}\) for \(\mu\) is the variance of \(\bar{X}\): \(\sigma^2/n\). Let us correct for bias, writing equation 1 above as:

\[E(\frac{n}{n-1}S_n^2)=\sigma^2\]

We can define the \textbf{sample variance} (corrected) \[S^2=\frac{n}{n-1}S_n^2=\frac{1}{n-1}\sum_{i=1}^ n(X_i-\bar{X})^2\]

which is an unbiased estimator of \(\sigma^2\) because \(E(S^2)=\sigma^2\).

We can also have inference problems when we are interested in the probability of the \textbf{sample variance} \(S^2\).

Consider a quality control process that requires the leads to be produced close to the specified value \(\mu\). We don't want cables that break too far from the mean.

If a sample of size \(8\) cables is very scattered (\(S^2>0.3\)), we stop production: the process is out of control.

What is the probability that the sample variance of a sample of size \(8\) Cables will be greater than the required \(0.3\)?

\hypertarget{probabilities-of-the-sample-variance}{%
\section{Probabilities of the sample variance}\label{probabilities-of-the-sample-variance}}

\textbf{Theorem:} If \(X\) follows a normal distribution
\[X \rightarrow N(\mu, \sigma^2)\]

The \textbf{statistic}:

\[W=\frac{(n-1)S^2}{\sigma^2} \rightarrow \chi^2(n-1)\]

has a distribution \(\chi^2\) (chi-square) with \(df=n-1\) degrees of freedom given by

\[f(w)=C_n w^{\frac{n-3}{2}} e^{-\frac{w}{2}}\]

where:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \(C_n=\frac{1}{2^{(n-1)/2\sqrt{\pi(n-1)}}}\) ensures \$\int\_\{-\infty\}\^{}\{\infty\} f (t)dt=\$1
\item
  \(\Gamma(x)\) is the Euler factorial for real numbers
\end{enumerate}

If we \textbf{know} the value of \(\sigma\), we can calculate the probabilities of \(S^2\) using the \(\chi^2\) distribution for \(W\).

\hypertarget{chi2-statistic}{%
\section{\texorpdfstring{\(\chi^2\)-statistic}{\textbackslash chi\^{}2-statistic}}\label{chi2-statistic}}

The probability density \(\chi^2\) has a parameter \(df=n-1\), called degrees of freedom. Let's look at some probability densities in the family of probability models \(\chi^2\)

\includegraphics{_main_files/figure-latex/unnamed-chunk-78-1.pdf}

\textbf{Example (variations in cable break)}

If we \textbf{know} that our cables
\[X \rightarrow N(\mu=13, \sigma^2=0.35^2)\]

so

\[W=\frac{(n-1)S^2}{\sigma^2}= \frac{7S^2}{0.35^2} \rightarrow \chi^2(n-1)\]

we can calculate \[P(S^2 > 0.3)=P(\frac{(n-1)S^2}{\sigma^2} > \frac{(n-1)0.3}{\sigma^2 } )\]
\(=P(W > \frac{7*0.3}{0.35^2})=P(W > 17.14286)\)

\(=1-P(W \leq 17.14286)\)

\(= 1- F_{\chi^2,df=7}(17.14286)=0.016\)

in R
1-pchisq(17.14286, df=7)=0.016

There is only a \(1\%\) chance of getting a value greater than \(s^2=0.3\). So \(s^2>0.3\) seems to be a good criteria to stop production and review the process.

If we take a random sample and get a value of \(s ^2\) that is greater than \(0.3\), it will be a rare observation if all is well. We tend to believe that the observed values are common, not rare, so we may think that something is not right.

When we perform random sampling we observe:

\begin{verbatim}
## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747
\end{verbatim}

Therefore, our observed value was \(s^2_{stock}=0.1275608\)

The sample is not very sparse because \(s^2_{stock} < 0.3\) and we believe that all is well and production is under control.

\hypertarget{questions-7}{%
\section{Questions}\label{questions-7}}

\textbf{1)} The sample mean is an unbiased estimator of the population mean because

\textbf{\(\qquad\)a:} The expected value of the sample mean is the population mean;
\textbf{\(\qquad\)b:} The expected value of the population mean is the sample mean;
\textbf{\(\qquad\)c:} The standard error approaches zero as \(n\) approaches infinity;
\textbf{\(\qquad\)d:} The variance of the sample mean approaches zero as \(n\) approaches infinity;

\textbf{2)} Why is the statistic \(S^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i -\bar{X})^2\) used? instead of \(S_n^2=\frac{1}{n}\sum_{i=1}^{n}(X_i -\bar{X})^2\) to estimate the variance of a random variable?

\textbf{\(\qquad\)a:} because its variance is \(0\);
\textbf{\(\qquad\)b:} because it is a consistent estimator of \(\sigma^2\);
\textbf{\(\qquad\)c:} because it is an unbiased estimator of \(\sigma^2\);
\textbf{\(\qquad\)d:} because it is the mean square distance to the sample mean (\(\bar{X}\));

\textbf{3)} What is the variance of the sample mean \(\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i\)?

\textbf{\(\qquad\)a:}\(\sigma\);
\textbf{\(\qquad\)b:}\(\frac{\sigma}{\sqrt{n}}\);
\textbf{\(\qquad\)c:}\(\sigma^2\);
\textbf{\(\qquad\)d:}\(\frac{\sigma^2}{n}\);

\textbf{4)} What is the mean and variance of the sample sum?

\textbf{\(\qquad\)a:}\(\mu\), \(n\sigma\);
\textbf{\(\qquad\)b:}\(n\mu\),\(n\sigma\);
\textbf{\(\qquad\)c:}\(\mu\), \(n\sigma^2\);
\textbf{\(\qquad\)d:}\(n\mu\), \(n\sigma^2\);

\textbf{5)} An inference question implies:

\textbf{\(\qquad\)a:} calculate the expected value of an estimator;
\textbf{\(\qquad\)b:} estimate the value of a parameter;
\textbf{\(\qquad\)c:} calculate a probability of an estimator;
\textbf{\(\qquad\)d:} fit a probability model;

\hypertarget{exercises-8}{%
\section{Exercises}\label{exercises-8}}

\hypertarget{exercise-1-7}{%
\subsubsection{Exercise 1}\label{exercise-1-7}}

An electronics company manufactures resistors that have an average resistance of 100 ohms and
a standard deviation of 10 ohms. The resistance distribution is normal.

\begin{itemize}
\item
  What is the sample mean of \(n=25\) resistors? (R:100)
\item
  What is the variance of the sample mean of \(n=25\) resistors? (R:4)
\item
  What is the standard error of the sample mean of \(n=25\) resistors? (R:2)
\item
  Find the probability
  that a random sample of \(n = 25\) resistors have an average resistance of less than \(95\) ohms (R: 0.0062)
\end{itemize}

\hypertarget{exercise-2-7}{%
\subsubsection{Exercise 2}\label{exercise-2-7}}

A battery model charges an average of \(75\%\) of its capacity in one hour with a standard deviation of \(15\%\).

\begin{itemize}
\item
  If the battery charge is a normal variable, what is the probability that the charge difference between the sample mean of \(25\) batteries and the mean charge is at most \(5\%\)? (R:0.9044)
\item
  If we charge \(100\) batteries, what is that probability? (R:0.9991)
\item
  If instead we only charge \(9\) batteries, what charge \(c\) is exceeded by the sample mean with probability \(0.015\)? (A:85.850)
\end{itemize}

  \bibliography{book.bib,packages.bib}

\end{document}
