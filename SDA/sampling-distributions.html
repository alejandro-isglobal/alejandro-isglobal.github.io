<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Sampling distributions | Statistical Data Analysis for Experimental Sciences</title>
  <meta name="description" content="This is a markdown book titled Statistical Data Analysis for Experimental Sciences by Alejandro Caceres" />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Sampling distributions | Statistical Data Analysis for Experimental Sciences" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a markdown book titled Statistical Data Analysis for Experimental Sciences by Alejandro Caceres" />
  <meta name="github-repo" content="alejandro-isglobal/master" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Sampling distributions | Statistical Data Analysis for Experimental Sciences" />
  
  <meta name="twitter:description" content="This is a markdown book titled Statistical Data Analysis for Experimental Sciences by Alejandro Caceres" />
  

<meta name="author" content="Alejandro CÃ¡ceres" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="normal-distribution.html"/>
<link rel="next" href="central-limit-theorem.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SDA</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#how-to-read-the-book"><i class="fa fa-check"></i><b>1.1</b> How to read the book</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.2</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-description.html"><a href="data-description.html"><i class="fa fa-check"></i><b>2</b> Data description</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-description.html"><a href="data-description.html#scientific-method"><i class="fa fa-check"></i><b>2.1</b> Scientific method</a></li>
<li class="chapter" data-level="2.2" data-path="data-description.html"><a href="data-description.html#data"><i class="fa fa-check"></i><b>2.2</b> Data</a></li>
<li class="chapter" data-level="2.3" data-path="data-description.html"><a href="data-description.html#types-of-outcomes"><i class="fa fa-check"></i><b>2.3</b> Types of outcomes</a></li>
<li class="chapter" data-level="2.4" data-path="data-description.html"><a href="data-description.html#random-experiments"><i class="fa fa-check"></i><b>2.4</b> Random experiments</a></li>
<li class="chapter" data-level="2.5" data-path="data-description.html"><a href="data-description.html#absolute-frequencies"><i class="fa fa-check"></i><b>2.5</b> Absolute frequencies</a></li>
<li class="chapter" data-level="2.6" data-path="data-description.html"><a href="data-description.html#relative-frequencies"><i class="fa fa-check"></i><b>2.6</b> Relative frequencies</a></li>
<li class="chapter" data-level="2.7" data-path="data-description.html"><a href="data-description.html#bar-chart"><i class="fa fa-check"></i><b>2.7</b> Bar chart</a></li>
<li class="chapter" data-level="2.8" data-path="data-description.html"><a href="data-description.html#pie-chart"><i class="fa fa-check"></i><b>2.8</b> Pie chart</a></li>
<li class="chapter" data-level="2.9" data-path="data-description.html"><a href="data-description.html#ordinal-categorical-outcomes"><i class="fa fa-check"></i><b>2.9</b> Ordinal categorical outcomes</a></li>
<li class="chapter" data-level="2.10" data-path="data-description.html"><a href="data-description.html#absolute-and-relative-cumulative-frequencies"><i class="fa fa-check"></i><b>2.10</b> Absolute and relative cumulative frequencies</a></li>
<li class="chapter" data-level="2.11" data-path="data-description.html"><a href="data-description.html#cumulative-frequency-graph"><i class="fa fa-check"></i><b>2.11</b> Cumulative frequency graph</a></li>
<li class="chapter" data-level="2.12" data-path="data-description.html"><a href="data-description.html#numerical-outcomes"><i class="fa fa-check"></i><b>2.12</b> Numerical outcomes</a></li>
<li class="chapter" data-level="2.13" data-path="data-description.html"><a href="data-description.html#transforming-continuous-data"><i class="fa fa-check"></i><b>2.13</b> Transforming continuous data</a></li>
<li class="chapter" data-level="2.14" data-path="data-description.html"><a href="data-description.html#frequency-table-for-a-continuous-variable"><i class="fa fa-check"></i><b>2.14</b> Frequency table for a continuous variable</a></li>
<li class="chapter" data-level="2.15" data-path="data-description.html"><a href="data-description.html#histogram"><i class="fa fa-check"></i><b>2.15</b> Histogram</a></li>
<li class="chapter" data-level="2.16" data-path="data-description.html"><a href="data-description.html#cumulative-frequency-graph-1"><i class="fa fa-check"></i><b>2.16</b> Cumulative frequency graph</a></li>
<li class="chapter" data-level="2.17" data-path="data-description.html"><a href="data-description.html#summary-statistics"><i class="fa fa-check"></i><b>2.17</b> Summary Statistics</a></li>
<li class="chapter" data-level="2.18" data-path="data-description.html"><a href="data-description.html#average-sample-mean"><i class="fa fa-check"></i><b>2.18</b> Average (sample mean)</a></li>
<li class="chapter" data-level="2.19" data-path="data-description.html"><a href="data-description.html#median"><i class="fa fa-check"></i><b>2.19</b> Median</a></li>
<li class="chapter" data-level="2.20" data-path="data-description.html"><a href="data-description.html#dispersion"><i class="fa fa-check"></i><b>2.20</b> Dispersion</a></li>
<li class="chapter" data-level="2.21" data-path="data-description.html"><a href="data-description.html#sample-variance"><i class="fa fa-check"></i><b>2.21</b> Sample variance</a></li>
<li class="chapter" data-level="2.22" data-path="data-description.html"><a href="data-description.html#interquartile-range-iqr"><i class="fa fa-check"></i><b>2.22</b> Interquartile range (IQR)</a></li>
<li class="chapter" data-level="2.23" data-path="data-description.html"><a href="data-description.html#boxplot"><i class="fa fa-check"></i><b>2.23</b> Boxplot</a></li>
<li class="chapter" data-level="2.24" data-path="data-description.html"><a href="data-description.html#questions"><i class="fa fa-check"></i><b>2.24</b> Questions</a></li>
<li class="chapter" data-level="2.25" data-path="data-description.html"><a href="data-description.html#exercises"><i class="fa fa-check"></i><b>2.25</b> Exercises</a></li>
<li class="chapter" data-level="2.26" data-path="data-description.html"><a href="data-description.html#practice"><i class="fa fa-check"></i><b>2.26</b> Practice</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#probability-mesurement"><i class="fa fa-check"></i><b>3.1</b> Probability mesurement</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#classical-probability"><i class="fa fa-check"></i><b>3.2</b> Classical probability</a></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#relative-frequencies-1"><i class="fa fa-check"></i><b>3.3</b> Relative frequencies</a></li>
<li class="chapter" data-level="3.4" data-path="probability.html"><a href="probability.html#frequentist-probability"><i class="fa fa-check"></i><b>3.4</b> Frequentist probability</a></li>
<li class="chapter" data-level="3.5" data-path="probability.html"><a href="probability.html#classical-and-frequentist-probabilities"><i class="fa fa-check"></i><b>3.5</b> Classical and frequentist probabilities</a></li>
<li class="chapter" data-level="3.6" data-path="probability.html"><a href="probability.html#sample-space"><i class="fa fa-check"></i><b>3.6</b> Sample space</a></li>
<li class="chapter" data-level="3.7" data-path="probability.html"><a href="probability.html#events"><i class="fa fa-check"></i><b>3.7</b> Events</a></li>
<li class="chapter" data-level="3.8" data-path="probability.html"><a href="probability.html#algebra-of-events"><i class="fa fa-check"></i><b>3.8</b> Algebra of events</a></li>
<li class="chapter" data-level="3.9" data-path="probability.html"><a href="probability.html#mutually-exclusive-events"><i class="fa fa-check"></i><b>3.9</b> Mutually exclusive events</a></li>
<li class="chapter" data-level="3.10" data-path="probability.html"><a href="probability.html#definition-of-probability"><i class="fa fa-check"></i><b>3.10</b> Definition of probability</a></li>
<li class="chapter" data-level="3.11" data-path="probability.html"><a href="probability.html#probability-table"><i class="fa fa-check"></i><b>3.11</b> Probability table</a></li>
<li class="chapter" data-level="3.12" data-path="probability.html"><a href="probability.html#joint-probabilities"><i class="fa fa-check"></i><b>3.12</b> Joint probabilities</a></li>
<li class="chapter" data-level="3.13" data-path="probability.html"><a href="probability.html#contingency-table"><i class="fa fa-check"></i><b>3.13</b> Contingency table</a></li>
<li class="chapter" data-level="3.14" data-path="probability.html"><a href="probability.html#the-addition-rule"><i class="fa fa-check"></i><b>3.14</b> The addition rule</a></li>
<li class="chapter" data-level="3.15" data-path="probability.html"><a href="probability.html#questions-1"><i class="fa fa-check"></i><b>3.15</b> Questions</a></li>
<li class="chapter" data-level="3.16" data-path="probability.html"><a href="probability.html#exercises-1"><i class="fa fa-check"></i><b>3.16</b> Exercises</a></li>
<li class="chapter" data-level="3.17" data-path="probability.html"><a href="probability.html#practice-1"><i class="fa fa-check"></i><b>3.17</b> Practice</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>4</b> Conditional probability</a>
<ul>
<li class="chapter" data-level="4.1" data-path="conditional-probability.html"><a href="conditional-probability.html#joint-probability"><i class="fa fa-check"></i><b>4.1</b> Joint probability</a></li>
<li class="chapter" data-level="4.2" data-path="conditional-probability.html"><a href="conditional-probability.html#statistical-independence-and-correlation"><i class="fa fa-check"></i><b>4.2</b> Statistical independence and correlation</a></li>
<li class="chapter" data-level="4.3" data-path="conditional-probability.html"><a href="conditional-probability.html#conditional-probability-1"><i class="fa fa-check"></i><b>4.3</b> Conditional probability</a></li>
<li class="chapter" data-level="4.4" data-path="conditional-probability.html"><a href="conditional-probability.html#conditional-contingency-table"><i class="fa fa-check"></i><b>4.4</b> Conditional contingency table</a></li>
<li class="chapter" data-level="4.5" data-path="conditional-probability.html"><a href="conditional-probability.html#statistical-independence"><i class="fa fa-check"></i><b>4.5</b> Statistical independence</a></li>
<li class="chapter" data-level="4.6" data-path="conditional-probability.html"><a href="conditional-probability.html#statistical-dependency"><i class="fa fa-check"></i><b>4.6</b> Statistical dependency</a></li>
<li class="chapter" data-level="4.7" data-path="conditional-probability.html"><a href="conditional-probability.html#diagnostic-test"><i class="fa fa-check"></i><b>4.7</b> Diagnostic test</a></li>
<li class="chapter" data-level="4.8" data-path="conditional-probability.html"><a href="conditional-probability.html#inverse-probabilities"><i class="fa fa-check"></i><b>4.8</b> Inverse probabilities</a></li>
<li class="chapter" data-level="4.9" data-path="conditional-probability.html"><a href="conditional-probability.html#bayes-theorem"><i class="fa fa-check"></i><b>4.9</b> Bayesâ Theorem</a></li>
<li class="chapter" data-level="4.10" data-path="conditional-probability.html"><a href="conditional-probability.html#questions-2"><i class="fa fa-check"></i><b>4.10</b> Questions</a></li>
<li class="chapter" data-level="4.11" data-path="conditional-probability.html"><a href="conditional-probability.html#exercises-2"><i class="fa fa-check"></i><b>4.11</b> Exercises</a></li>
<li class="chapter" data-level="4.12" data-path="conditional-probability.html"><a href="conditional-probability.html#practice-2"><i class="fa fa-check"></i><b>4.12</b> Practice</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html"><i class="fa fa-check"></i><b>5</b> Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="5.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#definition-of-a-random-variable"><i class="fa fa-check"></i><b>5.1</b> Definition of a Random Variable</a></li>
<li class="chapter" data-level="5.2" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#the-value-of-a-random-variable"><i class="fa fa-check"></i><b>5.2</b> The value of a random variable</a></li>
<li class="chapter" data-level="5.3" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#probability-of-random-variables"><i class="fa fa-check"></i><b>5.3</b> Probability of random variables</a></li>
<li class="chapter" data-level="5.4" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#probability-functions"><i class="fa fa-check"></i><b>5.4</b> Probability functions</a></li>
<li class="chapter" data-level="5.5" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#probability-mass-functions"><i class="fa fa-check"></i><b>5.5</b> Probability mass functions</a></li>
<li class="chapter" data-level="5.6" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#mean-or-expected-value"><i class="fa fa-check"></i><b>5.6</b> Mean or expected value</a></li>
<li class="chapter" data-level="5.7" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#variance"><i class="fa fa-check"></i><b>5.7</b> Variance</a></li>
<li class="chapter" data-level="5.8" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#probability-functions-for-functions-of-x"><i class="fa fa-check"></i><b>5.8</b> Probability functions for functions of <span class="math inline">\(X\)</span></a></li>
<li class="chapter" data-level="5.9" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#probability-distribution"><i class="fa fa-check"></i><b>5.9</b> Probability distribution</a></li>
<li class="chapter" data-level="5.10" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#probability-function-and-probability-distribution"><i class="fa fa-check"></i><b>5.10</b> Probability function and probability distribution</a></li>
<li class="chapter" data-level="5.11" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#quantiles"><i class="fa fa-check"></i><b>5.11</b> Quantiles</a></li>
<li class="chapter" data-level="5.12" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#summary"><i class="fa fa-check"></i><b>5.12</b> Summary</a></li>
<li class="chapter" data-level="5.13" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#questions-3"><i class="fa fa-check"></i><b>5.13</b> Questions</a></li>
<li class="chapter" data-level="5.14" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#exercises-3"><i class="fa fa-check"></i><b>5.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="continous-random-variables.html"><a href="continous-random-variables.html"><i class="fa fa-check"></i><b>6</b> Continous Random Variables</a>
<ul>
<li class="chapter" data-level="6.1" data-path="continous-random-variables.html"><a href="continous-random-variables.html#probabilities-of-continuous-random-variables"><i class="fa fa-check"></i><b>6.1</b> Probabilities of continuous random variables</a></li>
<li class="chapter" data-level="6.2" data-path="continous-random-variables.html"><a href="continous-random-variables.html#relative-frequencies-2"><i class="fa fa-check"></i><b>6.2</b> Relative frequencies</a></li>
<li class="chapter" data-level="6.3" data-path="continous-random-variables.html"><a href="continous-random-variables.html#probability-density-function"><i class="fa fa-check"></i><b>6.3</b> Probability Density Function</a></li>
<li class="chapter" data-level="6.4" data-path="continous-random-variables.html"><a href="continous-random-variables.html#total-area-under-the-curve"><i class="fa fa-check"></i><b>6.4</b> Total area under the curve</a></li>
<li class="chapter" data-level="6.5" data-path="continous-random-variables.html"><a href="continous-random-variables.html#probabilities-of-continous-variables"><i class="fa fa-check"></i><b>6.5</b> Probabilities of continous variables</a></li>
<li class="chapter" data-level="6.6" data-path="continous-random-variables.html"><a href="continous-random-variables.html#probability-distribution-1"><i class="fa fa-check"></i><b>6.6</b> Probability distribution</a></li>
<li class="chapter" data-level="6.7" data-path="continous-random-variables.html"><a href="continous-random-variables.html#probability-plots"><i class="fa fa-check"></i><b>6.7</b> Probability plots</a></li>
<li class="chapter" data-level="6.8" data-path="continous-random-variables.html"><a href="continous-random-variables.html#mean"><i class="fa fa-check"></i><b>6.8</b> Mean</a></li>
<li class="chapter" data-level="6.9" data-path="continous-random-variables.html"><a href="continous-random-variables.html#variance-1"><i class="fa fa-check"></i><b>6.9</b> Variance</a></li>
<li class="chapter" data-level="6.10" data-path="continous-random-variables.html"><a href="continous-random-variables.html#functions-of-x"><i class="fa fa-check"></i><b>6.10</b> Functions of <span class="math inline">\(X\)</span></a></li>
<li class="chapter" data-level="6.11" data-path="continous-random-variables.html"><a href="continous-random-variables.html#exercises-4"><i class="fa fa-check"></i><b>6.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="discrete-probability-models.html"><a href="discrete-probability-models.html"><i class="fa fa-check"></i><b>7</b> Discrete Probability Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="discrete-probability-models.html"><a href="discrete-probability-models.html#probability-model"><i class="fa fa-check"></i><b>7.1</b> Probability model</a></li>
<li class="chapter" data-level="7.2" data-path="discrete-probability-models.html"><a href="discrete-probability-models.html#parametric-models"><i class="fa fa-check"></i><b>7.2</b> Parametric models</a></li>
<li class="chapter" data-level="7.3" data-path="discrete-probability-models.html"><a href="discrete-probability-models.html#uniform-probability-mass-function-one-parameter"><i class="fa fa-check"></i><b>7.3</b> Uniform probability mass function (one parameter)</a></li>
<li class="chapter" data-level="7.4" data-path="discrete-probability-models.html"><a href="discrete-probability-models.html#uniform-probability-mass-function-two-parameters"><i class="fa fa-check"></i><b>7.4</b> Uniform probability mass function (two parameters)</a></li>
<li class="chapter" data-level="7.5" data-path="discrete-probability-models.html"><a href="discrete-probability-models.html#bernoulli-trial"><i class="fa fa-check"></i><b>7.5</b> Bernoulli trial</a></li>
<li class="chapter" data-level="7.6" data-path="discrete-probability-models.html"><a href="discrete-probability-models.html#binomial-experiment"><i class="fa fa-check"></i><b>7.6</b> Binomial experiment</a></li>
<li class="chapter" data-level="7.7" data-path="discrete-probability-models.html"><a href="discrete-probability-models.html#binomial-probability-function"><i class="fa fa-check"></i><b>7.7</b> Binomial probability function</a></li>
<li class="chapter" data-level="7.8" data-path="discrete-probability-models.html"><a href="discrete-probability-models.html#negative-binomial"><i class="fa fa-check"></i><b>7.8</b> Negative binomial</a></li>
<li class="chapter" data-level="7.9" data-path="discrete-probability-models.html"><a href="discrete-probability-models.html#geometric-distribution"><i class="fa fa-check"></i><b>7.9</b> Geometric distribution</a></li>
<li class="chapter" data-level="7.10" data-path="discrete-probability-models.html"><a href="discrete-probability-models.html#hypergeometric-model"><i class="fa fa-check"></i><b>7.10</b> Hypergeometric model</a></li>
<li class="chapter" data-level="7.11" data-path="discrete-probability-models.html"><a href="discrete-probability-models.html#questions-4"><i class="fa fa-check"></i><b>7.11</b> Questions</a></li>
<li class="chapter" data-level="7.12" data-path="discrete-probability-models.html"><a href="discrete-probability-models.html#exercises-5"><i class="fa fa-check"></i><b>7.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="poisson-and-exponential-models.html"><a href="poisson-and-exponential-models.html"><i class="fa fa-check"></i><b>8</b> Poisson and Exponential Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="poisson-and-exponential-models.html"><a href="poisson-and-exponential-models.html#discrete-probability-models-1"><i class="fa fa-check"></i><b>8.1</b> Discrete probability models</a></li>
<li class="chapter" data-level="8.2" data-path="poisson-and-exponential-models.html"><a href="poisson-and-exponential-models.html#poissson-experiment"><i class="fa fa-check"></i><b>8.2</b> Poissson experiment</a></li>
<li class="chapter" data-level="8.3" data-path="poisson-and-exponential-models.html"><a href="poisson-and-exponential-models.html#poisson-probability-mass-function"><i class="fa fa-check"></i><b>8.3</b> Poisson probability mass function</a></li>
<li class="chapter" data-level="8.4" data-path="poisson-and-exponential-models.html"><a href="poisson-and-exponential-models.html#continuous-probability-models"><i class="fa fa-check"></i><b>8.4</b> Continuous probability models</a></li>
<li class="chapter" data-level="8.5" data-path="poisson-and-exponential-models.html"><a href="poisson-and-exponential-models.html#exponential-process"><i class="fa fa-check"></i><b>8.5</b> Exponential process</a></li>
<li class="chapter" data-level="8.6" data-path="poisson-and-exponential-models.html"><a href="poisson-and-exponential-models.html#exponential-probability-density"><i class="fa fa-check"></i><b>8.6</b> Exponential probability density</a></li>
<li class="chapter" data-level="8.7" data-path="poisson-and-exponential-models.html"><a href="poisson-and-exponential-models.html#exponential-distribution"><i class="fa fa-check"></i><b>8.7</b> Exponential Distribution</a></li>
<li class="chapter" data-level="8.8" data-path="poisson-and-exponential-models.html"><a href="poisson-and-exponential-models.html#questions-5"><i class="fa fa-check"></i><b>8.8</b> Questions</a></li>
<li class="chapter" data-level="8.9" data-path="poisson-and-exponential-models.html"><a href="poisson-and-exponential-models.html#exercises-6"><i class="fa fa-check"></i><b>8.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="normal-distribution.html"><a href="normal-distribution.html"><i class="fa fa-check"></i><b>9</b> Normal Distribution</a>
<ul>
<li class="chapter" data-level="9.1" data-path="normal-distribution.html"><a href="normal-distribution.html#history"><i class="fa fa-check"></i><b>9.1</b> History</a></li>
<li class="chapter" data-level="9.2" data-path="normal-distribution.html"><a href="normal-distribution.html#normal-density"><i class="fa fa-check"></i><b>9.2</b> normal density</a></li>
<li class="chapter" data-level="9.3" data-path="normal-distribution.html"><a href="normal-distribution.html#definition"><i class="fa fa-check"></i><b>9.3</b> Definition</a></li>
<li class="chapter" data-level="9.4" data-path="normal-distribution.html"><a href="normal-distribution.html#probability-distribution-2"><i class="fa fa-check"></i><b>9.4</b> Probability distribution</a></li>
<li class="chapter" data-level="9.5" data-path="normal-distribution.html"><a href="normal-distribution.html#standard-normal-density"><i class="fa fa-check"></i><b>9.5</b> Standard normal density</a></li>
<li class="chapter" data-level="9.6" data-path="normal-distribution.html"><a href="normal-distribution.html#standard-distribution"><i class="fa fa-check"></i><b>9.6</b> Standard distribution</a></li>
<li class="chapter" data-level="9.7" data-path="normal-distribution.html"><a href="normal-distribution.html#standardization"><i class="fa fa-check"></i><b>9.7</b> Standardization</a></li>
<li class="chapter" data-level="9.8" data-path="normal-distribution.html"><a href="normal-distribution.html#questions-6"><i class="fa fa-check"></i><b>9.8</b> Questions</a></li>
<li class="chapter" data-level="9.9" data-path="normal-distribution.html"><a href="normal-distribution.html#exercises-7"><i class="fa fa-check"></i><b>9.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="sampling-distributions.html"><a href="sampling-distributions.html"><i class="fa fa-check"></i><b>10</b> Sampling distributions</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sampling-distributions.html"><a href="sampling-distributions.html#random-sample"><i class="fa fa-check"></i><b>10.1</b> Random sample</a></li>
<li class="chapter" data-level="10.2" data-path="sampling-distributions.html"><a href="sampling-distributions.html#parameter-estimation"><i class="fa fa-check"></i><b>10.2</b> Parameter estimation</a></li>
<li class="chapter" data-level="10.3" data-path="sampling-distributions.html"><a href="sampling-distributions.html#law-of-large-numbers"><i class="fa fa-check"></i><b>10.3</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="10.4" data-path="sampling-distributions.html"><a href="sampling-distributions.html#inference"><i class="fa fa-check"></i><b>10.4</b> Inference</a></li>
<li class="chapter" data-level="10.5" data-path="sampling-distributions.html"><a href="sampling-distributions.html#sample-mean"><i class="fa fa-check"></i><b>10.5</b> Sample mean</a></li>
<li class="chapter" data-level="10.6" data-path="sampling-distributions.html"><a href="sampling-distributions.html#prediction"><i class="fa fa-check"></i><b>10.6</b> Prediction</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="sampling-distributions.html"><a href="sampling-distributions.html#example-pacemaker-prediction"><i class="fa fa-check"></i><b>10.6.1</b> <strong>Example: Pacemaker Prediction</strong></a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="sampling-distributions.html"><a href="sampling-distributions.html#validation"><i class="fa fa-check"></i><b>10.7</b> Validation</a></li>
<li class="chapter" data-level="10.8" data-path="sampling-distributions.html"><a href="sampling-distributions.html#sample-sum"><i class="fa fa-check"></i><b>10.8</b> Sample Sum</a>
<ul>
<li class="chapter" data-level="10.8.1" data-path="sampling-distributions.html"><a href="sampling-distributions.html#example-cables"><i class="fa fa-check"></i><b>10.8.1</b> Example (Cables)</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="sampling-distributions.html"><a href="sampling-distributions.html#sample-variance-1"><i class="fa fa-check"></i><b>10.9</b> Sample Variance</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="sampling-distributions.html"><a href="sampling-distributions.html#example-cables-1"><i class="fa fa-check"></i><b>10.9.1</b> Example (Cables)</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="sampling-distributions.html"><a href="sampling-distributions.html#distribution-of-the-sample-variance"><i class="fa fa-check"></i><b>10.10</b> Distribution of the Sample Variance</a></li>
<li class="chapter" data-level="10.11" data-path="sampling-distributions.html"><a href="sampling-distributions.html#the-chi2-distribution"><i class="fa fa-check"></i><b>10.11</b> The <span class="math inline">\(\chi^2\)</span> Distribution</a></li>
<li class="chapter" data-level="10.12" data-path="sampling-distributions.html"><a href="sampling-distributions.html#questions-7"><i class="fa fa-check"></i><b>10.12</b> Questions</a></li>
<li class="chapter" data-level="10.13" data-path="sampling-distributions.html"><a href="sampling-distributions.html#exercises-8"><i class="fa fa-check"></i><b>10.13</b> Exercises</a>
<ul>
<li class="chapter" data-level="10.13.1" data-path="sampling-distributions.html"><a href="sampling-distributions.html#from-estimation-to-inference"><i class="fa fa-check"></i><b>10.13.1</b> From Estimation to Inference</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="central-limit-theorem.html"><a href="central-limit-theorem.html"><i class="fa fa-check"></i><b>11</b> Central limit theorem</a>
<ul>
<li class="chapter" data-level="11.1" data-path="central-limit-theorem.html"><a href="central-limit-theorem.html#margin-of-error"><i class="fa fa-check"></i><b>11.1</b> Margin of error</a></li>
<li class="chapter" data-level="11.2" data-path="central-limit-theorem.html"><a href="central-limit-theorem.html#averages-of-normal-variables"><i class="fa fa-check"></i><b>11.2</b> Averages of normal variables</a></li>
<li class="chapter" data-level="11.3" data-path="central-limit-theorem.html"><a href="central-limit-theorem.html#central-limit-theorem-1"><i class="fa fa-check"></i><b>11.3</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="11.4" data-path="central-limit-theorem.html"><a href="central-limit-theorem.html#sample-sum-and-clt"><i class="fa fa-check"></i><b>11.4</b> Sample sum and CLT</a></li>
<li class="chapter" data-level="11.5" data-path="central-limit-theorem.html"><a href="central-limit-theorem.html#unknown-sigma"><i class="fa fa-check"></i><b>11.5</b> Unknown <span class="math inline">\(\sigma\)</span></a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="central-limit-theorem.html"><a href="central-limit-theorem.html#example-pacemaker-prediction-1"><i class="fa fa-check"></i><b>11.5.1</b> <strong>Example: Pacemaker Prediction</strong></a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="central-limit-theorem.html"><a href="central-limit-theorem.html#t-statistic"><i class="fa fa-check"></i><b>11.6</b> T-statistic</a></li>
<li class="chapter" data-level="11.7" data-path="central-limit-theorem.html"><a href="central-limit-theorem.html#questions-8"><i class="fa fa-check"></i><b>11.7</b> Questions</a></li>
<li class="chapter" data-level="11.8" data-path="central-limit-theorem.html"><a href="central-limit-theorem.html#exercises-9"><i class="fa fa-check"></i><b>11.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html"><i class="fa fa-check"></i><b>12</b> Maximum likelihood</a>
<ul>
<li class="chapter" data-level="12.1" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#statistic"><i class="fa fa-check"></i><b>12.1</b> Statistic</a></li>
<li class="chapter" data-level="12.2" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#properties"><i class="fa fa-check"></i><b>12.2</b> Properties</a></li>
<li class="chapter" data-level="12.3" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#maximum-likelihood-1"><i class="fa fa-check"></i><b>12.3</b> Maximum likelihood</a></li>
<li class="chapter" data-level="12.4" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#maximum-likelihood-2"><i class="fa fa-check"></i><b>12.4</b> Maximum likelihood</a></li>
<li class="chapter" data-level="12.5" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#questions-9"><i class="fa fa-check"></i><b>12.5</b> Questions</a></li>
<li class="chapter" data-level="12.6" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#exercises-10"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="interval-estimation.html"><a href="interval-estimation.html"><i class="fa fa-check"></i><b>13</b> Interval estimation</a>
<ul>
<li class="chapter" data-level="13.1" data-path="interval-estimation.html"><a href="interval-estimation.html#revisiting-parameter-estimation-and-marging-of-error"><i class="fa fa-check"></i><b>13.1</b> Revisiting parameter estimation and marging of error</a></li>
<li class="chapter" data-level="13.2" data-path="interval-estimation.html"><a href="interval-estimation.html#interval-estimation-for-the-mean"><i class="fa fa-check"></i><b>13.2</b> Interval estimation for the mean</a></li>
<li class="chapter" data-level="13.3" data-path="interval-estimation.html"><a href="interval-estimation.html#confidence-interval-estimation"><i class="fa fa-check"></i><b>13.3</b> Confidence Interval Estimation</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="interval-estimation.html"><a href="interval-estimation.html#estimation-of-the-mean-for-normal-variables"><i class="fa fa-check"></i><b>13.3.1</b> Estimation of the mean for normal variables</a></li>
<li class="chapter" data-level="13.3.2" data-path="interval-estimation.html"><a href="interval-estimation.html#estimation-of-the-proportion-for-dichotomic-variables"><i class="fa fa-check"></i><b>13.3.2</b> Estimation of the proportion for dichotomic variables</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="interval-estimation.html"><a href="interval-estimation.html#estimation-of-the-variance"><i class="fa fa-check"></i><b>13.4</b> Estimation of the variance</a></li>
<li class="chapter" data-level="13.5" data-path="interval-estimation.html"><a href="interval-estimation.html#confidence-interval-for-the-variance"><i class="fa fa-check"></i><b>13.5</b> Confidence interval for the variance</a></li>
<li class="chapter" data-level="13.6" data-path="interval-estimation.html"><a href="interval-estimation.html#questions-10"><i class="fa fa-check"></i><b>13.6</b> Questions</a></li>
<li class="chapter" data-level="13.7" data-path="interval-estimation.html"><a href="interval-estimation.html#exercises-11"><i class="fa fa-check"></i><b>13.7</b> Exercises</a></li>
<li class="chapter" data-level="13.8" data-path="interval-estimation.html"><a href="interval-estimation.html#practice-3"><i class="fa fa-check"></i><b>13.8</b> Practice</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>14</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="14.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-formulation"><i class="fa fa-check"></i><b>14.1</b> Hypothesis formulation</a></li>
<li class="chapter" data-level="14.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>14.2</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="14.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-testing-for-the-mean"><i class="fa fa-check"></i><b>14.3</b> Hypothesis testing for the mean</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-test-with-a-confidence-interval"><i class="fa fa-check"></i><b>14.3.1</b> Hypothesis test with a confidence interval</a></li>
<li class="chapter" data-level="14.3.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-test-with-acceptancerejection-zones"><i class="fa fa-check"></i><b>14.3.2</b> Hypothesis test with acceptance/rejection zones</a></li>
<li class="chapter" data-level="14.3.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-test-with-a-p-value"><i class="fa fa-check"></i><b>14.3.3</b> Hypothesis test with a P-value</a></li>
<li class="chapter" data-level="14.3.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#upper-tail-hypothesis"><i class="fa fa-check"></i><b>14.3.4</b> Upper tail hypothesis</a></li>
<li class="chapter" data-level="14.3.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#paired-t-test"><i class="fa fa-check"></i><b>14.3.5</b> Paired t-test</a></li>
<li class="chapter" data-level="14.3.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#lower-tail-hypothesis"><i class="fa fa-check"></i><b>14.3.6</b> Lower tail hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-testing-for-the-proportion"><i class="fa fa-check"></i><b>14.4</b> Hypothesis testing for the proportion</a></li>
<li class="chapter" data-level="14.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#hypothesis-testing-for-the-variance"><i class="fa fa-check"></i><b>14.5</b> Hypothesis Testing for the Variance</a></li>
<li class="chapter" data-level="14.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#errors-in-hypothesis-testing"><i class="fa fa-check"></i><b>14.6</b> Errors in hypothesis testing</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>14.6.1</b> Sensitivity and Specificity</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#exercises-12"><i class="fa fa-check"></i><b>14.7</b> Exercises</a></li>
<li class="chapter" data-level="14.8" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#practice-4"><i class="fa fa-check"></i><b>14.8</b> Practice</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="contingency-tables.html"><a href="contingency-tables.html"><i class="fa fa-check"></i><b>15</b> Contingency tables</a>
<ul>
<li class="chapter" data-level="15.1" data-path="contingency-tables.html"><a href="contingency-tables.html#difference-between-proportions"><i class="fa fa-check"></i><b>15.1</b> Difference between proportions</a></li>
<li class="chapter" data-level="15.2" data-path="contingency-tables.html"><a href="contingency-tables.html#difference-between-proportions-1"><i class="fa fa-check"></i><b>15.2</b> Difference between proportions</a></li>
<li class="chapter" data-level="15.3" data-path="contingency-tables.html"><a href="contingency-tables.html#contingency-table-of-conditional-probabilities"><i class="fa fa-check"></i><b>15.3</b> Contingency table of conditional probabilities</a></li>
<li class="chapter" data-level="15.4" data-path="contingency-tables.html"><a href="contingency-tables.html#test-for-the-difference-between-proportions"><i class="fa fa-check"></i><b>15.4</b> Test for the difference between proportions</a></li>
<li class="chapter" data-level="15.5" data-path="contingency-tables.html"><a href="contingency-tables.html#chi2-test"><i class="fa fa-check"></i><b>15.5</b> <span class="math inline">\(\chi^2\)</span> test</a></li>
<li class="chapter" data-level="15.6" data-path="contingency-tables.html"><a href="contingency-tables.html#fishers-exact-test"><i class="fa fa-check"></i><b>15.6</b> Fisherâs exact test</a></li>
<li class="chapter" data-level="15.7" data-path="contingency-tables.html"><a href="contingency-tables.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>15.7</b> Hypergeometric distribution</a></li>
<li class="chapter" data-level="15.8" data-path="contingency-tables.html"><a href="contingency-tables.html#difference-between-several-proportions"><i class="fa fa-check"></i><b>15.8</b> Difference between several proportions</a></li>
<li class="chapter" data-level="15.9" data-path="contingency-tables.html"><a href="contingency-tables.html#goodness-of-fit"><i class="fa fa-check"></i><b>15.9</b> Goodness of fit</a></li>
<li class="chapter" data-level="15.10" data-path="contingency-tables.html"><a href="contingency-tables.html#questions-11"><i class="fa fa-check"></i><b>15.10</b> Questions</a></li>
<li class="chapter" data-level="15.11" data-path="contingency-tables.html"><a href="contingency-tables.html#practice-5"><i class="fa fa-check"></i><b>15.11</b> Practice</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html"><i class="fa fa-check"></i><b>16</b> Mean differences between two samples</a>
<ul>
<li class="chapter" data-level="16.1" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#difference-in-means-between-two-groups"><i class="fa fa-check"></i><b>16.1</b> Difference in means between two groups</a></li>
<li class="chapter" data-level="16.2" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#data-1"><i class="fa fa-check"></i><b>16.2</b> Data</a></li>
<li class="chapter" data-level="16.3" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#difference-between-means"><i class="fa fa-check"></i><b>16.3</b> Difference between means</a></li>
<li class="chapter" data-level="16.4" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#hypothesis-test"><i class="fa fa-check"></i><b>16.4</b> Hypothesis test</a></li>
<li class="chapter" data-level="16.5" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#estiamtor-of-the-mean-difference"><i class="fa fa-check"></i><b>16.5</b> Estiamtor of the mean difference</a></li>
<li class="chapter" data-level="16.6" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#standardized-error"><i class="fa fa-check"></i><b>16.6</b> Standardized error</a></li>
<li class="chapter" data-level="16.7" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#standardized-error-for-the-null"><i class="fa fa-check"></i><b>16.7</b> Standardized error for the null</a></li>
<li class="chapter" data-level="16.8" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#mean-differences-when-n-is-small"><i class="fa fa-check"></i><b>16.8</b> Mean differences when <span class="math inline">\(n\)</span> is small</a></li>
<li class="chapter" data-level="16.9" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#data-2"><i class="fa fa-check"></i><b>16.9</b> Data</a></li>
<li class="chapter" data-level="16.10" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#difference-between-means-1"><i class="fa fa-check"></i><b>16.10</b> Difference between means</a></li>
<li class="chapter" data-level="16.11" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#hypothesis-test-1"><i class="fa fa-check"></i><b>16.11</b> Hypothesis test</a></li>
<li class="chapter" data-level="16.12" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#estimator-of-the-mean-difference"><i class="fa fa-check"></i><b>16.12</b> Estimator of the mean difference</a></li>
<li class="chapter" data-level="16.13" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#standardized-error-for-the-null-1"><i class="fa fa-check"></i><b>16.13</b> Standardized error for the null</a></li>
<li class="chapter" data-level="16.14" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#mean-differences-with-unequall-variances"><i class="fa fa-check"></i><b>16.14</b> Mean differences with unequall variances</a></li>
<li class="chapter" data-level="16.15" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#data-3"><i class="fa fa-check"></i><b>16.15</b> Data</a></li>
<li class="chapter" data-level="16.16" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#questions-12"><i class="fa fa-check"></i><b>16.16</b> Questions</a></li>
<li class="chapter" data-level="16.17" data-path="mean-differences-between-two-samples.html"><a href="mean-differences-between-two-samples.html#practice-6"><i class="fa fa-check"></i><b>16.17</b> Practice</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html"><i class="fa fa-check"></i><b>17</b> Mean differences across several groups</a>
<ul>
<li class="chapter" data-level="17.1" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#different-means-among-several-conditions"><i class="fa fa-check"></i><b>17.1</b> Different means among several conditions</a></li>
<li class="chapter" data-level="17.2" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#data-4"><i class="fa fa-check"></i><b>17.2</b> Data</a></li>
<li class="chapter" data-level="17.3" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#difference-between-means-2"><i class="fa fa-check"></i><b>17.3</b> Difference between means</a></li>
<li class="chapter" data-level="17.4" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#hypothesis-test-2"><i class="fa fa-check"></i><b>17.4</b> Hypothesis test</a>
<ul>
<li class="chapter" data-level="17.4.1" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#distribution-of-group-means-under-h_0"><i class="fa fa-check"></i><b>17.4.1</b> Distribution of group means under <span class="math inline">\(H_0\)</span></a></li>
<li class="chapter" data-level="17.4.2" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#sources-of-variation"><i class="fa fa-check"></i><b>17.4.2</b> Sources of variation</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#variance-components-estimators"><i class="fa fa-check"></i><b>17.5</b> Variance components estimators</a></li>
<li class="chapter" data-level="17.6" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#analysis-of-variance-anova"><i class="fa fa-check"></i><b>17.6</b> Analysis of variance (ANOVA)</a></li>
<li class="chapter" data-level="17.7" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#anova-for-two-groups"><i class="fa fa-check"></i><b>17.7</b> ANOVA for Two Groups</a></li>
<li class="chapter" data-level="17.8" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#linear-model"><i class="fa fa-check"></i><b>17.8</b> Linear model</a></li>
<li class="chapter" data-level="17.9" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#way-anova"><i class="fa fa-check"></i><b>17.9</b> 2-way ANOVA</a></li>
<li class="chapter" data-level="17.10" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#data-5"><i class="fa fa-check"></i><b>17.10</b> Data</a></li>
<li class="chapter" data-level="17.11" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#modeling-residuals"><i class="fa fa-check"></i><b>17.11</b> Modeling residuals</a></li>
<li class="chapter" data-level="17.12" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#way-anova-linear-model"><i class="fa fa-check"></i><b>17.12</b> 2-way ANOVA linear model</a></li>
<li class="chapter" data-level="17.13" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#hypothesis-tests"><i class="fa fa-check"></i><b>17.13</b> Hypothesis tests</a></li>
<li class="chapter" data-level="17.14" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#variance-components"><i class="fa fa-check"></i><b>17.14</b> Variance components</a></li>
<li class="chapter" data-level="17.15" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#way-anova-with-interaction"><i class="fa fa-check"></i><b>17.15</b> 2-way ANOVA with interaction</a></li>
<li class="chapter" data-level="17.16" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#linear-model-1"><i class="fa fa-check"></i><b>17.16</b> Linear model</a></li>
<li class="chapter" data-level="17.17" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#hypothesis-tests-1"><i class="fa fa-check"></i><b>17.17</b> Hypothesis tests</a></li>
<li class="chapter" data-level="17.18" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#variance-components-1"><i class="fa fa-check"></i><b>17.18</b> Variance components</a></li>
<li class="chapter" data-level="17.19" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#questions-13"><i class="fa fa-check"></i><b>17.19</b> Questions</a></li>
<li class="chapter" data-level="17.20" data-path="mean-differences-across-several-groups.html"><a href="mean-differences-across-several-groups.html#practice-7"><i class="fa fa-check"></i><b>17.20</b> Practice</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html"><i class="fa fa-check"></i><b>18</b> Regression and Correlation</a>
<ul>
<li class="chapter" data-level="18.1" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#correlations"><i class="fa fa-check"></i><b>18.1</b> Correlations</a></li>
<li class="chapter" data-level="18.2" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#data-6"><i class="fa fa-check"></i><b>18.2</b> Data</a></li>
<li class="chapter" data-level="18.3" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#normal-bivariate"><i class="fa fa-check"></i><b>18.3</b> Normal bivariate</a></li>
<li class="chapter" data-level="18.4" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#estimators"><i class="fa fa-check"></i><b>18.4</b> Estimators</a></li>
<li class="chapter" data-level="18.5" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#correlation-coefficient"><i class="fa fa-check"></i><b>18.5</b> Correlation coefficient</a></li>
<li class="chapter" data-level="18.6" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#hypothesis-contrast"><i class="fa fa-check"></i><b>18.6</b> Hypothesis contrast</a></li>
<li class="chapter" data-level="18.7" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#regression-analysis"><i class="fa fa-check"></i><b>18.7</b> Regression analysis</a></li>
<li class="chapter" data-level="18.8" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#linear-model-2"><i class="fa fa-check"></i><b>18.8</b> Linear model</a></li>
<li class="chapter" data-level="18.9" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#hypothesis-contrast-1"><i class="fa fa-check"></i><b>18.9</b> Hypothesis contrast</a></li>
<li class="chapter" data-level="18.10" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#estimators-1"><i class="fa fa-check"></i><b>18.10</b> Estimators</a></li>
<li class="chapter" data-level="18.11" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>18.11</b> Hypothesis testing</a></li>
<li class="chapter" data-level="18.12" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#stratified-analysis"><i class="fa fa-check"></i><b>18.12</b> Stratified analysis</a></li>
<li class="chapter" data-level="18.13" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#multiple-regression"><i class="fa fa-check"></i><b>18.13</b> Multiple Regression</a></li>
<li class="chapter" data-level="18.14" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#multiple-regression-interaction"><i class="fa fa-check"></i><b>18.14</b> Multiple Regression interaction</a></li>
<li class="chapter" data-level="18.15" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#model-diagnostics"><i class="fa fa-check"></i><b>18.15</b> Model diagnostics</a></li>
<li class="chapter" data-level="18.16" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#questions-14"><i class="fa fa-check"></i><b>18.16</b> Questions</a></li>
<li class="chapter" data-level="18.17" data-path="regression-and-correlation.html"><a href="regression-and-correlation.html#practice-8"><i class="fa fa-check"></i><b>18.17</b> Practice</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="apendix.html"><a href="apendix.html"><i class="fa fa-check"></i><b>19</b> Apendix</a>
<ul>
<li class="chapter" data-level="19.1" data-path="apendix.html"><a href="apendix.html#solutions-to-questions"><i class="fa fa-check"></i><b>19.1</b> Solutions to Questions</a></li>
<li class="chapter" data-level="19.2" data-path="apendix.html"><a href="apendix.html#summary-tables-python-and-r-code"><i class="fa fa-check"></i><b>19.2</b> Summary tables, Python and R code</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="apendix.html"><a href="apendix.html#creating-data-frames-and-loading-text-files"><i class="fa fa-check"></i><b>19.2.1</b> Creating Data Frames and Loading Text Files</a></li>
<li class="chapter" data-level="19.2.2" data-path="apendix.html"><a href="apendix.html#python-and-r-functions-for-data-description"><i class="fa fa-check"></i><b>19.2.2</b> Python and R Functions for Data Description</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="apendix.html"><a href="apendix.html#summary-of-common-probability-models"><i class="fa fa-check"></i><b>19.3</b> Summary of common probability models</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="apendix.html"><a href="apendix.html#python-and-r-functions-for-probability-models"><i class="fa fa-check"></i><b>19.3.1</b> Python and R Functions for Probability Models</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="apendix.html"><a href="apendix.html#summary-of-hypothesis"><i class="fa fa-check"></i><b>19.4</b> Summary of hypothesis</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="apendix.html"><a href="apendix.html#python-and-r-functions-for-hypothesis-tesing"><i class="fa fa-check"></i><b>19.4.1</b> Python and R Functions for Hypothesis tesing</a></li>
<li class="chapter" data-level="19.4.2" data-path="apendix.html"><a href="apendix.html#python-libraries-required"><i class="fa fa-check"></i><b>19.4.2</b> Python Libraries Required</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Data Analysis
for
Experimental Sciences</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sampling-distributions" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">Chapter 10</span> Sampling distributions<a href="sampling-distributions.html#sampling-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>When making predictions, the probabilities of the outcomes of an experiment are considered fundamental properties of the system or process under study. To carry out these predictions, we require a model and its parameters to fully specify the probability mass (or density) function that describes the propensities of the outcomes.</p>
<p>These parameters may be derived from the experimental design and the underlying theory supporting the experiment. However, when our goal is not just to make predictions but to test a theory or model, we must be able to estimate the parameters from experimental data and compare them to the theoretical values. It is also common to encounter situations where no established theory predicts the value of a parameterâparticularly when it represents an unknown physical property of a system. In such cases, the primary objective is to estimate this parameter and draw conclusions based on its value.</p>
<p>As with probabilities, the parameters of a probabilistic model are abstract and unobservable, but they can be partially revealed through repeated experimentation.</p>
<p>In this chapter, we study the estimation of the mean and variance of normal distributions using random samples. These two quantities are the natural parameters of the normal distribution. The mean represents the expected value of the experiment and often directly corresponds to the physical property of interestâsuch as the mass or charge of the electron, or the longevity of a pacemaker. The variance, on the other hand, can reflect the uncertainty introduced by the experimental process itself (as in the case of J.J. Thomsonâs measurements), or variability among observational units (such as patients in a clinical trial of pacemakers).</p>
<p>We will introduce the sample mean (average) and the sample variance as random variables used to estimate these parameters. Both the sample mean and the sample variance have their own probability density functions, known as sampling distributions. We will use these distributions to quantify how precisely we can estimate the mean and variance of the random experiment from data.</p>
<div id="random-sample" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Random sample<a href="sampling-distributions.html#random-sample" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Example (Electronâs charge to mass ratio)</strong></p>
<p>In 1897, J. J. Thomson was the first to measure the charge-to-mass ratio (<span class="math inline">\(q/m\)</span>) of electrons <span class="citation">(<a href="#ref-Thomson1897eoverm">Thomson 1897</a>)</span>, demonstrating that they were the constituents of cathode rays and significantly smaller than hydrogen atoms. Although Thomson originally reported his results as the mass-to-charge ratio (<span class="math inline">\(m/e\)</span>), here we present them as <span class="math inline">\(q/m\)</span> values, expressed in units of <span class="math inline">\(Coulomb/Kg \times 10^{11}\)</span>, as illustrated below</p>
<p><img src="_main_files/figure-html/unnamed-chunk-123-1.png" width="672" /></p>
<p>We say that J. J. Thomson took a <strong>random sample</strong> of size 26, meaning he performed 26 independent repetitions of the random experiment: measuring the <span class="math inline">\(q/m\)</span> ratio under varying experimental conditions.</p>
<p>The experiments were conducted using different types of gases, yet the observations appeared to cluster around a single <strong>unknown</strong> value.</p>
<p><strong>Definition:</strong></p>
<p>A <strong>random sample</strong> of size <span class="math inline">\(n\)</span> is the <strong>repetition</strong> of a random experiment <span class="math inline">\(n\)</span> <strong>independent</strong> times.</p>
<ul>
<li><p>A random sample is an <span class="math inline">\(n\)</span>-dimensional <strong>random variable</strong>:</p>
<p><span class="math display">\[(X_1, X_2, \dots, X_n)\]</span></p></li>
</ul>
<p>where <span class="math inline">\(X_i\)</span> is the random variable from <em>i-th</em> repetition of the random experiment that measures the <span class="math inline">\(q/m\)</span> ratio, each subject to measurement error and with the same probability density function <span class="math inline">\(f(x)\)</span> for all <span class="math inline">\(i\)</span>.</p>
<ul>
<li><p><strong>An observation</strong> of a random sample is the set of <span class="math inline">\(n\)</span> values obtained from performing the experiment:</p>
<p><span class="math display">\[(x_1, x_2, \dots, x_n)\]</span></p></li>
</ul>
<p><strong>Example (Electron Measurement)</strong></p>
<p>The <strong>observation</strong> of a sample of size <span class="math inline">\(26\)</span> for the <span class="math inline">\(q/m\)</span> ratio is a vector with <span class="math inline">\(26\)</span> components:</p>
<p>(1.75, 2.94, 2.32, 3.12, 2.08, 2.50, 2.50, 2.85,
2.00, 2.50, 2.50, 2.56, 1.88, 2.12, 2.12, 1.88,
1.96, 1.85, 1.58, 1.88, 2.17, 1.63, 2.08, 1.11,
1.42, 1.00)</p>
<p>Clearly, the quantity <span class="math inline">\(q/m\)</span> is a parameter of the physical systemâan intrinsic property, a constant. However, the process of measuring this quantityânamely the random variable representing each specific experimental setup and the formulas used to compute the valuesâvaried in each repetition, producing different outcomes due to measurement error or experimental noise.</p>
<p>J.J. Thomson did not directly observe the value of <span class="math inline">\(q/m\)</span> but rather measurementsârealizations of the random variables <span class="math inline">\(X_i\)</span>âeach subject to experimental variability, with the expected value of these measurements equal to <span class="math inline">\(q/m\)</span>. At the time, there was no theoretical model predicting its value. The experiments were exploratory, and knowledge emerged not from theory, but from empirical observation.</p>
<p>How did J.J. Thomson estimate the value of <span class="math inline">\(q/m\)</span> from the data?</p>
<p>To move from raw measurements to a meaningful scientific conclusion, J.J. Thomson needed a systematic way to combine these varied observations into a single best estimate of the expected charge-to-mass ratio. In the next section, we will explore how statistical methods allow us to estimate unknown parameters from random samples like this one.</p>
</div>
<div id="parameter-estimation" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> Parameter estimation<a href="sampling-distributions.html#parameter-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To find likely values for parameters, we use data. Therefore, we take a <strong>random sample</strong>âthat is, we repeat the experiment <span class="math inline">\(n\)</span> times, collect data, and use it to <strong>estimate</strong> the parameters.</p>
<p><strong>Estimates of the Mean and Variance</strong></p>
<p>Recall that for a discrete random variable, the mean (or expected value) is defined using the number of possible outcomes <span class="math inline">\(m\)</span> and the probability mass function <span class="math inline">\(f(x)\)</span>:</p>
<p><span class="math display">\[
\mu = \sum_{i=1}^m x_i f(x_i)
\]</span></p>
<p>This formula represents the <strong>center of mass of the probabilities</strong>, where <span class="math inline">\(f(x_i)\)</span> is the probability of the outcome <span class="math inline">\(x_i\)</span>.</p>
<p>This concept is closely related to the <strong>sample average</strong>, which we define as:</p>
<p><span class="math display">\[
\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i
\]</span></p>
<p>Here, the <span class="math inline">\(x_i\)</span>âs are the <strong>observed values</strong> in a random sample of size <span class="math inline">\(n\)</span>. We can also express this average as a weighted sum over the distinct observed values <span class="math inline">\(x_i\)</span> with their associated <strong>relative frequencies</strong> <span class="math inline">\(f_i\)</span> (i.e., the proportion of times each outcome appears in the sample):</p>
<p><span class="math display">\[
\bar{x} = \sum_{i=1}^m x_i f_i
\]</span></p>
<p>If the relative frequencies <span class="math inline">\(f_i\)</span> converge to the probabilities <span class="math inline">\(f(x_i)\)</span> as <span class="math inline">\(n \to \infty\)</span>, then the average <span class="math inline">\(\bar{x}\)</span> will converge to the mean <span class="math inline">\(\mu\)</span>. For finite <span class="math inline">\(n\)</span>, we treat the average as an estimate of the mean:</p>
<p><span class="math display">\[
\hat{\mu} = \bar{x}
\]</span></p>
<p>Note that <span class="math inline">\(\hat{\mu}\)</span> is not the same as <span class="math inline">\(\mu\)</span>. The value <span class="math inline">\(\hat{\mu}\)</span> is a <strong>statistic</strong>âa quantity derived from the dataâwhile <span class="math inline">\(\mu\)</span> is a <strong>parameter</strong>, an inherent (but typically unknown) property of the underlying probability distribution. They are conceptually equal only in the limit as <span class="math inline">\(n \to \infty\)</span>, i.e., if we could repeat J.J. Thomsonâs experiment an infinite number of times under identical conditions.</p>
<p>When the average is calculated from a small sample, we know it will contain some estimation error, which we will accept for now without quantifying.</p>
<p>Similarly, the <strong>variance</strong> of a discrete random variable is defined in terms of the probability mass function as:</p>
<p><span class="math display">\[
\sigma^2 = \sum_{i=1}^m (x_i - \mu)^2 f(x_i),
\]</span></p>
<p>which represents the expected squared deviation from the mean.</p>
<p>From the observed values of a sample of size <span class="math inline">\(n\)</span>, we compute the <strong>sample variance</strong> as:</p>
<p><span class="math display">\[
s^2 = \frac{1}{n - 1} \sum_{i=1}^n (x_i - \bar{x})^2,
\]</span></p>
<p>The use of <span class="math inline">\(n - 1\)</span> will be explained later. If we express the sample variance in terms of the distinct values <span class="math inline">\(x_i\)</span> and their relative frequencies <span class="math inline">\(f_i\)</span>, we obtain:</p>
<p><span class="math display">\[
s^2 = \frac{n}{n - 1} \sum_{i=1}^m (x_i - \bar{x})^2 f_i.
\]</span></p>
<p>As the sample size <span class="math inline">\(n\)</span> increases, the sample variance <span class="math inline">\(s^2\)</span> converges to the variance <span class="math inline">\(\sigma^2\)</span>. Thus, we take <span class="math inline">\(s^2\)</span> as an estimate of <span class="math inline">\(\sigma^2\)</span> when working with finite samples.</p>
<p><strong>Example (Electron)</strong></p>
<p>We <strong>assume</strong> that the value obtained for the <span class="math inline">\(q/m\)</span> experiment follows a normal probability density function.</p>
<p><span class="math display">\[X \sim N(x; \mu, \sigma^2)\]</span>
Where <span class="math inline">\(E(X)=\mu\)</span> is the parameter <span class="math inline">\(\mu=q/m\)</span>.</p>
<p>the average of the sample is <span class="math inline">\(\bar{x}=2.09\)</span></p>
<pre><code>Python: np.mean(x)
R: mean(x)</code></pre>
<p>and <span class="math inline">\(s^2=0.51^2=0.26\)</span></p>
<pre><code>Python: np.var(x, ddof=1)
R: var(x)</code></pre>
<p>that we can take as the estimated values of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. So the <strong>fitted</strong> model is</p>
<p><span class="math display">\[X \sim N(x; \mu=2.09, \sigma^2=0.51^2)\]</span>
<img src="_main_files/figure-html/unnamed-chunk-124-1.png" width="672" /></p>
<p>The fitted distribution represents J.J. Thomsonâs experiment with its mean and its variance. The sample average was not expected to exactly equal the value of <span class="math inline">\(q/m\)</span>, as it is subject to random error. Had he taken another sample, the average would likely have been different. By contrast, <span class="math inline">\(q/m\)</span> appears to be a physical invariant with no associated dispersion. Todayâs more accurate value of <span class="math inline">\(q/m\)</span> is</p>
<p><span class="math inline">\(q/m \sim \frac{1.602176634\times 10^{-19}}{9.1093837015\times 10^{â31}} C/Kg =\)</span> <span class="math display">\[1.758820024\times 10^{11} C/Kg\]</span></p>
<p>The error that J.J. Thomsonâs made in his 1897 paper was <span class="math inline">\(0.33\times 10^{11} C/kg\)</span>, about <span class="math inline">\(18\%\)</span> in relation to the current estimate.</p>
<p>Is this an expected error of his experiment? If he was to re-do his experiment how close to the current value of <span class="math inline">\(q/m\)</span> he would typically be?</p>
</div>
<div id="law-of-large-numbers" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> Law of Large Numbers<a href="sampling-distributions.html#law-of-large-numbers" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we estimate parameters using dataâfor example, by using<br />
<span class="math display">\[
\hat{\mu} = \bar{x}
\]</span><br />
as an estimate of <span class="math inline">\(\mu\)</span>, or<br />
<span class="math display">\[
\hat{\sigma}^2 = s^2
\]</span><br />
as an estimate of <span class="math inline">\(\sigma^2\)</span>âwe know that we are introducing an error. If we were to take a different sample of <span class="math inline">\(q/m\)</span> measurements (say, of size 24), the estimate would likely change, because the sample average <span class="math inline">\(\bar{x}\)</span> depends on the data and is therefore variable.</p>
<p>This raises an important question:</p>
<p>When we perform an experiment, can we quantify how good our estimate of the parameter is?</p>
<p><strong>The Sample Mean as a Random Variable</strong></p>
<p>The first thing to recognize is that the numerical value we calculate, <span class="math inline">\(\bar{x}\)</span>, is just one observation of a random variable, which we denote by <span class="math inline">\(\bar{X}\)</span>.</p>
<p><strong>Definition:</strong></p>
<p>The <strong>sample mean</strong> (or average) of a random sample of size <span class="math inline">\(n\)</span> is defined as:
<span class="math display">\[
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span>
Here, <span class="math inline">\((X_1, X_2, \dots, X_n)\)</span> are independent and identically distributed (i.i.d.) random variables with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, and constitute the <span class="math inline">\(n\)</span>-dimensional random variable of the sample.</p>
<p>The average <span class="math inline">\(\bar{X}\)</span> is itself a random variable. For instance, in our sample of 24 measurements, we observed:
<span class="math display">\[
\bar{x} = 2.09
\]</span>
Had we taken another sample of size 24, the average would likely have been different.</p>
<p><strong>Why Use <span class="math inline">\(\bar{X}\)</span> to Estimate <span class="math inline">\(\mu\)</span>?</strong></p>
<p>The value <span class="math inline">\(\bar{x}\)</span> can be used to estimate the unknown parameter <span class="math inline">\(\mu = q/m\)</span> because is is an observation of the random variable <span class="math inline">\(\bar{X}\)</span> with two very important and general properties:</p>
<ol style="list-style-type: decimal">
<li><p>Unbiasedness:<br />
<span class="math display">\[
E(\bar{X}) = \mu
\]</span></p></li>
<li><p>Consistency as <span class="math inline">\(n \to \infty\)</span>:<br />
<span class="math display">\[
lim_{n \to \infty} V(\bar{X}) = 0
\]</span></p></li>
</ol>
<p><strong>Demonstration of the Properties</strong></p>
<ol style="list-style-type: decimal">
<li><p>Expected value of the mean:
<span class="math display">\[
E(\bar{X}) = E\left(\frac{1}{n} \sum_{i=1}^n X_i\right) = \frac{1}{n} \sum_{i=1}^n E(X_i) = \mu
\]</span>
This holds because the <span class="math inline">\(X_i\)</span> are i.i.d. random variables with expected value <span class="math inline">\(\mu\)</span>.</p></li>
<li><p>Variance of the mean:
<span class="math display">\[
V(\bar{X}) = V\left(\frac{1}{n} \sum_{i=1}^n X_i\right) = \frac{1}{n^2} \sum_{i=1}^n V(X_i) = \frac{\sigma^2}{n}
\]</span>
Since the variance shrinks as <span class="math inline">\(1/n\)</span>, the sample mean becomes more stable as the sample size increases.</p></li>
</ol>
<p><strong>Estimating <span class="math inline">\(\mu\)</span></strong></p>
<p>These two properties imply that <span class="math inline">\(\bar{x}\)</span>, the observed average, converges closer and closer to the value <span class="math inline">\(\mu\)</span> as the sample size <span class="math inline">\(n\)</span> increases.</p>
<p>This phenomenon is known as the <strong>law of large numbers</strong>. It holds regardless of the underlying probability distribution of the <span class="math inline">\(X_i\)</span>, provided they are independent and identically distributed with finite variance.</p>
<p>In other words:</p>
<ul>
<li><p>The estimation error in using <span class="math inline">\(\bar{x}\)</span> as an approximation for <span class="math inline">\(\mu\)</span> decreases as <span class="math inline">\(n\)</span> increases.</p></li>
<li><p>This is because the variability (variance) of <span class="math inline">\(\bar{X}\)</span> decreases with <span class="math inline">\(n\)</span>.</p></li>
</ul>
<p>So when we write:
<span class="math display">\[
\bar{x} = \hat{\mu}
\]</span>
we understand that this estimate becomes increasingly reliable as the sample grows.</p>
</div>
<div id="inference" class="section level2 hasAnchor" number="10.4">
<h2><span class="header-section-number">10.4</span> Inference<a href="sampling-distributions.html#inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We know that when we take large samples, the estimation error tends to be small. However, for a given sample size <span class="math inline">\(n\)</span>, we would like to quantify how far our estimate might be from the parameter value. This gives us an idea of how accurate our estimate is. Specifically, we may ask:</p>
<p>What is the probability of making an error of a given size when estimating <span class="math inline">\(\mu\)</span> with <span class="math inline">\(\bar{x}\)</span>?</p>
<p>When we calculate probabilities related to an estimator, we are performing <strong>inference</strong>. Inference problems often involve questions such as:</p>
<ul>
<li>What is the probability that the estimate <span class="math inline">\(\bar{x}\)</span> differs from the parameter <span class="math inline">\(\mu\)</span> by no more than some margin <span class="math inline">\(m\)</span>?</li>
</ul>
<p>Mathematically, this is expressed as:
<span class="math display">\[
P(-m \leq \bar{X} - \mu \leq m)
\]</span>
This represents the probability that the difference between the random variable <span class="math inline">\(\bar{X}\)</span> (the sample mean) and the parameter <span class="math inline">\(\mu\)</span> lies within a range of size <span class="math inline">\(2m\)</span>.</p>
<p>To calculate such probabilities, we need:</p>
<ol style="list-style-type: decimal">
<li><p>A probability model for <span class="math inline">\(\bar{X}\)</span> (i.e., its probability distribution)</p></li>
<li><p>The parameters of that model (e.g., the mean and variance of the distribution)</p></li>
</ol>
<p>What, then, is the probability distribution of <span class="math inline">\(\bar{X}\)</span>, and what about the distribution of <span class="math inline">\(S^2\)</span> (the sample variance), so that we can calculate the relevant probabilities?</p>
<p>These probability functions are known as <strong>sampling distributions</strong> because they describe the behavior of statistics like <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(S^2\)</span> when we take repeated samples. Understanding these distributions allows us to calculate their probabilities and make informed statements about how close our estimates are likely to be to the parameters.</p>
<p><strong>Example (Electron)</strong></p>
<p>We can pose an inference question based on J.J. Thomsonâs experiment. the mean <span class="math inline">\(\mu\)</span> is unknown, but the sample average from 24 repetitions is <span class="math inline">\(\bar{x} = 2.09\)</span> (in <span class="math inline">\(10^{11} C/kg\)</span> units). Assuming that each experimental measurement follows a normal distribution and that the standard deviation is approximately <span class="math inline">\(\hat{\sigma} = 0.51\)</span>, what is the probability that the sample mean <span class="math inline">\(\bar{X}\)</span> lies within <span class="math inline">\(0.33\)</span> of the mean <span class="math inline">\(\mu\)</span>?</p>
<p>In other words, we want to find the probability that the sample mean is within a <strong>margin of error</strong> of <span class="math inline">\(0.33\)</span> from <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display">\[
P(-0.33 \leq \bar{X} - \mu \leq 0.33)
\]</span></p>
<p>To calculate this probability, we need to know the probability distribution of <span class="math inline">\(\bar{X}\)</span>.</p>
</div>
<div id="sample-mean" class="section level2 hasAnchor" number="10.5">
<h2><span class="header-section-number">10.5</span> Sample mean<a href="sampling-distributions.html#sample-mean" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Theorem:</strong> If the random variable <span class="math inline">\(X\)</span> follows a normal distribution</p>
<p><span class="math display">\[
X \sim N(\mu, \sigma^2),
\]</span></p>
<p>then the sample mean <span class="math inline">\(\bar{X}\)</span> is also normally distributed as</p>
<p><span class="math display">\[
\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right).
\]</span></p>
<p>This fits perfectly with the law of large numbers because:</p>
<ol style="list-style-type: decimal">
<li>The <strong>mean</strong> of <span class="math inline">\(\bar{X}\)</span> is exactly the parameter <span class="math inline">\(\mu\)</span> we want to estimate:</li>
</ol>
<p><span class="math display">\[
E(\bar{X}) = \mu,
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>The <strong>variance</strong> of <span class="math inline">\(\bar{X}\)</span> gets smaller as the sample size <span class="math inline">\(n\)</span> grows:</li>
</ol>
<p><span class="math display">\[
V(\bar{X}) = \frac{\sigma^2}{n}.
\]</span></p>
<p>We say that <span class="math inline">\(\bar{X}\)</span> is an <em>unbiased estimator</em> of <span class="math inline">\(\mu\)</span>, meaning that on average it hits the valueâlike a dart player who on average hits the bullâs-eye.</p>
<p>It is also a <em>consistent estimator</em> because as <span class="math inline">\(n\)</span> increases, the variance (or âspreadâ) of <span class="math inline">\(\bar{X}\)</span> gets smallerâlike a dart player who improves their aim and gets closer to the bullâs-eye every time.</p>
<p>The quantity</p>
<p><span class="math display">\[
se = \sqrt{V(\bar{X})} = \frac{\sigma}{\sqrt{n}}
\]</span></p>
<p>is called the <strong>standard error</strong> of the sample mean. It tells us how much we expect our sample mean <span class="math inline">\(\bar{x}\)</span> to typically vary from the mean <span class="math inline">\(\mu\)</span>. Sometimes it is written as <span class="math inline">\(\sigma_{\bar{x}}\)</span>, to emphasize that it is the standard deviation of <span class="math inline">\(\bar{X}\)</span>.</p>
<p>Because <span class="math inline">\(\bar{X}\)</span> is normal when <span class="math inline">\(X\)</span> is normal, we can use the normal distribution to calculate probabilities related to <span class="math inline">\(\bar{X}\)</span>.</p>
<p>For example, suppose we want to find the probability that our estimate <span class="math inline">\(\bar{X}\)</span> lies within a margin of error <span class="math inline">\(m\)</span> from <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display">\[
P(-m \leq \bar{X} - \mu \leq m).
\]</span></p>
<p>Using the fact that <span class="math inline">\(\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)\)</span>, we can standardize this:</p>
<p><span class="math display">\[
P\left(-\frac{m}{\sigma/\sqrt{n}} \leq \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \leq \frac{m}{\sigma/\sqrt{n}}\right) = P(-z \leq Z \leq z),
\]</span></p>
<p>where</p>
<p><span class="math display">\[
z = \frac{m}{\sigma/\sqrt{n}},
\]</span></p>
<p>and <span class="math inline">\(Z\)</span> is a standard normal variable.</p>
<p>This allows us to calculate exactly how likely it is that our sample mean is close to the mean within the margin <span class="math inline">\(m\)</span>.</p>
<p><strong>Example (Electron)</strong></p>
<p>Suppose we want to know the probability that the sample mean <span class="math inline">\(\bar{X}\)</span> is within a margin of error <span class="math inline">\(m = 0.33\)</span> from the mean <span class="math inline">\(\mu\)</span>.</p>
<p>We assume the sample variance estimate <span class="math inline">\(\hat{\sigma} = 0.51\)</span> is close to <span class="math inline">\(\sigma\)</span>, and we have a sample size <span class="math inline">\(n = 24\)</span>. (Later, Gosset corrected for small samples using the t-distribution, but for now we treat <span class="math inline">\(n=24\)</span> as large enough.)</p>
<p>The standardized margin is</p>
<p><span class="math display">\[
z = \frac{0.33}{0.51 / \sqrt{24}} \approx 3.17.
\]</span></p>
<p>The probability that the sample mean is within <span class="math inline">\(\pm 0.33\)</span> of <span class="math inline">\(\mu\)</span> is then</p>
<p><span class="math display">\[
P\left(-3.17 \leq Z \leq 3.17\right) = \Phi(3.17) - \Phi(-3.17) \approx 0.99,
\]</span></p>
<p>where <span class="math inline">\(\Phi\)</span> is the cumulative distribution function (CDF) of the standard normal distribution.</p>
<p>In code:</p>
<pre><code>Python: norm.cdf(3.17) - norm.cdf(-3.17)
R: pnorm(3.17) - pnorm(-3.17)</code></pre>
<p>This means about 99% of the averages from repeated samples of size 24 would lie within 0.33 of the mean <span class="math inline">\(\mu\)</span>.</p>
<p>If we take the most recent and precise value of <span class="math inline">\(q/m \approx 1.75\)</span> (in <span class="math inline">\(10^{11} C/kg\)</span> units) as a better estimate to the mean, this tells us that most samples similar to J.J. Thomsonâs would produce averages close to that value with a precision of about <span class="math inline">\(0.33\)</span>.</p>
<p>Note that the value of <span class="math inline">\(q/m\)</span> is not directly observable and likely has infinite, unpredictable decimal places (i.e.Â is a normal number). What J.J. Thomson discovered was a consistent pattern in his measurements â a regularity that later researchers could replicate with increasing accuracy in other experiments.</p>
</div>
<div id="prediction" class="section level2 hasAnchor" number="10.6">
<h2><span class="header-section-number">10.6</span> Prediction<a href="sampling-distributions.html#prediction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>While some random experiments aim to infer a property of the system under study, others aim to make predictions about particular outcomes. However, if no theoretical knowledge is available about the parameters of the probabilistic model, these parameters must first be estimatedâoften by performing a large number of repetitions of the experiment.</p>
<div id="example-pacemaker-prediction" class="section level3 hasAnchor" number="10.6.1">
<h3><span class="header-section-number">10.6.1</span> <strong>Example: Pacemaker Prediction</strong><a href="sampling-distributions.html#example-pacemaker-prediction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Micra Transcatheter Pacemaker is a miniaturized pacemaker delivered percutaneously with a high success rate. A key measure of its clinical efficacy is the prediction of <strong>minimum virtual battery longevity (VBL)</strong>.</p>
<p>In an early study of 630 patients, the estimated battery longevityâbased on usage conditions observed at 12 monthsâwas <strong>12.1 years</strong>, with <strong>89%</strong> of patients projected to have a longevity of <strong>more than 10 years</strong> (corresponding to a standard deviation of <span class="math inline">\(1.71\)</span>) <span class="citation">(<a href="#ref-Duray2017Micra12mo">Duray et al. 2017</a>)</span>.</p>
<p>Devices with projected longevity less than 5 years are unlikely to be marketed or adopted, especially for young or high-risk patients. Fortunately, <strong>none of the patients</strong> in this study had a projected longevity below 5 years.</p>
<p><strong>Question:</strong> How can we compute the probability that a Micra pacemaker has a projected longevity of less than 5 years?</p>
<p>We <strong>assume</strong> the longevity of pacemakers follows a <strong>normal distribution</strong>:</p>
<p><span class="math display">\[
X \sim \mathcal{N}(\mu, \sigma^2)
\]</span></p>
<p>To compute <span class="math inline">\(P(X \leq 5)\)</span>, we need values for the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The reported mean is:<br />
<span class="math display">\[
  \bar{x} = 12.1
  \]</span></p>
<pre><code>Python: np.mean(x)
R: mean(x)</code></pre>
<p>and <span class="math inline">\(s^2=1.71^2=2.9\)</span></p>
<pre><code>Python: np.var(x, ddof=1)
R: var(x)</code></pre>
<p>that we can take as the estimated values of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. So the <strong>fitted</strong> model is</p>
<p><span class="math display">\[X \sim N(x; \mu=12.1, \sigma^2=1.71^2)\]</span></p>
<p><img src="_main_files/figure-html/unnamed-chunk-125-1.png" width="672" /></p>
<p>The probability that a patient is implanted with a pacemaker less than 5 years longevity can be computed using the cumulative distribution function of a normal distribution:</p>
<p><span class="math display">\[
P(X \leq 5) = F(5; \mu = 12.1, \sigma^2 = 1.71^2) = 0.00001
\]</span></p>
<pre><code>Python: norm.cdf(5, 12.1, 1.71)
R: pnorm(5, 12.1, 1.71)</code></pre>
<p>That is, the estimated probability is about <span class="math inline">\(0.001\%\)</span>.</p>
<p>This provides a probabilistic argument supporting the adoption of the pacemaker, even for children and high-risk patients. As of 2025, it is estimated that 200,000 Micra devices have been implanted. Therefore, we expect only about 2 patients with pacemaker longevity under 5 years.</p>
<p>If such rare cases occur, they may be considered unfortunate but extremely low-probability eventsâcomparable to or even lower than other everyday risks, such as dying in a car accident (roughly 1 in 100).</p>
<p>When inferring the parameters of a random experiment, we require as many repetitions as possible to produce reliable estimates. When parameters are entirely unknown, inferences from large studies are treated as the <strong>gold standard</strong>.</p>
</div>
</div>
<div id="validation" class="section level2 hasAnchor" number="10.7">
<h2><span class="header-section-number">10.7</span> Validation<a href="sampling-distributions.html#validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When the parameters of a random experiment have been estimated using random variables with reasonable distributional propertiesâsuch as clustering around the expected valueâthe stability of those estimates can be tested by performing a new set of experiments under different conditions. Repeated validation of parameters that represent properties of a system strengthens the notion that the property in question reflects a fundamental aspect of the system.</p>
<p><strong>Example (Pacemaker Validation)</strong></p>
<p>Random experiments are characterized by some experimental conditions. In the pacemaker study there are some inclusion and exclusion criteria, for example patients who are entirely pacemaker dependent were excluded for safety reasons. Pacemaker configuration can change for those patients. Imagine that we want to run a new study on 10 of those patients and measure the VBL for each of them. We then need to calculate the probability that the estimate we obtain for <span class="math inline">\(\mu\)</span> in our new study is within a margin of error from the gold standard taken as true values <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. We can compute, for instance, the probability that <span class="math inline">\(\bar{X}\)</span> is within 1 year from <span class="math inline">\(\mu\)</span>.</p>
<p><em>Probability densities for <span class="math inline">\(X\)</span> and <span class="math inline">\(\bar{X}\)</span></em></p>
<p>Remember that we have <strong>two probability functions</strong>:</p>
<ol style="list-style-type: decimal">
<li><p>The probability function of <span class="math inline">\(X\)</span> that is also the probability function of the <strong>random experiment</strong>.</p></li>
<li><p>The probability function of <span class="math inline">\(\bar{X}\)</span> that is probability function of the <strong>sample</strong> (a collection of independent random experiments).</p></li>
</ol>
<p>Let us assume that the the longevity of the pacemakers is</p>
<p><span class="math display">\[X \sim N(\mu=12.1, \sigma^2=1.70^2)\]</span></p>
<p><img src="_main_files/figure-html/unnamed-chunk-126-1.png" width="672" /></p>
<p>In our new study, we plan to obtain a sample of <span class="math inline">\(10\)</span> patients (like the blue crosses on the plot). Since <span class="math inline">\(X\)</span> is normally distributed, the sample mean <span class="math inline">\(\bar{X}\)</span> is also normally distributed. Therefore, we know the probability distribution of the sample mean <span class="math inline">\(\bar{X}\)</span></p>
<p><span class="math display">\[\bar{X} \sim N(12.1 , \frac{1.70^2}{10})\]</span></p>
<p><img src="_main_files/figure-html/unnamed-chunk-127-1.png" width="672" /></p>
<p>We can see that the outcomes of <span class="math inline">\(\bar{X}\)</span> (like the black dot in the plot) will be more tightly concentrated around <span class="math inline">\(\mu\)</span> than those of the random variable <span class="math inline">\(X\)</span>. In fact, when <span class="math inline">\(n\)</span> is large, the sample means become so close to <span class="math inline">\(\mu\)</span> and the distribution so sharply peaked that a single observed average (black dot) becomes a sufficiently accurate estimate of <span class="math inline">\(\mu\)</span>.</p>
<p>We want to calculate <strong>the probability</strong> that our estimate is within a margin of error of <span class="math inline">\(1\)</span> year. That is, within a distance of <span class="math inline">\(1\)</span> from the mean:</p>
<p><span class="math inline">\(P(-1 \leq \bar{X} - 12.1 \leq 1) = P(11.1 \leq \bar{X} \leq 13.1)\)</span></p>
<p>Remember the standard error is: <span class="math inline">\(se=\frac{\sigma}{\sqrt{n}}=\frac{1.7}{\sqrt{10}}= 0.53\)</span>.</p>
<p><span class="math display">\[=\Phi(13.1; 14.1, 0.53)-\Phi(10.1; 14.1, 0.53)=0.93\]</span></p>
<pre><code>Python: norm.cdf(14.1, 12.1, 0.53)-norm.cdf(10.1, 12.1, 0.53)
R: pnorm(13.1, 12.1, 1.7/sqrt(10))-pnorm(11.1, 12.1, 1.7/sqrt(10))</code></pre>
<p>We can, therefore, expect that <span class="math inline">\(93\%\)</span> of the sample means from new studies with a sample size of 10 will fall within the interval <span class="math inline">\((11.1, 13.1)\)</span>, providing reassurance about the safety of the device for high-risk patients in our new study.</p>
</div>
<div id="sample-sum" class="section level2 hasAnchor" number="10.8">
<h2><span class="header-section-number">10.8</span> Sample Sum<a href="sampling-distributions.html#sample-sum" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Some random experiments consist of the collection of several identical trials, where the aim is to measure the combined contributions of each of them. Think, for example, of the collective effect of identical non-interacting molecules in a gas. In such cases, we may be interested in the <strong>sum</strong> of the outcomes of each experiment.</p>
<p>The <strong>sample sum</strong> is the <strong>random variable</strong></p>
<p><span class="math display">\[
Y = n \bar{X} = \sum_{i=1}^n X_i
\]</span></p>
<p>which is a function of the random sample <span class="math inline">\((X_1, \ldots, X_n)\)</span>.</p>
<div id="example-cables" class="section level3 hasAnchor" number="10.8.1">
<h3><span class="header-section-number">10.8.1</span> Example (Cables)<a href="sampling-distributions.html#example-cables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Imagine that a client asks a metallurgical company to supply 8 cables that together can carry up to 96 tonsâthat is, 12 tons each. The engineer needs to guarantee that none of the cables will break when loaded with this weight.</p>
<p>In <strong>stock</strong>, there are cables, all produced under the same specifications, that might be suitable. So, the engineer selects 8 of these cables at random and loads each until it breaks. Here are the results (in tons):</p>
<p>13.34642, 13.32620, 13.01459, 13.10811, 12.96999, 13.55309, 13.75557, 12.62747</p>
<p>If the company intends to use all 8 cables simultaneously to carry a total of 96 tons, we should consider <strong>adding</strong> their individual capacities. The observed value for the sample sum is:</p>
<p><span class="math display">\[
y = 13.21268 \times 8 = 105.70144 \text{ tons}
\]</span></p>
<p>What is the probability that when we put all the cables together, they can carry a total weight within 2 tons of the total expected weight that the cables can support?</p>
<p><strong>Distribution of the sample sum</strong></p>
<p><strong>Theorem:</strong> if <span class="math inline">\(X\)</span> follows a normal distribution
<span class="math display">\[X \sim N(\mu, \sigma^2)\]</span></p>
<p>then <span class="math inline">\(Y\)</span> is normal</p>
<p><span class="math display">\[Y \sim N(n\mu, n\sigma^2)\]</span></p>
<p>and <span class="math inline">\(Y\)</span> has</p>
<ol style="list-style-type: decimal">
<li>mean <span class="math display">\[E(Y)=n\mu\]</span></li>
<li>variance <span class="math display">\[V(Y)=n\sigma^2\]</span></li>
</ol>
<p>As we have the distribution of <span class="math inline">\(Y\)</span> we can compute probabilities on it.</p>
<p><strong>Example (Cables)</strong></p>
<p>If the breaking load of the cables have been previously certified to be a normal variable</p>
<p><span class="math display">\[X \sim N(\mu=13, \sigma^2=0.35^2)\]</span></p>
<p>then the sample sum of size <span class="math inline">\(8\)</span> is normal</p>
<p><span class="math display">\[Y \sim N(n\mu=104, n\sigma^2=8\times 0.35^2)\]</span></p>
<p>with mean and variance</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E(Y)=n\mu=104\)</span></li>
<li><span class="math inline">\(V(Y)=n\sigma^2=8\times 0.35^2=0.98\)</span>; <span class="math inline">\(\sqrt{V(Y)}=0.9899495\)</span></li>
</ol>
<p>The probability that when we put all the cables together, they can carry a total weight between <span class="math inline">\(102=104-2\)</span> and <span class="math inline">\(106=104+2\)</span> can be computed using the normal distribution function</p>
<p><span class="math inline">\(P(102 \leq Y \leq 106)=\Phi(102; 104, 8\times 0.35^2)-\Phi(106; 104, 8\times 0.35^2)=0.956\)</span></p>
<p>We can calculate it as:</p>
<pre><code>Python: norm.cdf(106, 104, 0.9899495)-norm.cdf(102, 104, 0.9899495)
R: pnorm(106, 104, 0.9899495)-pnorm(102, 104, 0.9899495)
</code></pre>
<p>Therefore, we can expect that <span class="math inline">\(95.6\%\)</span> of the total weight that 8 cables can carry lies between <span class="math inline">\(102\)</span> and <span class="math inline">\(106\)</span> Tons, which represents a margin of error of <span class="math inline">\(2\)</span> Tons around the total mean <span class="math inline">\(n\mu = 104\)</span> Tons.</p>
<p>This probability is equal to the probability that a single cable breaks between <span class="math inline">\(13-\sqrt{2/8}\)</span> and <span class="math inline">\(13-\sqrt{2/8}=13.25\)</span> Tons.</p>
<p>The margin of error does not scale linearly with the number of cables; instead, it increases with the square root of the sample size, <span class="math inline">\(\sqrt{n}\)</span>. This is because the individual variations tend to cancel each other out when addedâsome cables break above the average strength, others below.</p>
<p>This slower growth of uncertainty ensures that collective measurements (like the total strength) become more accurate and reliable as the number of independent components increases, which is a fundamental principle of statistical inference.</p>
</div>
</div>
<div id="sample-variance-1" class="section level2 hasAnchor" number="10.9">
<h2><span class="header-section-number">10.9</span> Sample Variance<a href="sampling-distributions.html#sample-variance-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The variance is another key parameter of a random experiment. In some cases, it may reflect an intrinsic property of the system; in others, it may relate to the precision of the measurement process. When we estimate the variance of a random experiment using the observed value of the sample variance</p>
<p><span class="math display">\[
s^2 = \hat{\sigma}^2
\]</span></p>
<p>we are also introducing an estimation error. So how can we quantify the error we make?</p>
<p><strong>Definition</strong>
Just as we defined the <strong>sample mean</strong> <span class="math inline">\(\bar{X}\)</span> as an estimator of the population mean <span class="math inline">\(\mu\)</span>, we now turn to estimating the population variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The <strong>sample variance</strong> <span class="math inline">\(S^2\)</span> of a random sample of size <span class="math inline">\(n\)</span> is defined as</p>
<p><span class="math display">\[
S^2 = \frac{1}{n - 1} \sum_{i=1}^n (X_i - \bar{X})^2
\]</span></p>
<p>It measures the dispersion of the sample values around the sample mean <span class="math inline">\(\bar{X}\)</span>. The denominator <span class="math inline">\(n - 1\)</span>, rather than <span class="math inline">\(n\)</span>, ensures that <span class="math inline">\(S^2\)</span> is an unbiased estimator of the population variance <span class="math inline">\(\sigma^2\)</span>. This correction, known as Besselâs correction, accounts for the fact that we have used the sample mean <span class="math inline">\(\bar{X}\)</span> â an estimate of <span class="math inline">\(\mu\)</span> â in computing the deviations.</p>
<div id="example-cables-1" class="section level3 hasAnchor" number="10.9.1">
<h3><span class="header-section-number">10.9.1</span> Example (Cables)<a href="sampling-distributions.html#example-cables-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In a sample of <span class="math inline">\(8\)</span> cables, the sample variance took the value:</p>
<p><span class="math display">\[
s^2 = \frac{1}{n - 1} \sum_{i=1}^n (x_i - \bar{x})^2 = 0.1275608
\]</span></p>
<p>The sample variance <span class="math inline">\(S^2\)</span> is:</p>
<ol style="list-style-type: decimal">
<li><strong>Unbiased</strong>: Its expected value equals the true variance:<br />
<span class="math display">\[
E(S^2) = V(X) = \sigma^2
\]</span></li>
<li><strong>Consistent</strong>: Its variance decreases as the sample size increases:<br />
<span class="math display">\[
V(S^2) \to 0 \quad \text{as} \quad n \to \infty
\]</span></li>
</ol>
<p>Therefore, <span class="math inline">\(S^2\)</span> is both an <strong>unbiased</strong> and <strong>consistent</strong> estimator of <span class="math inline">\(\sigma^2\)</span>. This justifies using the observed sample variance <span class="math inline">\(s^2\)</span> as an estimate of the true variance:</p>
<p><span class="math display">\[
s^2 = \hat{\sigma}^2
\]</span></p>
<p>Just like <span class="math inline">\(\hat{\mu} = \bar{x}\)</span>, the error in estimating <span class="math inline">\(\sigma^2\)</span> with <span class="math inline">\(s^2\)</span> gets smaller as <span class="math inline">\(n\)</span> becomes larger.</p>
<p><strong>Why Do We Divide by <span class="math inline">\(n - 1\)</span>?</strong></p>
<p>One might propose estimating <span class="math inline">\(\sigma^2\)</span> using:</p>
<p><span class="math display">\[
S_n^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2
\]</span></p>
<p>However, this version is:</p>
<ol style="list-style-type: decimal">
<li><strong>Biased</strong>:<br />
<span class="math display">\[
E(S_n^2) = \sigma^2 - \frac{\sigma^2}{n} \neq \sigma^2
\]</span></li>
<li>but <strong>Consistent</strong>:<br />
<span class="math display">\[
V(S_n^2) \to 0 \quad \text{as} \quad n \to \infty
\]</span></li>
</ol>
<p>The bias appears because <span class="math inline">\(S_n^2\)</span> measures the spread around the sample mean <span class="math inline">\(\bar{X}\)</span>, not the true mean <span class="math inline">\(\mu\)</span>. The error introduced when substituting <span class="math inline">\(\bar{X}\)</span> for <span class="math inline">\(\mu\)</span> contributes an extra variance term <span class="math inline">\(\sigma^2 / n\)</span>.</p>
<p>We correct for this bias by multiplying <span class="math inline">\(S_n^2\)</span> by <span class="math inline">\(\frac{n}{n - 1}\)</span>:</p>
<p><span class="math display">\[
E\left( \frac{n}{n - 1} S_n^2 \right) = \sigma^2
\]</span></p>
<p>This gives us the corrected (unbiased) <strong>sample variance</strong>:</p>
<p><span class="math display">\[
S^2 = \frac{n}{n - 1} S_n^2 = \frac{1}{n - 1} \sum_{i=1}^n (X_i - \bar{X})^2
\]</span></p>
<p>which satisfies:</p>
<p><span class="math display">\[
E(S^2) = \sigma^2
\]</span></p>
<p><strong>Example (Quality Control)</strong></p>
<p>We encounter inference problems when we are interested in the probability of observing a certain value for the <strong>sample variance</strong> <span class="math inline">\(S^2\)</span>.</p>
<p>Consider a quality control process in which cables are expected to have breaking loads close to a specified target value <span class="math inline">\(\mu\)</span>. We want to avoid producing cables whose breaking strength varies too much from the mean.</p>
<p>Suppose we take a sample of 8 cables. If the sample variance is too high (specifically, <span class="math inline">\(S^2 &gt; 0.3\)</span>), we stop production and declare that the process is out of control.</p>
<p>What is the probability that the sample variance from a sample of 8 cables exceeds the threshold of <span class="math inline">\(0.3\)</span>?</p>
</div>
</div>
<div id="distribution-of-the-sample-variance" class="section level2 hasAnchor" number="10.10">
<h2><span class="header-section-number">10.10</span> Distribution of the Sample Variance<a href="sampling-distributions.html#distribution-of-the-sample-variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Theorem:</strong> If <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, then the random variable</p>
<p><span class="math display">\[
W = \frac{(n - 1) S^2}{\sigma^2}
\]</span></p>
<p>follows a chi-squared distribution with <span class="math inline">\(df = n - 1\)</span> degrees of freedom:</p>
<p><span class="math display">\[
W \sim \chi^2(n - 1)
\]</span></p>
<p>The probability density function (PDF) of the chi-squared distribution with <span class="math inline">\(n - 1\)</span> degrees of freedom is given by:</p>
<p><span class="math display">\[
f(w) = C_n \, w^{\frac{n - 3}{2}} e^{-w/2}
\]</span></p>
<p>where:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(C_n = \frac{1}{2^{(n-1)/2} \, \Gamma\left(\frac{n - 1}{2}\right)}\)</span> is a normalizing constant to ensure <span class="math inline">\(\int_0^\infty f(w) \, dw = 1\)</span>,</li>
<li><span class="math inline">\(\Gamma(x)\)</span> is the gamma function, a generalization of the factorial for real numbers.</li>
</ol>
<p>If the value of <span class="math inline">\(\sigma^2\)</span> is known, we can use the distribution of <span class="math inline">\(W\)</span> to compute probabilities related to the sample variance <span class="math inline">\(S^2\)</span>.</p>
</div>
<div id="the-chi2-distribution" class="section level2 hasAnchor" number="10.11">
<h2><span class="header-section-number">10.11</span> The <span class="math inline">\(\chi^2\)</span> Distribution<a href="sampling-distributions.html#the-chi2-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The chi-squared distribution has a parameter called the <strong>degrees of freedom</strong> <span class="math inline">\(df = n - 1\)</span>, which depends on the sample size. The shape of the distribution changes with different values of <span class="math inline">\(df\)</span>. The chi-squared distribution underpins the t-distribution used when inferring the mean with an unknown <span class="math inline">\(\sigma\)</span> and small <span class="math inline">\(n\)</span>, as we see in the following chapter. Below, we explore how the probability density function behaves for different values of <span class="math inline">\(df\)</span>.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-130-1.png" width="672" /></p>
<p><strong>Example (Variations in Cable Strength)</strong></p>
<p>If we <strong>know</strong> that our cables are certified as
<span class="math display">\[X \sim N(\mu=13, \sigma^2=0.35^2)\]</span></p>
<p>so</p>
<p><span class="math display">\[W=\frac{(n-1)S^2}{\sigma^2}= \frac{7S^2}{0.35^2} \sim \chi^2(n-1)\]</span></p>
<p>we can calculate <span class="math display">\[P(S^2 &gt; 0.3)=P(\frac{(n-1)S^2}{\sigma^2} &gt; \frac{(n-1)0.3}{\sigma^2 } )\]</span>
<span class="math inline">\(=P(W &gt; \frac{7\times0.3}{0.35^2})=P(W &gt; 17.14286)\)</span></p>
<p><span class="math inline">\(=1-P(W \leq 17.14286)\)</span></p>
<p><span class="math inline">\(= 1- F_{\chi^2,df=7}(17.14286)=0.016\)</span></p>
<pre><code>Python: 1-chi2.cdf(17.14286, df=7)
R: 1-pchisq(17.14286, 7)</code></pre>
<p>There is only a <span class="math inline">\(1\%\)</span> chance of getting a value greater than <span class="math inline">\(0.3\)</span>. So <span class="math inline">\(s^2&gt;0.3\)</span> seems to be a good criteria to stop production and review the process.</p>
<p>Consider that sample of <span class="math inline">\(8\)</span> cables that the engineer took was to perform a quality control test of the production line</p>
<p>13.34642, 13.32620, 13.01459, 13.10811, 12.96999, 13.55309, 13.75557, 12.62747</p>
<p>and the observed sample variance was <span class="math inline">\(s^2=0.1275608\)</span>. Therefore, the sample is not very dispersed because <span class="math inline">\(s^2 &lt; 0.3\)</span> and we believe that all is well and manufacturing is under control.</p>
</div>
<div id="questions-7" class="section level2 hasAnchor" number="10.12">
<h2><span class="header-section-number">10.12</span> Questions<a href="sampling-distributions.html#questions-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>1)</strong> The sample mean is an unbiased estimator of the population mean because</p>
<p><strong><span class="math inline">\(\qquad\)</span>a:</strong> The expected value of the sample mean is the population mean;
<strong><span class="math inline">\(\qquad\)</span>b:</strong> The expected value of the population mean is the sample mean;
<strong><span class="math inline">\(\qquad\)</span>c:</strong> The standard error approaches zero as <span class="math inline">\(n\)</span> approaches infinity;
<strong><span class="math inline">\(\qquad\)</span>d:</strong> The variance of the sample mean approaches zero as <span class="math inline">\(n\)</span> approaches infinity;</p>
<p><strong>2)</strong> Why is the statistic <span class="math inline">\(S^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i -\bar{X})^2\)</span> used? instead of <span class="math inline">\(S_n^2=\frac{1}{n}\sum_{i=1}^{n}(X_i -\bar{X})^2\)</span> to estimate the variance of a random variable?</p>
<p><strong><span class="math inline">\(\qquad\)</span>a:</strong> because its variance is <span class="math inline">\(0\)</span>;
<strong><span class="math inline">\(\qquad\)</span>b:</strong> because it is a consistent estimator of <span class="math inline">\(\sigma^2\)</span>;
<strong><span class="math inline">\(\qquad\)</span>c:</strong> because it is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>;
<strong><span class="math inline">\(\qquad\)</span>d:</strong> because it is the mean square distance to the sample mean (<span class="math inline">\(\bar{X}\)</span>);</p>
<p><strong>3)</strong> What is the variance of the sample mean <span class="math inline">\(\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i\)</span>?</p>
<p><strong><span class="math inline">\(\qquad\)</span>a:</strong><span class="math inline">\(\sigma\)</span>;
<strong><span class="math inline">\(\qquad\)</span>b:</strong><span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>;
<strong><span class="math inline">\(\qquad\)</span>c:</strong><span class="math inline">\(\sigma^2\)</span>;
<strong><span class="math inline">\(\qquad\)</span>d:</strong><span class="math inline">\(\frac{\sigma^2}{n}\)</span>;</p>
<p><strong>4)</strong> What is the mean and variance of the sample sum?</p>
<p><strong><span class="math inline">\(\qquad\)</span>a:</strong><span class="math inline">\(\mu\)</span>, <span class="math inline">\(n\sigma\)</span>;
<strong><span class="math inline">\(\qquad\)</span>b:</strong><span class="math inline">\(n\mu\)</span>,<span class="math inline">\(n\sigma\)</span>;
<strong><span class="math inline">\(\qquad\)</span>c:</strong><span class="math inline">\(\mu\)</span>, <span class="math inline">\(n\sigma^2\)</span>;
<strong><span class="math inline">\(\qquad\)</span>d:</strong><span class="math inline">\(n\mu\)</span>, <span class="math inline">\(n\sigma^2\)</span>;</p>
<p><strong>5)</strong> An inference question requires to</p>
<p><strong><span class="math inline">\(\qquad\)</span>a:</strong> calculate the expected value of an estimator;
<strong><span class="math inline">\(\qquad\)</span>b:</strong> estimate the value of a parameter;
<strong><span class="math inline">\(\qquad\)</span>c:</strong> calculate a probability of an estimator;
<strong><span class="math inline">\(\qquad\)</span>d:</strong> fit a probability model;</p>
</div>
<div id="exercises-8" class="section level2 hasAnchor" number="10.13">
<h2><span class="header-section-number">10.13</span> Exercises<a href="sampling-distributions.html#exercises-8" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exercise-1-7" class="section level4 hasAnchor" number="10.13.0.1">
<h4><span class="header-section-number">10.13.0.1</span> Exercise 1<a href="sampling-distributions.html#exercise-1-7" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Imagine that in the cable example the client asks to sell them one cables that can carry <span class="math inline">\(12\)</span> Tons.</p>
<p>The engineer takes <span class="math inline">\(8\)</span> sample of cables in stock at random, and load them until they break with the results</p>
<p>13.34642, 13.32620, 13.01459, 13.10811, 12.96999, 13.55309, 13.75557, 12.62747</p>
<p>If he does not know the certified breaking load, but he assumes that it is a normal variable, is it reasonable to sell one cable from the stock? (A: <span class="math inline">\(P(X&lt;12)=0.0003\)</span>).</p>
</div>
<div id="from-estimation-to-inference" class="section level3 hasAnchor" number="10.13.1">
<h3><span class="header-section-number">10.13.1</span> From Estimation to Inference<a href="sampling-distributions.html#from-estimation-to-inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far, we have seen how to use a sample to estimate population parameters such as the mean and variance. But these so called <strong>point estimates</strong> alone are not enough. Scientific conclusions require an understanding of how reliable those estimates are. This leads us to <strong>inference</strong> questions such as:</p>
<ul>
<li>How precise is our estimate?</li>
<li>What range of values is consistent with the data?</li>
<li>Could the observed estimate have occurred by chance?</li>
</ul>
<p>In the following chapters, we explore how the sampling distributions of estimators form the basis for answering these questions through <strong>confidence intervals</strong> and <strong>hypothesis tests</strong>.</p>
<div id="exercise-2-7" class="section level4 hasAnchor" number="10.13.1.1">
<h4><span class="header-section-number">10.13.1.1</span> Exercise 2<a href="sampling-distributions.html#exercise-2-7" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>An electronics company manufactures resistors that have an average resistance of 100 ohms and
a standard deviation of 10 ohms. The resistance distribution is normal.</p>
<ul>
<li><p>What is the sample mean of <span class="math inline">\(n=25\)</span> resistors? (R:100)</p></li>
<li><p>What is the variance of the sample mean of <span class="math inline">\(n=25\)</span> resistors? (R:4)</p></li>
<li><p>What is the standard error of the sample mean of <span class="math inline">\(n=25\)</span> resistors? (R:2)</p></li>
<li><p>Find the probability
that a random sample of <span class="math inline">\(n = 25\)</span> resistors have an average resistance of less than <span class="math inline">\(95\)</span> ohms (R: 0.0062)</p></li>
</ul>
</div>
<div id="exercise-3-5" class="section level4 hasAnchor" number="10.13.1.2">
<h4><span class="header-section-number">10.13.1.2</span> Exercise 3<a href="sampling-distributions.html#exercise-3-5" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A battery model charges an average of <span class="math inline">\(75\%\)</span> of its capacity in one hour with a standard deviation of <span class="math inline">\(15\%\)</span>.</p>
<ul>
<li><p>If the battery charge is a normal variable, what is the probability that the charge difference between the sample mean of <span class="math inline">\(25\)</span> batteries and the mean charge is at most <span class="math inline">\(5\%\)</span>? (R:0.9044)</p></li>
<li><p>If we charge <span class="math inline">\(100\)</span> batteries, what is that probability? (R:0.9991)</p></li>
<li><p>If instead we only charge <span class="math inline">\(9\)</span> batteries, what charge <span class="math inline">\(c\)</span> is exceeded by the sample mean with probability <span class="math inline">\(0.015\)</span>? (A:85.850)</p></li>
</ul>
</div>
<div id="exercise-4-4" class="section level4 hasAnchor" number="10.13.1.3">
<h4><span class="header-section-number">10.13.1.3</span> Exercise 4<a href="sampling-distributions.html#exercise-4-4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>On January 28, 1986, the space shuttle Challenger exploded just seconds after launch, killing all seven crew members. The night before, concerns were raised about the potential damage that the low temperature forecast at launch timeâ<span class="math inline">\(29Â°F\)</span> (<span class="math inline">\(â1.6Â°C\)</span>)âcould cause to two rubber O-rings that sealed sections of the solid rocket boosters. The failure of engineers to properly analyze available data, combined with pressure to avoid further delays in the mission, contributed to the disaster.</p>
<p>Engineers had access to data from previous shuttle launches where damage to the O-rings and they the temperatures on the days of those launches were recorded <span class="citation">(<a href="#ref-tufte1997visual">Tufte 1997</a>)</span>. Crucially ignoring the severity of the the damage and the launches with no damage, some arguments against delaying the mission were based on the temperatures of the launches with some damage only.</p>
<p>These are the the temperatures of the launches that registered some damage</p>
<p>53, 57, 58, 63, 70, 75</p>
<p>What is the probability that a launch with some damage is observed at temperature <span class="math inline">\(29Â°F\)</span>? (A: <span class="math inline">\(P(X&lt; 29)=3.10\times 10^{-5}\)</span>).</p>

</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Duray2017Micra12mo" class="csl-entry">
Duray, Gabor Z., Philippe Ritter, Mikhael El-Chami, Calambur Narasimhan, Razali Omar, Jose M. Tolosana, Shu Zhang, et al. 2017. <span>âLongâterm Performance of a Transcatheter Pacing System: 12âmonth Results from the Micra Transcatheter Pacing Study.â</span> <em>Heart Rhythm</em> 14 (5): 702â9. <a href="https://doi.org/10.1016/j.hrthm.2017.01.035">https://doi.org/10.1016/j.hrthm.2017.01.035</a>.
</div>
<div id="ref-Thomson1897eoverm" class="csl-entry">
Thomson, J. J. 1897. <span>âCathode Rays: Investigations on the Charge and Mass of Electrons.â</span> <em>Philosophical Magazine</em> 44 (263): 293â316. <a href="https://doi.org/10.1080/14786449708621070">https://doi.org/10.1080/14786449708621070</a>.
</div>
<div id="ref-tufte1997visual" class="csl-entry">
Tufte, Edward R. 1997. <em>Visual Explanations: Images and Quantities, Evidence and Narrative</em>. Cheshire, CT: Graphics Press.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="normal-distribution.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="central-limit-theorem.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/09-SamplingDistriutions.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
