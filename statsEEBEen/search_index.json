[["index.html", "EEBE stats Chapter 1 Objective 1.1 Recommended reading", " EEBE stats Alejandro Caceres 2023-09-20 Chapter 1 Objective This is the introduction course to the statistics of the EEBE (UPC). Statistics is a language that allows you to face new problems, on which we have no solution, and where the randomness plays a crucial role. In this course we will discuss the fundamental concepts of statistics. 3 hours of Theory per week: we will explain the concepts, we will exercise. 6 hours of Individual study per week: notes of course notes and resources in Athena. 2 hours of problem solving with R: face-to-face sessions (practices). Exam dates and additional study material can be found in ATENEA metacurso: Evaluation objectives: Q1 (10%): Test in computer Duration 2h on the indicated dates. Basic command knowledge (practices) Ability to calculate descriptive statistics and graphics, in specific situations (theory/practice) Knowledge about linear regression (practices) EP1 (25%): Written test (2-3 problems) Capacity to interpret statements in probability formulas (theory). Knowledge of the basic tools to solve problems of joint probability and conditional probability (theory). Mathematical knowledge of probability functions to calculate its basic properties (theory). Q2 (10%): Test in computer Duration 2h on the indicated dates Capacity to identify probability models in concrete problems (theory/practice). Use of R functions to calculate probabilities of probabilistic models (practice/theory) Identification of a sampling statistic and its properties (theory/practice) Knowledge of how to calculate the probability of sampling statistics (theory/practice) Use of R commands to calculate probabilities and make random sampling simulations (practice) EP2 (40%): Written test (2-3 problems) Mathematical capacity to determine specific estimators of probability models. Knowledge of the properties of specific estimators. Knowledge of confidence intervals and their properties (theory). Ability to identify the type of confidence interval in a specific problem (theory). Knowledge of hypothesis types to be used in a specific problems (theory). f.Use of R commands to solve confidence intervals and hypothesis tests (practice). CG (5%): Written test (2 questions about a text) Written expression capacity on a subject related to statistics. Coordinators: Luis Mujica (Luis.eduardo.mujica@upc.edu) Pablo Buenestado (Pablo.buenetado@upc.edu) 1.1 Recommended reading Class notes are our section will be accessible in Athena in PDF and HTML. Douglas C. Montgomery and George C. Runger. “Apply Statistics and Probability for Engineers” 4th Edition. Wiley 2007. "],["data-description.html", "Chapter 2 Data description 2.1 Scientific method 2.2 Statistics 2.3 Data 2.4 Result types 2.5 Random experiments 2.6 Absolute frequencies 2.7 Relative frequencies 2.8 Bar chart 2.9 Pie chart (pie) 2.10 Ordinal categorical variables 2.11 Accumulated absolute and relative frequencies 2.12 Cumulative frequency graph 2.13 Numeric variables 2.14 Transforming continuous data 2.15 Frequency table for a continuous variable 2.16 Histogram 2.17 Cumulative frequency graph 2.18 Summary Statistics 2.19 Average (sample mean) 2.20 Median 2.21 Dispersion 2.22 Sample variance 2.23 Interquartile range (IQR) 2.24 Boxplot 2.25 Questions 2.26 Exercises", " Chapter 2 Data description In this chapter, we will introduce tools for describing data. We will do so using tables, figures, and descriptive statistics of central tendency and dispersion. We will also introduce key concepts in statistics such as randomized experiments, observations, outcomes, and absolute and relative frequencies. 2.1 Scientific method One of the goals of the scientific method is to provide a framework for solving problems that arise in the study of natural phenomena or in the design of new technologies. Modern humans have developed a method over thousands of years that is still in development. The method has three main human activities: Observation characterized by the acquisition of data Reason characterized by the development of mathematical models Action characterized by the development of new experiments (technology) Their complex interaction and results are the basis of scientific activity. 2.2 Statistics Statistics deals with the interaction between models and data (the bottom part of the figure). The statistical questions are: What is the best model for my data (inference)? What are the data that a certain model (prediction) would produce? 2.3 Data The data is presented in the form of observations. An Observation or Realization is the acquisition of a number or characteristic of an experiment. For example, let’s take the series of numbers produced by repeating an experiment (1: success, 0: failure). … 1 0 0 1 0 1 0 1 1 … The number in bold is an observation in a repeat of the experiment An outcome is a possible observation that is the result of an experiment. 1 is one result, 0 is the other result of the experiment. Remember that the observation is concrete is the number you get one day in the laboratory. The abstract result is one of the characteristics of the type of experiment you are running. 2.4 Result types In statistics we are mainly interested in two types of results. Categorical: If the result of an experiment is a quality. They can be nominal (binary: yes, no; multiple: colors) or ordinal when the qualities can be ranked (severity of a disease). Numeric: If the result of an experiment is a number. The number can be discrete (number of emails received in an hour, number of leukocytes in the blood) or continuous (battery charge status, engine temperature). 2.5 Random experiments It can be said that the subject of study of statistics is random experiments, the means by which we produce data. Definition: A random experiment is an experiment that gives different results when repeated in the same way. Randomized experiments are of different types, depending on how they are conducted: on the same object (person): temperature, sugar levels. different objects but of the same size: the weight of an animal. about events: the number of hurricanes per year. 2.6 Absolute frequencies When we repeat a randomized experiment with categorical results, we record a list of results. We summarize observations by counting how many times we saw a particular result. Absolute frecuency: \\[ n_i \\] is the number of times we observe the result \\(i\\). Example (leukocytes) Let’s take a leukocyte from a donor and write down its type. Let’s repeat the experiment \\(N=119\\) times. (T cell, T cell, Neutrophil, ..., B cell) The second T cell in bold is the second observation. The last B cell is observation number 119. We can list the results (categories) in a frequency table: ## outcome ni ## 1 T Cell 34 ## 2 B cell 50 ## 3 basophil 20 ## 4 Monocyte 5 ## 5 Neutrophil 10 From the table, we can say that, for example, \\(n_1=34\\) is the total number of T cells observed in the repeat experiment. We also note that the total number of repetitions \\(N=\\sum_i n_i = 119\\). 2.7 Relative frequencies We can also summarize observations by calculating the proportion of how many times we saw a particular result. \\[ f_i = n_i /N\\] where \\(N\\) is the total number of observations In our example, \\(n_1=34\\) T cells were recorded, so we asked about the proportion of T cells out of the total \\(119\\). We can add these proportions \\(f_i\\) in the frequency table. ## outcome ni fi ## 1 T Cell 34 0.28571429 ## 2 B cell 50 0.42016807 ## 3 basophil 20 0.16806723 ## 4 Monocyte 5 0.04201681 ## 5 Neutrophil 10 0.08403361 Relative frequencies are fundamental in statistics. They give the proportion of one result in relation to the other results. Later we will understand them as the observations of probabilities. For absolute and relative frequencies we have the properties \\(\\sum_{i=1..M} n_i = N\\) \\(\\sum_{i=1..M} f_i = 1\\) where \\(M\\) is the number of results. 2.8 Bar chart When we have a lot of results and want to see which ones are most likely, we can use a bar chart that is a number of \\(n_i\\) Vs the results. 2.9 Pie chart (pie) We can also visualize the relative frequencies with a pie chart. The area of the circle represents 100% of the observations (proportion = 1) and the sections the relative frequencies of each result. 2.10 Ordinal categorical variables The leukocyte type in the above examples is a categorical nominal variable. Each observation belongs to a category (quality). The categories do not always have a certain order . Sometimes categorical variables can be sorted when they meet a natural ranking. This allows you to compute cumulative frequencies. Example (Misophonia) This is a clinical study on 123 patients who were examined for their degree of misophonia. Misophnia is uncontrolled anxiety/anger produced by certain sounds . Each patient was evaluated with a questionnaire (AMISO) and they were classified into 4 different groups according to severity. The results of the study are ## [1] 4 2 0 3 0 0 2 3 0 3 0 2 2 0 2 0 0 3 3 0 3 3 2 0 0 0 4 2 2 0 2 0 0 0 3 0 2 ## [38] 3 2 2 0 2 3 0 0 2 2 3 3 0 0 4 3 3 2 0 2 0 0 0 2 2 0 0 2 3 0 1 3 2 4 3 2 3 ## [75] 0 2 3 2 4 1 2 0 2 0 2 0 2 2 4 3 0 3 0 0 0 2 2 1 3 0 0 3 2 1 3 0 4 4 2 3 3 ## [112] 3 0 3 2 1 2 3 3 4 2 3 2 Each observation is the result of a randomized experiment: measurement of the level of misophonia in a patient. This data series can be summarized in terms of the results in the frequency table ## outcome ni fi ## 1 0 41 0.33333333 ## 2 1 5 0.04065041 ## 3 2 37 0.30081301 ## 4 3 31 0.25203252 ## 5 4 9 0.07317073 2.11 Accumulated absolute and relative frequencies Misophonia severity is categorical ordinal because its results can be ordered relative to its degree. When the results can be ordered, it is useful to ask how many observations were obtained up to a given result. We call this number the absolute cumulative frequency up to the result \\(i\\): \\[N_i =\\sum_{k= 1..i } n_k\\] It is also useful for calculating the proportion of observations up to a given result. \\[F_i =\\sum_{k= 1..i } f_k\\] We can add these frequencies in the frequency table ## outcome ni fi Ni Fi ## 0 0 41 0.33333333 41 0.3333333 ## 1 1 5 0.04065041 46 0.3739837 ## 2 2 37 0.30081301 83 0.6747967 ## 3 3 31 0.25203252 114 0.9268293 ## 4 4 9 0.07317073 123 1.0000000 Therefore, 67% of patients had misophonia up to severity 2 and 37% of patients had severity less than or equal to 1. 2.12 Cumulative frequency graph \\(F_i\\) is an important quantity because it allows us to define the accumulation of probabilities down to intermediate levels. The probability of an intermediate level \\(x\\) (\\(i\\leq x&lt; i+1\\)) is just the accumulation up to the lower level \\(F_x = F_i\\). \\(F_x\\) is therefore a function on a continuous range of values. We can draw it with respect to the results. Therefore, we can say that 67% of the patients had misophonia up to severity \\(2.3\\), although \\(2.3\\) is not an observed outcome. 2.13 Numeric variables The result of a random experiment can produce a number. If the number is discrete, we can generate a frequency table, with absolute, relative, and cumulative frequencies, and illustrate them with bar, pie, and cumulative charts. When the number is continuous the frequencies are not useful, we are most likely to observe or not observe a particular continuous number. Example (misophonia) The researchers wondered if the convexity of the jaw would affect the severity of misophonia. The scientific hypothesis is that the angle of convexity of the jaw can influence hearing and its sensitivity. These are the mandibular convexity results (degrees) for each patient: ## [1] 7.97 18.23 12.27 7.81 9.81 13.50 19.30 7.70 12.30 7.90 12.60 19.00 ## [13] 7.27 14.00 5.40 8.00 11.20 7.75 7.94 16.69 7.62 7.02 7.00 19.20 ## [25] 7.96 14.70 7.24 7.80 7.90 4.70 4.40 14.00 14.40 16.00 1.40 9.76 ## [37] 7.90 7.90 7.40 6.30 7.76 7.30 7.00 11.23 16.00 7.90 7.29 6.91 ## [49] 7.10 13.40 11.60 -1.00 6.00 7.82 4.80 11.00 9.00 11.50 16.00 15.00 ## [61] 1.40 16.80 7.70 16.14 7.12 -1.00 17.00 9.26 18.70 3.40 21.30 7.50 ## [73] 6.03 7.50 19.00 19.01 8.10 7.80 6.10 15.26 7.95 18.00 4.60 15.00 ## [85] 7.50 8.00 16.80 8.54 7.00 18.30 7.80 16.00 14.00 12.30 11.40 8.50 ## [97] 7.00 7.96 17.60 10.00 3.50 6.70 17.00 20.26 6.64 1.80 7.02 2.46 ## [109] 19.00 17.86 6.10 6.64 12.00 6.60 8.70 14.05 7.20 19.70 7.70 6.02 ## [121] 2.50 19.00 6.80 2.14 Transforming continuous data Since continuous outcomes cannot be counted (informatively), we transform them into ordered categorical variables. First we cover the range of observations in regular intervals of the same size (bins) ## [1] &quot;[-1.02,3.46]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; &quot;(16.8,21.3]&quot; Then we map each observation to its interval: creating a categorical variable ordered; in this case with 5 possible outcomes ## [1] &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; ## [6] &quot;(12.4,16.8]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; ## [11] &quot;(12.4,16.8]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; ## [16] &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; ## [21] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; ## [26] &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [31] &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; &quot;[-1.02,3.46]&quot; ## [36] &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [41] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; ## [46] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; ## [51] &quot;(7.92,12.4]&quot; &quot;[-1.02,3.46]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [56] &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; ## [61] &quot;[-1.02,3.46]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; ## [66] &quot;[-1.02,3.46]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;[-1.02,3.46]&quot; ## [71] &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; ## [76] &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; ## [81] &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; ## [86] &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; ## [91] &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; ## [96] &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; ## [101] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; ## [106] &quot;[-1.02,3.46]&quot; &quot;(3.46,7.92]&quot; &quot;[-1.02,3.46]&quot; &quot;(16.8,21.3]&quot; &quot;(16.8,21.3]&quot; ## [111] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; ## [116] &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [121] &quot;[-1.02,3.46]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; Therefore, instead of saying that the first patient had an angle of convexity of \\(7.97\\), we say that his angle was between the interval (or bin) \\((7.92,12.4]\\). No other patients had an angle of \\(7.97\\), but many had angles between \\((7.92,12.4]\\). 2.15 Frequency table for a continuous variable For a given regular partition of the interval of results into intervals, we can produce a frequency table as before ## outcome ni fi Ni Fi ## 1 [-1.02,3.46] 8 0.06504065 8 0.06504065 ## 2 (3.46,7.92] 51 0.41463415 59 0.47967480 ## 3 (7.92,12.4] 26 0.21138211 85 0.69105691 ## 4 (12.4,16.8] 20 0.16260163 105 0.85365854 ## 5 (16.8,21.3] 18 0.14634146 123 1.00000000 2.16 Histogram The histogram is the graph of \\(n_i\\) or \\(f_i\\) Vs the results in intervals (bins). The histogram depends on the size of the bins . This is a histogram with 20 bins . We see that most people have angles within \\((7, 8]\\) 2.17 Cumulative frequency graph We can also plot \\(F_x\\) against the results. Since \\(F_x\\) is of continuous range, we can order the observations (\\(x_1 &lt;... x_j &lt; x_{j+1} &lt; x_n\\)) and therefore \\[F_x = \\frac{k}{ n}\\] for \\(x_{k} \\leq x &lt; x_{k+ 1}\\) . \\(F_x\\) is known as the distribution of the data. \\(F_x\\) does not depend on the size of the bin. However, its resolution depends on the amount of data. 2.18 Summary Statistics Summary statistics are numbers calculated from the data that tell us important characteristics of the numerical variables (discrete or continuous). For example, we have statistics that describe extreme values: minimum: the minimum result observed maximum: the maximum result observed 2.19 Average (sample mean) An important statistic that describes the central value of the results (where to expect most observations) is the average \\[\\bar{x}=\\frac{1}{N} \\sum_{j= 1..N } x_j \\] where \\(x_j\\) is the observation \\(j\\) out of a total of \\(N\\). Example (Misophonia) The average convexity can be calculated directly from the observations in the usual way \\(\\bar{x}= \\frac{1}{ N}\\sum_j x_j\\) \\(= \\frac{1}{ N}( 7.97 + 18.23 + 12.27... + 6.80) = 10.19894\\) For categorically ordered variables, we can also use the relative frequencies to calculate the average \\(\\bar{x}=\\frac{1}{ N}\\sum_{i=1...N} x_j =\\frac{1}{N}\\sum_{i=1...M} x_i n_ {i}=\\) \\[\\sum_{i=1...M} x_i f_{i}\\] where we went from adding \\(N\\) observations to adding \\(M\\) results. The form \\(\\bar{x}= \\sum_{i = 1...M} x_i f_i\\) shows that the average is the center of gravity of the results. As if each result had a mass density given by \\(f_i\\). Example (Misophonia) The average severity of misophonia in the study can be calculated from the relative frequencies of the outcomes ## outcome ni fi ## 1 0 41 0.33333333 ## 2 1 5 0.04065041 ## 3 2 37 0.30081301 ## 4 3 31 0.25203252 ## 5 4 9 0.07317073 \\(\\bar{x}=0*f_ {0}+ 1*f_{1}+2*f_{2}+3*f_{3}+4*f_{4}=1.691057\\) The average is also the center of gravity for continuous variables. That is the point where the relative frequencies balance. 2.20 Median Another measure of centrality is the median. The median \\(x_m\\), or \\(q_{0.5}\\), is the value below which we find half of the observations. When we order the observations \\(x_1 &lt;... x_j &lt; x_{j+1} &lt; x_N\\), we count them until we find half of them. Therefore, \\(x_m\\) is the observation such that \\(m\\) satisfies \\[\\sum_{i\\leq m} 1 = \\frac{N}{2}\\] Example (Misophonia) If we order the angles of convexity, we see that \\(62\\) observations (individuals) (\\(N/2 \\sim 123/2\\)) are below \\(7.96\\). The median convexity is therefore \\(q_{0.5}= x_{62}=7.96\\) ## [1] -1.00 -1.00 1.40 1.40 1.80 2.46 2.50 3.40 3.50 4.40 4.60 4.70 ## [13] 4.80 5.40 6.00 6.02 6.03 6.10 6.10 6.30 6.60 6.64 6.64 6.70 ## [25] 6.80 6.91 7.00 7.00 7.00 7.00 7.02 7.02 7.10 7.12 7.20 7.24 ## [37] 7.27 7.29 7.30 7.40 7.50 7.50 7.50 7.62 7.70 7.70 7.70 7.75 ## [49] 7.76 7.80 7.80 7.80 7.81 7.82 7.90 7.90 7.90 7.90 7.90 7.94 ## [61] 7.95 7.96 ## [1] 7.96 7.97 8.00 8.00 8.10 8.50 8.54 8.70 9.00 9.26 9.76 9.81 ## [13] 10.00 11.00 11.20 11.23 11.40 11.50 11.60 12.00 12.27 12.30 12.30 12.60 ## [25] 13.40 13.50 14.00 14.00 14.00 14.05 14.40 14.70 15.00 15.00 15.26 16.00 ## [37] 16.00 16.00 16.00 16.14 16.69 16.80 16.80 17.00 17.00 17.60 17.86 18.00 ## [49] 18.23 18.30 18.70 19.00 19.00 19.00 19.00 19.01 19.20 19.30 19.70 20.26 ## [61] 21.30 We cut the data at ## [1] 7.96 to split them in half. In terms of frequencies, \\(q_{0.5}\\) makes the cumulative frequency \\(F_x\\) equal to \\(0.5\\) \\[\\sum_{i = 0, ... m} f_i =F_{q_{0.5}}=0.5\\] that is \\[q_{ 0.5}= F^{-1}(0.5)\\] This last equation means that, in the distribution graph, the median \\(q_{ 0.5}\\) is the value of \\(x\\) at which we have climbed half of the total height of \\(F\\). The mean and median are not always the same. 2.21 Dispersion Other important summary statistics for observations are the spread statistics. Many experiments may share their mean, but differ in how sparse the values are. The dispersion of the observations is a measure of the noise. 2.22 Sample variance The dispersion about the mean is measured by the sample variance \\[s^2=\\frac{ 1}{ N-1} \\sum_{j=1..N} ( x_j -\\bar{x})^2\\] This number measures the average squared distance of the observations from the average. The reason for \\(N-1\\) will be explained when we talk about inference, when we study the spread of \\(\\bar{x}\\), as well as the spread of the observations. In terms of the frequencies of the variables that are categorical and ordered, we can also calculate the sample variance as \\[s^2=\\frac{N}{N-1} \\sum_{i=1... M} (x_i -\\bar{x})^2 f_i\\] \\(s^2\\) can be considered as the moment of inertia of the observations. The square root of the sample variance, \\(s\\), is called standard deviation of the sample. Example (Misophonia) The standard deviation of the angle of convexity is \\(s= [\\frac{ 1}{123-1}((7.97-10.19894)^2+ (18.23-10.19894)^2\\) \\[+ (12.27-10.19894 )^ 2 + ...)]^{1/2} = 5.086707\\] The jaw convexity deviates from its mean by \\(5.086707\\). 2.23 Interquartile range (IQR) The spread of the data can also be measured with respect to the median using the interquartile range: We define the first quartile as the value \\(x_m\\) that makes the cumulative frequency \\(F_{q_{0.25}}\\) equal to \\(0.25\\) (or the value of \\(x\\) where we have accumulated a quarter of the observations, or the value that splits the first quarter of the observations) \\[q_{0.25}=F^{-1}(0.25)\\] We define the third quartile as the value \\(x_m\\) that makes the cumulative frequency \\(F_{q_{0.75}}\\) equal to \\(0.75\\) (or the value of \\(x\\) where we have accumulated three quarters of observations) \\[q_{0.75}=F^{-1}(0.75)\\] The interquartile range (IQR) is \\[IQR=q_{0.75} - q_{0.25}\\] This is the distance between the third and first quartiles and captures the central \\(50\\%\\) of the observations 2.24 Boxplot The interquartile range, median, and \\(5\\%\\) and \\(95\\%\\) of the data can be displayed in a box plot. In the boxplot, the values of the results are on the y-axis. The IQR is the box, the median is the middle line, and the whiskers mark the \\(5\\%\\) and \\(95\\%\\) of the data. 2.25 Questions 1) In the following boxplot, the first quartile and second quartile of the data are: \\(\\qquad\\)a: \\((-1.00, 21.30)\\); \\(\\qquad\\)b: \\((-1.00, 7.02)\\); \\(\\qquad\\)c: \\((7.02, 7.96)\\); \\(\\qquad\\)d: \\((7.02, 14.22)\\) 2) The main disadvantage of a histogram is that: \\(\\qquad\\) a : Depends on the size of the bin ; \\(\\qquad\\)b : Cannot be used for categorical variables; \\(\\qquad\\) c : Cannot be used when the bin size is small; \\(\\qquad\\) d : Used only for relative frequencies; 3) If the relative cumulative frequencies of a random experiment with outcomes \\(\\{1,2,3,4\\}\\) are: \\(F(1)=0.15, \\qquad F(2)=0.60, \\qquad F(3)=0.85, \\qquad F(4)=1\\). Then the relative frequency for the outcome \\(3\\) is \\(\\qquad\\)a: \\(0.15\\); \\(\\qquad\\)b: \\(0.85\\); \\(\\qquad\\)c: \\(0.45\\); \\(\\qquad\\)d: \\(0.25\\) 4) In a sample of size \\(10\\) from a random experiment we obtained the following data: \\(8, \\qquad 3, \\qquad 3, \\qquad 7, \\qquad 3, \\qquad 6, \\qquad 5, \\qquad 10, \\qquad 3, \\qquad 8\\). The first quartile of the data is: \\(\\qquad\\)a: \\(3.5\\); \\(\\qquad\\)b: \\(4\\); \\(\\qquad\\)c: \\(5\\); \\(\\qquad\\)d: \\(3\\) 5) Imagine that we collect data for two quantities that are not mutually exclusive, for example, the gender and nationality of passengers on a flight. If we want to make a single pie chart for the data, which of these statements is true? \\(\\qquad\\)a : We can only make a nationality pie chart because it has more than two possible outcomes; \\(\\qquad\\)b : We can make a pie graph for a new variable marking gender and nationality; \\(\\qquad\\)c : We can make a pie chart for the variable sex or the variable nationality; \\(\\qquad\\)d : We can only choose whether to make a pie chart for gender or a pie chart for nationality. 2.26 Exercises 2.26.0.1 Exercise 1 We have performed an experiment 8 times with the following results ## [1] 3 3 10 2 6 11 5 4 Answer the following questions: Calculate the relative frequencies of each result. Calculate the cumulative frequencies of each result. What is the average of the observations? What is the median? What is the third quartile? What is the first quartile? 2.26.0.2 Exercise 2 We have performed an experiment 10 times with the following results ## [1] 2.875775 7.883051 4.089769 8.830174 9.404673 0.455565 5.281055 8.924190 ## [9] 5.514350 4.566147 Consider 10 bins of size 1: [0,1], (1,2] …( 9,10). Answer the following questions: Calculate the relative frequencies of each result and draw the histogram Calculate the cumulative frequencies of each result and draw the cumulative graph. Draw a box plot . "],["probability.html", "Chapter 3 Probability 3.1 Random experiments 3.2 Measurement probability 3.3 Classical probability 3.4 Relative frequencies 3.5 Relative frequencies at infinity 3.6 Frequentist probability 3.7 Classical and frequentist probabilities 3.8 Definition of probability 3.9 Probabilities Table 3.10 Sample space 3.11 Events 3.12 Algebra of events 3.13 Mutually exclusive results 3.14 Joint probabilities 3.15 Contingency table 3.16 The addition rule: 3.17 Questions 3.18 Exercises", " Chapter 3 Probability In this chapter we will introduce the concept of probability from relative frequencies. We will define the events as the elements on which the probability is applied. Composite events will be defined using set algebra. Then we will discuss the concept of conditional probability derived from the joint probability of two events. 3.1 Random experiments Let’s remember the basic objective of statistics. Statistics deals with data that is presented in the form of observations. An observation is the acquisition of a number or characteristic from an experiment Observations are realizations of results. An outcome is a possible observation that is the result of an experiment. When conducting experiments, we often get different results. The description of the variability of the results is one of the objectives of statistics. A random experiment is an experiment that gives different results when repeated in the same way. The philosophical question behind it is how can we know something if every time we look at it it changes? 3.2 Measurement probability We would like to have a measure for the outcome of a randomized experiment that tells us how sure we are of observing the outcome when we perform a future randomized experiment. We will call this measure the probability of the outcome and assign values to it: 0, when we are sure that the observation will not occur. 1, when we are sure that the observation will happen. 3.3 Classical probability As long as a random experiment has \\(M\\) possible outcomes that are all equally likely, the probability of each \\(i\\) outcome is \\[P_i =\\frac{1}{ M}\\]. Classical probability was defended by Laplace (1814). Since every outcome is equally likely in this type of experiment, we declare complete ignorance and the best we can do is equally distribute the same probability for each outcome. We do not observe \\(P_i\\) We deduce \\(P_i\\) from our ratio and we don’t need to carry out any experiment to know it. Example (dice): What is the probability that we will get \\(2\\) on the roll of a die? \\(P_2=1/6=0.166666\\). 3.4 Relative frequencies What about random experiments whose possible outcomes are not equally likely? How then can we define the probabilities of the outcomes? Example (random experiment) Imagine that we repeat a random experiment \\(8\\) times and obtain the following observations 8 4 12 7 10 7 9 12 How sure are we of obtaining the result \\(12\\) in the following observation? The frequency table is ## outcome ni fi ## 1 4 1 0.125 ## 2 7 2 0.250 ## 3 8 1 0.125 ## 4 9 1 0.125 ## 5 10 1 0.125 ## 6 12 2 0.250 The relative frequency \\(f_i =\\frac{ n_ i }{ N}\\) seems like a reasonable probability measure because is a number between \\(0\\) and \\(1\\). measures the proportion of the total number of observations that we observe of a particular result. Since \\(f_{12}=0.25\\) then we would be one quarter sure, one out of every 4 observations, of getting \\(12\\). Question: How good is \\(f_i\\) as a measure of certainty of the result \\(i\\)? Example (random experiment with more repetitions) Let’s say we repeat the experiment 100,000 more times: The frequency table is now ## outcome ni fi ## 1 2 2807 0.02807 ## 2 3 5607 0.05607 ## 3 4 8435 0.08435 ## 4 5 11070 0.11070 ## 5 6 13940 0.13940 ## 6 7 16613 0.16613 ## 7 8 13806 0.13806 ## 8 9 10962 0.10962 ## 9 10 8402 0.08402 ## 10 11 5581 0.05581 ## 11 12 2777 0.02777 and the barplot is New results came out and \\(f_{12}\\) is now only \\(0.027\\), and so we are only \\(\\sim 3\\%\\) sure to get \\(12\\) in the next experiment. The probabilities measured by \\(f_i\\) change with \\(N\\). 3.5 Relative frequencies at infinity A crucial observation is that if we measure the probabilities of \\(f_i\\) in increasing values of \\(N\\) they converge! In this graph each vertical section gives the relative frequency of each observation. We see that after \\(N=1000\\) (\\(log10(N)=3\\)) the proportions hardly change with more \\(N\\). We find that each of the relative frequencies \\(f_i\\) converges to a constant value \\[lim_{N\\rightarrow \\infty} f_i = P_i\\] 3.6 Frequentist probability We call Probability \\(P_i\\) the limit as \\(N \\rightarrow \\infty\\) of the relative frequency of observing the outcome \\(i\\) in a random experiment. Defended by Venn (1876), the frequentist definition of probability is derived from (empirical) data/experience. We do not observe \\(P_i\\), we observe \\(f_i\\) We estimate \\(P_i\\) with \\(f_i\\) (usually when \\(N\\) is large), we write: \\[\\hat {P_ i}= f_i\\] Similar to the relationship between observation and result, we have the relationship between relative frequency and probability as a concrete value of an abstract quantity. 3.7 Classical and frequentist probabilities We have situations where classical probability can be used to find the limit of relative frequencies: If the results are equally probable, the classical probability gives us the limit: \\[P_i=lim_{N\\rightarrow \\infty} \\frac{n_i}{N}=\\frac{1}{M}\\] If the results in which we are interested can be derived from other equally probable results. We will see more about this when we study probability models. Example (sum of two dice) Our previous example is based on the sum of two dice. Although we perform the experiment many times, write down the results, and calculate the relative frequencies, we can know the exact value of probability. This probability follows from the fact that the outcome of each die is equally likely. From this assumption, we can find that (Exercise 1) \\[ P_i = \\begin{cases} \\frac{i-1}{36},&amp; i \\in \\{2,3,4,5,6, 7\\} \\\\ \\frac{13-i}{36},&amp; i \\in \\{8,9,10,11,12\\} \\\\ \\end{cases} \\] The motivation of the frequentist definition is empirical (data) while that of the classical definition is rational (models). We often combine both approaches (inference and deduction) to find out the probabilities of our random experiment. 3.8 Definition of probability A probability is a number that is assigned to each possible outcome of a random experiment and satisfies the following properties or axioms: when the results \\(E_1\\) and \\(E_2\\) are mutually exclusive; that is, only one of them can occur, so the probability of observing \\(E_1\\) or \\(E_1\\), written as \\(E_1\\cup E_2\\), is their sum: \\[ P( E_1\\cup E_2) = P(E_1) + P(E_2)\\] when \\(S\\) is the set of all possible outcomes, then its probability is \\(1\\) (at least something is observed): \\[P(S)=1\\] The probability of any outcome is between 0 and 1 \\[P(E) \\in [0,1]\\] Proposed by Kolmogorov’s less than 100 years ago (1933) 3.9 Probabilities Table Kolmogorov properties are the basic rules for building a probability table, similar to the relative frequency table. Example (dice) The probability table for the throw of a dice result probability \\(1\\) 1/6 \\(2\\) 1/6 \\(3\\) 1/6 \\(4\\) 1/6 \\(5\\) 1/6 \\(6\\) 1/6 \\(P( 1 \\cup 2\\cup ... \\cup 6)\\) 1 Let’s verify the axioms: Where \\(1 \\cup 2\\) is, for example, the event of rolling a \\(1\\) or a \\(2\\). So \\[ P( 1 \\cup 2)=P(1)+P(2)=2/6\\] Since \\(S= \\{ 1,2,3,4,5,6\\}\\) is made up of mutually exclusive outcomes, then \\[P(S)=P(1\\cup 2\\cup ... \\cup 6) = P(1)+P(2)+ ...+P(n)=1\\] The probabilities of each outcome are between \\(0\\) and \\(1\\). 3.10 Sample space The set of all possible outcomes of a random experiment is called the sample space and is denoted \\(S\\). The sample space can be made up of categorical or numerical outcomes. For example: human temperature: \\(S = (36, 42)\\) degrees Celsius. sugar levels in humans: \\(S =( 70-80) mg/ dL\\) the size of a production line screw: \\(S =( 70-72) mm\\) number of emails received in an hour: \\(S = \\{ 1, ...\\infty \\}\\) the throw of a dice: \\(S= \\{ 1, 2, 3, 4, 5, 6\\}\\) 3.11 Events An event \\(A\\) is a subset of the sample space. It is a collection of posible results. Examples of events: The event of a healthy temperature: \\(A=37-38\\) degrees Celsius The event of producing a screw with a size: \\(A=71.5mm\\) The event of receiving more than 4 emails in an hour: \\(A= \\{ 4, \\infty \\}\\) The event of obtaining a number less than or equal to 3 in the roll of a says: \\(A= \\{ 1,2,3\\}\\) An event refers to a possible set of outcomes. 3.12 Algebra of events For two events \\(A\\) and \\(B\\), we can construct the following derived events using the basic set operations: Complement \\(A&#39;\\): the event of not \\(A\\) Union \\(A \\cup B\\): the event of \\(A\\) or \\(B\\) Intersection \\(A \\ cap B\\): the event of \\(A\\) and \\(B\\) Example (dice) Let’s roll a die and look at the events (result set): a number less than or equal to three \\(A:\\{ 1,2,3\\}\\) an even number \\(B:\\{ 2,4,6\\}\\) Let’s see how we can build new events with set operations: a number not less than three: \\(A &#39;:\\{4,5,6\\}\\) a number less than or equal to three or even: \\(A \\cup B: \\{ 1,2,3,4,6\\}\\) a number less than or equal to three and even \\(A \\cap B: \\{ 2\\}\\) 3.13 Mutually exclusive results Outcomes like rolling \\(1\\) and \\(2\\) on a die are events that cannot occur at the same time. We say that they are mutually exclusive. In general, two events denoted as \\(E_1\\) and \\(E_2\\) are mutually exclusive when \\[E_1\\cap E_2=\\emptyset\\] Examples: The result of having a misophonia severity of \\(1\\) and a severity of \\(4\\). The results of obtaining \\(12\\) and \\(5\\) by adding the throw of two dice. According to the Kolmogorov properties , only mutually exclusive outcomes can be arranged in probability tables, as in relative frequency tables. 3.14 Joint probabilities The joint probability of \\(A\\) and \\(B\\) is the probability of \\(A\\) and \\(B\\). That’s \\[P( A \\cap B)\\] or \\(P(A,B)\\). To write joint probabilities of non mutually exclusive events (\\(A \\cap B \\neq \\emptyset\\)) into a probability table, we note that we can always decompose the sample space into mutually exclusive sets involving the intersections: \\(S=\\{A\\cap B, A \\cap B&#39;, A&#39;\\cap B, A&#39;\\cap B&#39;\\}\\) Let’s consider the Ven diagram for the example where \\(A\\) is the event that corresponds to drawing a number less than or equal to 3 and \\(B\\) corresponds to an even number: The marginals of \\(A\\) and \\(B\\) are the probability of \\(A\\) and the probability of \\(B\\), respectively: \\(P(A)=P(A\\cap B&#39;) + P(A \\cap B)=2/6+1/6=3/6\\) \\(P(B)=P(A&#39;\\cap B) +P(A \\cap B)=2/6+1/6=3/6\\) We can now write the probability table for the joint probabilities Result probability \\((A\\cap B)\\) \\(P(A \\cap B)=1/6\\) \\((A\\cap B&#39;)\\) \\(P(A \\cap B&#39;)=2/6\\) \\((A&#39;\\cap B)\\) \\(P(A&#39; \\cap B)=2/6\\) \\((A&#39;\\cap B&#39;)\\) \\(P(A&#39; \\cap B&#39;)=1/6\\) sum \\(1\\) Each result has \\(two\\) values (one for the feature of type \\(A\\) and one for type \\(B\\)) 3.15 Contingency table The joint probability table can also be written in a contingency table \\(B\\) \\(B&#39;\\) sum \\(A\\) \\(P(A \\cap B )\\) \\(P(A\\cap B&#39; )\\) \\(P(A)\\) \\(A&#39;\\) \\(P(A&#39;\\cap B )\\) \\(P(A&#39;\\cap B&#39; )\\) \\(P(A&#39;)\\) sum \\(P(B)\\) \\(P(B&#39;)\\) 1 Where the marginals are the sums in the margins of the table, for example: \\(P(A)=P(A \\cap B&#39;) + P(A \\cap B)\\) \\(P(B)=P(A&#39; \\cap B) +P(A \\cap B)\\) In our example, the contingency table is \\(B\\) \\(B&#39;\\) sum \\(A\\) \\(1/6\\) \\(2/6\\) \\(3/6\\) \\(A&#39;\\) \\(2/6\\) \\(1/6\\) \\(3/6\\) sum \\(3/6\\) \\(3/6\\) \\(1\\) 3.16 The addition rule: The addition rule allows us to calculate the probability of \\(A\\) or \\(B\\), \\(P( A \\cup B)\\), in terms of the probability of \\(A\\) and \\(B\\), \\(P(A \\cup B )\\). We can do this in three equivalent ways: Using the marginals and the joint probability \\[P(A \\cup B)=P(A) + P(B) - P(A\\cap B)\\] Using only joint probabilities \\[P( A \\cup B)=P(A \\cap B)+P(A\\cap B&#39;)+P(A&#39;\\cap B)\\] Using the complement of joint probability \\[P(A \\cup B)=1-P(A&#39;\\cap B&#39;)\\] Example (dice) Take the events \\(A:\\{ 1,2,3\\}\\), rolling a number less than or equal to \\(3\\), and \\(B:\\{2,4,6\\}\\), rolling an even number on the roll of a dice. Therefore: \\(P( A \\cup B)=P(A) + P(B) - P(A\\cap B)=3/6+3/6-1/6=5/6\\) \\(P(A \\cup B)=P(A \\cap B)+P(A\\cap B&#39;)+P(A&#39;\\cap B)=1/6+2/6+2/6=5/6\\) \\(P(A \\cup B)=1-P(A&#39;\\cap B&#39;)= 1-1/6=5/6\\) In the contingency table \\(P( A \\cup B)\\) corresponds to the cells in bold (method 2 above). That is all cells but 1/6 from the bottom right (method 3). \\(B\\) \\(B&#39;\\) \\(A\\) 1/6 2/6 \\(A&#39;\\) 2/6 1/6 3.17 Questions We collect the age and category of 100 athletes in a competition \\(age:junior\\) \\(age:senior\\) \\(category:1st\\) \\(14\\) \\(12\\) \\(category:2nd\\) \\(21\\) \\(18\\) \\(category:3rd\\) \\(22\\) \\(13\\) 1) What is the estimated probability that an athlete is 2nd category and senior? \\(\\qquad\\)a: \\(18/100\\); \\(\\qquad\\)b: \\(18/43\\); \\(\\qquad\\)c: \\(18\\); \\(\\qquad\\)d: \\(18/39\\) 2) What is the estimated probability that the athlete is not in the third category and is senior? \\(\\qquad\\)a: \\(35/100\\); \\(\\qquad\\)b: \\(30/100\\); \\(\\qquad\\)c: \\(22/100\\); \\(\\qquad\\)d: \\(13/100\\) 3) What is the marginal probability of the third category? \\(\\qquad\\)a: \\(13/100\\); \\(\\qquad\\)b: \\(35/100\\); \\(\\qquad\\)c: \\(22/100\\); \\(\\qquad\\)d: \\(13/22\\) 4) What is the marginal probability of being senior? \\(\\qquad\\)a: \\(13/100\\); \\(\\qquad\\)b: \\(43/100\\); \\(\\qquad\\)c: \\(43/57\\); \\(\\qquad\\)d: \\(57/100\\) 5) What is the probability of being senior or third category? \\(\\qquad\\)a: \\(65/100\\); \\(\\qquad\\)b: \\(86/100\\); \\(\\qquad\\)c: \\(78/100\\); \\(\\qquad\\)d: \\(13/100\\) 3.18 Exercises 3.18.0.1 Classical probability: Exercise 1 Write the table of joint probability for the results of rolling two dice; In the rows write the results of the first die and in the columns the results of the second die. What is the probability of drawing \\((3, 4)\\) ? (R:1/36) What is the probability of rolling \\(3\\) and \\(4\\) with any of the two dice? (R:2/36) What is the probability of rolling \\(3\\) on the first die or \\(4\\) on the second? (To:11/36) What is the probability of rolling \\(3\\) or \\(4\\) with any dice? (R:20/36) Write the probability table for the result of the add of two dice. Assume that the outcome of each die is equally likely. Verify that it is: \\[ P_i= \\begin{cases} \\frac{i-1}{36},&amp; i \\in \\{2,3,4,5,6, 7\\} \\\\ \\frac{13-i}{36},&amp; i \\in \\{8,9,10,11,12\\} \\\\ \\end{cases} \\] 3.18.0.2 Frequentist probability: Exercise 2 The result of a randomized experiment is to measure the severity of misophonia and the state of depression of a patient. Misophonia severity: \\(S_M:\\{M_ 0,M _1,M_2,M_3,M_4\\}\\) Depression: \\(S_ D:\\{ D&#39;, D\\}\\)) Write the contingency table for the absolute frequencies (\\(n_{ M,D }\\)) for a study on a total of 123 patients in which it was observed 100 individuals did not have depression. No individual with misophonia 4 and without depression. 5 individuals with grade 1 misophonia and no depression. The same number as the previous case for individuals with depression and without misophonia . 25 individuals without depression and grade 3 misophonia . The number of misophonics without depression for grades 2 and 0 were distributed equally . The number of individuals with depression and misophonia increased progressively in multiples of three, starting at 0 individuals for grade 1. Answer the following questions: How many individuals had misophonia ? (A:83) How many individuals had grade 3 misophonia ? (R:31) How many individuals had grade 2 misophonia without depression? (R:35) Write down the contingency table for relative frequencies \\(f_{ M,D }\\). Suppose \\(N\\) is large and the absolute frequencies estimate the probabilities \\(f_{ M,D }=\\hat {P}(M \\cap D)\\). Answer the following questions: What is the marginal probability of severity 2 misophonia ? (R: 0.3) What is the probability of not being misophonic and not being depressed? (R:0.284) What is the probability of being misophonic or depressed? (R: 0.715) What is the probability of being misophonic and being depressed? (R: 0.146) Describe in spoken language the results with probability 0. 3.18.0.3 Exercise 3 We have carried out a randomized experiment \\(10\\) times, which consists of recording the sex and vital status of patients with some type of cancer after 10 years of diagnosis. We got the following results ## A B ## 1 male dead ## 2 male dead ## 3 male dead ## 4 female alive ## 5 male dead ## 6 female alive ## 7 female dead ## 8 female alive ## 9 male alive ## 10 male alive Create the contingency table for the number (\\(n_{ i,j }\\)) of observations of each result (\\(A,B\\)) Create the contingency table for the relative frequency (\\(f_{ i,j }\\)) of the results What is the marginal frequency of being a man? (R/0.6) What is the marginal frequency of being alive? (R/0.5) What is the frequency of being alive or being a woman? (R/0.6) 3.18.0.4 Theory: Exercise 4 From the second form of the addition rule, obtain the first and the third form. What is the third form addition rule for the probability of three events \\(P(A \\cup B \\cup C)\\)? "],["conditional-probability.html", "Chapter 4 Conditional probability 4.1 Joint probability 4.2 Statistical independence 4.3 The conditional probability 4.4 Conditional contingency table 4.5 Statistical independence 4.6 Statistical dependency 4.7 Diagnostic test 4.8 Inverse probabilities 4.9 Bayes’ Theorem 4.10 Exercises 4.11 Questions", " Chapter 4 Conditional probability In this chapter, we will introduce conditional probability. We will use conditional probability to define statistical independence. We will discuss Bayes’ theorem and we will discuss one of its main applications, which is the predictive efficiency of a diagnostic tool. 4.1 Joint probability Recall that the joint probability of two events \\(A\\) and \\(B\\) is defined as their intersection \\[P( A ,B )=P(A \\cap B)\\] Now imagine randomized experiments that measure two different types of outcomes. height and weight of an individual: \\((h, w)\\) time and position of an electric charge: \\((p, t)\\) the throw of two dice: (\\(n_1\\), \\(n_2\\)) cross two green traffic lights: (\\(\\bar{ R_ 1}\\) , \\(\\bar{R_2}\\)) We are often interested in whether the values of one result condition the values of the other. 4.2 Statistical independence In many cases, we are interested in whether two events often tend to occur together. We want to be able to discern between two cases. Independence between events. For example, rolling a 1 on one die does not make it more likely to roll another 1 on a second die. Correlation between events. For example, if a man is tall, he is probably heavy. Example (conductor) We conducted an experiment to find out if observing structural flaws in a material affects its conductivity. The data would look like Conductor Structure conductivity \\(c_1\\) flaws low \\(c_2\\) no flaws high \\(c_3\\) flaws low … … … \\(c_i\\) no flaws low* … … … … … … \\(c_n\\) flaws high* We can expect low conductivity to occur more often with flaws than without flaws if the flaws affect conductivity. Let’s imagine that from the data we obtain the following contingency table of estimated joint probabilities with flaws (F) no flaws (F’) sum low (L) \\(0.005\\) \\(0.045\\) \\(0.05\\) high (L’) \\(0.095\\) \\(0.855\\) \\(0.95\\) sum \\(0.1\\) \\(0.9\\) 1 where, for example, the joint probability of \\(L\\) and \\(F\\) is \\(P(L,F )=0.005\\) and the marginal probabilities are \\(P(L)=P(L, F) + P(L, F&#39;)=0.05\\) \\(P(F)=P(L, F) + P(L&#39;, F)= 0.1\\). 4.3 The conditional probability Low conductivity is independent of having structural flaws if the probability of having low conductivity (\\(L\\)) is the same whether it has flaws (\\(F\\)) or not (\\(F&#39;\\) ) . Let us first consider only the materials that have flaws. Among those materials that have flaws (\\(F\\)), what is the estimated probability that they have low conductivity? \\(\\hat{P}(L| F)= \\frac{n_{L,F}}{n_{F}}=\\frac{n_{L,F}/n}{n_{F}/n}= \\frac{f_{L,F}}{f_{F}}\\) \\[= \\frac{\\hat{P}( L,F )}{\\hat{P}(F)}\\] Therefore, in the limit when \\(N \\rightarrow \\infty\\), we have \\[P(L| F)= \\frac{P(L,F)}{P(D)}=\\frac{P(L\\cap F)}{P(D)}\\] Definition: The conditional probability of an event \\(B\\) given an event \\(A\\), denoted \\(P(A| B)\\) , is \\[P(A|B) = \\frac{P(A\\cap B)}{P(B)}\\] We can prove that conditional probability satisfies the axioms of probability. The conditional probability can be understood as a probability with a sample space given by \\(B\\): \\(S_B\\). In our example, the materials with stuctural flaw. 4.4 Conditional contingency table If we divide the columns of the joint probability table by the marginal probabilities of the conditioning effects (\\(F\\) and \\(F&#39;\\)), we can write a conditional contingency table F F’ L P( L | F) P(L | F’) L’ P(L’ | F) P(L’ | F’) sum 1 1 where the column probabilities sum to one. The first column shows the probabilities of low conductivity or not only of the materials that have flaws (first condition: \\(F\\)). The second column shows the probabilities only for the materials that have no flaws (second condition: \\(F&#39;\\)). Conditional probabilities are the probabilities of the event within each condition. We read them as: \\(P(L| F)\\): Probability of having low conductivity if it has flaws \\(P(L&#39;| F)\\): Probability of not having low conductivity if it has flaws \\(P(L|F &#39;)\\): Probability of having low conductivity if it has no flaws \\(P(L&#39;|F &#39;)\\): Probability of not having low conductivity if it has no flaws 4.5 Statistical independence In our example, the conditional contingency table is F F’ L P(L|F) = 0.05 P(L|F’)= 0.05 L’ P(L’|F)=0.95 P(L’|F’)=0.95 sum 1 1 We note that the marginal and conditional probabilities are the same! \\(P(L| F)= P(L|F&#39;)=P(L)\\) \\(P(L&#39;| F)= P(L&#39;|F&#39;)=P(L&#39;)\\) This means that the probability of observing a low conductivity is not dependent on having a structural flaw or not. We conclude that low conductivity is not affected by having a structural flaw. Definition Two events \\(A\\) and \\(B\\) are statistically independent if either of the equivalent cases occurs. \\(P(A| B)= P(A)\\); \\(A\\) is independent of \\(B\\) \\(P(B| A)= P(B)\\); \\(B\\) is independent of \\(A\\) and by the definition of conditional probability \\(P(A\\cap B)=P(A|B)P(B)=P(A)P(B)\\) This third form is a statement about joint probabilities. It says that we can obtain joint probabilities by multiplying the marginal ones. In our original joint probability table F F’ sum L \\(0.005\\) \\(0.045\\) \\(0.05\\) L’ \\(0.095\\) \\(0.855\\) \\(0.95\\) sum \\(0.1\\) \\(0.9\\) 1 we can confirm that all the entries of the matrix are the product of the marginal ones. For example: \\(P(F)P(L)= P( L \\cap F)\\) and \\(P(L&#39;)P(F&#39;)=P(L&#39; \\cap F&#39;)\\). Therefore, low conductivity is independent of having a structural flaw. Example (Coins) We want to confirm that the results of tossing two coins are independent. We consider all outcomes to be equally likely: result probability \\(( H,T )\\) 1/4 \\(( H,H )\\) 1/4 \\(( T,T )\\) 1/4 \\(( T, H )\\) 1/4 sum 1 where \\(( H,T )\\) is, for example, the event of heads on the first coin and tails on the second coin. The contingency table for the joint probabilities is: H T sum H \\(1/4\\) \\(1/4\\) \\(1/2\\) T \\(1/4\\) \\(1/4\\) \\(1/2\\) sum \\(1/2\\) \\(1/2\\) 1 From this table, we see that the probability of getting a head and then a tail is the product of the marginals \\(P( H, T)=P(H)*P(T)=1/4\\). Therefore, the events of heads in the first coin and tails in the second are independent. If we build the conditional contingency table on the toss of the first coin, we will see that obtaining tails in the second coin is not conditioned by having obtained heads in the first coin: \\(P(T| H)= P(T) =1 / 2\\) 4.6 Statistical dependency An important example of statistical dependency is found in the performance of diagnostic tools, where we want to determine the state of a system(s) with results satisfactory (yes) unsatisfactory (not) with a test (t) with results positive negative For example, we test a battery to see how long it can last. We load a cable to find out if it resists carrying a certain load. We run a PCR to see if someone is infected. 4.7 Diagnostic test Let’s consider diagnosing an infection with a new test. Infection status: yes (infected) no (not infected) Test: positive negative The conditional contingency table is what we get in a controlled environment (laboratory) Infection: yes Infection: No Test: positive P(positive | yes) P(positive | no) Test: negative P(negative | yes) P(negative | no) sum 1 1 Let’s look at the table entries 1) Rate of true positives (Sensitivity): The probability of testing positive if you have the disease \\(P(positive| yes)\\) Rate of true negatives (Specificity): The probability of testing negative if you do not have the disease \\(P(negative| no)\\) False positive rate: the probability of testing positive if you do not have the disease \\(P(positive| no)\\) False negative rate: the probability of testing negative if you have the disease \\(P(negative| yes)\\) High correlation (statistical dependence) between test and infection means high values for probabilities 1 and 2 and low values for probabilities 3 and 4. Example (COVID) Now let’s consider a real situation. In the early days of the coronavirus pandemic, there was no measure of the effectiveness of PCRs in detecting the virus. One of the first published studies (https://www.nejm.org/doi/full/10.1056/NEJMp2015897) found that The PCR had a sensitivity of 70%, in infection condition. The PCR had a specificity of 94%, in non-infected condition. The conditional contingency table is Infection: yes Infection: No Test: positive P(positive| yes)= 0.7 P(positive|no)=0.06 Test: negative P(negative| yes)= 0.3 P(negative|no)=0.94 sum 1 1 Therefore, the errors in the diagnostic tests were: The false positive rate is \\(P(positive| no)= 0.06\\) The false negative rate is \\(P(negative| yes)= 0.3\\) 4.8 Inverse probabilities We are interested in finding the probability of being infected if the test is positive: \\[P(si| positive)\\] For that: We recover the contingency table for joint probabilities, multiplying by the marginal \\(P(yes)\\) and \\(P(no)\\) that we need to know Infection: yes Infection: No sum Test: positive P( positive | yes)P(yes) P(positive | no)P(no) P(positive) Test: negative P( negative | yes)P(yes) P(negative | no) P(no) P(negative) sum P(yes) P(no) 1 We use the definition of conditional probabilities for rows instead of columns (we divide by the marginal of the test results) Infection: yes Infection: No sum Test: positive P(yes|positive) P(no|positive) 1 Test: negative P(yes|negative) P(no|negative) 1 For example: \\[P(yes| positive)= \\frac{P(positive|yes)P(yes)}{P(positive)}\\] To apply this formula we need the marginals \\(P(yes)\\) (prevalence) and \\(P(positive)\\). The prevalence \\(P(yes)\\) needs to be given from another study. The first prevalence study in Spain showed that during confinement \\(P(yes)=0.05\\), \\(P(no)=0.95\\), before the summer of 2020. To find the marginal of positives \\(P(positive)\\), we can then use the definition of marginal and conditional probability: \\(P(positive)= P(positive \\cap yes) + P(positive \\cap no)\\) \\[= P(positive| yes)P (yes)+P(positive|no)P(no)\\] This last relation of the marginals is called rule of total probability. 4.9 Bayes’ Theorem After substituting the total probability rule into \\(P(yes| positive)\\) , we have \\[P(yes| positive)= \\frac{P(positive|yes)P(yes)}{P(positive|yes)P(yes)+P(positive|no)P(no)}\\] This expression is known as Bayes’ theorem. It allows us to reverse the conditionals: \\[P(positive|yes) \\rightarrow P(yes| positive)\\] Or assess a test in a controlled condition (infection) and then use it to infer the probability of the condition when the test is positive. Example (COVID): The test performance was: Sensitivity: \\(P(positive| yes)= 0.70\\) False positive rate: \\(P(positive| no)= 1- P(negative|no)=0.06\\) The study in the Spanish population gave: \\(P(yes)=0.05\\) \\(P(no)=1-P(yes)=0.95\\). Therefore, the probability of being infected in case of testing positive was: \\[P(yes| positive)= 0.38\\] We concluded that at that time PCR was not very good at confirming infections. However, let us now apply Bayes’ theorem to the probability of not being infected if the test was negative. \\[P(no|negative) = \\frac{P(negative|no) P(no )}{ P(negative|no) P(no)+P(negative|yes)P(yes)}\\] Substituting all values gives \\[P(no| negative)= 0.98\\] So the tests were good for ruling out infections and a fair requirement for travel. Bayes’s theorem In general, we can have more than two conditioning events. Therefore, Baye’s theorem says: If \\(E1, E2, ..., Ek\\) are \\(k\\) mutually exclusive and exhaustive events and \\(B\\) is any event, then the probability inverse \\(P(Ei| B)\\) is \\[P(Ei| B)= \\frac{P(B|Ei)P(Ei)}{P(B|E1)P(E1) +...+ P(B|Ek)P(Ek)}\\] The denominator is the total probability rule for the marginal \\(P(B)\\), in terms of the marginals \\(P(E1), P(E2), ... P(Ek)\\). \\[P(B)=P(B|E 1)P (E1) +...+ P(B|Ek)P(Ek)\\] Conditional tree The total probability rule can also be illustrated using a conditional tree. Rule of total probability for the marginal of \\(B\\): In how many ways can I get the result \\(B\\)? \\(P(B)=P(B|A)P(A)+P(B|A&#39;)P(A&#39;)\\) 4.10 Exercises 4.11 Questions We collect the age and category of 100 athletes in a competition \\(junior\\) \\(senior\\) \\(1st\\) \\(14\\) \\(12\\) \\(2nd\\) \\(21\\) \\(18\\) \\(3rd\\) \\(22\\) \\(13\\) 1) What is the estimated probability that the athlete is in the third category if the athlete is a junior? \\(\\qquad\\)a: \\(22\\); \\(\\qquad\\)b: \\(22/100\\); \\(\\qquad\\)c: \\(22/57\\); \\(\\qquad\\)d: \\(22/35\\); 2) What is the estimated probability that the athlete is a junior and is in the 1st category if the athlete is not in the 3rd category? \\(\\qquad\\)a: \\(14/35\\); \\(\\qquad\\)b: \\(14/65\\); \\(\\qquad\\)c: \\(14/100\\); \\(\\qquad\\)d: \\(14/26\\) 3) A diagnostic test has a probability of \\(8/9\\) of detecting a disease if the patients are sick and a probability of \\(3/9\\) of detecting the disease if the patients are healthy. If the probability of being sick is \\(1/9\\). What is the probability that a patient is sick if a test detects the disease? \\(\\qquad\\)a: \\(\\frac{8/9}{8/9+3/9}*1/9\\); \\(\\qquad\\)b: \\(\\frac{3/9}{8/9+3/9}*1/9\\); \\(\\qquad\\)c: \\(\\frac{3/9*8/9}{8/9*1/9+3/9*8/9}\\); \\(\\qquad\\)d: \\(\\frac{8/9*1/9}{8/9*1/9+3/9*8/9}\\); 4) As discussed in the notes, a PCR test for coronavirus had a sensitivity of 70% and a specificity of 94% and in Spain during confinement there was an incidence of 5%. With these data, what was the probability of testing positive in Spain (\\(P(positive)\\)) \\(\\qquad\\)a: \\(0.035\\); \\(\\qquad\\)b: \\(0.092\\); \\(\\qquad\\)c: \\(0.908\\); \\(\\qquad\\)d: \\(0.95\\) 5) With the same data as in question 4, testing positive in the PCR and being infected are not independent events because: \\(\\qquad\\) a: Sensitivity is 70%; \\(\\qquad\\)b: Sensitivity and false positive rate are different; \\(\\qquad\\)c: The false positive rate is 0.06%; \\(\\qquad\\)d: the specificity is 96% 4.11.0.1 Exercise 1 A machine is tested for its performance in producing high-quality turning rods. These are the test results Rounded: yes Rounded: No smooth surface: yes 200 1 smooth surface: no 4 2 What is the estimated probability that the machine will produce a rod that does not satisfy any quality control? (A: 2/207) What is the estimated probability that the machine will produce a rod that fails at least one quality check? (A: 7/207) What is the estimated probability that the machine will produce rods with a rounded and smooth surface? (A: 200/207) What is the estimated probability that the bar is rounded if the bar is smooth? (A: 200/201) What is the estimated probability that the rod is smooth if it is rounded? (A: 200/204) What is the estimated probability that the rod is neither smooth nor rounded if it does not satisfy at least one quality check? (A: 2/7) Are smoothness and roundness independent events? (No) 4.11.0.2 Exercise 2 We developed a test to detect the presence of bacteria in a lake. We found that if the lake contains the bacteria, the test is positive 70% of the time. If there are no bacteria, the test is negative 60% of the time. We implemented the test in a region where we know that 20% of the lakes have bacteria. What is the probability that a lake that tests positive is contaminated with bacteria? (R: 0.30) 4.11.0.3 Exercise 3 Two machines are tested for their performance in producing high-quality turning rods. These are the test results Machine 1 Rounded: yes Rounded: No smooth surface: yes 200 1 smooth surface: no 4 2 Machine 2 Rounded: yes Rounded: No smooth surface: yes 145 4 smooth surface: no 8 6 What is the probability that the bar is rounded? (A: 357/370) What is the probability that the rod was produced by machine 1? (A: 207/370) What is the probability that the rod is not smooth? (R: 20/370) What is the probability that the rod is smooth or rounded or produced by machine 1? (A: 364/370) What is the probability that the rod will be rounded if it is smoothed and from machine 1? (A: 200/201) What is the probability that the rod is not rounded if it is not smooth and it is from machine 2? (A: 6/8) What is the probability that the rod has come out of machine 1 if it is smooth and rounded? (R: 200/345) What is the probability that the rod came from machine 2 if it fails at least one of the quality controls? (R:0.72) 4.11.0.4 Exercise 4 We want to cross an avenue with two traffic lights. The probability of finding the first red light is 0.6. If we stop at the first traffic light, the probability of stopping at the second is 0.15. While the probability of stopping at the second if we don’t stop at the first is 0.25. When we try to cross both traffic lights: What is the probability of having to stop at each traffic light? (R:0.09) What is the probability of having to stop at at least one traffic light? (R:0.7) What is the probability of having to stop at a single traffic light? (R:0.61) If I stopped at the second traffic light, what is the probability that I would have to stop at the first? (R: 0.47) If you were to stop at any traffic light, what is the probability that you would have to stop twice? (R: 0.12) Is stopping at the first traffic light an independent event from stopping at the second traffic light? (No) Now, we want to cross an avenue with three traffic lights. The probability of finding a red light only depends on the previous one. Specifically, the probability of finding a red traffic light given that the previous one was red is 0.15. While the probability of finding a fair traffic in red given that the previous one was in green is 0.25. Furthermore, the probability of finding the first red light is 0.6. What is the probability of having to stop at each traffic light? (R:0.013) - What is the probability of having to stop at at least one traffic light? (R:0.775) - What is the probability of having to stop at a single traffic light? (R:0.5425) tips: If the probability that a traffic light is red depends only on the previous one, then \\(P(R_3|R_2,R_1)=P(R_3|R_2,\\bar{R}_1)=P(R_3|R_2)\\) and \\(P(R_3|\\bar{R}_2,R_1)=P(R_3|\\bar{R}_2,\\bar{R}_1)=P(R_3|\\bar{R}_2)\\) The joint probability of finding three red lights can be written as: \\(P(R_ 1,R _2,R_3)=P(R_3|R_2)P(R_2|R_1)P(R_1)\\) 4.11.0.5 Exercise 5 A quality test on a random brick is defined by the events: Pass the quality test: \\(E\\), fail the quality test: \\(\\bar{E}\\) Defective: \\(D\\), non-defective: \\(\\bar{D}\\) If the diagnostic test has sensitivity \\(P(E|\\bar{D })= 0.99\\) and specificity \\(P(\\bar{E}|D)=0.98\\), and the probability of passing the test is \\(P(E) =0.893\\) then What is the probability that a randomly chosen brick is defective \\(P(D)\\)? (R:0.1) What is the probability that a brick that has passed the test is actually defective? (R:0.022) The probability that a brick is not defective and that it fails the test (R:0.009) Are \\(D\\) and \\(\\bar{E}\\) statistically independent? (No) "]]
