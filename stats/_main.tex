% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\title{Stats theory (SDA)}
\author{Alejandro Caceres}
\date{2022-09-16}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Stats theory (SDA)},
  pdfauthor={Alejandro Caceres},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{about}{%
\chapter{About}\label{about}}

The course is divided into \textbf{theory} and \textbf{practical} classes (Bootcamps). The classes on theory are subdivided into statistics (Stats), machine learning, and Bayesian inference. Here, are the times, schedules, and content for the statistics theory classes.

\textbf{Stats theory} classes comprise a total of 30 hours: 24 plenary lectures (24 hours) divided in

\begin{itemize}
\tightlist
\item
  Descriptive statistics and probability (4 days)
\item
  Inference (4 days)
\end{itemize}

and 2 group work sessions (6 hours)

\hypertarget{schedule}{%
\section{Schedule:}\label{schedule}}

\hypertarget{recommended-reading-list}{%
\section{Recommended reading list}\label{recommended-reading-list}}

\begin{itemize}
\tightlist
\item
  Douglas C. Montgomery and George C. Runger. ``Applied Statistics and Probability for Engineers'' 4th Edition. Wiley 2007.
\end{itemize}

\hypertarget{data-description}{%
\chapter{Data description}\label{data-description}}

\hypertarget{objective}{%
\section{Objective}\label{objective}}

\begin{itemize}
\tightlist
\item
  Data: discrete, continuous
\item
  Summarizing data in tables and figures
\end{itemize}

\hypertarget{statistics}{%
\section{Statistics}\label{statistics}}

\begin{itemize}
\item
  Solve problems in a systematic way (science, engineering and technology)
\item
  Modern humans use a general \textbf{method} historically developed for thousands of years! \ldots{} and still under development.
\item
  It has three main components: observation, logic, and generation of new knowledge
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{scientific-method}{%
\section{Scientific method}\label{scientific-method}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{outcome}{%
\section{Outcome}\label{outcome}}

\textbf{Observation} or \emph{Realization}

\begin{itemize}
\tightlist
\item
  an \textbf{observation} is the acquisition of a number or a characteristic from an experiment
\end{itemize}

\ldots{} 1 0 0 1 0 \textbf{1} 0 1 1 \ldots{} (the number in bold is an observation in a repetition of the experiment)

\textbf{Outcome}

\begin{itemize}
\tightlist
\item
  An \textbf{outcome} is a possible observation that is the result of an experiment.
\end{itemize}

\textbf{1} is an outcome, \textbf{0} is the other outcome

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{types-of-outcome}{%
\section{Types of outcome}\label{types-of-outcome}}

\begin{itemize}
\tightlist
\item
  \textbf{Categorical}: If the result of an experiment can only take discrete values (number of car pieces produced per hour, number of leukocytes in blood)
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Continuous}: If the result of an experiment can only take continuous values (battery state of charge, engine temperature).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{random-experiments}{%
\section{Random experiments}\label{random-experiments}}

\textbf{Definition:}

A \textbf{random experiment} is an experiment that gives different outcomes when repeated in the same manner.

\textbf{Examples:}

\begin{itemize}
\tightlist
\item
  on the same object (person): temperature, sugar levels.\\
\item
  on different objects but the same measurement: the weight of an animal.
\item
  on events: a number of emails received in an hour.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{absolute-frequencies}{%
\section{Absolute frequencies}\label{absolute-frequencies}}

When we repeat a random experiment, we record a list of outcomes.

We summarize the \textbf{categorical} observations by counting how many times we saw a particular outcome.

\textbf{Absolute frequency}:

\[n_i\]

is the number of times we observed the outcome \(i\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example}{%
\section{Example}\label{example}}

\textbf{Random experiment}: Extract a leukocyte from \textbf{one} donor and write down its type. Repeat experiment \(N=119\) times.

\begin{verbatim}
(T cell, Tcell, Neutrophil, ..., B cell)
\end{verbatim}

\begin{verbatim}
##      outcome ni
## 1     T Cell 34
## 2     B cell 50
## 3   basophil 20
## 4   Monocyte  5
## 5 Neutrophil 10
\end{verbatim}

\begin{itemize}
\tightlist
\item
  For instance: \(n_1=34\) is total number of T cells
\item
  \(N=\sum_i n_i=119\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{relative-frequencies}{%
\section{Relative frequencies}\label{relative-frequencies}}

We can also summarize the observations by computing the \textbf{proportion} of how many times we saw a particular outcome.

\[f_i=n_i/N\] where \(N\) is the total number of observations

In our example there are recorded \(n_1=34\) T cells, so we ask for the proportion of T cells from the total of \(119\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-1}{%
\section{Example}\label{example-1}}

\begin{verbatim}
##      outcome ni         fi
## 1     T Cell 34 0.28571429
## 2     B cell 50 0.42016807
## 3   basophil 20 0.16806723
## 4   Monocyte  5 0.04201681
## 5 Neutrophil 10 0.08403361
\end{verbatim}

We have

\(\sum_{i=1..M} n_i = N\)

\(\sum_{i=1..M} f_i = 1\)

where \(M\) is the number of outcomes.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bar-plot}{%
\section{Bar plot}\label{bar-plot}}

We can plot \(n_i\) Vs the outcomes, giving us a bar plot

\includegraphics{_main_files/figure-latex/unnamed-chunk-3-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{pie-chart}{%
\section{Pie chart}\label{pie-chart}}

We can visualize the relative frequencies with a pie chart

\begin{itemize}
\tightlist
\item
  Where the area of the circle represents 100\% of observations (proportion = 1) and the sections the relative frequencies of all the outcomes.
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-4-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{categorical-and-ordered-variables}{%
\section{Categorical and ordered variables}\label{categorical-and-ordered-variables}}

Cell types are not meaningfully ordered concerning the outcomes. However, sometimes \textbf{categorical} variables can be \textbf{ordered}.

Misophonia study:

\begin{itemize}
\item
  123 patients were examined for misophonia: anxiety/anger produced by certain sounds
\item
  They were categorized into 4 different groups according to severity.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-2}{%
\section{Example}\label{example-2}}

The results of the study are:

\begin{verbatim}
##   [1] 4 2 0 3 0 0 2 3 0 3 0 2 2 0 2 0 0 3 3 0 3 3 2 0 0 0 4 2 2 0 2 0 0 0 3 0 2
##  [38] 3 2 2 0 2 3 0 0 2 2 3 3 0 0 4 3 3 2 0 2 0 0 0 2 2 0 0 2 3 0 1 3 2 4 3 2 3
##  [75] 0 2 3 2 4 1 2 0 2 0 2 0 2 2 4 3 0 3 0 0 0 2 2 1 3 0 0 3 2 1 3 0 4 4 2 3 3
## [112] 3 0 3 2 1 2 3 3 4 2 3 2
\end{verbatim}

And its frequency table

\begin{verbatim}
##   outcome ni         fi
## 1       0 41 0.33333333
## 2       1  5 0.04065041
## 3       2 37 0.30081301
## 4       3 31 0.25203252
## 5       4  9 0.07317073
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{absolute-and-relative-cumulative-frequencies}{%
\section{Absolute and relative cumulative frequencies}\label{absolute-and-relative-cumulative-frequencies}}

Misophonia severity is \textbf{categorical} and \textbf{ordered}.

When outcomes can be ordered then it is useful to ask how many observations were obtained up to a given outcome we call this number the absolute cumulative frequency up to the outcome \(i\):
\[N_i=\sum_{k=1..i} n_k\]
It is also useful to compute the \textbf{proportion} of the observations that was obtained up to a given outcome

\[F_i=\sum_{k=1..i} f_k\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{frequency-table}{%
\section{Frequency table}\label{frequency-table}}

\begin{verbatim}
##   outcome ni         fi  Ni        Fi
## 0       0 41 0.33333333  41 0.3333333
## 1       1  5 0.04065041  46 0.3739837
## 2       2 37 0.30081301  83 0.6747967
## 3       3 31 0.25203252 114 0.9268293
## 4       4  9 0.07317073 123 1.0000000
\end{verbatim}

\begin{itemize}
\item
  \textbf{67\%} of patients had misophonia up to severity \textbf{2}
\item
  \textbf{37\%} of patients have severity less or equal than \textbf{1}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{cumulative-frequency-plot}{%
\section{Cumulative frequency plot}\label{cumulative-frequency-plot}}

We can also plot the cumulative frequency Vs the outcomes

\includegraphics{_main_files/figure-latex/unnamed-chunk-8-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-variables}{%
\section{Continuous variables}\label{continuous-variables}}

The result of a random experiment can also give continuous outcomes.

In the misophonia study, the researchers asked whether the convexity of the jaw would affect the misophonia severity (the scientific hypothesis is that the convexity angle of the jaw can influence the ear and its sensitivity). These are the results for the convexity of the jaw (degrees)

\begin{verbatim}
##   [1]  7.97 18.23 12.27  7.81  9.81 13.50 19.30  7.70 12.30  7.90 12.60 19.00
##  [13]  7.27 14.00  5.40  8.00 11.20  7.75  7.94 16.69  7.62  7.02  7.00 19.20
##  [25]  7.96 14.70  7.24  7.80  7.90  4.70  4.40 14.00 14.40 16.00  1.40  9.76
##  [37]  7.90  7.90  7.40  6.30  7.76  7.30  7.00 11.23 16.00  7.90  7.29  6.91
##  [49]  7.10 13.40 11.60 -1.00  6.00  7.82  4.80 11.00  9.00 11.50 16.00 15.00
##  [61]  1.40 16.80  7.70 16.14  7.12 -1.00 17.00  9.26 18.70  3.40 21.30  7.50
##  [73]  6.03  7.50 19.00 19.01  8.10  7.80  6.10 15.26  7.95 18.00  4.60 15.00
##  [85]  7.50  8.00 16.80  8.54  7.00 18.30  7.80 16.00 14.00 12.30 11.40  8.50
##  [97]  7.00  7.96 17.60 10.00  3.50  6.70 17.00 20.26  6.64  1.80  7.02  2.46
## [109] 19.00 17.86  6.10  6.64 12.00  6.60  8.70 14.05  7.20 19.70  7.70  6.02
## [121]  2.50 19.00  6.80
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bins}{%
\section{Bins}\label{bins}}

Continuous outcomes cannot be counted!

We transform them into ordered categorical variables

\begin{itemize}
\tightlist
\item
  We cover the range of the observations into regular intervals of the same size (bins)
\end{itemize}

\begin{verbatim}
## [1] "[-1.02,3.46]" "(3.46,7.92]"  "(7.92,12.4]"  "(12.4,16.8]"  "(16.8,21.3]"
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{create-a-categorical-variable-from-a-continuous-one}{%
\section{Create a categorical variable from a continuous one}\label{create-a-categorical-variable-from-a-continuous-one}}

\begin{itemize}
\tightlist
\item
  We map each observation to its interval: creating an \textbf{ordered categorical} variable; in this case with 5 possible outcomes
\end{itemize}

\begin{verbatim}
##   [1] "(7.92,12.4]"  "(16.8,21.3]"  "(7.92,12.4]"  "(3.46,7.92]"  "(7.92,12.4]" 
##   [6] "(12.4,16.8]"  "(16.8,21.3]"  "(3.46,7.92]"  "(7.92,12.4]"  "(3.46,7.92]" 
##  [11] "(12.4,16.8]"  "(16.8,21.3]"  "(3.46,7.92]"  "(12.4,16.8]"  "(3.46,7.92]" 
##  [16] "(7.92,12.4]"  "(7.92,12.4]"  "(3.46,7.92]"  "(7.92,12.4]"  "(12.4,16.8]" 
##  [21] "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(16.8,21.3]"  "(7.92,12.4]" 
##  [26] "(12.4,16.8]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]" 
##  [31] "(3.46,7.92]"  "(12.4,16.8]"  "(12.4,16.8]"  "(12.4,16.8]"  "[-1.02,3.46]"
##  [36] "(7.92,12.4]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]" 
##  [41] "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(7.92,12.4]"  "(12.4,16.8]" 
##  [46] "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(12.4,16.8]" 
##  [51] "(7.92,12.4]"  "[-1.02,3.46]" "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]" 
##  [56] "(7.92,12.4]"  "(7.92,12.4]"  "(7.92,12.4]"  "(12.4,16.8]"  "(12.4,16.8]" 
##  [61] "[-1.02,3.46]" "(12.4,16.8]"  "(3.46,7.92]"  "(12.4,16.8]"  "(3.46,7.92]" 
##  [66] "[-1.02,3.46]" "(16.8,21.3]"  "(7.92,12.4]"  "(16.8,21.3]"  "[-1.02,3.46]"
##  [71] "(16.8,21.3]"  "(3.46,7.92]"  "(3.46,7.92]"  "(3.46,7.92]"  "(16.8,21.3]" 
##  [76] "(16.8,21.3]"  "(7.92,12.4]"  "(3.46,7.92]"  "(3.46,7.92]"  "(12.4,16.8]" 
##  [81] "(7.92,12.4]"  "(16.8,21.3]"  "(3.46,7.92]"  "(12.4,16.8]"  "(3.46,7.92]" 
##  [86] "(7.92,12.4]"  "(12.4,16.8]"  "(7.92,12.4]"  "(3.46,7.92]"  "(16.8,21.3]" 
##  [91] "(3.46,7.92]"  "(12.4,16.8]"  "(12.4,16.8]"  "(7.92,12.4]"  "(7.92,12.4]" 
##  [96] "(7.92,12.4]"  "(3.46,7.92]"  "(7.92,12.4]"  "(16.8,21.3]"  "(7.92,12.4]" 
## [101] "(3.46,7.92]"  "(3.46,7.92]"  "(16.8,21.3]"  "(16.8,21.3]"  "(3.46,7.92]" 
## [106] "[-1.02,3.46]" "(3.46,7.92]"  "[-1.02,3.46]" "(16.8,21.3]"  "(16.8,21.3]" 
## [111] "(3.46,7.92]"  "(3.46,7.92]"  "(7.92,12.4]"  "(3.46,7.92]"  "(7.92,12.4]" 
## [116] "(12.4,16.8]"  "(3.46,7.92]"  "(16.8,21.3]"  "(3.46,7.92]"  "(3.46,7.92]" 
## [121] "[-1.02,3.46]" "(16.8,21.3]"  "(3.46,7.92]"
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{frequency-table-for-a-continuous-variable}{%
\section{Frequency table for a continuous variable}\label{frequency-table-for-a-continuous-variable}}

\begin{verbatim}
##        outcome ni         fi  Ni         Fi
## 1 [-1.02,3.46]  8 0.06504065   8 0.06504065
## 2  (3.46,7.92] 51 0.41463415  59 0.47967480
## 3  (7.92,12.4] 26 0.21138211  85 0.69105691
## 4  (12.4,16.8] 20 0.16260163 105 0.85365854
## 5  (16.8,21.3] 18 0.14634146 123 1.00000000
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{histogram}{%
\section{Histogram}\label{histogram}}

The histogram is the plot of \(n_i\) or \(f_i\) Vs the outcomes (bins). The histogram depends on the size of the bins

\includegraphics{_main_files/figure-latex/unnamed-chunk-13-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{histogram-1}{%
\section{Histogram}\label{histogram-1}}

The histogram is the plot of \(n_i\) or \(f_i\) Vs the outcomes (bins). The histogram depends on the size of the bins

\includegraphics{_main_files/figure-latex/unnamed-chunk-14-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{cumulative-frequency-plot-continous-variables}{%
\section{Cumulative frequency plot: Continous variables}\label{cumulative-frequency-plot-continous-variables}}

We can also plot the cumulative frequency Vs the outcomes

\includegraphics{_main_files/figure-latex/unnamed-chunk-15-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{summary-statistics}{%
\section{Summary statistics}\label{summary-statistics}}

The summary statistics are numbers computed from the data that tell us important features of numerical variables (categorical or continuous).

Limiting values:

\begin{itemize}
\tightlist
\item
  minimum: the minimum outcome observed
\item
  maximum: the maximum outcome observed
\end{itemize}

Central value for the outcomes

\begin{itemize}
\tightlist
\item
  The average is defined as
\end{itemize}

\[\bar{x}=\frac{1}{N} \sum_{j=1..N} x_j\]

where \(x_j\) is the \textbf{observation} \(j\) (convexity) from a total of \(N\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average}{%
\section{Average}\label{average}}

The average convexity can be computed directly from the \textbf{observations}

\(\bar{x}= \frac{1}{N}\sum_j x_j\)

\(= \frac{1}{N}(7.97 + 18.23 + 12.27... + 6.80) = 10.19894\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-categorical-ordered}{%
\section{Average (categorical ordered)}\label{average-categorical-ordered}}

For \textbf{categorical ordered} variables we can use the frequency table to compute the average

\begin{verbatim}
##   outcome ni         fi
## 1       0 41 0.33333333
## 2       1  5 0.04065041
## 3       2 37 0.30081301
## 4       3 31 0.25203252
## 5       4  9 0.07317073
\end{verbatim}

The average \textbf{severity} of misophonia in the study
can \textbf{also} be computed from the relative frequencies of the \textbf{outcomes}

\(\bar{x}=\frac{1}{N}\sum_{i=1...N} x_j=\frac{1}{N}\sum_{i=1...M} x_i*n_{i}=\sum_{i=1...M} x_i*f_{i}\)

\(=0*f_{0}+1*f_{1}+2*f_{2}+3*f_{3}+4*f_{4}=1.691057\)

(note the change from \(N\) to \(M\) in the second summation)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-categorical-ordered-1}{%
\section{Average (categorical ordered)}\label{average-categorical-ordered-1}}

In terms of the \textbf{outcomes} of categorical ordered variables, the \textbf{average} can be written as

\[\bar{x}= \sum_{i = 1...M} x_i f_i\]

from a total of \(M\) possible outcomes (number of severity levels).

\(\bar{x}\) is the \textbf{central value} or center of gravity of the outcomes. As if each outcome had a mass density given by \(f_i\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-1}{%
\section{Average}\label{average-1}}

\begin{itemize}
\item
  The average is not the result of one observation (random experiment).
\item
  It is the result of a series of observations (sample).
\item
  It describes the number where the observed values balance.
\end{itemize}

That is why we hear, for instance, that a patient with an infection can infect an average of 2.5 people.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-2}{%
\section{Average}\label{average-2}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-17-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{median}{%
\section{Median}\label{median}}

Another measure of centrality is the median. The median \(q_{0.5}\) is the value \(x_p\)

\[median(x)=q_{0.5}=x_p\]

below which we find half of the observations

\[\sum_{x\leq x_p} 1 = \frac{N}{2}\]

or in terms of the frequencies, is the value \(x_p\) that makes the cumulative frequency \(F_p\) equal to \(0.5\)

\[q_{0.5}=\sum_{x\leq x_p} f_x =F_p=0.5\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{median-vs-average}{%
\section{Median Vs Average}\label{median-vs-average}}

\begin{itemize}
\tightlist
\item
  Average: Center of mass (compensates distant values)
\item
  Median: Half of the mass
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-18-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{dispersion}{%
\section{Dispersion}\label{dispersion}}

An important measure of the outcomes is their \textbf{dispersion}. Many experiments can share their mean but differ on how dispersed the values are.

\includegraphics{_main_files/figure-latex/unnamed-chunk-19-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{dispersion-1}{%
\section{Dispersion}\label{dispersion-1}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-20-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-variance}{%
\section{Sample variance}\label{sample-variance}}

Dispersion about the mean is measured with the

\begin{itemize}
\tightlist
\item
  The sample variance:
\end{itemize}

\[s^2=\frac{1}{N-1} \sum_{j=1..N} (x_j-\bar{x})^2\]

It measures the average square distance of the \textbf{observations} to the average. The reason for \(N-1\) will be explained when we talk about inference.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-variance-1}{%
\section{Sample variance}\label{sample-variance-1}}

\begin{itemize}
\tightlist
\item
  In terms of the frequencies of \textbf{categorical and ordered} variables
\end{itemize}

\[s^2=\frac{N}{N-1} \sum_{x} (x-\bar{x})^2 f_x\]

\(s^2\) can be thought of as the moment of inertia of the observations.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-deviation}{%
\section{Standard deviation}\label{standard-deviation}}

The squared root of the sample variance is called the \textbf{standard deviation} \(s\).

The standard deviation of the convexity angle is

\(s= [\frac{1}{123-1}((7.97-10.19894)^2+ (18.23-10.19894)^2\)\\
\(+ (12.27-10.19894)^2 + ...)]^{1/2} = 5.086707\)

The jaw convexity deviates from its mean by \(5.086707\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{iqr}{%
\section{IQR}\label{iqr}}

\begin{itemize}
\item
  Dispersion of data can also be measured with respect to the median by the \textbf{interquartile range}
\item
  We define the \textbf{first} quartile as the value \(x_p\) that makes the cumulative frequency \(F_p\) equal to \(0.25\)
\end{itemize}

\[q_{0.25}=\sum_{x\leq x_p} f_x =F_p=0.25\]

\begin{itemize}
\tightlist
\item
  We also define the \textbf{third} quartile as the value \(x_p\) that makes the cumulative frequency \(F_p\) equal to \(0.75\)
\end{itemize}

\[q_{0.75}=\sum_{x\leq x_p} f_x =F_p=0.75\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{iqr-1}{%
\section{IQR}\label{iqr-1}}

The distance between the third quartile and the first quartile is called the \textbf{interquartile range} (IQR) and captures the central 50\% of the observations

\includegraphics{_main_files/figure-latex/unnamed-chunk-21-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{box-plot}{%
\section{Box plot}\label{box-plot}}

The interquartile range, the median, and the 5\% and 95\% of the data can be visualized in a \textbf{boxplot}, here the values of the outcomes are on the y-axis. The IQR is the box, the median is the line in the middle and the whiskers mark the 5\% and 95\% of the data.

\includegraphics{_main_files/figure-latex/unnamed-chunk-22-1.pdf}

\hypertarget{probability}{%
\chapter{Probability}\label{probability}}

\hypertarget{objective-1}{%
\section{Objective}\label{objective-1}}

\begin{itemize}
\tightlist
\item
  Definition of probability
\item
  Probability algebra
\item
  Joint probability
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{random-experiments-1}{%
\section{Random experiments}\label{random-experiments-1}}

\textbf{Observation}

\begin{itemize}
\tightlist
\item
  An \textbf{observation} is the acquisition of a number or a characteristic from an experiment
\end{itemize}

\textbf{Outcome}

\begin{itemize}
\tightlist
\item
  An \textbf{outcome} is a possible observation that is the result of an experiment.
\end{itemize}

\textbf{Random experiment}

\begin{itemize}
\tightlist
\item
  An experiment that gives \textbf{different} outcomes when repeated in the same manner.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-1}{%
\section{Probability}\label{probability-1}}

The \textbf{probability} of an outcome is a measure of how sure we are to observe that outcome when performing a random experiment.

\begin{itemize}
\item
  0: We are sure that the observation will \textbf{not} happen.
\item
  1: We are sure that the observation will happen.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-3}{%
\section{Example}\label{example-3}}

\begin{itemize}
\tightlist
\item
  Consider the following observations of a random experiment:
\end{itemize}

1 5 1 2 2 1 2 2

\begin{itemize}
\tightlist
\item
  How sure we are to obtain \(2\) in the following observation?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-4}{%
\section{Example}\label{example-4}}

The frequency table is

\begin{verbatim}
##   outcome ni    fi
## 1       1  3 0.375
## 2       2  4 0.500
## 3       5  1 0.125
\end{verbatim}

The \textbf{relative frequency} \(f_i\)

\begin{itemize}
\tightlist
\item
  is a number between \(0\) and \(1\).
\item
  measures the proportion of total observations that we observed a particular outcome.
\item
  seems a reasonable probability measure.
\end{itemize}

As \(f_2=0.5\) then we would be half certain to obtain a \(2\) in the next repetition of the experiment.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{relative-frequency}{%
\section{Relative frequency}\label{relative-frequency}}

As a measure of certainty is \(f_i\) enough?

Say we repeated the experiment 12 times more:

1 5 1 2 2 1 2 2 \textbf{3 1 1 3 3 1 6 3 5 6 4 4}

The frequency table is now

\begin{verbatim}
##   outcome ni  fi
## 1       1  6 0.3
## 2       2  4 0.2
## 3       3  4 0.2
## 4       4  2 0.1
## 5       5  2 0.1
## 6       6  2 0.1
\end{verbatim}

New outcomes appeared and \(f_2\) is now \(0.2\), we are now a fifth certain of obtaining \(2\) in the next experiment\ldots{} probability should not depend on \(N\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{at-infinity}{%
\section{At infinity}\label{at-infinity}}

Say we repeated the experiment 1000 times:

\begin{verbatim}
##   outcome  ni    fi
## 1       1 159 0.159
## 2       2 183 0.183
## 3       3 160 0.160
## 4       4 170 0.170
## 5       5 161 0.161
## 6       6 167 0.167
\end{verbatim}

We find that \(f_i\) is converging to a constant value

\[lim_{N\rightarrow \infty} f_i = P_i\]

\includegraphics{_main_files/figure-latex/unnamed-chunk-26-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{frequentist-probability}{%
\section{Frequentist probability}\label{frequentist-probability}}

We call \textbf{Probability} \(P_i\) to the limit when \(N \rightarrow \infty\) of the \textbf{relative frequency} of observing the outcome \(i\) in a random experiment.

Championed by \href{https://plato.stanford.edu/entries/probability-interpret/\#ClaPro}{Venn (1876)}

The frequentist interpretation of probabilities is derived from data/experience (empirical).

\begin{itemize}
\tightlist
\item
  We do not observe \(P_i\), we observe \(f_i\)
\item
  When we \textbf{estimate} \(P_i\) with \(f_i\) (typically when \(N\) is large), we write: \[\hat{P_i}=f_i\]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{classical-probability}{%
\section{Classical Probability}\label{classical-probability}}

Whenever a random experiment has \(M\) possible outcomes that are all \textbf{equally likely}, the probability of each outcome is \(\frac{1}{M}\).

Championed by \href{https://plato.stanford.edu/entries/probability-interpret/\#ClaPro}{Laplace (1814)}.

Since each outcome is \textbf{equally probable} we declare complete ignorance and the best we can do is to fairly distribute the same probability to each outcome.

What if I told you that our experiment was the throw of the dice? then

\(P_2=1/6=0.166666\).

\[P_i=lim_{N\rightarrow \infty} \frac{n_i}{N}=\frac{1}{M}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{classical-and-frequentist-probabilities}{%
\section{Classical and frequentist probabilities}\label{classical-and-frequentist-probabilities}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-2}{%
\section{Probability}\label{probability-2}}

Probability is a number between \(0\) and \(1\) that is assigned to each member \(E\) of a collection of \textbf{events} of a \textbf{sample space} (\(S\)) from a random experiment.

\[P(E) \in (0,1)\]

where \(E \in S\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-space}{%
\section{Sample space}\label{sample-space}}

We start by reasoning what are all the possible values (outcomes) that a random experiment could give.

Note that we do not have to observe them in a particular experiment: We are using \textbf{reason/logic} and not observation.

\textbf{Definition:}

\begin{itemize}
\item
  The set of all possible outcomes of a random experiment is called the \textbf{sample space}
  of the experiment.
\item
  The sample space is denoted as \(S\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{examples-of-sample-spaces}{%
\section{Examples of sample spaces}\label{examples-of-sample-spaces}}

\begin{itemize}
\tightlist
\item
  temperature 35 and 42 degrees Celcius
\item
  sugar levels: 70-80mg/dL
\item
  the size of one screw from a production line: 70mm-72mm
\item
  number of emails received in an hour: 0-100
\item
  a dice throw: 1, 2, 3, 4, 5, 6
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{discrete-and-continuous-sample-spaces}{%
\section{Discrete and continuous sample spaces}\label{discrete-and-continuous-sample-spaces}}

\begin{itemize}
\item
  A sample space is discrete if it consists of a finite or countable infinite set of outcomes.
\item
  A sample space is continuous if it contains an interval (either finite or infinite in length) of
  real numbers.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{event}{%
\section{Event}\label{event}}

\textbf{Definition:}

An \textbf{event} is a \textbf{subset} of the sample space of a random experiment. It is a \textbf{collection} of outcomes.

Examples of events:

\begin{itemize}
\tightlist
\item
  The event of a healthy temperature: temperature 37-38 degrees Celsius
\item
  The event of producing a screw with a size: of 71.5mm
\item
  The event of receiving more than 4 emails in an hour.
\item
  The event of obtaining a number less than 3 in the throw of a dice
\end{itemize}

One event refers to a possible set of \textbf{outcomes}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{event-operations}{%
\section{Event operations}\label{event-operations}}

For two events \(A\) and \(B\), we can construct the following derived events:

\begin{itemize}
\tightlist
\item
  Complement \(A'\): the event of \textbf{not} \(A\)
\item
  Union \(A \cup B\): the event of \(A\) \textbf{or} \(B\)\\
\item
  Intersection \(A \cap B\): the event of \(A\) \textbf{and} \(B\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{event-operations-example}{%
\section{Event operations example}\label{event-operations-example}}

Take

\begin{itemize}
\tightlist
\item
  Event \(A:\{1,2,3\}\) a number less or equal to three in the throw of a dice\\
\item
  Event \(B:\{2,4,6\}\) an even number in the throw of a dice
\end{itemize}

New events:

\begin{itemize}
\tightlist
\item
  Not less than three: \(A':\{4,5,6\}\)
\item
  Less or equal to three \textbf{or} even: \(A \cup B: \{1,2,3,4,6\}\)
\item
  Less or equal to three \textbf{and} even \(A \cap B: \{2\}\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{outcomes}{%
\section{Outcomes}\label{outcomes}}

Outcomes are events that are \textbf{mutually exclusive}

\textbf{Definition:}

Two events denoted as \(E_1\) and \(E_2\), such that

\[E_1\cap E_2=\emptyset\]
They cannot occur at the same time.

Example:

\begin{itemize}
\item
  The outcome of obtaining \(1\) \textbf{and} the outcome of obtaining \(5\) in the throw of one dice are mutually exclusive:
\item
  The event of obtaining \(1\) and \(5\) is empty:\[\{1\}\cap \{5\}=\emptyset\]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-definition}{%
\section{Probability definition}\label{probability-definition}}

A probability is a number that is assigned to each possible event (\(E\)) of a sample space (\(S\)) of a random experiment that satisfies the following properties:

\begin{itemize}
\tightlist
\item
  \(P(S)=1\)
\item
  \(0 \leq P(E) \leq 1\)
\item
  when \(E_1\cap E_2=\emptyset\) \[P(E_1\cup E_2) = P(E_1) + P(E_2)\]
\end{itemize}

Proposed by Kolmogorov's (1933)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-properties}{%
\section{Probability properties}\label{probability-properties}}

Kolmogorov says that we can build a probability table (likewise the relative frequency table)

\begin{longtable}[]{@{}cc@{}}
\toprule
outcome & Probability \\
\midrule
\endhead
\(1\) & 1/6 \\
\(2\) & 1/6 \\
\(3\) & 1/6 \\
\(4\) & 1/6 \\
\(5\) & 1/6 \\
\(6\) & 1/6 \\
\(P(1 \cup 2\cup ... \cup 6)\) & 1 \\
\bottomrule
\end{longtable}

As \(\{1,2,3,4,5,6\}\) are mutually exclusive then

\[P(S)=P(1\cup 2\cup ... \cup 6) = P(1)+P(2)+ ...+P(n)=1\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{addition-rule}{%
\section{Addition Rule}\label{addition-rule}}

When \(A\) and \(B\) are not mutually exclusive then:

\[P(A \cup B)=P(A) + P(B) - P(A\cap B)\]

Where \(P(A)\) and \(P(B)\) are called the \textbf{marginal probabilities}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-addition-rule}{%
\section{Example Addition Rule}\label{example-addition-rule}}

Take

\begin{itemize}
\tightlist
\item
  Event \(A:\{1,2,3\}\) a number less or equal to three in the throw of a dice\\
\item
  Event \(B:\{2,4,6\}\) an even number in the throw of a dice
\end{itemize}

then:

\begin{itemize}
\tightlist
\item
  \(P(A): P(1) + P(2) + P(3)=3/6\)
\item
  \(P(B): P(2) + P(4) + P(6)=3/6\)
\item
  \(P(A \cap B): P(2) = 1/6\)
\end{itemize}

\(P(A \cup B)=P(A) + P(B) - P(A\cap B)=3/6+3/6-1/6=5/6\)

Note: \(P(2)\) appears in \(P(A)\) and \(P(B)\) that's why we subtract it with the intersection

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{venn-diagram}{%
\section{Venn diagram}\label{venn-diagram}}

Note that can always break down the sample space in \textbf{mutually exclusive} sets involving the intersections:

\(S=\{A\cap B, A \cap B', A'\cap B, A'\cap B'\}\)

Marginals:

\begin{itemize}
\tightlist
\item
  \(P(A)=P(A\cap B') + P(A \cap B)=2/6+1/6=3/6\)
\item
  \(P(B)=P(A'\cap B) +P(A \cap B)=2/6+1/6=3/6\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-table}{%
\section{Probability table}\label{probability-table}}

Let's look at the probability table

\begin{longtable}[]{@{}cc@{}}
\toprule
outcome & Probability \\
\midrule
\endhead
\(A\cap B\) & \(P(A\cap B)\) \\
\(A\cap B'\) & \(P(A\cap B')\) \\
\(A'\cap B\) & \(P(A'\cap B)\) \\
\(A'\cap B'\) & \(P(A'\cap B')\) \\
sum & \(1\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-probability-table}{%
\section{Example probability table}\label{example-probability-table}}

We also write \(A \cap B\) as \((A,B)\) and call it the \textbf{joint probability} of \(A\) and \(B\)

In our example:

\begin{longtable}[]{@{}cc@{}}
\toprule
outcome & Probability \\
\midrule
\endhead
\((A, B)\) & \(P(A, B)=1/6\) \\
\((A, B')\) & \(P(A, B')=2/6\) \\
\((A', B)\) & \(P(A', B)=2/6\) \\
\((A', B')\) & \(P(A', B')=1/6\) \\
sum & \(1\) \\
\bottomrule
\end{longtable}

Note: each outcome has \(two\) values (one for the characteristic of type \(A\) and another for type \(B\))

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{contingency-table}{%
\section{Contingency table}\label{contingency-table}}

We can organize the probability of \textbf{joint outcomes} in a \textbf{contingency table}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& \(B\) & \(B'\) & sum \\
\midrule
\endhead
\(A\) & \(P(A, B )\) & \(P(A, B' )\) & \(P(A)\) \\
\(A'\) & \(P(A', B )\) & \(P(A', B' )\) & \(P(A')\) \\
sum & \(P(B)\) & \(P(B')\) & 1 \\
\bottomrule
\end{longtable}

Marginals:

\begin{itemize}
\tightlist
\item
  \(P(A)=P(A, B') + P(A, B)\)
\item
  \(P(B)=P(A', B) +P(A, B)\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-contingency-table}{%
\section{Example contingency table}\label{example-contingency-table}}

\begin{itemize}
\tightlist
\item
  Event \(A:\{1,2,3\}\) a number less or equal to three in the throw of a dice\\
\item
  Event \(B:\{2,4,6\}\) an even number in the throw of a dice
\end{itemize}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& \(B\) & \(B'\) & sum \\
\midrule
\endhead
\(A\) & \(1/6\) & \(2/6\) & \(3/6\) \\
\(A'\) & \(2/6\) & \(1/6\) & \(3/6\) \\
sum & \(3/6\) & \(3/6\) & 1 \\
\bottomrule
\end{longtable}

Three forms of the \textbf{addition rule}:

\(P(A \cup B)\)\[=P(A) + P(B) - P(A\cap B)\]
\[=P(A \cap B)+P(A\cap B')+P(A'\cap B)\]
\[=1-P(A'\cap B')\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{misophonia-study}{%
\section{Misophonia study}\label{misophonia-study}}

In the misophonia study, the patients were assessed for their misophonia severity \textbf{and} if they were depressed.

The outcome of one random experiment is to measure the misophonia severity \textbf{and} depression status of one patient. The repetition of the random experiment was to perform the same two measurements on another patient.

\begin{verbatim}
##     Misofonia.dic depresion.dic
## 1               4             1
## 2               2             0
## 3               0             0
## 4               3             0
## 5               0             0
## 6               0             0
## 7               2             0
## 8               3             0
## 9               0             1
## 10              3             0
## 11              0             0
## 12              2             0
## 13              2             1
## 14              0             0
## 15              2             0
## 16              0             0
## 17              0             0
## 18              3             0
## 19              3             0
## 20              0             0
## 21              3             0
## 22              3             0
## 23              2             0
## 24              0             0
## 25              0             0
## 26              0             0
## 27              4             1
## 28              2             0
## 29              2             0
## 30              0             0
## 31              2             0
## 32              0             0
## 33              0             0
## 34              0             0
## 35              3             0
## 36              0             0
## 37              2             0
## 38              3             1
## 39              2             0
## 40              2             0
## 41              0             0
## 42              2             0
## 43              3             0
## 44              0             0
## 45              0             0
## 46              2             0
## 47              2             0
## 48              3             0
## 49              3             0
## 50              0             0
## 51              0             0
## 52              4             1
## 53              3             0
## 54              3             1
## 55              2             1
## 56              0             1
## 57              2             0
## 58              0             0
## 59              0             0
## 60              0             0
## 61              2             0
## 62              2             0
## 63              0             0
## 64              0             0
## 65              2             0
## 66              3             1
## 67              0             0
## 68              1             0
## 69              3             0
## 70              2             0
## 71              4             1
## 72              3             0
## 73              2             1
## 74              3             0
## 75              0             1
## 76              2             0
## 77              3             0
## 78              2             0
## 79              4             1
## 80              1             0
## 81              2             0
## 82              0             0
## 83              2             0
## 84              0             0
## 85              2             0
## 86              0             1
## 87              2             0
## 88              2             0
## 89              4             1
## 90              3             0
## 91              0             1
## 92              3             0
## 93              0             0
## 94              0             0
## 95              0             0
## 96              2             0
## 97              2             0
## 98              1             0
## 99              3             0
## 100             0             0
## 101             0             0
## 102             3             1
## 103             2             0
## 104             1             0
## 105             3             0
## 106             0             0
## 107             4             1
## 108             4             1
## 109             2             0
## 110             3             0
## 111             3             0
## 112             3             1
## 113             0             0
## 114             3             0
## 115             2             0
## 116             1             0
## 117             2             0
## 118             3             1
## 119             3             0
## 120             4             1
## 121             2             0
## 122             3             0
## 123             2             0
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{contingency-table-for-frequencies}{%
\section{Contingency table for frequencies}\label{contingency-table-for-frequencies}}

\begin{itemize}
\tightlist
\item
  For the number of observations \(n_{i,j}\) of each outcome \((x_i, y_i)\), misophonia: \(x\in \{0,1,2,3,4\}\) and depression \(y\in \{0,1\}\) (no:\(0\), yes:\(1\))
\end{itemize}

\begin{verbatim}
##               
##                Depression:0 Depression:1
##   Misophonia:4            0            9
##   Misophonia:3           25            6
##   Misophonia:2           34            3
##   Misophonia:1            5            0
##   Misophonia:0           36            5
\end{verbatim}

\begin{itemize}
\tightlist
\item
  For the relative frequencies \(f_{i,j}\)
\end{itemize}

\begin{verbatim}
##               
##                Depression:0 Depression:1
##   Misophonia:4   0.00000000   0.07317073
##   Misophonia:3   0.20325203   0.04878049
##   Misophonia:2   0.27642276   0.02439024
##   Misophonia:1   0.04065041   0.00000000
##   Misophonia:0   0.29268293   0.04065041
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{heat-map}{%
\section{Heat map}\label{heat-map}}

The contingency table can be plotted as a \textbf{heat map}

\includegraphics{_main_files/figure-latex/unnamed-chunk-30-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continous-variables}{%
\section{Continous variables}\label{continous-variables}}

In the misophonia study, the jaw protrusion was also measured as a possible cephalometric factor for de disease.

\begin{verbatim}
##     Angulo_convexidad protusion.mandibular
## 1                7.97                13.00
## 2               18.23                -5.00
## 3               12.27                11.50
## 4                7.81                16.80
## 5                9.81                33.00
## 6               13.50                 2.00
## 7               19.30                -3.90
## 8                7.70                16.80
## 9               12.30                 8.00
## 10               7.90                28.80
## 11              12.60                 3.00
## 12              19.00                -7.90
## 13               7.27                28.30
## 14              14.00                 4.00
## 15               5.40                22.20
## 16               8.00                 0.00
## 17              11.20                15.00
## 18               7.75                17.00
## 19               7.94                49.00
## 20              16.69                 5.00
## 21               7.62                42.00
## 22               7.02                28.00
## 23               7.00                 9.40
## 24              19.20               -13.20
## 25               7.96                23.00
## 26              14.70                 2.30
## 27               7.24                25.00
## 28               7.80                 4.90
## 29               7.90                92.00
## 30               4.70                 6.00
## 31               4.40                17.00
## 32              14.00                 3.30
## 33              14.40                10.30
## 34              16.00                 6.30
## 35               1.40                19.50
## 36               9.76                22.00
## 37               7.90                 5.00
## 38               7.90                78.00
## 39               7.40                 9.30
## 40               6.30                50.60
## 41               7.76                18.00
## 42               7.30                18.00
## 43               7.00                10.00
## 44              11.23                 4.00
## 45              16.00                13.30
## 46               7.90                48.00
## 47               7.29                23.50
## 48               6.91                37.60
## 49               7.10                15.00
## 50              13.40                 5.10
## 51              11.60                -2.20
## 52              -1.00                32.00
## 53               6.00                25.00
## 54               7.82                24.00
## 55               4.80                33.60
## 56              11.00                 3.30
## 57               9.00                31.50
## 58              11.50                12.80
## 59              16.00                 3.00
## 60              15.00                 6.00
## 61               1.40                21.40
## 62              16.80               -10.00
## 63               7.70                19.00
## 64              16.14                32.00
## 65               7.12                15.00
## 66              -1.00                10.00
## 67              17.00               -16.90
## 68               9.26                 2.00
## 69              18.70               -10.10
## 70               3.40                12.20
## 71              21.30               -11.00
## 72               7.50                 5.20
## 73               6.03                16.00
## 74               7.50                 5.80
## 75              19.00                 5.20
## 76              19.01                13.00
## 77               8.10                13.60
## 78               7.80                16.10
## 79               6.10                33.20
## 80              15.26                 4.00
## 81               7.95                12.00
## 82              18.00                -1.50
## 83               4.60                18.30
## 84              15.00                 3.00
## 85               7.50                15.80
## 86               8.00                27.10
## 87              16.80               -10.00
## 88               8.54                25.00
## 89               7.00                27.10
## 90              18.30                -8.00
## 91               7.80                12.00
## 92              16.00                -8.00
## 93              14.00                23.00
## 94              12.30                 5.00
## 95              11.40                 1.00
## 96               8.50                18.90
## 97               7.00                15.00
## 98               7.96                22.00
## 99              17.60                -3.50
## 100             10.00                20.00
## 101              3.50                12.20
## 102              6.70                14.70
## 103             17.00                -5.00
## 104             20.26                -4.15
## 105              6.64                11.00
## 106              1.80                -4.00
## 107              7.02                25.00
## 108              2.46                35.00
## 109             19.00                -5.00
## 110             17.86               -30.00
## 111              6.10                12.20
## 112              6.64                19.00
## 113             12.00                 1.60
## 114              6.60                20.00
## 115              8.70                17.10
## 116             14.05                24.00
## 117              7.20                 7.10
## 118             19.70               -11.00
## 119              7.70                21.30
## 120              6.02                 5.00
## 121              2.50                12.90
## 122             19.00                 5.90
## 123              6.80                 5.80
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{heat-map-for-continuous-variables}{%
\section{Heat map for continuous variables}\label{heat-map-for-continuous-variables}}

\begin{itemize}
\item
  Two dimensional \textbf{histogram}.
\item
  It illustrates the ``continuous contingency'' table for continuous variables
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-32-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{scatter-plot}{%
\section{Scatter plot}\label{scatter-plot}}

\begin{itemize}
\item
  The \textbf{histogram} depends on the size of the bin (pixel).
\item
  If the pixel is small enough to contain a single observation then the heat map results in a \textbf{scatter plot}
\end{itemize}

The scatter plot is the illustration of a ``contingency table'' for continuous variables when the bin (pixel) is small enough to contain one single observation (consisting of a pair of values).

\includegraphics{_main_files/figure-latex/unnamed-chunk-33-1.pdf}

\hypertarget{conditional-probability}{%
\chapter{Conditional Probability}\label{conditional-probability}}

\hypertarget{objective-2}{%
\section{Objective}\label{objective-2}}

\begin{itemize}
\tightlist
\item
  Conditional probability
\item
  Independence
\item
  Bayes' theorem
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{joint-probability}{%
\section{Joint Probability}\label{joint-probability}}

The joint probability of two events \(A\) and \(B\) is
\[P(A,B)=P(A \cap B)\]

Let's imagine a random experiment that measures two different types of outcomes.

\begin{itemize}
\item
  height and weight of an individual: \((h, w)\)
\item
  time and place of an electric charge: \((p, t)\)
\item
  a throw of two dice: (\(n_1\),\(n_2\))
\item
  cross two traffic lights in green: (\(\bar{R_1}\), \(\bar{R_2}\))
\end{itemize}

In many cases, we are interested in finding out whether the values of one outcome \textbf{condition} the values of the other.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{diagnostics}{%
\section{Diagnostics}\label{diagnostics}}

Let's consider a \textbf{diagnostic tool}

We want to find the state of a system (s):

\begin{itemize}
\tightlist
\item
  inadequate (yes)
\item
  adequate (no)
\end{itemize}

with a test (t):

\begin{itemize}
\tightlist
\item
  positive
\item
  negative
\end{itemize}

We test a battery to find how long it can live. We stress a cable to find if it resists carrying a certain load. We perform a PCR to see if someone is infected.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{diagnostics-test}{%
\section{Diagnostics Test}\label{diagnostics-test}}

Let's consider diagnosing infection with a new test.

Infection status:

\begin{itemize}
\tightlist
\item
  yes (infected)
\item
  no (not infected)
\end{itemize}

Test:

\begin{itemize}
\tightlist
\item
  positive
\item
  negative
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{observations}{%
\section{Observations}\label{observations}}

Each individual is a random experiment with two measurements: (Infection, Test)

\begin{longtable}[]{@{}ccc@{}}
\toprule
Subject & Infection & Test \\
\midrule
\endhead
\(s_1\) & yes & positive \\
\(s_2\) & no & negative \\
\(s_3\) & yes & positive \\
\ldots{} & \ldots{} & \ldots{} \\
\(s_i\) & no & positive* \\
\ldots{} & \ldots{} & \ldots{} \\
\ldots{} & \ldots{} & \ldots{} \\
\(s_n\) & yes & negative* \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{contingency-tables}{%
\section{Contingency tables}\label{contingency-tables}}

\begin{itemize}
\tightlist
\item
  For the number of observations of each outcome
\end{itemize}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Infection: yes & Infection: no & sum \\
\midrule
\endhead
Test: positive & 18 & 12 & 30 \\
Test: negative & 30 & 300 & 330 \\
sum & 48 & 312 & 360 \\
\bottomrule
\end{longtable}

\begin{itemize}
\tightlist
\item
  For the relative frequencies, if \(N>>0\) we will take \(f_{i,j}=\hat{P}(x_i, y_j)\)
\end{itemize}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Infection: yes & Infection: no & sum \\
\midrule
\endhead
Test: positive & 0.05 & 0.0333 & 0.0833 \\
Test: negative & 0.0833 & 0.833 & 0.9166 \\
sum & 0.133 & 0.866 & 1 \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conditional-probability-1}{%
\section{Conditional probability}\label{conditional-probability-1}}

Let's think first in terms of those who are \textbf{infected}

Within those who are infected (\textbf{yes}), what is the probability of those who tested positive?

\begin{itemize}
\tightlist
\item
  Sensitivity (true positive rate)
\end{itemize}

\[\hat{P}(positive|yes)=\frac{n_{positive,yes}}{n_{yes}}\]

\[=\frac{\frac{n_{positive,yes}}{N}}{\frac{n_{yes}}{N}}=\frac{f_{positive,yes}}{f_{yes}}\]

Therefore, in the limit, we expect to have a probability of the type

\[P(positive|yes)=\frac{P(positive, yes)}{P(yes)}=\frac{P(positive \cap yes)}{P(yes)}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conditional-probability-2}{%
\section{Conditional probability}\label{conditional-probability-2}}

\textbf{Definition:}
The conditional probability of an event B given an event A, denoted as \(P(A|B)\), is
\[P(A|B) = \frac{P(A\cap B)}{P(B)}\]

\begin{itemize}
\tightlist
\item
  you can prove that the conditional probability satisfies the axioms of probability.
\item
  it is the probability with the sampling space given by \(B\): \(S_B\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conditional-contingency-table}{%
\section{Conditional contingency table}\label{conditional-contingency-table}}

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Infection: Yes & Infection: No \\
\midrule
\endhead
Test: positive & P(positive {\textbar{}} yes) & P(positive {\textbar{}} no) \\
Test: negative & P(negative {\textbar{}} yes) & P(negative {\textbar{}} no) \\
sum & 1 & 1 \\
\bottomrule
\end{longtable}

\begin{itemize}
\item
  True positive rate (Sensitivity): The probability of testing positive \textbf{if} having the disease \(P(positive|yes)\)
\item
  True negative rate (Specificity): The probability of testing negative \textbf{if} not having the disease \(P(negative|no)\)
\item
  False-positive rate: The probability of testing positive \textbf{if} not having the disease \(P(positive|no)\)
\item
  False-negative rate: The probability of testing negative \textbf{if} having the disease \(P(negative|yes)\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-conditional-contingency-table}{%
\section{Example conditional contingency table}\label{example-conditional-contingency-table}}

Taking the frequencies as estimates of the probabilities then

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Infection: Yes & Infection: No \\
\midrule
\endhead
Test: positive & 18/48 = 0.375 & 12/312 = 0.038 \\
Test: negative & 30/48 = 0.625 & 300/312 =0.962 \\
sum & 1 & 1 \\
\bottomrule
\end{longtable}

Our diagnostic tool has low sensitivity (0.375) but high
specificity (0.962).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multiplication-rule}{%
\section{Multiplication rule}\label{multiplication-rule}}

Now let's imagine the real situation.

\begin{itemize}
\item
  PCRs for coronavirus were (performed){[}\url{https://www.nejm.org/doi/full/10.1056/NEJMp2015897}{]} in people in the hospital who we are sure to be infected. They have a sensitivity of 70\%. They have also been tested in the lab in conditions of no infection with 96\% specificity
\item
  A prevalence study in Spain showed that \(P(yes)=0.05\), \(P(no)=0.95\) before summer.
\end{itemize}

With this data, what was the probability that a randomly selected person in the population tested positive \textbf{and} was infected: \(P(yes \cap positive)=P(yes, positive)\)?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{diagnostic-performance}{%
\section{Diagnostic performance}\label{diagnostic-performance}}

To study the performance of a new diagnostic test:

\begin{itemize}
\item
  you select specimens that are inadequate (disease: \textbf{yes}) and apply the test, trying to find its sensitivity: \(P(positive|yes)\) (0.70 for PCRs)
\item
  you select specimens that are adequate (disease: \textbf{no}) and apply the test, trying to find its specificity: \(P(negative|no)\) (0.96 for PCRs)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Infection: Yes & Infection: No \\
\midrule
\endhead
Test: positive & P(positive{\textbar{}}yes)=0.7 & P(positive{\textbar{}}no)=0.06 \\
Test: negative & P(negative{\textbar{}}yes)=0.3 & P(negative{\textbar{}}no)=0.94 \\
sum & 1 & 1 \\
\bottomrule
\end{longtable}

From this matrix, can we obtain \(P(yes, positive)\)?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multiplication-rule-1}{%
\section{Multiplication rule}\label{multiplication-rule-1}}

How do you recover the joint probability from the conditional probability?

For two events \(A\) and \(B\) we have the multiplication rule

\[P(A, B) =  P(A|B) P(B)\]

that follows from the definition of the conditional probability.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{contingency-table-in-terms-of-conditional-probabilities}{%
\section{Contingency table in terms of conditional probabilities}\label{contingency-table-in-terms-of-conditional-probabilities}}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Infection: Yes & Infection: No & sum \\
\midrule
\endhead
Test: positive & P(positive {\textbar{}} yes)P(yes) & P(positive {\textbar{}} no)P(no) & P(positive) \\
Test: negative & P(negative {\textbar{}} yes)P(yes) & P(negative {\textbar{}} no) P(no) & P(negative) \\
sum & P(yes) & P(no) & 1 \\
\bottomrule
\end{longtable}

For instance the probability of testing \(positive\) and being infected \(yes\):

\begin{itemize}
\tightlist
\item
  \(P(positive, yes)=P(positive \cap yes) = P(positive|yes) P(yes)\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conditional-tree}{%
\section{Conditional tree}\label{conditional-tree}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{contingency-table-in-terms-of-conditional-probabilities-1}{%
\section{Contingency table in terms of conditional probabilities}\label{contingency-table-in-terms-of-conditional-probabilities-1}}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Infection: yes & Infection: no & sum \\
\midrule
\endhead
Test: positive & 0.035 & 0.057 & 0.092 \\
Test: negative & 0.015 & 0.893 & 0.908 \\
sum & 0.05 & 0.95 & 1 \\
\bottomrule
\end{longtable}

\begin{itemize}
\tightlist
\item
  \(P(positive,yes)= 0.035\)
\end{itemize}

But we also found the marginal of being positive:

\begin{itemize}
\tightlist
\item
  \(P(positive)=0.092\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{total-probability-rule}{%
\section{Total probability rule}\label{total-probability-rule}}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Infection: Yes & Infection: No & sum \\
\midrule
\endhead
Test: positive & P(positive {\textbar{}} yes)P(yes) & P(positive {\textbar{}} no)P(no) & P(positive) \\
Test: negative & P(negative {\textbar{}} yes)P(yes) & P(negative {\textbar{}} no) P(no) & P(negative) \\
sum & P(yes) & P(no) & 1 \\
\bottomrule
\end{longtable}

When we write the unknown marginals in terms of their conditional probabilities we call it the \textbf{total probability rule}

\begin{itemize}
\tightlist
\item
  \(P(positive)=P(positive|yes)P(yes)+P(positive|no)P(no)\)
\item
  \(P(negative)=P(negative|yes)P(yes)+P(negative|no)P(no)\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conditional-tree-1}{%
\section{Conditional tree}\label{conditional-tree-1}}

\textbf{Total probability rule} for the marginal of \(B\): In how many ways I can obtain the outcome \(B\)?

\(P(B)=P(B|A)P(A)+P(B|A')P(A')\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{finding-reverse-probabilities}{%
\section{Finding reverse probabilities}\label{finding-reverse-probabilities}}

From the conditional contingency table

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Infection: Yes & Infection: No \\
\midrule
\endhead
Test: positive & P(positive {\textbar{}} yes) & P(positive {\textbar{}} no) \\
Test: negative & P(negative {\textbar{}} yes) & P(negative {\textbar{}} no) \\
sum & 1 & 1 \\
\bottomrule
\end{longtable}

How can we calculate the probability of being infected if tested positive: \(P(yes|positive)\)?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{recover-joint-probabilities}{%
\section{Recover joint probabilities}\label{recover-joint-probabilities}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We recover the contingency table for joint probabilities
\end{enumerate}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Infection: Yes & Infection: No & sum \\
\midrule
\endhead
Test: positive & P(positive {\textbar{}} yes)P(yes) & P(positive {\textbar{}} no)P(no) & P(positive) \\
Test: negative & P(negative {\textbar{}} yes)P(yes) & P(negative {\textbar{}} no) P(no) & P(negative) \\
sum & P(yes) & P(no) & 1 \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{reverse-conditionals}{%
\section{Reverse conditionals}\label{reverse-conditionals}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We compute the conditional probabilities for the test:
\end{enumerate}

\[P(infection|test)=\frac{P(test|infection)P(infection)}{P(test)}\]

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Infection: Yes & Infection: No & sum \\
\midrule
\endhead
Test: positive & P(yes{\textbar{}}positive) & P(no{\textbar{}}positive) & 1 \\
Test: negative & P(yes{\textbar{}}negative) & P(no{\textbar{}}negative) & 1 \\
\bottomrule
\end{longtable}

For instance:
\[P(yes|positive)=\frac{P(positive|yes)P(yes)}{P(positive)}\]
since we usually don't have \(P(positive)\) we use the \textbf{total probability} rule in the denominator

\[P(yes|positive)=\frac{P(positive|yes)P(yes)}{P(positive|yes)P(yes)+P(positive|no)P(no)}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bayes-theorem}{%
\section{Baye's theorem}\label{bayes-theorem}}

The expression:

\[P(yes|positive)=\frac{P(positive|yes)P(yes)}{P(positive|yes)P(yes)+P(positive|no)P(no)}\]

is called the \textbf{Bayes theorem}

\textbf{Theorem}

If \(E1, E2, ..., Ek\) are \(k\) mutually exclusive and exhaustive events and \(B\) is any event,

\[P(Ei|B)=\frac{P(B|Ei)P(Ei)}{P(B|E1)P(E1) +...+ P(B|Ek)P(Ek)}\]

It allows to reverse the conditionals:

\[P(B|A) \rightarrow P(A|B)\]

Or \textbf{design} a test \(B\) in controlled condition \(A\) and then use it to \textbf{infer} the probability of the condition when the test is positive.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-bayes-theorem}{%
\section{Example: Bayes' theorem}\label{example-bayes-theorem}}

Baye's theorem:

\[P(yes|positive)  = \frac{P(positive|yes) P(yes)}{P(possitive|yes)P(yes)+P(positive|no)P(no)}\]

we know:

\begin{itemize}
\item
  \(P(positive|yes)=0.70\)
\item
  \(P(positive|no)=1- P(negative|no)=0.06\)
\item
  the probability of infection and not infection in the population: \(P(yes)=0.05\) and \(P(no)=1-P(yes)=0.95\).
\end{itemize}

Therefore:

\[P(yes|positive)=0.47\]

Tests are not so good to \textbf{confirm} infections.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-bayes-theorem-1}{%
\section{Example: Bayes' theorem}\label{example-bayes-theorem-1}}

Let's now apply it to the probability of not being infected if the test is negative

\[P(no|negative)  = \frac{P(negative|no)  P(no)}{P(negative|no) P(no)+P(negative|yes)P(yes)}\]

Substitution of all the values gives

\[P(no|negative)=0.98\]

Tests are good to \textbf{rule out} infections.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistical-independence}{%
\section{Statistical independence}\label{statistical-independence}}

In many applications, we want to know if the knowledge of one event conditions the outcome of another event.

\begin{itemize}
\tightlist
\item
  there are cases where we want to know if the events are not conditioned
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistical-independence-1}{%
\section{Statistical independence}\label{statistical-independence-1}}

Consider conductors for which we measure their surface flaws and if their conduction capacity is defective

The estimated \textbf{joint probabilities} are

\begin{longtable}[]{@{}cccc@{}}
\toprule
& flaws (F) & no flaws (F') & sum \\
\midrule
\endhead
defective (D) & \(0.005\) & \(0.045\) & \(0.05\) \\
no defective (D') & \(0.095\) & \(0.855\) & \(0.95\) \\
sum & \(0.1\) & \(0.9\) & 1 \\
\bottomrule
\end{longtable}

where, for instance, the joint probability of \(F\) and \(D\) is

\begin{itemize}
\tightlist
\item
  \(P(D,F)=0.005\)
\end{itemize}

The marginal probabilities are

\begin{itemize}
\tightlist
\item
  \(P(D)=P(D, F) + P(D, F')=0.05\)
\item
  \(P(F)=P(D, F) + P(D', F)= 0.1\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistical-independence-2}{%
\section{Statistical independence}\label{statistical-independence-2}}

What is the \textbf{conditional probability} of observing a defective conductor if they have a flaw?

\begin{longtable}[]{@{}ccc@{}}
\toprule
& F & F' \\
\midrule
\endhead
D & P(D{\textbar{}}F) = 0.05 & P(D{\textbar{}}F')=0.05 \\
D' & P(D'{\textbar{}}F)=0.95 & P(D'{\textbar{}}F')=0.95 \\
sum & 1 & 1 \\
\bottomrule
\end{longtable}

The marginals and the conditional probabilities are the same!

\begin{itemize}
\tightlist
\item
  \(P(D|F)=P(D|F')=P(D)\)
\item
  \(P(D'|F)=P(D'|F')=P(D')\)
\end{itemize}

The probability of observing a defective conductor \textbf{does not} depend on having observed or not a flaw.

\[P(D) = P(D|F)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistical-independence-3}{%
\section{Statistical independence}\label{statistical-independence-3}}

Two events \(A\) and \(B\) are statistically independent if

\begin{itemize}
\tightlist
\item
  \(P(A|B)=P(A)\); \(A\) is independent of \(B\)
\item
  \(P(B|A)=P(B)\); \(B\) is independent of \(A\)
\end{itemize}

and by the multiplication rule, their joint probability is

\begin{itemize}
\tightlist
\item
  \(P(A\cap B)=P(A|B)P(B)=P(A)P(B)\)
\end{itemize}

the multiplication of their marginal probabilities.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{products-of-marginals-products}{%
\section{Products of marginals products}\label{products-of-marginals-products}}

\begin{longtable}[]{@{}cccc@{}}
\toprule
& F & F' & sum \\
\midrule
\endhead
D & \(0.005\) & \(0.045\) & \(0.05\) \\
D' & \(0.095\) & \(0.855\) & \(0.95\) \\
sum & \(0.1\) & \(0.9\) & 1 \\
\bottomrule
\end{longtable}

Confirm that all the entries of the matrix are the product of the marginals.

For example:

\begin{itemize}
\tightlist
\item
  \(P(F)P(D)= P(D \cap F)\)
\item
  \(P(D')P(F')=P(D' \cap F')\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-5}{%
\section{Example}\label{example-5}}

Outcomes of throwing two coins: \(S={(H,H), (H,T), (T,H), (T,T)}\)

\begin{longtable}[]{@{}cccc@{}}
\toprule
& H & T & sum \\
\midrule
\endhead
H & \(1/4\) & \(1/4\) & \(1/2\) \\
T & \(1/4\) & \(1/4\) & \(1/2\) \\
sum & \(1/2\) & \(1/2\) & 1 \\
\bottomrule
\end{longtable}

\begin{itemize}
\tightlist
\item
  Obtaining a head in the first coin does not condition obtaining a tail in the result of the second coin \(P(T|H)=P(T)=1/2\)
\item
  the probability of obtaining a head and then a tail is the product of each independent outcome \(P(H, T)=P(H)*P(T)=1/4\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{discrete-random-variables}{%
\chapter{Discrete Random Variables}\label{discrete-random-variables}}

\hypertarget{objective-3}{%
\section{Objective}\label{objective-3}}

\begin{itemize}
\tightlist
\item
  Random variables
\item
  Probability mass function
\item
  Mean and variance
\item
  Probability distribution
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{how-do-we-assign-probability-values-to-outcomes}{%
\section{How do we assign probability values to outcomes?}\label{how-do-we-assign-probability-values-to-outcomes}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{random-variable}{%
\section{Random variable}\label{random-variable}}

\textbf{Definition:}

A \textbf{random variable} is a function that assigns a real \textbf{number} to each \textbf{outcome} in the sample space of a random experiment.

\begin{itemize}
\tightlist
\item
  Most commonly a random variable is the value of the \textbf{measurement} of interest that is made in a random experiment.
\end{itemize}

A random variable can be:

\begin{itemize}
\tightlist
\item
  Discrete (nominal, ordinal)
\item
  Continuous (interval, ratio)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{random-variable-1}{%
\section{Random variable}\label{random-variable-1}}

A \textbf{value} (or \textbf{outcome}) of a random variable is one of the possible numbers that the variable can take in a random experiment.

We write the random variable in \textbf{capitals}.

Example:

If \(X \in \{0,1\}\), we then say \(X\) is a random variable that can take the values \(0\) or \(1\).

\textbf{Observation} of a random variable

\begin{itemize}
\tightlist
\item
  An observation is the \textbf{acquisition} of the value of a random variable in a random experiment
\end{itemize}

Example:

1 0 0 1 0 \textbf{1} 0 1 1

The number in bold is an observation of \(X\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{events-of-observing-a-random-variable}{%
\section{Events of observing a random variable}\label{events-of-observing-a-random-variable}}

\begin{itemize}
\tightlist
\item
  \(X=1\) is the \textbf{event} of observing the random variable \(X\) with value \(1\)
\item
  \(X=2\) is the \textbf{event} of observing the random variable \(X\) with value \(2\)
\end{itemize}

\ldots{}

\textbf{In general:}

\begin{itemize}
\item
  \(X=x\) is the \textbf{event} of observing the random variable \(X\) with value \(x\) (little \(x\))
\item
  Any two values of a random variable define two \textbf{mutually exclusive} events.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-of-random-variables}{%
\section{Probability of random variables}\label{probability-of-random-variables}}

We are interested in assigning probabilities to the values of a random variable.

We have already done this for the dice: \(X \in \{1,2,3,4,5,6\}\) (classical interpretation of pribability)

\begin{longtable}[]{@{}cc@{}}
\toprule
\(X\) & Probability \\
\midrule
\endhead
\(1\) & \(P(X=1)=1/6\) \\
\(2\) & \(P(X=2)=1/6\) \\
\(3\) & \(P(X=3)=1/6\) \\
\(4\) & \(P(X=4)=1/6\) \\
\(5\) & \(P(X=5)=1/6\) \\
\(6\) & \(P(X=6)=1/6\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-functions}{%
\section{Probability functions}\label{probability-functions}}

\begin{itemize}
\tightlist
\item
  We can write the probability table
\item
  plot it
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-34-1.pdf}

\begin{itemize}
\tightlist
\item
  or write as the function
\end{itemize}

\[f(x)=P(X=x)=1/6\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-functions-1}{%
\section{Probability functions}\label{probability-functions-1}}

We can \textbf{create} any type of probability function if we respect the probability rules:

\includegraphics{_main_files/figure-latex/unnamed-chunk-35-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-functions-2}{%
\section{Probability functions}\label{probability-functions-2}}

For a discrete random variable \(X \in \{x_1 , x_2 , .. , x_M\}\) , a \textbf{probability mass function}

is always positive

\begin{itemize}
\tightlist
\item
  \(f(x_i)\geq 0\)
\end{itemize}

is used to compute probabilities

\begin{itemize}
\tightlist
\item
  \(f(x_i)=P(X=x_i)\)
\end{itemize}

and its sum over all the values of the variable is \(1\):

\begin{itemize}
\tightlist
\item
  \(\sum_{i=1}^M f(x_i)=1\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-functions-3}{%
\section{Probability functions}\label{probability-functions-3}}

\begin{itemize}
\item
  Note that the definition of \(X\) and its probability mass function is general \textbf{without reference} to any experiment. The functions live in the model (abstract) space.
\item
  \(X\) and \(f(x)\) are abstract objects that may or may not map to an experiment
\item
  We have the freedom to construct them as we want as long as we respect their definition.
\item
  They have some \textbf{properties} that are derived exclusively from their definition.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-probability-mass-function}{%
\section{Example: Probability mass function}\label{example-probability-mass-function}}

Consider the following random variable \(X\) over the outcomes

\begin{longtable}[]{@{}cc@{}}
\toprule
outcome & \(X\) \\
\midrule
\endhead
\(a\) & 0 \\
\(b\) & 0 \\
\(c\) & 1.5 \\
\(d\) & 1.5 \\
\(e\) & 2 \\
\(f\) & 3 \\
\bottomrule
\end{longtable}

If each outcome is equally probable then what is the probability mass function of \(x\)?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-table-for-equally-likely-outcomes}{%
\section{Probability table for equally likely outcomes}\label{probability-table-for-equally-likely-outcomes}}

\begin{longtable}[]{@{}cc@{}}
\toprule
outcome & Probability(outcome) \\
\midrule
\endhead
\(a\) & \(1/6\) \\
\(b\) & \(1/6\) \\
\(c\) & \(1/6\) \\
\(d\) & \(1/6\) \\
\(e\) & \(1/6\) \\
\(f\) & \(1/6\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-table-for-x}{%
\section{\texorpdfstring{Probability table for \(X\)}{Probability table for X}}\label{probability-table-for-x}}

\begin{longtable}[]{@{}cc@{}}
\toprule
\(X\) & \(f(x)=P(X=x)\) \\
\midrule
\endhead
\(0\) & \(P(X=0)=2/6\) \\
\(1.5\) & \(P(X=1.5)=2/6\) \\
\(2\) & \(P(X=2)=1/3\) \\
\(3\) & \(P(X=3)=1/3\) \\
\bottomrule
\end{longtable}

We can compute, for instance, the following probabilities for events on the values of \(X\)

\begin{itemize}
\tightlist
\item
  \(P(X>3)\)
\item
  \(P(X=0\, \cup \, X=2 )\)
\item
  \(P(X \leq 2)\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-6}{%
\section{Example}\label{example-6}}

Consider:

\begin{itemize}
\tightlist
\item
  we do not know what the primary events with equal probabilities are.
\item
  we then \textbf{estimate} the probability mass function from the relative frequencies observed for a random variable
\end{itemize}

\begin{longtable}[]{@{}cc@{}}
\toprule
\(X\) & \(f_i\) \\
\midrule
\endhead
\(-2\) & \(0.132\) \\
\(-1\) & \(0.262\) \\
\(0\) & \(0.240\) \\
\(1\) & \(0.248\) \\
\(2\) & \(0.118\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-7}{%
\section{Example}\label{example-7}}

\textbf{Probability model:}

These probabilities are consistent with the following experiment: In one urn put \(8\) balls and:

\begin{itemize}
\tightlist
\item
  mark \(1\) ball with \(-2\)
\item
  mark \(2\) balls with \(-1\)
\item
  mark \(2\) balls with \(0\)
\item
  mark \(2\) balls with \(1\)
\item
  mark \(1\) ball with \(2\)
\end{itemize}

\textbf{experiment:} Take one ball and read the number.

\begin{longtable}[]{@{}cc@{}}
\toprule
\(X\) & \(P(X=x)\) \\
\midrule
\endhead
\(-2\) & \(1/8=0.125\) \\
\(-1\) & \(2/8=0.25\) \\
\(0\) & \(2/8=0.25\) \\
\(1\) & \(2/8=0.25\) \\
\(2\) & \(1/8=0.125\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probabilities-and-frequencies}{%
\section{Probabilities and frequencies}\label{probabilities-and-frequencies}}

For computing the relative frequencies \(f_i\) you have to

\begin{itemize}
\tightlist
\item
  \textbf{repeat} the experiment \(N\) times (you have to put the ball back in the urn each time) and at the end compute
\end{itemize}

\[f_i=n_i/N\]

We are assuming that:

\[lim_{N \rightarrow \infty} f_i = f(x_i)=P(X=x_i)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probabilities-and-relative-frequencies}{%
\section{Probabilities and relative frequencies}\label{probabilities-and-relative-frequencies}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-36-1.pdf}

\begin{itemize}
\tightlist
\item
  In this example we \textbf{know} the probability \textbf{model} \(f(x)=P(X=x)\) by design.
\item
  We never observe \(f(x)\)
\item
  We can use relative frequencies to estimate the probabilities
  \[f_i = \hat{f}(x_i)=\hat{P}(X=x_i)\] (\(f_i\) depends on \(N\))
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-and-variance}{%
\section{Mean and Variance}\label{mean-and-variance}}

The probability mass functions \(f(x)\) have two main properties

\begin{itemize}
\tightlist
\item
  its center
\item
  its spread
\end{itemize}

We can ask,

\begin{itemize}
\item
  around which values of \(X\) the probability concentrated?
\item
  How dispersed are the values of \(X\) in relation to their probabilities?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-and-variance-1}{%
\section{Mean and Variance}\label{mean-and-variance-1}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean}{%
\section{Mean}\label{mean}}

Remember that the \textbf{average} in terms of the relative frequencies of the values of \(x_i\) (categorical ordered outcomes) can be written as

\[\bar{x}= \sum_{i=1}^M x_i \frac{n_i}{N}=\sum_{i=1}^M x_i f_i\]

\textbf{Definition}

The \textbf{mean} (\(\mu\)) or expected value of a discrete random variable \(X\), \(E(X)\), with mass function \(f(x)\) is given by

\[ \mu = E(X)= \sum_{i=1}^M x_i f(x_i) \]

It is the center of gravity of the \textbf{probabilities}: The point where probability loadings on a road are balanced

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-mean}{%
\section{Example: Mean}\label{example-mean}}

What is the mean of \(X\) if its probability mass function \(f(x)\) is given by

\(P(X=0)=1/16\)
\(P(X=1)=4/16\)
\(P(X=2)=6/16\)
\(P(X=3)=4/16\)
\(P(X=4)=1/16\)

\includegraphics{_main_files/figure-latex/unnamed-chunk-37-1.pdf}

\[ \mu =E(X)=\sum_{i=1}^m x_i f(x_i) \]

\(E(X)=\)\textbf{0} * 1/16 + \textbf{1} * 4/16 + \textbf{2} * 6/16 + \textbf{3} * 4/16 + \textbf{4} * 1/16 =2

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{variance}{%
\section{Variance}\label{variance}}

In similar terms we define the mean squared distance from the mean:

\textbf{Definition}

The variance, written as \(\sigma^2\) or \(V(X)\), of a discrete random variable \(X\) with mass function \(f(x)\) is given by

\[\sigma^2 = V(X)= \sum_{i=1}^M (x_i-\mu)^2 f(x_i)\]

\begin{itemize}
\item
  \(\sigma=\sqrt{V(X)}\) is called the \textbf{standard deviation} of the random variable
\item
  Think of it as the moment of inertia of probabilities about the mean.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-variance}{%
\section{Example: Variance}\label{example-variance}}

What is the variance of \(X\) if its probability mass function \(f(x)\) is given by

\(P(X=0)=1/16\)
\(P(X=1)=4/16\)
\(P(X=2)=6/16\)
\(P(X=3)=4/16\)
\(P(X=4)=1/16\)

\[\sigma^2 =V(X)=\sum_{i=1}^m (x_i-\mu)^2 f(x_i)\]

\(V(X)=\)\textbf{(0-2)}\(^2\)* 1/16 + \textbf{(1-2)}\(^2\)* 4/16 + \textbf{(2-2)}\(^2\)* 6/16 + \textbf{(3-2)}\(^2\)* 4/16 + \textbf{(4-2)}\(^2\)* 1/16 =1

\[V(X)=\sigma^2=1\]
\[\sigma=1\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{functions-of-x}{%
\section{\texorpdfstring{Functions of \(X\)}{Functions of X}}\label{functions-of-x}}

\textbf{Definition}

For any function \(h\) of a random variable \(X\), with mass function \(f(x)\), its expected value is given by

\[ E[h(X)]= \sum_{i=1}^M h(x_i) f(x_i) \]

This is an important definition that allows us to prove three important properties of the median and variance:

\begin{itemize}
\item
  The mean of a linear function is the linear function fo the mean: \[E(a\times X +b)= a\times E(X) +b\] for \(a\) and \(b\) scalars (numbers).
\item
  The variance of a linear function of \(X\) is:\[V(a\times X +b)= a^2\times V(X)\]
\item
  The variance \textbf{about the origin} is the variance \textbf{about the mean} plus the mean squared: \[E(X^2)=V(X)+E(X)^2\]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-variance-about-the-origin}{%
\section{Example: Variance about the origin}\label{example-variance-about-the-origin}}

What is the variance \(X\) about the origin, \(E(X^2)\), if its probability mass function \(f(x)\) is given by

\(P(X=0)=1/16\)
\(P(X=1)=4/16\)
\(P(X=2)=6/16\)
\(P(X=3)=4/16\)
\(P(X=4)=1/16\)

\[E(X^2) =\sum_{i=1}^m x_i^2 f(x_i)\]

\(E(X^2)=\)\textbf{(0)}\(^2\)* 1/16 + \textbf{(1)}\(^2\)* 4/16 + \textbf{(2)}\(^2\)* 6/16 + \textbf{(3)}\(^2\)* 4/16 + \textbf{(4)}\(^2\)* 1/16 =5

We can also verify:

\[E(X^2)=V(X)+E(X)^2\]

\(5=1+2^2\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-distribution}{%
\section{Probability distribution}\label{probability-distribution}}

\textbf{Definition:}

The \textbf{probability distribution} function is defined as

\[F(x)=P(X\leq x)=\sum_{x_i\leq x} f(x_i) \]

That is the accumulated probability up to a given value \(x\)

\(F(x)\) satisfies:

\begin{itemize}
\tightlist
\item
  \(0\leq F(x) \leq 1\)
\item
  If \(x \leq y\), then \(F(x) \leq F(y)\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-probability-distribution}{%
\section{Example: Probability distribution}\label{example-probability-distribution}}

For the probability mass function:

\(f(0)=P(X=0)=1/16\)
\(f(1)=P(X=1)=4/16\)
\(f(2)=P(X=2)=6/16\)
\(f(3)=P(X=3)=4/16\)
\(f(4)=P(X=4)=1/16\)

The probability distribution is:

\[
    F(x)=
\begin{cases}
    1/16,& \text{if } x < 1\\
    5/16,& 1\leq x < 2\\
    11/16,& 2\leq x < 3\\
    15/16,& 3\leq x < 4\\
    16/16,&  x \leq 5\\
\end{cases}
\]

For\(X \in \mathbb{Z}\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-distribution-1}{%
\section{Probability distribution}\label{probability-distribution-1}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-38-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-function-and-probability-distribution}{%
\section{Probability function and Probability distribution}\label{probability-function-and-probability-distribution}}

Compute the mass probability function of the following probability distribution:

\(F(0)=1/16\), \(F(1)=5/16\), \(F(2)=11/16\), \(F(3)=15/16\), \(F(4)=16/16\),

Let's work backward.

\(f(0)=F(0)=1/16\)
\(f(1)=F(1)-f(0)=5/32-1/32=4/16\)
\(f(2)=F(2)-f(1)-f(0)=F(2)-F(1)=6/16\)
\(f(3)=F(3)-f(2)-f(1)-f(0)=F(3)-F(2)=4/16\)
\(f(4)=F(4)-F(3)=1/16\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-function-and-probability-distribution-1}{%
\section{Probability function and Probability distribution}\label{probability-function-and-probability-distribution-1}}

The Probability distribution is another way to specify the probability of a random variable

\[f(x_i)=F(x_i)-F(x_{i-1})\]

with

\[f(x_1)=F(x_1)\]

for \(X\) taking values in \(x_1 \leq x_2 \leq ... \leq x_n\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{quantiles}{%
\section{Quantiles}\label{quantiles}}

We define the \textbf{q-quantile} as the value \(x_{p}\) \textbf{under} which we have accumulated q*100\% of the probability

\[q=\sum_{i=1}^p f(x_i) = F (x_p)\]

\begin{itemize}
\tightlist
\item
  The \textbf{median} is value \(x_m\) such that \(q=0.5\)
\end{itemize}

\[F(x_{m})=0.5\]

\begin{itemize}
\tightlist
\item
  The \(0.05\)-quantile is the value \(x_{r}\) such that \(q=0.05\)
\end{itemize}

\[F(x_{r})=0.05\]

\begin{itemize}
\tightlist
\item
  The \(0.95\)-quantile is the value \(x_{s}\) such that \(q=0.95\)
\end{itemize}

\[F(x_{s})=0.95\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.43}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.30}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.26}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\centering
quantity names
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
model (unobserved)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
data (observed)
\end{minipage} \\
\midrule
\endhead
probability mass function // relative frequency & \(f(x_i)=P(X=x_i)\) & \(f_i=\frac{n_i}{N}\) \\
probability distribution // cumulative relative frequency & \(F(x_i)=P(X \leq x_i)\) & \(F_i=\sum_{k\leq i} f_k\) \\
mean // average & \(\mu=E(X)=\sum_{i=1}^M x_i f(x_i)\) & \(\bar{x}=\sum_{j=1}^N x_j/N\) \\
variance // sample variance & \(\sigma^2=V(X)=\sum_{i=1}^M (x_i-\mu)^2 f(x_i)\) & \(s^2=\sum_{j=1}^N (x_j-\bar{x})^2/(N-1)\) \\
standard deviation // sample sd & \(\sigma=\sqrt{V(X)}\) & \(s\) \\
variance about the origin // 2nd sample moment & \(E(X^2)=\sum_{i=1}^M x_i^2 f(x_i)\) & \(m_2= \sum_{j=1}^N x_j^2/n\) \\
\bottomrule
\end{longtable}

Note that:

\begin{itemize}
\tightlist
\item
  \(i=1...M\) is an \textbf{outcome} of the random variable \(X\).
\item
  \(j=1...N\) is an \textbf{observation} of the random variable \(X\).
\end{itemize}

Properties:

\begin{itemize}
\tightlist
\item
  \(\sum_{i=1...N} f(x_i)=1\)
\item
  \(f(x_i)=F(x_i)-F(x_{i-1})\)
\item
  \(E(a\times X +b)= a\times E(X) +b\); for \(a\) and \(b\) scalars.
\item
  \(V(a\times X +b)= a^2\times V(X)\)
\item
  \(E(X^2)=V(X)+E(X)^2\)
\end{itemize}

\hypertarget{continous-random-variables}{%
\chapter{Continous Random Variables}\label{continous-random-variables}}

\hypertarget{objective-4}{%
\section{Objective}\label{objective-4}}

\begin{itemize}
\tightlist
\item
  Probability density function
\item
  Mean and variance
\item
  Probability distribution
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-random-variable}{%
\section{Continuous random variable}\label{continuous-random-variable}}

What happens with continuous random variables?

Let's reconsider the convexity angle of misophonia patients (Section 2.21).

\begin{itemize}
\tightlist
\item
  We redefined the outcomes as little regular intervals (bins) and computed the relative frequency for each of them as we did in the discrete case.
\end{itemize}

\begin{verbatim}
##        outcome ni         fi
## 1 [-1.02,3.46]  8 0.06504065
## 2  (3.46,7.92] 51 0.41463415
## 3  (7.92,12.4] 26 0.21138211
## 4  (12.4,16.8] 20 0.16260163
## 5  (16.8,21.3] 18 0.14634146
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-random-variable-1}{%
\section{Continuous random variable}\label{continuous-random-variable-1}}

Let's consider again that their relative frequencies are the probabilities when \(N \rightarrow \infty\)

\[f_i=\frac{n_i}{N} \rightarrow f(x_i)=P(X=x_i)\]

The probability depends now on the length of the bins \(\Delta x\). If we make the bins smaller and smaller then the frequencies get smaller and therefore

\(P(X=x_i) \rightarrow 0\) when \(\Delta x \rightarrow 0\), because \(n_i \rightarrow 0\)

\begin{verbatim}
##          outcome ni         fi
## 1  [-1.02,0.115]  2 0.01626016
## 2   (0.115,1.23]  0 0.00000000
## 3    (1.23,2.34]  3 0.02439024
## 4    (2.34,3.46]  3 0.02439024
## 5    (3.46,4.58]  2 0.01626016
## 6    (4.58,5.69]  4 0.03252033
## 7     (5.69,6.8] 11 0.08943089
## 8     (6.8,7.92] 34 0.27642276
## 9    (7.92,9.04] 12 0.09756098
## 10   (9.04,10.2]  4 0.03252033
## 11   (10.2,11.3]  3 0.02439024
## 12   (11.3,12.4]  7 0.05691057
## 13   (12.4,13.5]  2 0.01626016
## 14   (13.5,14.6]  6 0.04878049
## 15   (14.6,15.7]  4 0.03252033
## 16   (15.7,16.8]  8 0.06504065
## 17     (16.8,18]  4 0.03252033
## 18     (18,19.1]  9 0.07317073
## 19   (19.1,20.2]  3 0.02439024
## 20   (20.2,21.3]  2 0.01626016
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-random-variable-2}{%
\section{Continuous random variable}\label{continuous-random-variable-2}}

We define a quantity at a point \(x\) that is the amount of probability per unit distance that we would find in an \textbf{infinitesimal} bin \(dx\) at \(x\)

\[f(x)= \frac{P(x\leq X \leq x+dx)}{dx}\]

\(f(x)\) is called the probability \textbf{density} function.

Therefore, the probability of observing \(x\) between \(x\) and \(x+dx\)
is given by

\[P(x\leq X \leq x+dx)= f(x) dx\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-random-variable-3}{%
\section{Continuous random variable}\label{continuous-random-variable-3}}

\textbf{Definition}

For a continuous random variable \(X\), a \textbf{probability density} function is such that

The function is positive:

\begin{itemize}
\tightlist
\item
  \(f(x) \geq 0\)
\end{itemize}

The probability of observing a value within an interval is the \textbf{area under the curve}:

\begin{itemize}
\tightlist
\item
  \(P(a\leq X \leq b)=\int_{a}^{b} f(x) dx\)
\end{itemize}

The probability of observing \textbf{any} value is 1:

\begin{itemize}
\tightlist
\item
  \(\int_{-\infty}^{\infty} f(x) dx = 1\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-random-variable-4}{%
\section{Continuous random variable}\label{continuous-random-variable-4}}

\begin{itemize}
\item
  The probability density function is a step forward in the abstraction of probabilities: we add the continuous limit (\(dx \rightarrow 0\)).
\item
  All the properties of probabilities are translated in terms of densities (\(\sum \rightarrow \int\)).
\item
  Assignment of probabilities to a random variable can be done with equiprobability (classical) arguments.
\item
  Densities are mathematical quantities some will map to experiments some will not. \emph{Which density will map best to my experiment?}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{total-area-under-the-curve}{%
\section{Total area under the curve}\label{total-area-under-the-curve}}

Example: take the \textbf{probability density} that may describe the random variable that measures where a raindrop falls in a rain gutter of length \(100cm\).

\[
    f(x)= 
\begin{cases}
    \frac{1}{100},& \text{if } x\in (0,100)\\
    0,& otherwise 
\end{cases}
\]

Then the probability of \textbf{any} observation is the total \textbf{area under the curve}

\(P(-\infty\leq X \leq \infty)= \int_{-\infty}^{\infty} f(x) dx = 100*0.01= 1\)

\includegraphics{_main_files/figure-latex/unnamed-chunk-41-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{area-under-the-curve}{%
\section{Area under the curve}\label{area-under-the-curve}}

The probability of observing \(x\) in an interval is the \textbf{area under the curve} within the interval

\begin{itemize}
\tightlist
\item
  \(P(20 \leq X \leq 60) = \int_{20}^{60} f(x) dx = (60-20)*0.01=0.4\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-42-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{area-under-the-curve-1}{%
\section{Area under the curve}\label{area-under-the-curve-1}}

In general \(f(x)\) should satisfy:

\begin{itemize}
\tightlist
\item
  \(0 \leq P(a \leq X \leq b) = \int_{a}^{b} f(x) dx \leq 1\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-43-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-distribution-2}{%
\section{Probability distribution}\label{probability-distribution-2}}

The probability accumulated up to \(b\) is defined by the probability distribution \(F\)

\begin{itemize}
\tightlist
\item
  \(F(b) = P(X \leq b)=\int_{-\infty}^bf(x)dx\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-44-1.pdf}

The probability accumulated up to \(a\) is

\begin{itemize}
\tightlist
\item
  \(F(a) = P(X \leq a)\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-45-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-distribution-3}{%
\section{Probability distribution}\label{probability-distribution-3}}

The probability between \(a\) and \(b\) is defined by the probability distribution \(F\)

\begin{itemize}
\tightlist
\item
  \(P(a\leq X \leq b) = \int_a^b f(x)dx=F(b)-F(a)\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-46-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-distribution-4}{%
\section{Probability distribution}\label{probability-distribution-4}}

The probability distribution of a continuous random variable is defined as\\
\(F(a)=P(X\leq a) =\int_{-\infty} ^a f(x)dx\)

with the properties that:

It is between \(0\) and \(1\):

\begin{itemize}
\tightlist
\item
  \(F(-\infty)= 0\) and \(F(\infty)=1\)
\end{itemize}

It always increases:

\begin{itemize}
\tightlist
\item
  if \(a\leq b\) then \(F(a)\leq F(b)\)
\end{itemize}

It can be used to compute probabilities:

\begin{itemize}
\tightlist
\item
  \(P(a \leq X \leq b)=F(b)-F(a)\)
\end{itemize}

It recovers the probability density:

\begin{itemize}
\tightlist
\item
  \(f(x)=\frac{dF(x)}{dx}\)
\end{itemize}

We use \textbf{probability distributions} to \textbf{compute probabilities} of a random variable with intervals

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-distribution-5}{%
\section{Probability distribution}\label{probability-distribution-5}}

For the uniform density function:

\[
    f(x)= 
\begin{cases}
    \frac{1}{100},& \text{if } x\in (0,100)\\
    0,& otherwise 
\end{cases}
\]

The probability distribution is

\[
    F(a)= 
\begin{cases}
    0,& a \leq 0 \\
    \frac{a}{100},& \text{if } a\in (0,100)\\
    1, & 10 \leq a \\
    \\
\end{cases}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-graphics}{%
\section{Probability graphics}\label{probability-graphics}}

The probability \(P(20<X<60)\) is the \emph{area} under the \textbf{density} curve

\includegraphics{_main_files/figure-latex/unnamed-chunk-47-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-graphics-1}{%
\section{Probability graphics}\label{probability-graphics-1}}

The probability \(P(20<X<60)\) is the \emph{difference} in \textbf{distribution} values

\includegraphics{_main_files/figure-latex/unnamed-chunk-48-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-1}{%
\section{Mean}\label{mean-1}}

As in the discrete case, the \textbf{mean} measures the center of the distribution

\textbf{Definition}

Suppose \(X\) is a continuous random variable with probability \textbf{density} function \(f(x)\). The mean or expected value of \(X\), denoted as \(\mu\) or \(E(X)\), is

\[\mu=E(X)=\int_{-\infty}^\infty x f(x) dx\]

It is the continuous version of the center of mass.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-2}{%
\section{Mean}\label{mean-2}}

\[
    f(x)= 
\begin{cases}
    \frac{1}{100},& \text{if } x\in (0,100)\\
    0,& otherwise 
\end{cases}
\]

\(E(X)=50\)

\includegraphics{_main_files/figure-latex/unnamed-chunk-49-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{variance-1}{%
\section{Variance}\label{variance-1}}

As in the discrete case, the variance measures the dispersion about the mean

\textbf{Definition}

Suppose \(X\) is a continuous random variable with probability density function \(f(x)\). The variance of \(X\), denoted as \(\sigma^2\) or \(V(X)\), is

\[\sigma^2=V(X)=\int_{-\infty}^\infty (x-\mu)^2 f(x) dx\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{functions-of-x-1}{%
\section{\texorpdfstring{Functions of \(X\)}{Functions of X}}\label{functions-of-x-1}}

\textbf{Definition}

For any function \(h\) of a random variable \(X\), with mass function \(f(x)\), its expected value is given by

\[E[h(X)]= \int_{-\infty}^{\infty} h(x) f(x)dx\]

And we have the same properties as in the discrete case

\begin{itemize}
\item
  The mean of a linear function is the linear function fo the mean: \[E(a\times X +b)= a\times E(X) +b\] for \(a\) and \(b\) scalars.
\item
  The variance of a linear function of \(X\) is:\[V(a\times X +b)= a^2\times V(X)\]
\item
  The variance about the origin is the variance about the mean plus the mean squared: \[E(X^2)=V(X)+E(X)^2\]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-8}{%
\section{Example}\label{example-8}}

\begin{itemize}
\tightlist
\item
  for the probability density
\end{itemize}

\[
    f(x)= 
\begin{cases}
    \frac{1}{100},& \text{if } x\in (0,100)\\
    0,& otherwise 
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  compute the mean
\item
  compute variance using \(E(X^2)=V(X)+E(X)^2\)
\item
  compute \(P(\mu-\sigma\leq X \leq \mu+\sigma)\)
\item
  What are the first and third quartiles?
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-50-1.pdf}

\hypertarget{discrete-probability-models}{%
\chapter{Discrete Probability Models}\label{discrete-probability-models}}

\hypertarget{objective-5}{%
\section{Objective}\label{objective-5}}

Discrete probability models:

\begin{itemize}
\tightlist
\item
  Uniform and Bernoulli probability functions
\item
  Binomial and negative binomial probability functions
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-mass-function}{%
\section{Probability mass function}\label{probability-mass-function}}

A probability mass function of a \textbf{discrete random variable} \(X\) with possible values \(x_1 , x_2 , .. , x_M\) is \textbf{any function} such that

Positive:

\begin{itemize}
\tightlist
\item
  \(f(x_i)\geq 0\)
\end{itemize}

\emph{Allow us to compute probabilities:}

\begin{itemize}
\tightlist
\item
  \(f(x_i)=P(X=x_i)\)
\end{itemize}

The probability of any outcome is \(1\)

\begin{itemize}
\tightlist
\item
  \(\sum_{i=1}^M f(x_i)=1\)
\end{itemize}

\textbf{Properties:}

Central tendency:

\begin{itemize}
\tightlist
\item
  \(E(X)= \sum_{i=1}^M x_i f(x_i)\)
\end{itemize}

Dispersion:

\begin{itemize}
\tightlist
\item
  \(V(X)= \sum_{i=1}^M (x_i-\mu)^2 f(x_i)\)
\end{itemize}

They are abstract objects with general properties that may or may not \textbf{describe} a natural or engineered process.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-model}{%
\section{Probability model}\label{probability-model}}

A \textbf{probability model} is a probability mass function that may represent the probabilities of a random experiment.

\textbf{Examples:}

\begin{itemize}
\item
  \(f(x)=P(X=x)=1/6\) represents the probability of the outcomes of \textbf{one} throw of a dice.
\item
  The probability mass function
\end{itemize}

\begin{longtable}[]{@{}cc@{}}
\toprule
\(X\) & \(f(x)\) \\
\midrule
\endhead
\(-2\) & \(1/8\) \\
\(-1\) & \(2/8\) \\
\(0\) & \(2/8\) \\
\(1\) & \(2/8\) \\
\(2\) & \(1/8\) \\
\bottomrule
\end{longtable}

Represents the probability of drawing \textbf{one} ball from an urn where there are two balls per label: \(-1, 0, 1\) and one ball per label: \(-2, 2\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{parametric-models}{%
\section{Parametric models}\label{parametric-models}}

When we perform a random experiment and \textbf{do not} know the probabilities of the outcomes:

\begin{itemize}
\tightlist
\item
  We can always formulate the model given by the relative frequencies: \(\hat{P}(X=x_i)=f_i\) ( where \(i=1...M\)).
\end{itemize}

We need to find \(M\) numbers each depending on \(N\).

In many cases:

\begin{itemize}
\tightlist
\item
  We can formulate probability functions \(f(x)\) that depend on \textbf{very few} numbers only.
\end{itemize}

\textbf{Example:}

A random experiment with \(M\) equally likely outcomes has a probability mass function:
\[f(x)=P(X=x)=1/M\]

We only need to know \(M\).

The numbers we \textbf{need to know} to fully determine a probability function are called \textbf{parameters}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{uniform-distribution-one-parameter}{%
\section{Uniform distribution (one parameter)}\label{uniform-distribution-one-parameter}}

\textbf{Definition}
A random variable \(X\) with outcomes \(\{1,...M\}\) has a discrete \textbf{uniform distribution} if all its \(M\) outcomes have the same probability

\[f(x)=\frac{1}{M}\]

With mean and variance:

\(E(X)= \frac{M+1}{2}\)

\(V(X)= \frac{M^2-1}{12}\)

Note: \(E(X)\) and \(V(X)\) are also \textbf{parameters}. If we know any of them then we can fully determine the distribution.

\[f(x)=\frac{1}{2E(X)-1}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{uniform-distribution}{%
\section{Uniform distribution}\label{uniform-distribution}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-51-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{uniform-distribution-two-parameters}{%
\section{Uniform distribution (two parameters)}\label{uniform-distribution-two-parameters}}

Let's introduce a new uniform probability model with \textbf{two parameters}: The minimum and maximum outcomes.

If the random variable takes values in \(\{a, a+1, ...b\}\), where \(a\) and \(b\) are integers and all the outcomes are equally probable then

\[f(x)=\frac{1}{b-a+1}\]

as \(M=b-a+1\).

\begin{itemize}
\tightlist
\item
  We then say that \(X\) distributes uniformly between \(a\) and \(b\) and write
\end{itemize}

\[X \rightarrow Unif(a,b)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{uniform-distribution-two-parameters-1}{%
\section{Uniform distribution (two parameters)}\label{uniform-distribution-two-parameters-1}}

\textbf{Example:}

What is the probability of observing a child of a particular age in a primary school (if all classes have the same amount of children)?

From the experiment we know: \(a=6\) and \(b=11\) then

\[X \rightarrow Unif(a=6, b=11)\] that is

\[f(x)=\frac{1}{6}\] for \(x\in \{6,7,8,9,10,11\}\), and \(0\) otherwise

\includegraphics{_main_files/figure-latex/unnamed-chunk-52-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{uniform-distribution-1}{%
\section{Uniform distribution}\label{uniform-distribution-1}}

The probability model of a random variable \(X\)

\[f(x)=\frac{1}{b-a+1}\]

for \(x \in \{a, a+1, ...b\}\)

has mean and variance:

\begin{itemize}
\item
  \(E(X)= \frac{b+a}{2}\)
\item
  \(V(X)= \frac{(b-a+1)^2-1}{12}\)
\end{itemize}

(Change variables \(X=Y+a-1\), \(y \in \{1,...M\}\))

We can either specify \(a\) and \(b\) or \(E(X)\) and \(V(X)\).

In our example:

\begin{itemize}
\tightlist
\item
  \(E(X)=(11+6)/2=8.5\)
\item
  \(V(X)=(6^2-1)/12=2.916667\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{uniform-distribution-two-parameter}{%
\section{Uniform distribution (two-parameter)}\label{uniform-distribution-two-parameter}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-53-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{parameters-and-models}{%
\section{Parameters and Models}\label{parameters-and-models}}

\begin{itemize}
\item
  A \textbf{model} is a particular function \(f(x)\) that \textbf{describes} our experiment
\item
  If the model is a \textbf{known} function that depends on a few parameters then changing the value of the parameters we produce a \textbf{family of models}
\item
  Knowledge of \(f(x)\) is reduced to the knowledge of the value of the parameters
\item
  Ideally, the model and the parameters are \textbf{interpretable}
\end{itemize}

\emph{Example:}

\textbf{Model}: The data of our experiment is produced by a random process in which each age has the \textbf{same probability} of being observed.

\textbf{Parameters}: \(a\) is the minimum age, \(E(X)\) is the expected age \ldots{} they are \textbf{physical properties} of the experiment.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{parameters-and-models-1}{%
\section{Parameters and Models}\label{parameters-and-models-1}}

\textbf{Example:}

A \textbf{family} of models obtained from two-parameter uniform distributions changing the \textbf{variances} and keeping a constant mean (\(E(X)=8.5\)). It results on \textbf{changing} both \textbf{minimum} and \textbf{maximum} outcomes.

\begin{itemize}
\tightlist
\item
  Note: Only one model makes sense for our experiment (only one model can represent the ages of children in a school).
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-54-1.pdf}

\begin{itemize}
\tightlist
\item
  We can think of \textbf{families} that change only the \textbf{mean}, only the \textbf{minimum}, or only the \textbf{maximum}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bernoulli-trial}{%
\section{Bernoulli trial}\label{bernoulli-trial}}

Let's try to advance from the equal probability case and suppose a model with two outcomes (\(A\) and \(B\)) that have \textbf{unequal} probabilities

\textbf{Examples:}

\begin{itemize}
\item
  Writing down the sex of a patient who goes into an emergency room of a hospital (\(A:male\) and \(B:female\)).
\item
  Recording whether a manufactured machine is defective or not (\(A:defective\) and \(B:fine\)).
\item
  Hitting a target (\(A:success\) and \(B:failure\)).
\item
  Transmitting one pixel correctly (\(A:yes\) and \(B:no\)).
\end{itemize}

In these examples, the probability of outcome \(A\) is usually \textbf{unknown}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bernoulli-trial-1}{%
\section{Bernoulli trial}\label{bernoulli-trial-1}}

We will introduce the probability of an outcome (\(A\)) as the \textbf{parameter} of the model:

\begin{itemize}
\tightlist
\item
  outcome A (success): has probability \(p\) (parameter)
\item
  outcome B (failure): has a probability \(1-p\)
\end{itemize}

Or write, the probability mass function of \(K\) taking values \(\{0, 1\}\) for \(A\) and \(B\)

\[
    f(k)= 
\begin{cases}
    1-p,&  k=0\, (event\, B)\\
    p,& k=1\, (event\, A) 
\end{cases}
\]

or more shortly

\[f(k; p)=p^k(1-p)^{1-k} \]

for \(k=(0,1)\)

We only need to know \(p\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bernoulli-trial-2}{%
\section{Bernoulli trial}\label{bernoulli-trial-2}}

A Bernoulli variable \(K\) with outcomes \(\{0, 1\}\) has a probability mass function

\[f(k; p)=p^k(1-p)^{1-k} \]
With mean and variance:

\begin{itemize}
\item
  \(E(K)=p\)
\item
  \(V(K)=(1-p)p\)
\end{itemize}

Note:

\begin{itemize}
\item
  The probability of the outcome \(A\) is the parameter \(p\)
  which is the same as \(f(0)=P(X=0)\).
\item
  As \(p\) is usually \textbf{unknown} we typically estimated it by the relative frequency (more on this in the inference sections): \(\hat{p}=f_A=\frac{n_A}{N}\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bernoulli-trial-3}{%
\section{Bernoulli trial}\label{bernoulli-trial-3}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-55-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution}{%
\section{Binomial distribution}\label{binomial-distribution}}

When we are interested in learning about a particular Bernoulli trial

\begin{itemize}
\item
  We repeat the Bernoulli trial \(N\) times and count how many times we obtained \(A\) (\(n_A\)).
\item
  We define a random variable \(X=n_A\) taking values \(x \in {0,1,...N}\)
\end{itemize}

We now ask for the probability of observing \(x\) events of type \(A\) in the repetition of \(n\) independent Bernoulli trials, when the probability of observing \(A\) is \(p\).

\[P(X=x)=f(x)=?\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{examples-binomial-distribution}{%
\section{Examples: Binomial distribution}\label{examples-binomial-distribution}}

\begin{itemize}
\item
  Writing down the sex of \(n=10\) patients who go into an emergency room of a hospital. What is the probability that \(x=6\) patients are men when \(p=0.9\)?
\item
  Trying \(n=5\) times to hit a target (\(A:success\) and \(B:failure\)). What is the probability that I hit the target \(x=5\) times when I usually hit it \(25\%\) of the times (\(p=0.25\))?
\item
  Transmitting \(n=100\) pixels correctly (\(A:yes\) and \(B:no\)). What is the probability that \(x=2\) pixels are errors, when the probability of error is \(p=0.1\)?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution-1}{%
\section{Binomial distribution}\label{binomial-distribution-1}}

What is the probability of observing \(X=4\) errors when transmitting \(4\) pixels, if the probability of an error is \(p\)?

Consider \(4\) random variables: \(K_1\), \(K_2\), \(K_3\) and \(K_4\) that record whether an error has been made in the \(1^{st}\), \(2^{nd}\), \(3^{rd}\) and \(4^{th}\) pixel.

Then

\begin{itemize}
\item
  \(k_i\) takes values \(\{correct:0; error:1\}\)
\item
  \(X=\sum_{i=1}^4 K_i\) takes values \(\{0,1,2,3,4\}\)
\end{itemize}

Then the probability of observing \(4\) errors is:

\begin{itemize}
\tightlist
\item
  \(P(X=4)=P(1,1,1,1)=p*p*p*p=p^4\) because \(K_i\) are independent.
\end{itemize}

The probability of \(0\) errors is:

\begin{itemize}
\tightlist
\item
  \(P(X=0)=P(0,0,0,0)=(1-p)(1-p)(1-p)(1-p)=(1-p)^4\)
\end{itemize}

The probability of \(3\) errors is:

\(P(X=3)=P(0,1,1,1)+P(1,0,1,1)+P(1,1,0,1)+P(1,1,1,0)\)
\(=4p^3(1-p)^1\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution-2}{%
\section{Binomial distribution}\label{binomial-distribution-2}}

Therefore the probability of \(x\) errors is

\[
    f(x)= 
\begin{cases}
    1*p^0(1-p)^4,&  x=0 \\
    4*p^1(1-p)^3,&  x=1 \\
    6*p^2(1-p)^2,&  x=2 \\
    4*p^3(1-p)^1,&  x=3 \\
    1*p^4(1-p)^0,&  x=4 \\
\end{cases}
\]

or more shortly

\[f(x)=\binom 4 x p^x(1-p)^{4-x}\]
for \(x=0,1,2,3,4\)

where \(\binom 4 x\) is the number of possible outcomes (transmissions of \(4\) pixels) with \(x\) errors.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution-definition}{%
\section{Binomial distribution: Definition}\label{binomial-distribution-definition}}

The binomial probability function is the probability mass function of observing \(x\) outcomes of type \(A\) in \(n\) independent Bernoulli trials, where \(A\) has the same probability \(p\) in each trial.

The function is given by

\(f(x)=\binom n x p^x(1-p)^{n-x}\), \(x=0,1,...n\)

\(\binom n x= \frac{n!}{x!(n-x)!}\) is called \textbf{the binomial coefficient} and gives the number of ways one can obtain \(x\) events of type \(A\) in a set of \(n\).

When a variable \(X\) has a binomial probability function we say it distributes binomially and write

\[X\rightarrow Bin(n,p)\]

where \(n\) and \(p\) are parameters.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution-mean-and-variance}{%
\section{Binomial distribution: Mean and Variance}\label{binomial-distribution-mean-and-variance}}

The mean and variance of \(X\hookrightarrow Bin(n,p)\) are

\begin{itemize}
\item
  \(E(X)=np\)
\item
  \(V(X)=np(1-p)\)
\item
  Since \(X\) is the sum of \(n\) independent Bernoulli variables
\end{itemize}

\(E(X)=E(\sum_{i=1}^n K_i)=np\)

and

\(V(X)=V(\sum_{i=1}^n K_i)=n(1-p)p\)

Example:

\begin{itemize}
\item
  The expected value for the number of errors in the transmission of 4 pixels is \(np=4*0.1=0.4\) when the probability of an error is \(0.1\).
\item
  The variance is \(n(1-p)p=0.36\)
\end{itemize}

\textbf{Remember}: We can specify either the parameters \(n\) and \(p\), or the parameters \(E(X)\) and \(V(X)\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-1-1}{%
\section{Example 1}\label{example-1-1}}

Now let's answer:

\begin{itemize}
\tightlist
\item
  What is the probability of observing \(4\) errors when transmitting \(4\) pixels, if the probability of an error is \(0.1\)?
\end{itemize}

Since we are repeating a Bernoulli trial \(n=4\) times and counting the number of events of type \(A\) (errors), when \(P(A)=p=0.1\) then

\[X \rightarrow Bin(n=4, p=0.1)\]
That is \[f(x)=\binom 4 x 0.1^x(1-0.1)^{4-x}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-1-2}{%
\section{Example 1}\label{example-1-2}}

\begin{itemize}
\tightlist
\item
  We want to compute:
\end{itemize}

\(P(X=4)=f(4)=\binom 4 4 0.1^4 0.9^{0}=10^{-4}\)

In R dbinom(4,4,0.1)

\begin{itemize}
\tightlist
\item
  We can also compute:
\end{itemize}

\(P(X=2)=\binom 4 2 0.1^2 0.9^2=0.0486\)

In R dbinom(2,4,0.1)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-2-1}{%
\section{Example 2}\label{example-2-1}}

\begin{itemize}
\tightlist
\item
  What is the probability of observing at least \(8\) voters of the ruling party in an election poll of size \(10\), if the probability of a positive vote is \(0.9\)
\end{itemize}

For this case

\[X \rightarrow Bin(n=8, p=0.9)\]

That is \[f(x)=\binom {10} x 0.9^x(0.1)^{4-x}\]

We want to compute:
\(P(X\le 8)=F(8)= \sum_{i=1..8} f(x_i)=0.2639011\)

in R pbinom(8,10, 0.9)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution-3}{%
\section{Binomial distribution}\label{binomial-distribution-3}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-56-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{negative-binomial-distribution}{%
\section{Negative binomial distribution}\label{negative-binomial-distribution}}

Now let us imagine that we are interested in counting the well-transmitted pixels before a \textbf{given number} of errors occur. Say we can \textbf{tolerate} \(r\) errors in transmission.

\begin{itemize}
\item
  Experiment: Suppose performing Bernoulli trials until we observe the outcome \(A\) appears \(r\) times.
\item
  Random variable: We count the number of events \(B\)
\item
  Example: What is the probability of observing \(y\) well-transmitted (\(B\)) pixels before \(r\) errors (\(A\))?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{negative-binomial-distribution-1}{%
\section{Negative binomial distribution}\label{negative-binomial-distribution-1}}

Let's first find the probability of one particular transmission with \(y\) number of correct pixels (\(B\)) and \(r\) number of errors (\(A\)).

\((0,0,1,., 0,1,...0,1)\) (there are \(y\) zeros, and \(r\) ones)

We observe \(y\) correct pixels in a total of \(y + r\) trials.

Then

\begin{itemize}
\tightlist
\item
  \(P(0,0,1,., 0,1,...0,1)=p^r(1-p)^y\) (Remember: \(p\) is the probability of error)
\end{itemize}

How many transmissions can have \(y\) correct pixels before \(r\) errors?

Note:

\begin{itemize}
\item
  The last bit is fixed (marks the end of transmission)
\item
  The total number of transmissions with \(y\) number of correct pixels (\(B\)) that we can obtain in \(y + r-1\) trials is: \(\binom {y + r-1} y\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{negative-binomial-distribution-2}{%
\section{Negative binomial distribution}\label{negative-binomial-distribution-2}}

Therefore, the probability of observing \(y\) events of type \(B\) before \(r\) events of type \(A\) (with probability \(p\)) is

\[P(Y=y)=f(y)=\binom {y+r-1} y p^r(1-p)^y\]

for \(y=0,1,...\)

We then say that \(Y\) follows a negative binomial distribution and we write

\[Y\rightarrow NB(r,p)\]

where \(r\) and \(p\) are parameters representing the tolerance and the probability of a single error.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-and-variance-2}{%
\section{Mean and Variance}\label{mean-and-variance-2}}

A random variable with \(Y\rightarrow NB(r,p)\) has

\begin{itemize}
\item
  mean: \(E(Y)= r\frac{1-p}{p}\)
\item
  variance: \(V(Y)= r\frac{1-p}{p^2}\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{geometric-distribution}{%
\section{Geometric distribution}\label{geometric-distribution}}

We call \textbf{geometric distribution} to the negative binomial distribution with \(r=1\)

The probability of observing \(B\) events before observing the \textbf{first} event of type \(A\) is

\[P(Y=y)=f(y)= p(1-p)^y\]

\[Y\rightarrow Geom(p)\]
with mean

\begin{itemize}
\item
  mean: \(E(Y)= \frac{1-p}{p}\)
\item
  variance: \(V(Y)= \frac{1-p}{p^2}\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-9}{%
\section{Example}\label{example-9}}

\begin{itemize}
\item
  A website has three servers.
\item
  One server operates at a time and only when a request fails another server is used.
\item
  If the probability of failure for a request is known to be \(p=0.0005\) then
\item
  what is the expected number of successful requests before the three computers fail?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-10}{%
\section{Example}\label{example-10}}

Since we are repeating a Bernoulli trial until \(r=3\) events of type \(A\) (failure) are observed (each with \(P(A)=p=0.0005\)) and are counting the number of events of type \(B\) (successful requests) then

\[Y \rightarrow NB(r=3, p=0.0005)\]

Therefore, the expected number of requests before the system fails is:

\(E(Y)=r\frac{1-p}{p}=3\frac{1-0.0005}{0.0005}=5997\)

\begin{itemize}
\tightlist
\item
  Note that there are actually \(6000\) trials
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-11}{%
\section{Example}\label{example-11}}

What is the probability of dealing with at most \(5\) successful requests before the system fails?

Recall the cumulative function distribution \(F(y)=P(Y\leq 5)\)

\(F(5)=P(Y\leq 5)=\Sigma_{y=0}^5 f(y)\)

\(=\sum_{y=0}^5\binom {y+2} y 0.0005^r0.9995^y\)

\(=\binom {2} 0 0.0005^3 0.9995^0 +\binom {3} 1 0.0005^3 0.9995^1\)

\(+\binom {4} 2 0.0005^3 0.9995^2 +\binom {5} 3 0.0005^3 0.9995^3\)

\(+\binom {6} 4 0.0005^3 0.9995^4 +\binom {7} 5 0.0005^3 0.9995^5\)

\(= 6.9\times 10^{-9}\)

In R pnbinom(5,3,0.0005)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{examples}{%
\section{Examples}\label{examples}}

With the negative binomial probability function:

\[f(y)=\binom {y+r-1} y p^r (1-p)^y\]

We can now answer questions like:

\begin{itemize}
\tightlist
\item
  What is the probability of observing \(10\) correct pixels before \(2\) errors, if the probability of an error is \(0.1\)?
\end{itemize}

\(f(10; r=2, p=0.1)=0.03835463\)

in R dnbinom(10, 2, 0.1)

\begin{itemize}
\tightlist
\item
  What is the probability that \(2\) girls enter the class before \(4\) boys if the probability that a girl enters is \(0.5\)?
\end{itemize}

\(f(2; r=4, p=0.5)=0.15625\)

in R dnbinom(2, 4, 0.5)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{negative-binomial-distribution-3}{%
\section{Negative binomial distribution}\label{negative-binomial-distribution-3}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-57-1.pdf}

\hypertarget{poisson-and-exponential-models}{%
\chapter{Poisson and Exponential Models}\label{poisson-and-exponential-models}}

\hypertarget{objective-6}{%
\section{Objective}\label{objective-6}}

Discrete probability model:

\begin{itemize}
\tightlist
\item
  Poisson
\end{itemize}

Continuous probability model:

\begin{itemize}
\tightlist
\item
  Exponential
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{discrete-probability-models-1}{%
\section{Discrete probability models}\label{discrete-probability-models-1}}

We are building up more complex models from simple ones:

\textbf{Uniform}: Classical interpretation of probability
\(\downarrow\)
\textbf{Bernoulli}: Introduction of a \textbf{parameter} \(p\) (family of models)
\(\downarrow\)
\textbf{Binomial}: \textbf{Repetition} of a random experiment (\(n\)-times Bernoulli trials)
\(\downarrow\)
\textbf{Poisson}: Repetition of random experiment within a continuous interval, having \textbf{no control} on when/where the Bernouilli trial occurs.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{counting-events}{%
\section{Counting events}\label{counting-events}}

Imagine that we are observing events that \textbf{depend} on time or distance \textbf{intervals}.

\begin{itemize}
\tightlist
\item
  cars arriving at a traffic light
\item
  getting messages on your mobile phone
\item
  impurities occurring at random in a copper wire
\end{itemize}

Suppose that the events are outcomes of \textbf{independent} Bernoulli trials each appearing randomly on a continuous interval, and we want to \textbf{count} them.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{counting-events-1}{%
\section{Counting events}\label{counting-events-1}}

What is the probability of observing \(X\) events in an interval's unit (time or distance)?

Imagine that some impurities in a copper wire deposit randomly along a wire

\begin{itemize}
\tightlist
\item
  at each centimeter, you would count an average of \(\lambda=10/cm\).
\item
  divide the centimeter into micrometers (\(0.0001cm\))
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution}{%
\section{Poisson distribution}\label{poisson-distribution}}

micrometers are small enough so

\begin{itemize}
\tightlist
\item
  either there is or there is not an impurity in each micrometer
\item
  each micrometer can be considered a \textbf{Bernoulli trial}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution-1}{%
\section{Poisson distribution}\label{poisson-distribution-1}}

The probability of observing \(X\) impurities in \(n=10,000\mu\) (1cm) approximately follows a binomial distribution

\(P(X=x)=\binom n x p^x(1-p)^{n-x}\)

where \(p\) is the probability of finding an impurity in a micrometer.

Remember that
\(E(X)=np\)
so for \(\lambda=np\) (average number of impurities per 1cm), we can write

\[P(X=x)=\binom n x \big(\frac{\lambda}{n}\big)^x(1-\frac{\lambda}{n})^{n-x}\]

\begin{itemize}
\tightlist
\item
  There \textbf{could} still be two impurities in a micrometer so we need to increase the partition of the wire and \(n \rightarrow \infty\).
\end{itemize}

Then in the limit:

\[P(X=x)= \frac{e^{-\lambda}\lambda^x}{x!}\]

Where \(\lambda\) is constant because it is the density of impurities per centimeter, a \textbf{physical property} of the system.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution-derivation-details}{%
\section{Poisson distribution: Derivation details}\label{poisson-distribution-derivation-details}}

For \(P(X=x)=\binom n x \big(\frac{\lambda}{n}\big)^x(1-\frac{\lambda}{n})^{n-x}\)

in the limit (\(n \rightarrow \infty\))

\begin{itemize}
\tightlist
\item
  \(\frac{1}{n^x}\binom n x =\frac{1}{n^x}\frac{n!}{x! (n-x)!}=\frac{(n-x)!(n-x+1)...(n-1)n}{n^x x! (n-x)!}=\frac{n(n-1)..(n-x+1)}{n^x x!} \rightarrow \frac{1}{x!}\)
\item
  \((1-\frac{\lambda}{n})^{n} \rightarrow e^{-\lambda}\) (definition of exponential)
\item
  \((1-\frac{\lambda}{n})^{-x} \rightarrow 1\)
\end{itemize}

Therefore
\(P(X=x)= \frac{e^{-\lambda}\lambda^x}{x!}\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution-2}{%
\section{Poisson distribution}\label{poisson-distribution-2}}

\textbf{Definition}

Given

\begin{itemize}
\tightlist
\item
  an interval in the real numbers
\item
  counts occur at random in the interval
\item
  the average number of counts on the interval is known (\(\lambda\))
\item
  if one can find a small regular partition of the interval such that each of them can be considered Bernoulli trials
\end{itemize}

Then\ldots{}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution-3}{%
\section{Poisson distribution}\label{poisson-distribution-3}}

\textbf{Definition}

The random variable \(X\) that counts events across the interval is a \textbf{Poisson} variable with probability mass function\\
\[f(x)= \frac{e^{-\lambda}\lambda^x}{x!}, \lambda>0\]

\textbf{Properties:}

\begin{itemize}
\tightlist
\item
  mean \(E(X)= \lambda\)
\item
  variance \(V(X)= \lambda\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution-4}{%
\section{Poisson distribution}\label{poisson-distribution-4}}

With the Poisson probability function:

\[f(x)= \frac{e^{-\lambda}\lambda^x}{x!}\] for \(x \in \{0, 1, ...\}\)

We can now answer questions like:

\begin{itemize}
\tightlist
\item
  What is the probability of receiving 4 emails in an hour, when the average number of emails in \textbf{two} hours is \(1\)?
\end{itemize}

\(f(4; \lambda=0.5)=0.001579507\)

in R dpois(2,0.5)

\begin{itemize}
\tightlist
\item
  What is the probability of counting at least \(10\) cars arriving at a road toll in a minute, when the average number of cars that arrive at the toll in a minute is \(5\);
  \(P(X \leq 10)=F(10; \lambda=5)=0.9863047\)?
\end{itemize}

in R ppois(10,5)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution-5}{%
\section{Poisson distribution}\label{poisson-distribution-5}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-58-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-probability-models}{%
\section{Continuous probability models}\label{continuous-probability-models}}

Continuous probability models are probability density functions \(f(x)\) of a continuous random variables that we \textbf{believe} describe real random experiments.

Definition:

Positive:

\begin{itemize}
\tightlist
\item
  \(f(x) \geq 0\)
\end{itemize}

\emph{Allows us to compute probabilities using the area under the curve:}

\begin{itemize}
\tightlist
\item
  \(P(a\leq X \leq b)=\int_{a}^{b} f(x) dx\)
\end{itemize}

The probability of any value is \(1\):

\begin{itemize}
\tightlist
\item
  \(\int_{-\infty}^{\infty} f(x) dx = 1\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-density}{%
\section{Exponential density}\label{exponential-density}}

Let's go back to the Poisson probability for the number of events (\(k\)) in an interval

\[f(k)=\frac{e^{-\lambda}\lambda^k}{k!}, \lambda>0\]

\begin{itemize}
\item
  Let's now consider only two consecutive (length/time) events
\item
  the distance between them is a \textbf{continuous} random variable.
\end{itemize}

We can ask for the probability that the first event is at distance \(X\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-density-1}{%
\section{Exponential density}\label{exponential-density-1}}

The probability of \(0\) counts \textbf{if} an interval has unit \(x\) is

\[f(0|x)=\frac{e^{-x\lambda}x\lambda^0}{0!}\]

or

\[f(0|x)=e^{-x\lambda}\]

We can treat this as the conditional probability of \(0\) events in a distance \(x\): \(f(K=0|X=x)\) and apply the Bayes theorem to reverse it:

\[f(x|0)=C f(0|x)=C e^{-x\lambda}\]

So we can calculate the \textbf{probability of observing a distance} a distance \(x\) with \(0\) counts (this is the distance between any two events or the distance until the first event).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-density-2}{%
\section{Exponential density}\label{exponential-density-2}}

In a Poisson process with parameter \(\lambda\) the probability of waiting a distance/time \(X\) between two counts is given by the \textbf{probability density}

\[f(x)= C e^{-x\lambda}\]

\begin{itemize}
\item
  \(C\) is a constant that ensures: \(\int_{-\infty}^{\infty} f(x) dx =1\)
\item
  by integration \(C=\lambda\)
\end{itemize}

Therefore

\[f(x)=\lambda e^{-\lambda x}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-density-3}{%
\section{Exponential density}\label{exponential-density-3}}

An exponential random variable \(X\) has a probability density

\[f(x)=\lambda e^{-\lambda x}, x\geq 0\]

\textbf{Properties:}

\begin{itemize}
\tightlist
\item
  Mean: \(E(X)=\frac{1}{\lambda}\)
\item
  Variance: \(V(Y)=\frac{1}{\lambda^2}\)
\end{itemize}

Where \(\lambda\) is its single parameter, known as a \textbf{decay rate}.

\textbf{Note:} The exponential model is a general model. It can describe the time/length until the first count in a Poisson process of the size of a whole made by a drill.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-density-4}{%
\section{Exponential density}\label{exponential-density-4}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-59-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-distribution}{%
\section{Exponential Distribution}\label{exponential-distribution}}

In a Poisson process: ¿What is the probability of observing an interval \textbf{smaller} than size \(a\) until the first count?

Remember that this probability \(F(a)=P(X \leq a)\) is the probability density

\[F(a)=\lambda \int_\infty^a e^{-x\lambda}dx=1-e^{-a\lambda}\]

\begin{itemize}
\tightlist
\item
  ¿What is the probability of observing an interval \textbf{larger} than size \(a\) until the first event?
\end{itemize}

\[P(X > a)=1- P(X \leq a)= 1- F(a) = e^{-a\lambda}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-distribution-1}{%
\section{Exponential Distribution}\label{exponential-distribution-1}}

With the exponential density function:

\[f(x)=\lambda e^{-\lambda x}\]

We can answer questions like:

\begin{itemize}
\tightlist
\item
  What is the probability that we have to wait for a bus for more than \(1\) hour when on average there are two buses per hour?
\end{itemize}

\[P(X > 1)=1-P(X \le 1) = 1-F(1,\lambda=2)=0.1353353\]
in R 1-pexp(1,2)

\begin{itemize}
\tightlist
\item
  What is the probability of having to wait less than \(2\) seconds to detect one particle when the radioactive decay rate is \(2\) particles each second; \(F(2,\lambda=2)\)
\end{itemize}

\[P(X \le 2)=F(2,\lambda=2)=0.9816844\]

in R pexp(2,2)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-distribution-2}{%
\section{Exponential Distribution}\label{exponential-distribution-2}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-60-1.pdf}

The median \(x_m\) is such that \(F(x_m)=0.5\). That is \(x_m=\frac{\log(2)}{\lambda}\)

\hypertarget{normal-distribution}{%
\chapter{Normal Distribution}\label{normal-distribution}}

\hypertarget{objective-7}{%
\section{Objective}\label{objective-7}}

Continuous probability model:

\begin{itemize}
\tightlist
\item
  Normal distribution
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continuous-probability-models-1}{%
\section{Continuous probability models}\label{continuous-probability-models-1}}

Continuous probability models are probability density functions \(f(x)\) of a continuous random variables that we \textbf{believe} describe real random experiments.

Definition:

Positive:

\begin{itemize}
\tightlist
\item
  \(f(x) \geq 0\)
\end{itemize}

\emph{Allows us to compute probabilities using the area under the curve:}

\begin{itemize}
\tightlist
\item
  \(P(a\leq X \leq b)=\int_{a}^{b} f(x) dx\)
\end{itemize}

The probability of any value is \(1\):

\begin{itemize}
\tightlist
\item
  \(\int_{-\infty}^{\infty} f(x) dx = 1\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-density}{%
\section{Normal density}\label{normal-density}}

In 1801 Gauss analyzed the orbit of Ceres (large asteroid between Mars and Jupiter).

\begin{itemize}
\tightlist
\item
  People suspected it was a new planet.
\item
  The measurements had errors.
\item
  He was interested in finding how the observations were distributed so he could find the most probable orbit.
\item
  He wanted to predict where astronomers should point their telescopes to find it a few months after it had passed behind the Sun.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-density-1}{%
\section{Normal density}\label{normal-density-1}}

Errors due to measurement.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-density-2}{%
\section{Normal density}\label{normal-density-2}}

He assumed that

\begin{itemize}
\tightlist
\item
  small errors were more likely than large errors
\item
  error at a distance \(-\epsilon\) or \(\epsilon\) from the most likely measurement were equally likely
\item
  the most \textbf{likely} altitude of Ceres at a given time in the sky was the \textbf{average} of multiple altitude measurements at that latitude.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-density-3}{%
\section{Normal density}\label{normal-density-3}}

That was enough to show that the random deviations \(y\) \textbf{from the orbit} distributed like

\[f(y)=\frac{h}{\sqrt{\pi}}e^{-h^2y^2}\]

*The evolution of the Normal distribution, Saul Stahl, Mathematics Magazine, 2006.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-density-4}{%
\section{Normal density}\label{normal-density-4}}

Let's write the distribution of errors

\[f(y)=\frac{h}{\sqrt{\pi}}e^{-h^2y^2}\]

for the errors of measurements from the horizon \(X\) then \(y=x-x_0\)

\[f(x)=\frac{h}{\sqrt{\pi}}e^{-h^2(x-x_0)^2}\]

\begin{itemize}
\tightlist
\item
  The \textbf{mean} of this probability density is:
\end{itemize}

\(E(X)=\mu=x_0\), that represents the \textbf{true} position of Ceres from the horizon (property of the physical system).

\begin{itemize}
\tightlist
\item
  The \textbf{variance} is:
\end{itemize}

\(V(X)=\sigma^2=\frac{1}{2h^2}\), that represents the dispersion of the error in the observations (property of the measurement system).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{definition}{%
\section{Definition}\label{definition}}

A random variable \(X\) defined in the real numbers has a \textbf{Normal} density if it takes the form

\[f(x; \mu, \sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, x \in {\Bbb R}\]

with mean and variance:

\begin{itemize}
\tightlist
\item
  \(E (X) = \mu\)
\item
  \(V (X) = \sigma^2\)
\end{itemize}

\(\mu\) and \(\sigma\) are the \textbf{two parameters} that fully describe the normal density function and their \textbf{interpretation} depends on the random experiment.

When \(X\) follows a Normal density, i.e.~distributes normally, we write

\[X\rightarrow N(\mu,\sigma^2)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-probability-density-gaussian}{%
\section{Normal probability density (Gaussian)}\label{normal-probability-density-gaussian}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-61-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-1}{%
\section{Normal distribution}\label{normal-distribution-1}}

The probability distribution of the Normal density:

\[F_{normal}(a)=P(Z \leq a)\]

is the \textbf{error} function defined by the area under the curve from \(-\infty\) to \(a\)

\[F_{normal}(a)=\int_{-\infty}^{a}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx\]
The function is found in most computer programs.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-2}{%
\section{Normal distribution}\label{normal-distribution-2}}

When \[X \rightarrow N(\mu, \sigma^2)\]

We can ask questions like:

\begin{itemize}
\tightlist
\item
  What is the probability that a woman in the population is at most \(150cm\) tall if women have a mean height of \(165cm\) with standard deviation of \(8cm\)?
\end{itemize}

\(P(X\le 150)=F(150, \mu=165, \sigma=8)=0.03039636\)

in R pnorm(150, 165, 8)

\begin{itemize}
\tightlist
\item
  What is the probability that a woman's height in the population is between \(165cm\) and \(170cm\)?
\end{itemize}

\(P(165 \le X \le 170)=F(170, \mu=165, \sigma=8)-F(165, \mu=165, \sigma=8)=0.2340145\)

in R pnorm(170, 165, 8)-pnorm(165, 165, 8)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-3}{%
\section{Normal distribution}\label{normal-distribution-3}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-62-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-4}{%
\section{Normal distribution}\label{normal-distribution-4}}

\begin{itemize}
\tightlist
\item
  the mean \(\mu\) is also the median as it splits the measurements in two
\item
  \(x\) values that fall farther than 2\(\sigma\) are considered \textbf{rare} \(5\%\)
\item
  \(x\) values that fall farther than 3\(\sigma\) are considered \textbf{extremely rare} \(0.2\%\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-5}{%
\section{Normal distribution}\label{normal-distribution-5}}

We can define the limits of \textbf{common observations} for the distribution of women's height in the population.

\begin{itemize}
\tightlist
\item
  \(P(165-8 \leq X \leq 165-8)=P(157 \leq X \leq 173)=0.68\)
\item
  \(P(165-2 \times 8 \leq X \leq 165-2\times 8)=P(149 \leq X \leq 181)=0.95\)
\item
  \(P(165-3 \times 8 \leq X \leq 165-3\times 8)=P(141 \leq X \leq 189)=0.997\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-63-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-normal-density}{%
\section{Standard normal density}\label{standard-normal-density}}

Let's change variables to a \textbf{standardized variable}

\[Z=\frac{X-\mu}{\sigma}\]

in the density

\[f(x; \mu, \sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, x \in {\Bbb R}\]

replacing \(x=\sigma z+\mu\) and \(dx=\sigma dz\) in the probability expression we have

\(P(x\leq X \leq x +dx)=P(z\leq Z \leq z +dz)\)
\[=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx\] \[=\frac{1}{ \sqrt{2\pi}}e^{-\frac{z^2}{2}} dz\]

we obtain the \textbf{standardized} form of the normal density.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-normal-density-1}{%
\section{Standard normal density}\label{standard-normal-density-1}}

\textbf{Definition}

A random variable \(Z\) defined in the real numbers has a \textbf{standard} density if it takes the form

\[f(z)=\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}} dz,z \in {\Bbb R}\]

with mean and variance

\begin{itemize}
\item
  \(E (X) = 0\)
\item
  \(V (X) =1\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-normal-density-2}{%
\section{Standard normal density}\label{standard-normal-density-2}}

The standard density:

\[f(z)=\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}} dz,z \in {\Bbb R}\]

\begin{itemize}
\item
  is the normal density \(N(\mu=0,\sigma^2=1)\)
\item
  any normally distributed variable \(X\) can be transformed to a variable \(Z\)
\end{itemize}

\[Z=\frac{x-\mu}{\sigma}\]

that follows a standard distribution:

\[Z \rightarrow N(0,1)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-6}{%
\section{Normal distribution}\label{normal-distribution-6}}

All normal densities can be obtained from the standard density with the values of \(\mu\) and \(\sigma\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-distribution}{%
\section{Standard distribution}\label{standard-distribution}}

The probability distribution of the standard density:

\[\phi(a)=F_{standard}(a)=P(Z \leq a)\]

is the \textbf{error} function defined by

\[\phi(a)=\int_{-\infty}^{a} \frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}} dz\]

You can find it in most computer programs

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-normal-density-3}{%
\section{Standard normal density}\label{standard-normal-density-3}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-normal-density-4}{%
\section{Standard normal density}\label{standard-normal-density-4}}

We define the limits of the \textbf{most common observations} for the standard variable

\begin{itemize}
\tightlist
\item
  \(P(-0.67 \leq X \leq 0.67)=0.50\)
\item
  \(P(-1.96 \leq X \leq 1.96)=0.95\)
\item
  \(P(-2.58 \leq X \leq 2.58)=0.99\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-64-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-and-standard-distributions}{%
\section{Normal and standard distributions}\label{normal-and-standard-distributions}}

For any normally distributed variable \(X\), such that

\[X\rightarrow N(\mu, \sigma^2)\]

its distribution \(F(a)=P(X \leq a)\) can be computed from

\[F(a)= \phi \big(\frac{a-\mu}{\sigma}\big)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-7}{%
\section{Normal distribution}\label{normal-distribution-7}}

For computing \(P(a\leq X \leq b)\), we use the property of the probability distributions

\[F(b)-F(a)=P(X\leq b)-P(X\leq a)\]

Let's standardize

\(=P(\frac{X-\mu}{\sigma}\leq \frac{a-\mu}{\sigma})-P(\frac{X-\mu}{\sigma}\leq \frac{b-\mu}{\sigma})\)

\(=P(Z \leq \frac{b-\mu}{\sigma})-P(Z \leq \frac{a-\mu}{\sigma}\big)\)

\(=\phi \big(\frac{b-\mu}{\sigma}\big)-\phi \big(\frac{a-\mu}{\sigma}\big)\)

Then

\[F(b)-F(a)=\phi \big(\frac{b-\mu}{\sigma}\big)-\phi \big(\frac{a-\mu}{\sigma}\big)\]

The probabilities of \textbf{any normal variable} can be obtained from the \textbf{standard distribution}, after standardization (subtract the mean and divide by the standard deviation).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{summary-of-probability-models}{%
\section{Summary of probability models}\label{summary-of-probability-models}}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.22}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.25}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.12}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.10}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.10}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.10}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.10}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\centering
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
X
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
range of x
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f(x)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
E(X)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
V(X)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
R
\end{minipage} \\
\midrule
\endhead
Uniform & integer or real number & \([a, b]\) & \(\frac{1}{n}\) & \(\frac{b+a}{2}\) & \(\frac{(b-a+1)^2-1}{12}\) & rep(1/n, n), dunif(x, a, b) \\
Bernoulli & event A & 0,1 & \(p^x(1-p)^{1-x}\) & \(p\) & \(p(1-p)\) & c(1-p,p) \\
Binomial & \# of A events in \(n\) repetitions of Bernoulli trials & 0,1,\ldots{} & \(\binom n x p^x(1-p)^{n-x}\) & \(np\) & \(np(1-p)\) & dbimon(x,n,p) \\
Negative Binomial for events & \# of B events in Bernoulli repetitions before \(r\) As are observed & 0,1,.. & \(\binom {x+r-1} x p^r(1-p)^x\) & \(\frac{r(1-p)}{p}\) & \(\frac{r(1-p)}{p^2}\) & dnbinom(x,r,p) \\
Poisson & \# of events A in an interval & 0,1, .. & \(\frac{e^{-\lambda}\lambda^x}{x!}\) & \(\lambda\) & \(\lambda\) & dpoiss(x, lambda) \\
Exponential & Interval between two events A & \([0,\infty)\) & \(\lambda e^{-\lambda x}\) & \(\frac{1}{\lambda}\) & \(\frac{1}{\lambda^2}\) & dexp(x, lambda) \\
Normal & measurement with symmetric errors whose most likely value is the average & \((-\infty, \infty)\) & \(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\) & \(\mu\) & \(\sigma^2\) & dnomr(x, mu, sigma) \\
\bottomrule
\end{longtable}

\hypertarget{sampling-distributions}{%
\chapter{Sampling Distributions}\label{sampling-distributions}}

\hypertarget{objective-8}{%
\section{Objective}\label{objective-8}}

Distributions for

\begin{itemize}
\tightlist
\item
  Sample mean
\item
  sample sum
\item
  Sample variance
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-8}{%
\section{Normal distribution}\label{normal-distribution-8}}

When we have a normal random variable

\[X \rightarrow N(x; \mu, \sigma^2)\]

How do we estimate \(\mu\) and \(\sigma^2\)?

\begin{itemize}
\item
  we need to take a \textbf{random sample}
\item
  we need to \textbf{estimate} each parameter
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-12}{%
\section{Example}\label{example-12}}

Imagine a client asking your metallurgical company to sell them \(8\) cables that can carry up to \(96\) Tons; that is \(12\) Tons each.

\begin{itemize}
\tightlist
\item
  You have in \textbf{stock} a set of cables that could do the job.
\end{itemize}

Can you use the cables in stock or do you need to produce new ones?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-13}{%
\section{Example}\label{example-13}}

you take a sample of \(8\) random experiments, each of which consists of loading a cable until it breaks and recording the breaking load.

These are the results: The observation of a \textbf{sample} of size \(8\)

\begin{verbatim}
## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747
\end{verbatim}

\begin{itemize}
\item
  None of them broke at \(12\) Tons.
\item
  There was one that broke at \(12.62747\) Tons.
\end{itemize}

Do you take the risk and sell a random sample of \(8\) cables from your stock?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{_main_files/figure-latex/unnamed-chunk-66-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{random-sample}{%
\section{Random sample}\label{random-sample}}

A \textbf{random sample} of size \(n\) is the \textbf{repetition} of a random experiment \(n\) \textbf{independent} times.

\begin{itemize}
\tightlist
\item
  A random sample is a \(n\)-dimensional \textbf{random variable}
\end{itemize}

\[(X_1, X_2, ... X_n)\]

where \(X_i\) is the \emph{i-th} repetition of the random experiment with comon distribution \(f(x; \theta)\) for any \(i\)

\begin{itemize}
\tightlist
\item
  \textbf{One observation} of a random sample is the set of \(n\) values obtained from the experiments
\end{itemize}

\[(x_1, x_2, ... x_n)\]
Our \textbf{observation} of the sample of \(8\) cables was

\begin{verbatim}
## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-14}{%
\section{Example}\label{example-14}}

We would like to compute \(P(X<12)\).

We are going to \textbf{assume} that the braking point is \textbf{normally distributed}.

\[X \rightarrow N(x; \mu, \sigma^2)\]

\begin{itemize}
\item
  For computing \(P(X <12)\) we need the parameters \(\mu\) and \(\sigma^2\).
\item
  How do we estimate the parameters from the observed sample?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-or-sample-mean}{%
\section{Average or sample mean}\label{average-or-sample-mean}}

\textbf{Definition}

The sample mean (or average) of a \textbf{random sample} of size \(n\) is defined as

\[\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i\]

The average is a \textbf{random variable} that in our \(8\)-size sample took the value

\[\bar{x}_{stock}=13.21\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-as-estimator}{%
\section{Average as estimator}\label{average-as-estimator}}

This number can be used to \textbf{estimate} the unkown parameter \(\mu\) because:

\begin{itemize}
\tightlist
\item
  \(E(\bar{X})=E(X)=\mu\)
\item
  \(V(\bar{X})=\frac{V(X)}{n}=\frac{\sigma^2}{n}\)
\end{itemize}

(since each random experiment in the sample is independent)

as

\begin{itemize}
\tightlist
\item
  \(n \rightarrow \infty\), \(V(\bar{X}) \rightarrow 0\)
\end{itemize}

then

\begin{itemize}
\tightlist
\item
  \(\bar{x}\) \textbf{concentrates closer and closer} to \(\mu\) as \(n\) increases.
\end{itemize}

We can take one value of \(\bar{x}\) as estimation for \(\mu\) or

\[\bar{x}=\hat{\mu}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{outcome-probability-density-and-probability-density-of-the-average}{%
\section{Outcome probability density and probability density of the average}\label{outcome-probability-density-and-probability-density-of-the-average}}

If we \textbf{knew} that the \textbf{true} parameters were \(\mu=13\) and \(\sigma=0.35\) this is what we would see

\includegraphics{_main_files/figure-latex/unnamed-chunk-68-1.pdf} \includegraphics{_main_files/figure-latex/unnamed-chunk-68-2.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-variance-2}{%
\section{Sample variance}\label{sample-variance-2}}

\textbf{Definition}

The \textbf{sample variance} \(S^2\) of a random sample of size \(n\)

\[S^2= \frac{1}{n-1}\sum_{i=1}^n (X_i-\bar{X})^2\]

is the dispersion of the measurements about \(\bar{X}\). In our \(8\)-size sample \(S^2\) took the value

\[s_{stock}^2=\frac{1}{n-1}\sum_{i=1}^n (x_i-\bar{x})^2=0.1275608\]

The expected value of \(S^2\) is

\begin{itemize}
\tightlist
\item
  \(E(S^2)=V(X)=\sigma^2\)
\end{itemize}

and therefore \(S^2\) is

\begin{itemize}
\tightlist
\item
  an estimator of \(V(X)\)
\item
  it also concentrates around \(\sigma^2\) because as \(n \rightarrow \infty\), \(V(\bar{S^2}) \rightarrow 0\)
\end{itemize}

We can take one value of \(s^2\) as estimation for \(\sigma^2\) or

\[s^2=\hat{\sigma}^2\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-variance-3}{%
\section{Sample variance}\label{sample-variance-3}}

\(S^2\) aims to estimate the dispersion of the outcomes about \(\mu\) (the variance)

If we use \(\bar{X}\) as an estimator of \(\mu\) we need to correct for its dispersion (i.e.~mean squared error of \(\bar{X}\)).

The correction is achieved by dividing by \(n-1\) and not \(n\) in the definition of \(S^2\)

For:

\(S_n^2=\frac{1}{n}\sum_{i=1}^n (X_i-\bar{X})^2\)

\(E(S_n^2) = \sigma^2-\frac{\sigma^2}{n} \neq \sigma^2\) (we say that \(S_n^2\) is \textbf{biased})

\includegraphics{_main_files/figure-latex/unnamed-chunk-69-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{fitting-a-model}{%
\section{Fitting a model}\label{fitting-a-model}}

We \textbf{fit a model} when we

\begin{itemize}
\tightlist
\item
  \textbf{estimate} the parameters of the model
\end{itemize}

We also say we \textbf{train} a model (machine learning)

\textbf{Assuming} that

\[X \rightarrow N(x; \mu, \sigma^2)\]

Since we do not know the parameters, we \textbf{plugin} the estimates \(\bar{x}\) and \(s^2\) as the values of \(\mu\)
and \(\sigma^2\)

\[X \rightarrow N(x; \mu=13.21, \sigma^2=0.3571565^2)\]

\includegraphics{_main_files/figure-latex/unnamed-chunk-70-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{prediction}{%
\section{Prediction}\label{prediction}}

We \textbf{predict} the value of an \textbf{outcome} when we compute its \textbf{probability}

What is the probability that the cable breaks at \(12\) Tons?

If we assumed the random variable

\[X \rightarrow N(x; \mu, \sigma^2)\]
We plug in the estimates \(\bar{x}\) and \(s^2\) into the probability distribution

\[P(X \leq 12)= F_{normal}(12; \mu=13.21, \sigma^2=0.1275608)\]

In R pnorm(12,13.21, 0.3571565)\(=0.000352188\)

Given the \textbf{observed} sample, there is an estimated probability of \(0.03\%\) that a single cable will brake at \(12\) Tons.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference}{%
\section{Inference}\label{inference}}

We \textbf{infer} the value of an \textbf{estimator} when we compute its \textbf{probability}

\textbf{Example:}

\begin{itemize}
\tightlist
\item
  Imagine that our cables are certified to break with at a mean load of \(\mu = 13\) Tons with variance \(\sigma^2=0.35^2\).
\item
  Can we claim that we actually produce stronger cables because we obtained \(\bar{x}=13.21\) in our \(8\)-sample average?
\end{itemize}

We need to compute probabilities of \(\bar{X}\).

When we make inferences, we usually ask the question:

How \textbf{confident} are we that the value of the estimator \textbf{is close} to the \textbf{true parameter}?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-mean-distribution}{%
\section{Sample mean distribution}\label{sample-mean-distribution}}

When \(X\) follows a normal distribution \(X \rightarrow N(\mu, \sigma^2)\)

\(\bar{X}\) is normal:

\[\bar{X} \rightarrow N(\mu, \frac{\sigma^2}{n})\]
Then, if we \textbf{know} \(\mu\) and \(\sigma\) we can compute the true \textbf{probabilities of \(\bar{X}\)} using the normal distribution.

The mean and variance of \(\bar{X}\) are

\begin{itemize}
\tightlist
\item
  \(E(\bar{X})=\mu\)
\item
  \(V(\bar{X})=\frac{\sigma^2}{n}\)
\end{itemize}

The errors in estimation are

\begin{itemize}
\tightlist
\item
  bias: \(E(X)-E(\bar{X})=0\)
\item
  standard error: \(se= \frac{\sigma}{\sqrt{n}}\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference-on-the-average}{%
\section{Inference on the average}\label{inference-on-the-average}}

\textbf{Example:}

If we \textbf{know} that for our cables trully distribute as \[X \rightarrow N(\mu=13, \sigma^2=0.35^2)\] then

\[\bar{X} \rightarrow N(13, \frac{0.35^2}{8})\]

\begin{itemize}
\tightlist
\item
  \(E(\bar{X})=13\)
\item
  \(V(X)=\frac{0.35^2}{8}=0.01530169\); \(se=\frac{0.35}{\sqrt{8}}=0.1237\)
\end{itemize}

Our \textbf{observed error} in the estimation of the mean is the difference

\[\bar{x}_{stock}-\mu=13.21-13=0.21\]

We ask: Is this a \textbf{typical} error?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{outcome-probability-density-and-probability-density-of-the-average-1}{%
\section{Outcome probability density and probability density of the average}\label{outcome-probability-density-and-probability-density-of-the-average-1}}

If we \textbf{knew} that the \textbf{true} parameters were \(\mu=13\) and \(\sigma=0.35\) this is the error we would see

\includegraphics{_main_files/figure-latex/unnamed-chunk-71-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probabilities-of-barx}{%
\subsection{\texorpdfstring{Probabilities of \(\bar{X}\)}{Probabilities of \textbackslash bar\{X\}}}\label{probabilities-of-barx}}

\textbf{If} we \textbf{know} that the braking load of our cables \textbf{truly} distribute as \[\bar{X} \rightarrow N(\mu=13, \frac{\sigma^2}{n}=0.1237^2)\]

What is the probability of observing an \textbf{error in estimation} of \(\mu\) (distance between \(\bar{X}\) and \(\mu\)) smaller than \(0.21\)?

We want to compute \[P(-0.21 \leq \bar{X} - 13\leq 0.21)=P(12.79 \leq \bar{X} \leq 13.21)\]

\(=F_{normal}(13.21; \mu, se^2)-F_{normal}(12.79; \mu, se^2)\)

In R we can compute it as:

pnorm(13.21, 13, 0.1237)-pnorm(12.79, 13, 0.1237)=0.9104.

\(91.0\%\) of the errors are less than \(0.21\), therefore the \textbf{observed} error does not seem to be too typical (only \(9\%\) of the errors are higher). Maybe we have stronger cables than we thought.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-sum}{%
\subsection{Sample sum}\label{sample-sum}}

If we are interested in using all the \(8\) cables at the same time to carry a total of \(96\) Tons, then we should consider adding their individual contributions.

The \textbf{sample sum} is the \textbf{statistic}:

\[Y=n \bar{X}=\sum_{i=1}^n X_i\]

if \(X \rightarrow N(\mu, \sigma^2)\) then \[Y \rightarrow N(n\mu, n\sigma^2)\]

With mean and variance:

\begin{itemize}
\tightlist
\item
  \(E(Y)=n\mu\)
\item
  \(V(Y)=n\sigma^2\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference-on-the-sample-sum}{%
\subsection{Inference on the sample sum}\label{inference-on-the-sample-sum}}

If we \textbf{know} that for our cables \[X \rightarrow N(\mu=13, \sigma^2=0.35^2)\] then

\[Y \rightarrow N(n\mu=104, n\sigma^2=8\times 0.35^2)\]

\begin{itemize}
\tightlist
\item
  \(E(Y)=104\)
\item
  \(V(Y)=8\times 0.35^2=0.98\)
\end{itemize}

For our \(8\)-sample, we observed

\begin{itemize}
\tightlist
\item
  \(y_{stock}=105.7014\)
\end{itemize}

and, therefore, the \textbf{observed error} in the estimation of the mean of the \textbf{true} total braking load (\(n\mu\)) of \(8\) cables was

\begin{itemize}
\tightlist
\item
  \(y_{stock}-n\mu= 1.7014\)
\end{itemize}

Is this a \textbf{typical} error?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probabilities-of-the-sample-sum-propagation-of-error}{%
\subsection{Probabilities of the sample sum: Propagation of error}\label{probabilities-of-the-sample-sum-propagation-of-error}}

What is the probability of observing a difference \(Y-E(Y)\) smaller than \(1.7014\)?

We want to compute the probability

\[P(-1.7014 \leq \bar{Y} - 104 \leq 1.7014)=P(102.2986 \leq Y \leq 105.7014)\]

\(=F_{normal}(105.7014; n\mu, n\sigma^2)-F_{normal}(102.2986; n\mu, n\sigma^2)\)

In R we can compute it as:

pnorm(105.7014, 104, sqrt(0.98)) - pnorm(102.2986, 104, sqrt(0.98))=0.914.

\(91.4\%\) are smaller than \(1.7014\), a higher proportion than the proportion for individual cables because their individual errors accumulated.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference-in-the-sample-variance}{%
\section{Inference in the sample variance}\label{inference-in-the-sample-variance}}

Consider a quality control process that requires that the cables are produced close to the specified value \(\mu\).

If a sample of \(8\) cables is too dispersed (\(S^2>0.3\)), we stop production: the process is out of control.

What is the probability that the sample variance of a sample of \(8\) cables is greater than the required \(0.3\)?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probabilities-of-the-sample-variance}{%
\section{Probabilities of the sample variance}\label{probabilities-of-the-sample-variance}}

When \(X\) follows a normal distribution
\[X \rightarrow N(\mu, \sigma^2)\]

The \textbf{statistic}:

\[W=\frac{(n-1)S^2}{\sigma^2} \rightarrow \chi^2(n-1)\]

has a \(\chi^2\) (chi-squared) distribution with \(df=n-1\) degrees of freedom given by

\[f(w)=C_n  w^{\frac{n-3}{2}} e^{-\frac{w}{2}}\]

where:

\begin{itemize}
\item
  \(C_n=\frac{1}{2^{(n-1)/2\sqrt{\pi(n-1)}}}\) ensures \(\int_{-\infty}^{\infty} f(t)dt=1\)
\item
  \(\Gamma(x)\) is Euler's factorial for real numbers
\item
  If we \textbf{know} the true values of \(\mu\) and \(\sigma\) we can compute probabilities of \(S^2\) using the \(\chi^2\) distribution for \(W\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{chi2-statistic}{%
\section{\texorpdfstring{\(\chi^2\)-statistic}{\textbackslash chi\^{}2-statistic}}\label{chi2-statistic}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-72-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{chi2-statistic-1}{%
\section{\texorpdfstring{\(\chi^2\)-statistic}{\textbackslash chi\^{}2-statistic}}\label{chi2-statistic-1}}

If we \textbf{know} that our cables trully distribute as \[X \rightarrow N(\mu=13, \sigma^2=0.35^2)\]

then we can compute \[P(S^2 > 0.2)=P(\frac{(n-1)S^2}{\sigma^2} > \frac{(n-1)0.3}{\sigma^2})\]
\(=P(W > \frac{(n-1)0.3}{\sigma^2})\)

\(=1-P(W \leq \frac{(n-1)0.3}{\sigma^2})=1-P(W\leq \frac{(8-1)0.3}{0.1225})\)

\(= 1- F_{\chi^2,df=7}(17.14286)=0.016\)

In R
1-pchisq(17.14286, df=7)=0.016

There is only a probability of \(1\%\) of obtaining a value greater than \(s^2=0.3\).

\begin{itemize}
\item
  \(s^2>0.3\) seems to be a good criterion to stop production and revise the process.
\item
  our observed value was \(s^2_{stock}=0.1275608\)
\item
  the sample is not too dispersed and we believe that the production is under control.
\end{itemize}

\hypertarget{point-estimators}{%
\chapter{Point Estimators}\label{point-estimators}}

\hypertarget{objective-9}{%
\section{Objective}\label{objective-9}}

\begin{itemize}
\tightlist
\item
  Random sample
\item
  Statistic
\item
  Point estimators
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{parameters}{%
\section{Parameters}\label{parameters}}

When we want to compute \textbf{probabilities} for the outcome of a random experiment \textbf{we need} a model and its parameter:

\[X \rightarrow f(x; \theta)\]

But we usually \textbf{don't know} \(\theta\)

Let's look at a known example that will help us to introduce \textbf{terminology}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bernoulli-trial-4}{%
\section{Bernoulli trial}\label{bernoulli-trial-4}}

Writing down the sex of a patient who goes into an emergency room of a hospital is a Bernoulli variable \(K\) with outcomes (0:female and 1:male), which has a probability mass function

\[f(k; p)=p^k(1-p)^{1-k} \]
The parameter \(p\)

\begin{itemize}
\item
  is the probability that one patient is male
\item
  is usually \textbf{unknown}
\end{itemize}

What do we do?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution-4}{%
\section{Binomial distribution}\label{binomial-distribution-4}}

We repeat the Bernoulli trial \(n\) times and count how many times we obtained \(A\) from the total number of repetitions:

\[f_A=\frac{n_A}{n}\]

\begin{itemize}
\tightlist
\item
  From \(n=100\) patients we count how many are men \(n_{man}\).
\end{itemize}

\(f_A\) is the \textbf{observation} of the \textbf{average} over Bernoulli trials \[\bar{K}=\frac{1}{n}\sum_i^n K_i\]
That is

\[f_{man}=\bar{k}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution-5}{%
\section{Binomial distribution}\label{binomial-distribution-5}}

The average over \(n\) Bernoulli trials \(\bar{K}\) is a random variable with mean and variance:

\begin{itemize}
\tightlist
\item
  \(E(\bar{K})=p\)
\item
  \(V(\bar{K})=\frac{p(1-p)}{n}\) (Remember: \(V(aK)=a^2K\))
\end{itemize}

Therefore:

\begin{itemize}
\tightlist
\item
  as \(n \rightarrow \infty\), \(V(\bar{K}) \rightarrow 0\)
\end{itemize}

and

\begin{itemize}
\tightlist
\item
  \(\bar{K}\) \textbf{concentrates closer and closer} to \(p\) as \(n\) increases.
\end{itemize}

We can take one value of \(\bar{k}\) as estimation for \(p\) or

\[\bar{k}=\hat{p}\]

Remember: \(\bar{k}=f_{male}\) and \(p=P(male)\), therefore \(lim_{n\rightarrow \infty} f_{male}=P(male)\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-3}{%
\section{Average}\label{average-3}}

Example:

If we observe \[(1, 0, 0, 1, 1, 0, 1, 0, 1, 0)\]

after the repetition of \(n\) Bernoulli trials: Determining the sex of \(10\) patients (\(0\):female, \(1\):male).

The \textbf{random variable} \(\bar{K}\) takes the value \(\bar{k}= 5/10=0.5\) and we use it

\[\hat{p}=0.5\]

to \textbf{estimate} the unobserved probability (\(p\)) that one patient entering the emergency room is male (parameter of the Bernoulli trial).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-4}{%
\section{Average}\label{average-4}}

\textbf{Situation 1:}

If we wait for \(10\) other patients then \[(1, 1, 1, 1, 0, 1, 0, 1, 1, 0)\]

\(\bar{K}=7/10\) and \[\hat{p}=0.7\] changes because \(\bar{K}\) is random.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-5}{%
\section{Average}\label{average-5}}

\textbf{Situation 2:}

If we observe the event \[(1, 0, ..., 1)\]
with \(N=10000\) and \(\bar{k}= 6675/10000\) then \[\hat{p}= 0.6675\]

\textbf{However}, when we repeat the \(10000\)-sampling

\[\hat{p}=0.6698\]

The two estimates get \textbf{closer and closer} because \(V(\bar{K}) \rightarrow 0\)

\textbf{Solution:} to estimate the probability that a man enters an emergency room, repeat the experiment many many times and take the average.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-6}{%
\section{Average}\label{average-6}}

\textbf{Situation 1: small n}

We record the sex of \(10\) patients going into an emergency room

\includegraphics{_main_files/figure-latex/unnamed-chunk-73-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-7}{%
\section{Average}\label{average-7}}

\textbf{Situation 2: large n}

We record the sex of \(500\) patients going into an emergency room

\includegraphics{_main_files/figure-latex/unnamed-chunk-74-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{random-sample-1}{%
\section{Random sample}\label{random-sample-1}}

A \textbf{random sample} of size \(n\) is the \textbf{repetition} of a random experiment \(n\) \textbf{independent} times.

\begin{itemize}
\tightlist
\item
  A random sample is a \(n\)-dimensional \textbf{random variable}
\end{itemize}

\[(X_1, X_2, ... X_n)\]

where \(X_i\) is the \emph{i-th} repetition of the random experiment with comon distribution \(f(x; \theta)\) for any \(i\)

\begin{itemize}
\tightlist
\item
  \textbf{One observation} of a random sample is the set of \(n\) values obtained from the experiments
\end{itemize}

\[(x_1, x_2, ... x_n)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{random-sample-2}{%
\section{Random sample}\label{random-sample-2}}

We repeat the random experiment \(n\) times to \textbf{learn from experience} and then we can

\begin{itemize}
\item
  Describe properties of the data and the underlying distribution model (Descriptive statistics)
\item
  \textbf{Find} \(\theta\) (Estimation)
\item
  Make hypotheses on \(\theta\) (Inference)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistic}{%
\section{Statistic}\label{statistic}}

A \textbf{statistic} is any function of a \textbf{random sample}
\[T(X_1,X_2, ..., X_n)\]
It usually returns a number.

\begin{itemize}
\item
  Statistics are \textbf{random variables}
\item
  The \textbf{probability distributions} of statistics are called \textbf{sampling distributions}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistics-examples-1}{%
\section{Statistics Examples 1}\label{statistics-examples-1}}

Statistics of \textbf{location} (center) of outcomes of random experiments

\begin{itemize}
\tightlist
\item
  \textbf{Average}:
\end{itemize}

\[\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i\]

\begin{itemize}
\tightlist
\item
  \textbf{Median}:
\end{itemize}

\[Q_{0.5}=X_m\]

such that: \(F_m=\sum_{i<m} \frac{n_i}{n}=0.5\)

\begin{itemize}
\tightlist
\item
  \textbf{Mode}:
\end{itemize}

\[\text{Mode}= X_{m}\]

such that: \(\max_m\{\frac{n_i}{n}\}\)

Remember: They are random variables. Every time we take another sample they change their value.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistics-examples-2}{%
\section{Statistics Examples 2}\label{statistics-examples-2}}

Statistics of \textbf{spread} of outcomes of random experiments

\begin{itemize}
\item
  \textbf{Sample variance}:
  \[S^2=\frac{1}{N-1}\sum_{i=1}^n (X_i-\bar{X})^2\]
\item
  \textbf{Inter quantile range}:
  \[Q_{0.75}-Q_{0.25}\]
\item
  \textbf{Range}:
  \[\max\{X_i\}-\min\{X_i\}\]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistics-examples-3}{%
\section{Statistics Examples 3}\label{statistics-examples-3}}

statistics with important \textbf{distribution properties}:

\begin{itemize}
\tightlist
\item
  \textbf{Standard}:
\end{itemize}

\[Z=\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\]

\begin{itemize}
\tightlist
\item
  \textbf{t-statistics}:
\end{itemize}

\[T=\frac{\bar{X}-\mu}{S}\]

\begin{itemize}
\tightlist
\item
  \textbf{\(\chi^2\)-statistics}:
\end{itemize}

\[\chi^2=\frac{(n-1)S^2}{\sigma^2}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{uses-of-statistics}{%
\section{Uses of Statistics}\label{uses-of-statistics}}

\begin{itemize}
\item
  \textbf{Description} of a sample's data

  \begin{itemize}
  \tightlist
  \item
    location: \(\bar{X}\)
  \item
    Minimum: \(\min\{X_i\}\)
  \item
    Maximum: \(\max\{X_i\}\)
  \end{itemize}
\item
  \textbf{Estimation} of a probability model's \textbf{parameters}

  \begin{itemize}
  \tightlist
  \item
    mean: \(\bar{X}\) for \(\mu\)
  \item
    variance: \(S^2\), for for \(\sigma^2\)
  \end{itemize}
\item
  \textbf{Inference} to say something about the parameters given the data

  \begin{itemize}
  \tightlist
  \item
    mean: \(Z\), \(T\)
  \item
    variance: \(\chi^2\)
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimation}{%
\section{Estimation}\label{estimation}}

We \textbf{assume} a probability model for the distribution of \(X\),

\[X \rightarrow f(x; \theta)\]

where \(\theta\) is a parameter

\textbf{Main question}:

\begin{itemize}
\tightlist
\item
  What is the value of \(\theta\), so we can compute the probability of an outcome?
\end{itemize}

\textbf{Example:} We observe \(n\) patients come in the emergency room, we take the relative frequency \(\bar{k}=\frac{n_{men}}{n}\) and that is an estimation for the parameter \(p\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{point-estimators-1}{%
\section{Point estimators}\label{point-estimators-1}}

\textbf{Point estimators} are statistics that are used to \textbf{learn about the unknown parameters} of probability models:
\[X \rightarrow f(x; \theta)\]

\textbf{Notation}:

\begin{itemize}
\tightlist
\item
  A \textbf{parameter} of the distribution is a \textbf{number}
\end{itemize}

\[\theta\]

\begin{itemize}
\tightlist
\item
  A \textbf{point estimator} is a statistic (function of the random sample) and a \textbf{random variable}
\end{itemize}

\[\Theta\]

\begin{itemize}
\tightlist
\item
  An \textbf{estimate} is an \textbf{observed value} of the estimator
\end{itemize}

\[\hat{\theta}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{point-estimators-2}{%
\section{Point estimators}\label{point-estimators-2}}

For the Bernoulli trial

\[K \rightarrow Bernouilli(p)\]

\textbf{Notation}:

\begin{itemize}
\item
  The \(p\) is the \textbf{parameter} of the distribution
\item
  A \textbf{point estimator} of \(p\) is the \textbf{random variable}
\end{itemize}

\[\bar{K}= \frac{1}{n}\sum_{i=1}^n K_i\]

\begin{itemize}
\tightlist
\item
  An \textbf{estimate} of \(p\) is an \textbf{observed value} of the estimator
\end{itemize}

\[\hat{p}=\bar{k}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{point-estimators-3}{%
\section{Point estimators}\label{point-estimators-3}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{properties-of-estimators}{%
\section{Properties of estimators}\label{properties-of-estimators}}

As \textbf{random variables} estimators have their own probability functions:

\[\Theta \rightarrow f(\hat{\theta}; \beta)\]

and parameters \(\beta\)

They have their own mean and variance

\begin{itemize}
\tightlist
\item
  \(E(\Theta)\)\\
\item
  \(V(\Theta)\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-15}{%
\section{Example:}\label{example-15}}

For the Bernoulli trial if we take \(\bar{K}\) as the estimator for \(p\) then

\begin{itemize}
\tightlist
\item
  \(E(\bar{K})=p\)
\item
  \(V(\bar{K})=\frac{p(1-p)}{n}\)
\end{itemize}

\textbf{Remember:} The numbers we need to know to fully determine a probability function are call parameters, therefore \(p\), \(n\), are parameters for the probability function of \(\bar{K}\)

\[\bar{K} \rightarrow f(\bar{k}; n, p)\]
in particular, we know that

\[Y=n\bar{K} \rightarrow Binom(n, p)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bias-accuracy}{%
\section{Bias (Accuracy)}\label{bias-accuracy}}

Some important properties of estimators are their \textbf{errors} when estimating:

Let's \textbf{assume that we know} the parameter \(\theta\) that we want to estimate.

The \textbf{bias} of \(\Theta\) is

\[bias=E(\Theta)-\theta\]

\begin{itemize}
\item
  how much the expectation of \(\Theta\) differs from the parameter \(\theta\).
\item
  \(bias\) is a property of \(\Theta\).
\item
  \(\Theta\) is \textbf{unbiased} if
\end{itemize}

\[E(\Theta)=\theta\]

\textbf{Example:}

For the Bernoulli trial if we take \(\bar{K}\) as the estimator for \(p\) then

\begin{itemize}
\tightlist
\item
  \(E(\bar{K})=p\) and therefore it is \textbf{unbiased}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{a-biased-inaccurate-estimator}{%
\section{A biased (inaccurate) estimator}\label{a-biased-inaccurate-estimator}}

Imagine another estimator for \(p\) that we call \(T\)

\begin{itemize}
\tightlist
\item
  \(p\): bullseye (parameter)
\item
  \(t\): dart (estimate from a estatistic \(T\))
\item
  \(bias\): error in accuracy (\(E(T)-p\))
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-75-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-error-precision}{%
\section{Standard Error (Precision)}\label{standard-error-precision}}

The \textbf{standard error} \(se\) of an estimator \(\Theta\) is its standard deviation

\[se=\sqrt{V(\Theta)}\]

\begin{itemize}
\tightlist
\item
  What \(\Theta\) varies from its mean \(E(\theta)\)
\end{itemize}

\textbf{Example:}

For the Bernoulli trial if we take \(\bar{K}\) as the estimator for \(p\) then its standard error is

\[se=\sqrt{V(\bar{K})}= \sqrt{\frac{p(1-p)}{n}}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{an-unprecise-estimator-of-p}{%
\section{\texorpdfstring{An unprecise estimator of \(p\)}{An unprecise estimator of p}}\label{an-unprecise-estimator-of-p}}

\begin{itemize}
\tightlist
\item
  \(p\): bullseye (parameter)
\item
  \(t\): dart (estimate from a estatistic \(T\))
\item
  \(se\): error in precision (\(\sqrt{V(T)}\))
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-76-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-squared-error}{%
\section{Mean squared error}\label{mean-squared-error}}

The \(mse\) of \(\Theta\) is its expected squared difference from the parameter

\[mse(\Theta)=E([\Theta - \theta]^2)\]

or equivalently is the sum of the errors

\[mse(\Theta)=se^2 + bias^2\]

\begin{itemize}
\tightlist
\item
  what \(\Theta\) varies from the parameter \(\theta\)
\end{itemize}

\textbf{Example}:

For the Bernoulli trial if we take \(\bar{K}\) as an \textbf{unbiased} estimator for \(p\) then its \(mse\) is

\[mse=V(\bar{K})=\frac{p(1-p)}{n}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{an-unprecise-and-inaccurate-estimator-of-p}{%
\section{\texorpdfstring{An unprecise and inaccurate estimator of \(p\)}{An unprecise and inaccurate estimator of p}}\label{an-unprecise-and-inaccurate-estimator-of-p}}

\begin{itemize}
\tightlist
\item
  \(p\): bullseye (parameter)
\item
  \(t\): dart (estimate)
\item
  \(mse\): total error
\item
  \(bias\): error in accuracy
\item
  \(se\): error in precision
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-77-1.pdf}

\hypertarget{central-limit-theorem}{%
\chapter{Central limit theorem}\label{central-limit-theorem}}

\hypertarget{objective-10}{%
\section{Objective}\label{objective-10}}

\begin{itemize}
\tightlist
\item
  Margin of errors
\item
  Central limit theorem
\item
  t-statistic
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{margin-of-error}{%
\section{Margin of error}\label{margin-of-error}}

When deciding whether an \textbf{observed error} is large or not we usually compare it with a \textbf{predefined} tolerance.

\begin{itemize}
\tightlist
\item
  The \textbf{margin of error} at \(5\%\) level is the distance \(m\) such that distribution of \(\bar{X}\) captures \(95\%\) of the estimations:
\end{itemize}

\[P(-m \leq \bar{X}-\mu \leq m)=P(\mu-m \leq \bar{X} \leq\mu + m)=0.95\]

\begin{itemize}
\tightlist
\item
  or that \(95\%\) of the values of \(\bar{X}\) are a distance \(m\) from \(\mu\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{margin-of-error-1}{%
\section{Margin of error}\label{margin-of-error-1}}

Let's continue with the braking load example.

for the \(8\)-sample

\begin{verbatim}
## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747
\end{verbatim}

the \textbf{observed error} is the difference

\[\bar{x}_{stock}-\mu=13.21-13=0.21\]
Is this value below the margin of error at \(5\%\)?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{z-statistic}{%
\section{Z-statistic}\label{z-statistic}}

If we \textbf{know} that for our cables truly distribute as
\[X \rightarrow N(\mu=13, \sigma^2=0.35^2)\] then,

\[\bar{X} \rightarrow N(\mu, \frac{\sigma^2}{n})\]

and the \(5\%\) margin of error for the average in our \(8\)-sample can be computed from the \textbf{standardized statistic}:

\[Z=\frac{\bar{X}-E(\bar{X})}{\sqrt{V(\bar{X})}} =\frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}  \rightarrow N(0,1)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{z-statistic-1}{%
\section{Z-statistic}\label{z-statistic-1}}

to compute the margin of error \(m\) at \(5\%\) level we standardize (subtract \(\mu\) and divide by \(\sigma/\sqrt{n}\))

\(P(\mu-m \leq \bar{X} \leq\mu + m)=P(-\frac{m}{\sigma/\sqrt{n}} \leq \frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\leq\frac{m}{\sigma/\sqrt{n}})\)
\[=P(-\frac{m}{\sigma/\sqrt{n}} \leq Z \leq\frac{m}{\sigma/\sqrt{n}})=0.95\]

(compare it with the plot) we have

\[m=z_{0.025} \frac{\sigma}{\sqrt{n}}=1.96\times se=1.96\frac{0.35}{\sqrt{8}}=0.24\]
where \(z_{0.025}=1.96\) is the value \(Z\) that leaves \(2.5\%\) at each side of standard normal density (\(0.025\)-quantile)

Our observed error \(0.21\)

\begin{itemize}
\item
  is less than the margin of error \(0.24\) at level \(5\%\).
\item
  and, therefore, it is expected within the \(95\%\) of errors.
\end{itemize}

If an observation of \(\bar{x}\) distance more than \(\sim 2\) times the \(se\) we say that the error is \textbf{unusually} large.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{z-statistic-2}{%
\section{Z-statistic}\label{z-statistic-2}}

\textbf{Definition}

For a normal random variable \(X\)

\[X \rightarrow N(\mu, \sigma^2)\] with \textbf{known} \(\sigma\)

The \(Z\) statistic:

\[Z=\frac{\bar{X}-E(\bar{X})}{\sqrt{V(\bar{X})}}\]
is a standard random variable whose \(1-\alpha/2\)-quantiles (\(z_{1-\alpha/2}\)) give a measure of the margin of error of \(\bar{X}\) at \(1-\alpha\) level

\[m=z_{1-\alpha/2}\frac{\sigma}{\sqrt{n}}\]

\textbf{A common situation:}

\begin{itemize}
\tightlist
\item
  What happens when \(X\) is not normally distributed?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{central-limit-theorem-1}{%
\section{Central Limit Theorem}\label{central-limit-theorem-1}}

For any random variable \(X\) with \textbf{unknown} (any type of) distribution

\[X \rightarrow f(x; \theta)\]
the standardized statistic

\[Z=\frac{\bar{X}-E(\bar{X})}{\sqrt{V(\bar{X})}}\]

approximates to a standard distribution

\[Z \rightarrow_d N(0,1)\] when \(n\rightarrow \infty\)

Therefore:

\begin{itemize}
\tightlist
\item
  We can compute probabilities for \(\bar{X}\) if \(n\) is large, using the normal distribution:
\end{itemize}

\[\bar{X} \sim_{aprox}  N(E(X), \frac{V(X)}{n})\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{central-limit-theorem-2}{%
\section{Central Limit Theorem}\label{central-limit-theorem-2}}

\textbf{Example:}

Consider an experiment where we measure the concentration in blood of a drug after 10-hour administration in \(30\) patients. We obtain the following results:

\begin{verbatim}
##  [1] 0.42172863 0.28830514 0.66452743 0.01578868 0.02810549 0.15825061
##  [7] 0.15711365 0.07263340 1.36311823 0.01457672 0.50241503 0.24010736
## [13] 0.14050681 0.18855892 0.09414202 0.42489306 0.78160177 0.23938021
## [19] 0.29546742 2.02050586 0.42157487 0.48293561 0.74263790 0.67402224
## [25] 0.58426449 0.80292617 0.74837143 0.78532627 0.01588387 0.29892485
\end{verbatim}

\begin{itemize}
\item
  the average is \(\bar{x}=0.56\)
\item
  the histogram of the results is:
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-80-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{central-limit-theorem-3}{%
\section{Central Limit Theorem}\label{central-limit-theorem-3}}

If we \textbf{know} that levels follow an exponential distribution \[X \rightarrow exp(\lambda=2)\]

The mean and variance are:

\begin{itemize}
\tightlist
\item
  \(E(X)=\frac{1}{\lambda}=0.5\)
\item
  \(V(X)=\frac{1}{\lambda^2}=0.25\)
\end{itemize}

Therefore the mean and variance of \(\bar{X}\) are:

\begin{itemize}
\tightlist
\item
  \(E(\bar{X})=\frac{1}{\lambda}=0.5\)
\item
  \(V(\bar{X})=\frac{V(X)}{n}=\frac{1}{n\lambda^2}=0.25/30\)
\end{itemize}

As \(n \geq 30\)

\[Z=\frac{\bar{X}-\lambda}{\sqrt{\frac{1}{n\lambda^2}}}\]

is a standard normal variable and: \(\bar{X} \sim_{aprox} N(\lambda, \frac{1}{n\lambda^2})\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{_main_files/figure-latex/unnamed-chunk-81-1.pdf} \includegraphics{_main_files/figure-latex/unnamed-chunk-81-2.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{margin-of-error-with-clt}{%
\section{Margin of error with CLT}\label{margin-of-error-with-clt}}

Since

\[\bar{X} \sim_{aprox}  N(E(X), \frac{V(X)}{n})\]

The margin of error at \(5\%\) level

\[P(E(X)-m \leq \bar{X} \leq E(X) + m)=0.95\]

can be computed again with the standard distribution

\[m=z_{0.025} \sqrt{\frac{V(X)}{n}}=1.96\sqrt{\frac{0.25}{30}}=0.1789227\]

We \textbf{observed} \(\bar{x}=0.5638725\) therefore the \textbf{observed error} in estimation is

\[\bar{x} - E(X)=0.5638725-0.5=0.063\]

which is within the margin of error.

The error that we observed is common and within the \(95\%\) of errors.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-sum-and-clt}{%
\section{Sample sum and CLT}\label{sample-sum-and-clt}}

For any random variable \(X\) with \textbf{unknown} (any type of) distribution

\[X \rightarrow f(x; \theta)\]

the standardized statistic

\[Z=\frac{\bar{X}-E(\bar{X})}{\sqrt{V(\bar{X})}}=\frac{n\bar{X}-nE(\bar{X})}{\sqrt{nV(\bar{X})}}\]

approximates to a standard distribution

\[Z \rightarrow_d N(0,1)\] when \(n\rightarrow \infty\)

Therefore:

\begin{itemize}
\tightlist
\item
  We can compute probabilities for the sample sum \(Y=n\bar{X}\) if \(n\) is large, using the normal distribution:
\end{itemize}

\[\bar{Y} \sim_{aprox}  N(nE(X), nV(X))\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{unknown-sigma-but-large-n}{%
\section{\texorpdfstring{Unknown \(\sigma\) but large \(n\)}{Unknown \textbackslash sigma but large n}}\label{unknown-sigma-but-large-n}}

For any random variable \(X\) with \textbf{unknown} (any type of) distribution

\[X \rightarrow f(x; \theta)\]

with \textbf{unknown} variance \(V(X)\), we can estimate the standard error (\(se=\sqrt{V(X)/n}\)) by the sample standard deviation

\[\hat{se}=\frac{s}{\sqrt{n}}\] and write the standardized statistic

\[Z=\frac{\bar{X}-E(\bar{X})}{\frac{s}{\sqrt{n}}} \]

\[Z \rightarrow_d N(0,1)\] to recover the CLT when \(n\rightarrow \infty\) (a good aproximation is when \(n>30\))

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{t-statistic}{%
\section{T-statistic}\label{t-statistic}}

When

\begin{itemize}
\tightlist
\item
  \(\sigma\) is \textbf{unknown}
\end{itemize}

and

\begin{itemize}
\tightlist
\item
  \(n\) is small (cannot apply CLT)
\end{itemize}

However, if \(X\) is normal

\[X \rightarrow N(\mu, \sigma^2)\] then the standardized statistic

\[T=\frac{\bar{X}-\mu}{\frac{S}{\sqrt{n}}} \]

Follows a \(t\)-distribution with \(n-1\) degrees of freedom, and we can compute probabilities on \(\bar{X}\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{t-statistic-1}{%
\section{T-statistic}\label{t-statistic-1}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-82-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{t-statistic-2}{%
\section{T-statistic}\label{t-statistic-2}}

To compute the margin of error \(m\) at \(5\%\) level when \(n\) is small, \(\sigma\) unknown but \(X\) normal

\(P(\mu-m \leq \bar{X} \leq\mu + m)=P(-\frac{m}{s/\sqrt{n}} \leq \frac{\bar{X}-\mu}{\frac{s}{\sqrt{n}}} \leq\frac{m}{s/\sqrt{n}})\)
\[=P(-\frac{m}{s/\sqrt{n}} \leq T \leq\frac{m}{s/\sqrt{n}})=0.95\]

We use the \(t\)-distribution

\[m=t_{0.025, n-1} \frac{s}{\sqrt{n}}\]
where \(t_{0.025, n-1}\) is the value \(T\) that leaves \(2.5\%\) at each side of \(t\)-distribution with \(n-1\) degrees of freedom (\(0.025\)-quantile)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-1-3}{%
\section{Example 1}\label{example-1-3}}

Going back to the braking load example, we computed the margin of error with \textbf{known} \(\sigma^2=0.35^2\).

\[m=z_{0.025} \frac{\sigma}{\sqrt{n}}=1.96\times se=1.96\frac{0.35}{\sqrt{8}}=0.24\]

\begin{itemize}
\tightlist
\item
  In most applications we \textbf{do not know} the parameters
\end{itemize}

If we only assumed that the braking load is a normal random variable

\[X \rightarrow N(\mu, \sigma^2)\]
with \textbf{unknown} \(\mu\) and \(\sigma^2\) then from the data

\begin{itemize}
\tightlist
\item
  \(s_{stock}=\sqrt{0.1275608}\)
\end{itemize}

and the margin of error is \[m=t_{0.025, n-1} \frac{s}{\sqrt{n}}=2.36\times \hat{se}=2.36\frac{0.3571565}{\sqrt{8}}=0.29\]
where \(t_{0.025, n-1}=2.36\)

in R is qt(1-0.025, 7)

It increased from the value we obtained with \textbf{known} \(\sigma\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-2-2}{%
\section{Example 2}\label{example-2-2}}

We can also ask for the probability of observing an error in the estimation of \(\mu\) (distance between \(\bar{X}\) and \(\mu\)) smaller than the observed value \(0.21\)?

We thus want to compute \[P(-0.21 \leq \bar{X} - \mu\leq 0.21)=P(\frac{-0.21}{s/\sqrt{n}} \leq T \leq \frac{0.21}{s/\sqrt{n}})\]

\(=P(\frac{-0.21}{0.3571565/\sqrt{8}} \leq T \leq \frac{0.21}{0.3571565/\sqrt{8}})\)

\(=F_{t, n-1}(0.21)-F_{t, n-1}(-0.21)\)

In R we can compute it as:

pt(1.663052, 7)-pt(-1.663052, 7)=0.859.

\(85.9\%\) of the errors are less than \(0.21\), therefore the \textbf{observed} error seems more typical than the \(91\%\) that we obtain with \(\sigma^2=0.35^2\).

Note that in the calculations we have substituted \(\sigma=0.35\) by a higher estimate \(s=0.3571565\) \textbf{obtained from data}.

\hypertarget{maximum-likelihood}{%
\chapter{Maximum likelihood}\label{maximum-likelihood}}

\hypertarget{objective-11}{%
\section{Objective}\label{objective-11}}

\begin{itemize}
\tightlist
\item
  Maximum likelihood
\item
  Method of Moments
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistic-1}{%
\section{Statistic}\label{statistic-1}}

\textbf{Definition}

Given a random sample \(X_1,...X_n\) a \textbf{statistic} is any real value function of the random variables that define the random sample: \(f(X_1,...X_n)\)

\begin{itemize}
\tightlist
\item
  \(\bar{X}=\frac{1}{N} \sum_{j=1..N} X_j\)
\item
  \(S^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar{X})^2\)
\item
  \(\max{X_1, X_n}\)
\end{itemize}

are statistics

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimator}{%
\section{Estimator}\label{estimator}}

\textbf{Definition}

An \textbf{estimator} is a statistic \(\Theta\) whose values \(\hat{\theta}\) are measures of a parameter \(\theta\) of the population distribution on which the sample is defined: \(E(\Theta)\sim \theta\)

\[X \rightarrow f(x; \theta)\]

Then

\begin{itemize}
\tightlist
\item
  \(\theta\) is a \textbf{parameter} of the population distribution \(f(x; \theta)\)
\item
  \(\Theta\) is an \textbf{estimator} of \(\theta\): A random variable
\item
  \(\hat{\theta}\) is the \textbf{estimate} of \(\theta\): A realized value of \(\Theta\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimator-1}{%
\section{Estimator}\label{estimator-1}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{examples-1-average-sample-mean}{%
\section{Examples 1: Average (Sample mean)}\label{examples-1-average-sample-mean}}

When \[X \rightarrow N(\mu_X, \sigma^2_X)\]

For the mean:

\begin{itemize}
\tightlist
\item
  \(\mu_X\) is a \textbf{parameter} of the population distribution \(N(\mu_X, \sigma^2_X)\)
\item
  \(\bar{X}\) is an \textbf{estimator} of \(\mu_X\)
\item
  \(\bar{x}=\hat{\mu}=13.21 \, Tons\) is the \textbf{estimate} of \(\mu_X\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{examples-2-sample-variance}{%
\section{Examples 2: Sample Variance}\label{examples-2-sample-variance}}

When \[X \rightarrow N(\mu_X, \sigma^2_X)\]

For the variance:

\begin{itemize}
\tightlist
\item
  \(\sigma^2_X\) is a \textbf{parameter} of the population distribution \(N(\mu_X, \sigma^2_X)\)
\item
  \(S^2\) is an \textbf{estimator} of \(\sigma^2_X\)
\item
  \(s^2=\hat{\sigma^2_X}=0.127 \, Tons^2\) is the \textbf{estimate} of \(\sigma^2_X\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bias}{%
\section{Bias}\label{bias}}

An estimator is unbiased if \(E(\Theta)=\theta\)

\begin{itemize}
\item
  \(\bar{X}\) is an \textbf{unbiased} estimator of \(\mu_X\) because \(E(\bar{X})=\mu_X\)
\item
  \(S^2\) is an \textbf{unbiased} estimator of \(\sigma^2_X\) because \(E(S^2)=\sigma^2_X\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{consistency}{%
\section{Consistency}\label{consistency}}

An estimator is consistent if \(V(\Theta) \rightarrow 0\) when \(n \rightarrow \infty\)

\begin{itemize}
\item
  \(\bar{X}\) is \textbf{consistent} because \(V(\bar{X})=\frac{\sigma_X}{n}\rightarrow 0\) when \(n \rightarrow \infty\).
\item
  \(S^2\) is also \textbf{consistent} (we will not show).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{maximum-likelihood-1}{%
\section{Maximum likelihood}\label{maximum-likelihood-1}}

How can we \textbf{estimate} the parameter of \textbf{any} parametric model?

\begin{itemize}
\item
  Imagine we design a laser with a diameter of 1mm that we want to use for clinical applications.
\item
  We want to characterize the diameter of a piercing in a tissue made with the laser
\item
  and take a random sample of 30 cuts made with the laser
\end{itemize}

\begin{verbatim}
##  [1] 1.11 1.64 1.20 1.79 1.89 1.01 1.31 1.81 1.34 1.25 1.92 1.24 1.49 1.36 1.03
## [16] 1.82 1.09 1.01 1.14 1.91 1.80 1.51 1.44 1.98 1.46 1.53 1.33 1.39 1.12 1.04
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-16}{%
\section{Example}\label{example-16}}

with histogram

\includegraphics{_main_files/figure-latex/unnamed-chunk-84-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-density}{%
\section{Probability density}\label{probability-density}}

We consider that maximum probability should be given to diameters of \(x=1mm\), and that the diameters should decrease exponentially in probability as they get larger with a limit of \(2mm\) beyond which the probability is \(0\).

A suitable probability density distribution is

\[
    f(x)= 
\begin{cases}
\frac{1}{\alpha}(x-1)^{\frac{1-\alpha}{\alpha}},& \text{if } x \in (1,2)\\
    0,& x \notin (1,2)\\
\end{cases}
\]

Where \(\alpha\) is a parameter.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-density-1}{%
\section{Probability density}\label{probability-density-1}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-85-1.pdf}

If we were to perform a \(n\)-sample: \(X_1,...X_n\)

\begin{itemize}
\tightlist
\item
  The \textbf{maximum likelihood} is a method that \textbf{gives us the estimator} for \(\alpha\)
\end{itemize}

\[\hat{\alpha}_{ml}\]
How should we combine the data for obtaining the best value of \(\hat{\alpha}_{ml}\)?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-maximum-likelihood}{%
\section{Example: Maximum likelihood}\label{example-maximum-likelihood}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-86-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{maximum-likelihood-2}{%
\section{Maximum likelihood}\label{maximum-likelihood-2}}

The objective is to find the value of the parameter that we \textbf{believe} can represent \textbf{best} the data.

We search for the parameter that makes the \textbf{observation} of the sample the most \textbf{probable}.

Note:

\begin{itemize}
\tightlist
\item
  Probabilities are assigned to observations.
\item
  Probabilities are not assigned to parameters (we assign beliefs, and likelihoods).
\end{itemize}

Parameters are not supposed to change, they are properties of the system.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-step-1}{%
\section{Method step 1}\label{method-step-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We calculate the probability of having observed the \(n\)-sample: \(x_1,...x_n\)
\end{enumerate}

\(P(x_1,...x_n)=P(x_1)P(X_2)...P(x_n)\)
\[=f(x_1;\alpha)f(x_2;\alpha) ...f(x_n;\alpha)\]
- Once the data is observed they are \textbf{fixed}.
- The unknown is \(\alpha\)
- This probability as a function of the \(\alpha\) we call it the \textbf{likelihood function}

\[L(\alpha)= \Pi_{i=1..n} f(x_i; \alpha)\]

then in our case

\(L(\alpha;x_1,..x_n)= \frac{1}{\alpha^n} \Pi_{i=1..n} (x_i-1)^{\frac{1-\alpha}{\alpha}}= \frac{1}{\alpha^n} \{(x_1-1)(x_2-1)...(x_n-1)\}^{\frac{1-\alpha}{\alpha}}\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-step-2}{%
\section{Method step 2}\label{method-step-2}}

We want to maximize \(L(\alpha)\) with respect to \(\alpha\).

Since we have the multiplication of many factors is easier to maximize the logarithm of \(L(\alpha)\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Take the logarithm, obtain the \textbf{Log-likelihood}
\end{enumerate}

\[\ln L(\alpha;x_1,..x_n)= -n \ln(\alpha) + {\frac{1-\alpha}{\alpha}} \Sigma_{i=1...n} \ln (x_i-1)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-step-3}{%
\section{Method step 3}\label{method-step-3}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Maximize the log-likelihood with respect to the parameter
\end{enumerate}

Therefore,

\begin{itemize}
\tightlist
\item
  we differentiate with respect to \(\alpha\)
\end{itemize}

\(\frac{d \ln L(\alpha)}{d \alpha}= -\frac{n}{\alpha} - \frac{1}{\alpha^2} \Sigma_{i=1...n} \ln (x_i)\)

\begin{itemize}
\tightlist
\item
  The maximum is where the derivative is \(0\). This maximum is the value of our estimator \(\hat{\alpha}_{ml}\).
\end{itemize}

\(\hat{\alpha}_{ml}=-\frac{1}{n}\Sigma_{i=1...n} \ln (x_i-1)\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-step-3-1}{%
\section{Method step 3}\label{method-step-3-1}}

\(\hat{\alpha}_{ml}=-\frac{1}{n}\Sigma_{i=1...n} \ln (x_i-1)\)

is the \textbf{statistic} that estimate the parameter.

In our example we compute:

\(\hat{\alpha}_{ml}=-\frac{1}{n}\{ \ln (1.11-1)+ \ln (1.64-1)+...\ln (1.04-1)\}=1.320\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimation-1}{%
\section{Estimation}\label{estimation-1}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-87-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimation-2}{%
\section{Estimation}\label{estimation-2}}

\begin{itemize}
\item
  This is the log-likelihood for our 30 laser cuts.
\item
  If we take another sample this function changes and so does its maximum.
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-88-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{maximum-likelihood-history}{%
\section{Maximum likelihood: History}\label{maximum-likelihood-history}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{maximum-likelihood-history-1}{%
\section{Maximum likelihood: History}\label{maximum-likelihood-history-1}}

\begin{itemize}
\item
  Ceres was thought to be a planet
\item
  It disappeared behind the Sun
\item
  Predictions were needed to know where in the sky to look for it after it passed behind the sun
\item
  The trajectory (parallel to the planets) would determine if it was likely a planet
\item
  With several observations with errors, what would be the best representative of the true position of Ceres at a given time?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{maximum-likelihood-history-2}{%
\section{Maximum likelihood: History}\label{maximum-likelihood-history-2}}

What is the statistic that best represents the true position of Ceres?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{maximum-likelihood-history-3}{%
\section{Maximum likelihood: History}\label{maximum-likelihood-history-3}}

Gauss proposed that at a \textbf{given} time

\begin{itemize}
\tightlist
\item
  the \textbf{true} position of Ceres was the mean \(\mu\)
\item
  the probabilities around the mean were symmetrical.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{maximum-likelihood-history-4}{%
\section{Maximum likelihood: History}\label{maximum-likelihood-history-4}}

Gauss discovered that if the average (\(\bar{x}\)) is the \textbf{most likely} value for the real position of Ceres (\(\mu\)), then the probability density for the errors is

\[\frac{h}{\sqrt{\pi}}e^{-h^2(y-\mu_Y)^2}\]

which we call the Gaussian and Pearson (1920) baptized it as the normal curve.

Note: We assume that the \textbf{true} position of Ceres exists \(\mu\).

Can we say the same about the height of men? is there a \textbf{true} mean height? (Galton)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-9}{%
\section{Normal distribution}\label{normal-distribution-9}}

Imagine that we take a \(8\)-sample for the breaking load of cables and

\begin{itemize}
\item
  \textbf{assume} that \[X \rightarrow N(\mu, \sigma^2)\].
\item
  What are the estimators of \(\mu\) and \(\sigma^2\) from the sample?
\end{itemize}

Which values of the parameters best describe the data?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-10}{%
\section{Normal distribution}\label{normal-distribution-10}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The likelihood function, the probability of having observed \((x_1, ....x_n)\) is
\end{enumerate}

\(L(\mu, \sigma^2)=\Pi_{i=1..n} N(x_i;\mu,\sigma)\)

\(=\big( \frac{1}{\sigma \sqrt{2 \pi}}\big)^n e^{-\frac{1}{2\sigma^2} \sum_i(x_i-\mu)^2}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We can take the log of \(L\), and compute the \textbf{log-likelihood}
\end{enumerate}

\(\ln L(\mu, \sigma^2)=-n \ln(\sigma \sqrt{2 \pi})-\frac{1}{2\sigma^2} \Sigma_i(x_i-\mu)^2\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-11}{%
\section{Normal distribution}\label{normal-distribution-11}}

The estimates of \(\mu\), \(\sigma^2\) are where the likelihood is maximum, and give the highest probability for the data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  we differentiate with respect to \(\mu\) and \(\sigma^2\)
\end{enumerate}

\begin{itemize}
\item
  \(\frac{d \ln L(\mu, \sigma^2)}{d\mu}=\frac{1}{\sigma^2} \sum_i(x_i-\mu)\)
\item
  \(\frac{d \ln L(\mu, \sigma^2)}{d\sigma^2}=-\frac{n}{2 \sigma^2}+\frac{1}{2\sigma^4} \sum_i(x_i-\mu)^2\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-12}{%
\section{Normal distribution}\label{normal-distribution-12}}

The derivatives are \(0\) at the maxima

\begin{itemize}
\tightlist
\item
  \(\frac{1}{\hat{\sigma}^2} \sum_i(x_i-\hat{\mu})=0\)
\item
  \(-\frac{n}{2 \hat{\sigma}^2}+\frac{1}{2\hat{\sigma}^4} \sum_i(x_i-\hat{\mu})^2=0\)
\end{itemize}

solving for the parameters we find

\begin{itemize}
\tightlist
\item
  \(\hat{\mu}_{ml}=\frac{1}{n}\sum_i x_i=\bar{x}\) (the average)
\item
  \(\hat{\sigma^2}_{ml}=\frac{1}{n}\sum_i(x_i-\bar{x})^2\) (the \textbf{uncorrected} sample variance)
\end{itemize}

The maximum likelihood estimator of \(\sigma^2\) is a \textbf{biased} estimator as: \(E(\hat{\sigma}^2_{ml})\neq \sigma^2\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-of-moments}{%
\section{Method of Moments}\label{method-of-moments}}

The method of maximum likelihood is aimed to produce the estimators of probability distributions from data.

\begin{itemize}
\tightlist
\item
  Is there another way to produce those estimators? would they be equal?
\end{itemize}

Let's look again at the maximum likelihood estimators for \(\mu\) and \(\sigma^2\) for a random variable that distributes normally: \[X \rightarrow N(\mu, \sigma^2)\]

\begin{itemize}
\tightlist
\item
  \(\hat{\mu}=\frac{1}{n}\sum_i x_i\)
\item
  \(\hat{\sigma^2}=\frac{1}{n}\sum_i(x_i-\bar{x})^2\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-of-moments-1}{%
\section{Method of Moments}\label{method-of-moments-1}}

Let's re-write the estimators in terms of the values of \(X\) (outcomes), not of the observations

For instance:

\[\hat{\mu}=\frac{1}{n}\sum_i x_i= \sum_x x \frac{n_x}{n}\]

and remember that in the limit \(n \rightarrow \infty\) the frequentist interpretation requires \(\frac{n_x}{n} \rightarrow P(X=x)\) and therefore in the limit

\[\hat{\mu}=\frac{1}{n}\sum_i x_i \rightarrow E(X)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-of-moments-2}{%
\section{Method of Moments}\label{method-of-moments-2}}

The method os moments says that we can take the \textbf{observed} value of the average \(\bar{X}= \frac{1}{n}\sum_i X_i\) as an estimator of \(E(X)=\mu\)

\[ E(X)\sim \bar{x}\]

\(\bar{X}\) is called the first \textbf{sample moment}

the estimator of the parameter \(\theta\) is then obtained from the equation:

\[E(X; \hat{\theta})=\bar{x}\]
Example: If
\[X \rightarrow N(\mu, \sigma^2)\] then

\(E(X; \mu, \sigma^2)=\mu\)

Method of moments:

\begin{itemize}
\tightlist
\item
  \(\hat{\mu}=\bar{x}=\frac{1}{n}\sum_i x_i\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-of-moments-3}{%
\section{Method of Moments}\label{method-of-moments-3}}

Suppose that we have several batteries (new and old) that we charge over the period of 1 hour. We measure the state of charge of the battery, being 1 a 100\% charge.

The state of charge of a battery is a random variable that may have a uniform distribution, where we do not know the minimum value that \(x\) can take, but we know that the maximum is 1
\[
f(x)=
\begin{cases}
    \frac{1}{1-a},& \text{if } x\in (a,1)\\
    0,& x\notin (a,1)
\end{cases}
\]

What is the estimator of \(a\)?

\begin{itemize}
\tightlist
\item
  We run an experiment and obtain \(x_1,...x_n\) how can we estimate \(a\) form the data?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-of-moments-4}{%
\section{Method of Moments}\label{method-of-moments-4}}

The distribution has one parameter. The method of moments gives us one equation

\[E(X; \hat{a})=\bar{x}\]
That is

\[\frac{\hat{a}+1}{2}=\bar{x}\]

That we solve for \(\hat{a}\)

\[\hat{a}=2\bar{x}-1\]

This is the estimator of the minimum charge we may observe.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-of-moments-5}{%
\section{Method of Moments}\label{method-of-moments-5}}

Note that taking the minimum of the measurements is clearly suboptimal.

The method gave us a clever answer:

\begin{itemize}
\tightlist
\item
  we can compute \(\bar{x}\) with increasing precision given by \(n\)
\item
  We know that no measurement surpasses \(b=1\)
\item
  Then compute the distance between \(\bar{x}\) and \(b\): \(1-\bar{x}\)
\item
  Subtract it from \(\bar{x}\): \(\bar{x}-(1-\bar{x})=2\bar{x}-1\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-of-moments-6}{%
\section{Method of Moments}\label{method-of-moments-6}}

The method says that an estimator for the parameter \(\theta\) of \(f(x;\theta)\) can be found from the equation:

\[E(X, \hat{\theta})=\frac{1}{n}\sum_i x_i\]

If there are more parameters, we use the higher \textbf{sample moments}

\begin{itemize}
\tightlist
\item
  The second sample moment is
\end{itemize}

\[\frac{1}{n}\sum_i X^2_i\]

as such, an observation of this moment is

\(E(X ^ 2)~\frac{1}{n}\sum_i x^2_i\).

The method says that an estimation for the the parameters \(\theta_1\) and \(\theta_2\) of \(f(x;\theta_1,\theta_2)\) can be found from the equations:

\begin{itemize}
\item
  \(E(X; \hat{\theta_1}, \hat{\theta_2})= \frac{1}{n}\sum_i x_i\)
\item
  \(E(X^2; \hat{\theta_1}, \hat{\theta_2})=\frac{1}{n}\sum_i x^2_i\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution-13}{%
\section{Normal distribution}\label{normal-distribution-13}}

If \(X\) distributes normally\\
\[X \rightarrow N(\mu, \sigma^2)\]

then it has mean and variance:

\(E(X; \mu, \sigma^2)=\mu\) and
\(V(X; \mu, \sigma^2)=\sigma^2\)

Method of moments gives the equations:

\begin{itemize}
\tightlist
\item
  \(E(X)=\frac{1}{n}\sum_i x_i\)
\item
  \(E(X^2)=\frac{1}{n}\sum_i x^2_i\)
\end{itemize}

A substitution of \(E(X)\) into the first equation gives the estimator for the mean \(\mu\).

\begin{itemize}
\tightlist
\item
  \(\hat{\mu}=\frac{1}{n}\sum_i x_i\)
\end{itemize}

\(E(X^2)\) follows from the property: \(E(X^2) = \hat{\mu}^2 + V(X)= \hat{\mu}^2+\hat{\sigma}^2\)

then

\begin{itemize}
\tightlist
\item
  \(\hat{\sigma}^2= \frac{1}{n} \sum_i x^2_i -\hat{\mu}^2\)
\end{itemize}

which can also be written as:
\(\frac{1}{n} \sum_i(x_i-\hat{\mu})^2\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-of-moments-7}{%
\section{Method of Moments}\label{method-of-moments-7}}

What is the estimator of parameter \(\alpha\) for the laser cut given by the method of moments?

\[
    f(x; \alpha)= 
\begin{cases}
\frac{1}{\alpha}(x-1)^{\frac{1-\alpha}{\alpha}},& \text{if } x \in (1,2)\\
    0,& x \notin (1,2)\\
\end{cases}
\]

Where \(\alpha\) is a parameter.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-of-moments-8}{%
\section{Method of Moments}\label{method-of-moments-8}}

The method says that an estimator for the parameter \(\alpha\) of \(f(x;\alpha)\) can be found from the equation:

\[E(X; \hat{\alpha})=\frac{1}{n}\sum_i x_i\]

We need to compute the expected value \(E(X)\)

\[E(X)=\int_{-\infty}^{\infty} x f(x;\alpha)dx\]

and equate it to the average \(\bar{x}\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-of-moments-9}{%
\section{Method of Moments}\label{method-of-moments-9}}

Consider a change of variables \(Z=X-1\) then \(E(X)=E(Z)+1\) and

\(E(Z)= \frac{1}{\alpha} \int_0^1 z z^{\frac{1-\alpha}{\alpha}}dz= \frac{1}{\alpha} \int_0^1 z^{1+\frac{1-\alpha}{\alpha}}dz\)

\(= \frac{1}{\alpha} \frac{z^{2+\frac{1-\alpha}{\alpha}}}{{2+\frac{1-\alpha}{\alpha}}} |_0^1=\frac{1}{1+\alpha}\)

Therefore, the method of moments gives us the equation

\[\frac{1}{1+\hat{\alpha}}+1=\bar{x}\]

which solving for \(\hat{\alpha}\) gives us the estimate

\(\hat{\alpha}_m=\frac{1}{\bar{x}-1}-1\)

For our 30 lasers, this is

\(\hat{\alpha}_m=1.314\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{method-of-moments-10}{%
\section{Method of Moments}\label{method-of-moments-10}}

Note that this is an example for which the estimates by maximum likelihood and the method of moments are different

\begin{itemize}
\item
  \(\hat{\alpha}_{ml}=-\frac{1}{n}\sum_{i=1}^n \ln (x_i-1)=1.320\)
\item
  \(\hat{\alpha}_m=\frac{1-\bar{x}}{\bar{x}}=1.314\)
\end{itemize}

We need simulation studies, where we know the true value of the parameter \(\alpha\), to find which of these statistics have less mean squared error.

Note: the data for 30 laser piercings were simulated with \(\alpha=2\), therefore we should prefer the maximum likelihood estimate.

To obtain better estimates of \(\alpha\) we need to increase the size of the sample.

\hypertarget{interval-estimation}{%
\chapter{Interval estimation}\label{interval-estimation}}

\hypertarget{objective-12}{%
\section{Objective}\label{objective-12}}

\begin{itemize}
\tightlist
\item
  Interval estimation for the mean and the proportion
\item
  Interval estimation for the variance
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{average-or-sample-mean-1}{%
\section{Average or sample mean}\label{average-or-sample-mean-1}}

\textbf{Definition}

The sample mean (or average) of a \textbf{random sample} of size \(n\)

\[\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i\]

Because each random experiment is independent, the mean and variance of \(\bar{X}\) are

\begin{itemize}
\tightlist
\item
  \(E(\bar{X})=E(X)\)
\item
  \(V(\bar{X})=\frac{V(X)}{n}\)
\end{itemize}

\(\bar{X}\) is therefore

\begin{itemize}
\item
  an \textbf{estimator} of \(E(X)\), that is \(\mu\).
\item
  a random variable
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference-on-the-average-1}{%
\section{Inference on the average}\label{inference-on-the-average-1}}

\textbf{Example:}

You perform \(8\) random experiments: Load a cable until it breaks and record the breaking load. These are the results.

\begin{verbatim}
## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747
\end{verbatim}

If we \textbf{know} that our cables truly distribute as \[X \rightarrow N(\mu=13, \sigma^2=0.35^2)\] then

\[\bar{X} \rightarrow N(13, \frac{0.35^2}{8})\]

\begin{itemize}
\tightlist
\item
  \(E(\bar{X})=13\)
\item
  \(V(X)=\frac{0.35^2}{8}=0.01530169\); \(se=\frac{0.35}{\sqrt{8}}=0.1237\)
\end{itemize}

then the \textbf{observed error} in the estimation is the difference

\[\bar{x}_{stock}-\mu=13.21-13=0.21\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{margin-of-error-2}{%
\section{Margin of error}\label{margin-of-error-2}}

When deciding whether the \textbf{error} in estimation: \(\bar{X}-\mu\) is large or not we usually compare it with a predefined tolerance.

\begin{itemize}
\tightlist
\item
  The \textbf{margin of error} at \(5\%\) level is the distance \(m\) such that distribution of \(\bar{X}\) captures \(95\%\) of the estimations:
\end{itemize}

\[P(-m \leq \bar{X}-\mu \leq m)=P(\mu-m \leq \bar{X} \leq\mu + m)=0.95\]

\begin{itemize}
\tightlist
\item
  or that \(95\%\) of the values of \(\bar{X}\) are a distance \(m\) from \(\mu\).
\end{itemize}

In our example, we assume that \(\bar{X}\) is normally distributed then

\[m=z_{0.025} \frac{\sigma}{\sqrt{n}}=1.96*se=1.96\frac{0.35}{\sqrt{8}}=0.24\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{outcome-probability-density-vs-sample-mean-probability-density}{%
\section{Outcome probability density Vs sample mean probability density}\label{outcome-probability-density-vs-sample-mean-probability-density}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-90-1.pdf}

\includegraphics{_main_files/figure-latex/unnamed-chunk-91-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{real-life}{%
\section{Real life}\label{real-life}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-92-1.pdf}

\includegraphics{_main_files/figure-latex/unnamed-chunk-93-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-1}{%
\section{Interval estimation}\label{interval-estimation-1}}

We could estimate the margin of errors and errors because we ``knew'':

\begin{itemize}
\tightlist
\item
  A distribution for \(X\)
\item
  with known \(\mu\)
\item
  and known \(\sigma^2\)
\end{itemize}

In real life we do not know any, but:

\begin{itemize}
\tightlist
\item
  We can assume a distribution of \(X\)
\item
  We can estimate parameters
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-2}{%
\section{Interval estimation}\label{interval-estimation-2}}

From the margin of error equation:

\[P(-m \leq \bar{X} - \mu \leq  m)=0.95\]

let's solve for \(\mu\) (the real unknown)

\[P(\bar{X} - m \leq \mu \leq \bar{X} + m)=0.95\]

The left and right limits of the inequality are random variables which motivate the definition for the \textbf{random confidence interval at \(95\%\)}

\begin{itemize}
\tightlist
\item
  \((L,U)\) such that \(P(L \leq \mu \leq U )=0.95\).
\end{itemize}

When the interval captures the \textbf{error}: \((\bar{X}-\mu)\) then

\[(L,U)=(\bar{X} - m,\bar{X} + m)\]

This interval is a \textbf{random variable} and it has by definition a probability of \(0.95\) to contain \(\mu\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-3}{%
\section{Interval estimation}\label{interval-estimation-3}}

When we perform \(n\)-random experiments (\(n\)-sample) we can calculate \(m\) if \(X\) is normal and we know \(\sigma^2\).

\begin{itemize}
\tightlist
\item
  the interval that we obtain from the experiment is
\end{itemize}

\((l,u)=(\bar{x} - m,\bar{x} + m)\) (script size)

\begin{itemize}
\item
  this interval either contains or does not the parameter \(\mu\): we will \textbf{never know}!
\item
  We say that we have a confidence of \(95\%\) that the interval \((l,u)\) will capture the true unknown parameter \(\mu\). Think of buying a lottery ticket for which you do not know the result.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-4}{%
\section{Interval estimation}\label{interval-estimation-4}}

In our example, we assume that \(\bar{X}\) is normally distributed then

\[m=z_{0.025} \frac{\sigma}{\sqrt{n}}\]

and the \(95\%\) confidence interval is

\[(l,u)=(\bar{x}-z_{0.025} \frac{\sigma}{\sqrt{n}}, \bar{x}+z_{0.025} \frac{\sigma}{\sqrt{n}})= (12.97,13.45)\]

or

\[\hat{\mu}=13.21 \pm 0.24\]
It also means that, in the estimation, we are confident about the units but not so much about the decimal places.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-5}{%
\section{Interval estimation}\label{interval-estimation-5}}

For a sample of 8 observations, we have one estimate of the mean and one confidence interval

\includegraphics{_main_files/figure-latex/unnamed-chunk-94-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-6}{%
\section{Interval estimation}\label{interval-estimation-6}}

Every time that we obtain a new sample then the estimates change. If we perform 100 samples then \(95%
\) of the confidence intervals will contain \(\mu\) (we do not know which!)

\includegraphics{_main_files/figure-latex/unnamed-chunk-95-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-7}{%
\section{Interval estimation}\label{interval-estimation-7}}

If the \(95\%\) confidence interval at confidence limit of \(\alpha=0.05%
\) (the amount of probability that is left out in the probability distribution) when \(X \rightarrow N(\mu, \sigma)\) and known \(\sigma^2\) is

\[(l,u) = (\bar{x} - z_{0.025}\frac{\sigma}{\sqrt{n}},\bar{x} +z_{0.025} \frac{\sigma}{\sqrt{n}})\]

Likewise, the \(99\%\) confidence interval at confidence limit \(\alpha=0.01\) is

\((l,u) = (\bar{x} - z_{0.005}\frac{\sigma}{\sqrt{n}},\bar{x} + z_{0.005}\frac{\sigma}{\sqrt{n}})\)

\[= (\bar{x} - 2.58\frac{\sigma}{\sqrt{n}},\bar{x} + 2.58\frac{\sigma}{\sqrt{n}})\]

or

\[\hat{\mu}=\bar{x} \pm 2.58\frac{\sigma}{\sqrt{n}}\]

If we want to be more confident then we need larger confidence intervals!

For our cables:

\[\hat{\mu}= 13.21 \pm 0.31\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-8}{%
\section{Interval estimation}\label{interval-estimation-8}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-17}{%
\section{Example}\label{example-17}}

A metallic material is tested for impact to measure the energy required to cut it at a given temperature.

\begin{itemize}
\item
  Ten specimens of A238 steel were cut at 60ºC at the following impact energies (J)
\item
  \(64.1, 64.7, 64.5, 64.6, 64.5, 64.3, 64.6, 64.8, 64.2, 64.3\)
\item
  If we know that the impact energy is randomly distributed with \(\sigma=1J\) what is the \(95\%\) CI for the mean of these data?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-18}{%
\section{Example}\label{example-18}}

We know

\begin{itemize}
\tightlist
\item
  \(x_i=\{64.1, 64.7, 64.5, 64.6, 64.5, 64.3, 64.6, 64.8, 64.2, 64.3\}\)
\item
  \(X \rightarrow N(\mu, \sigma^2)\)
\item
  \(\sigma=1J\)
\item
  \(\alpha=0.05\)
\end{itemize}

The confidence interval is then

\[CI=(\bar{x}-1.96 \frac{\sigma}{\sqrt{n}}, \bar{x}+1.96  \frac{\sigma}{\sqrt{n}})\]
\[=(64.46-1.96 \frac{1}{\sqrt{10}}, 64.46+1.96  \frac{1}{\sqrt{10}})=(63.84,65.08)\]

or

\[\hat{\mu}=64.46 \pm 0.61\]

this tells us that we can be sure on the first digit (6), somewhat confident on the second (4), and unsure on the decimals (46).

What if \(\sigma^2\) is unknown?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{t-statistic-3}{%
\section{T-statistic}\label{t-statistic-3}}

When

\begin{itemize}
\tightlist
\item
  \(\sigma\) is \textbf{unknown}
\end{itemize}

However, if \(X\) is normal

\[X \rightarrow N(\mu, \sigma^2)\] then the standardized statistic

\[T=\frac{\bar{X}-\mu}{\frac{S}{\sqrt{n}}}\]

Follows a \(t\)-distribution with \(n-1\) degrees of freedom, and we can compute probabilities on \(\bar{X}\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{t-statistic-4}{%
\section{T-statistic}\label{t-statistic-4}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-96-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{t-statistic-5}{%
\section{T-statistic}\label{t-statistic-5}}

To compute the margin of error \(m\) at \(5\%\) level when \(n\) is small, \(\sigma^2\) unknown but \(X\) normal

\(P(\mu-m \leq \bar{X} \leq\mu + m)\)
\[=P(-\frac{m}{s/\sqrt{n}} \leq T \leq\frac{m}{s/\sqrt{n}})=0.95\]

We use the \(t\)-distribution

\[m=t_{0.025, n-1} \frac{s}{\sqrt{n}}\]
where \(t_{0.025, n-1}\) is the value \(T\) that leaves \(2.5\%\) at each side of \(t\)-distribution with \(n-1\) degrees of freedom (\(0.025\)-quantile)

the \(95\%\) confidence interval is then

\[(l,u)=(\bar{x}-t_{0.025, n-1} \frac{s}{\sqrt{n}}, \bar{x}+t_{0.025, n-1} \frac{s}{\sqrt{n}})\]
in R: \(t_{0.025, n-1}\)=qt(1-0.025, n-1)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-19}{%
\section{Example}\label{example-19}}

A metallic material is tested for impact to measure the energy required to cut it at a given temperature.

\begin{itemize}
\item
  Ten specimens of A238 steel were cut at 60ºC at the following impact energies (J)
\item
  \(64.1, 64.7, 64.5, 64.6, 64.5, 64.3, 64.6, 64.8, 64.2, 64.3\)
\item
  If we know that the impact energy is randomly distributed but we \textbf{do not know} the variance what is the \(95\%\) CI for the mean of these data?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-20}{%
\section{Example}\label{example-20}}

\begin{itemize}
\tightlist
\item
  \(\bar{x}=64.46\)
\item
  \(s=0.227\)
\item
  \(\alpha=0.05\)
\item
  \(t_{0.025,9}=2.26\) obtained from \(P(T \leq t_{0.025,9})=0.975\); qt(1-0.025, 9)
\end{itemize}

The CI interval is then

\(CI=(\bar{x}- t_{0.025,9}\frac{s}{\sqrt{n}},\bar{x}+t_{0.025,9} \frac{s}{\sqrt{n}})\)

\[=(64.46-2.26 \frac{0.227}{\sqrt{10}},64.46+2.26 \frac{0.227}{\sqrt{10}})\] \[=(64.29,64.62)\]

but \(CI=(63.84,65.08)\) when \(\sigma=1\). Data suggests \(\sigma<1\).

R: t.test(c(64.1,64.7,64.5,64.6,64.5,64.3,64.6,64.8,64.2,64.3))

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{ic-with-clt}{%
\section{IC with CLT}\label{ic-with-clt}}

If we do not know how \(X\) distributes but take a large sample \(n\ge 30\) then we can use the CLT to find the CI intervals.

the \(95\%\) confidence interval is then

\[(l,u)=(\bar{x}-z_{0.025} \frac{s}{\sqrt{n}}, \bar{x}+z_{0.025} \frac{s}{\sqrt{n}})\]
since \(t_{0.025, n-1} \rightarrow z_{0.025}\) for \(n \rightarrow \infty\) then it is also ok to use the T distribution for large \(n\) and unknown distribution of \(X\).

Note: This is why R only implements t.test and not z.test in the base functions to compute CI.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Example:}

Consider an experiment where we measure the concentration in blood of a drug after 10-hour administration in \(30\) patients. We obtain the following results:

\begin{verbatim}
##  [1] 0.42172863 0.28830514 0.66452743 0.01578868 0.02810549 0.15825061
##  [7] 0.15711365 0.07263340 1.36311823 0.01457672 0.50241503 0.24010736
## [13] 0.14050681 0.18855892 0.09414202 0.42489306 0.78160177 0.23938021
## [19] 0.29546742 2.02050586 0.42157487 0.48293561 0.74263790 0.67402224
## [25] 0.58426449 0.80292617 0.74837143 0.78532627 0.01588387 0.29892485
\end{verbatim}

\begin{itemize}
\item
  the average is \(\bar{x}=0.4556198\)
\item
  the standard deviation is \(s=0.4335571\)
\item
  the histogram of the results is:
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-98-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{central-limit-theorem-4}{%
\section{Central Limit Theorem}\label{central-limit-theorem-4}}

We \textbf{assumed} that \(X \rightarrow exp(\lambda=2)\)

With mean and variance:

\begin{itemize}
\tightlist
\item
  \(E(X)=\frac{1}{\lambda}=0.5\)
\item
  \(V(X)=\frac{1}{\lambda^2}=0.25\)
\end{itemize}

The error was

\[\bar{x}-E(X)=0.4556198-0.5=-0.0443802\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{ci-with-clt}{%
\section{CI with CLT}\label{ci-with-clt}}

What happens if we do not know the value of \(E(X)\)?

\begin{itemize}
\tightlist
\item
  We use a 95\% CI to estimate it
\end{itemize}

Since \(n \geq 30\)

\[\bar{X} \sim_{aprox}  N(\lambda, \frac{1}{n\lambda^2})\]

and the \(95\%\) confidence interval is then

\[(l,u)=(\bar{x}-z_{0.025} \frac{s}{\sqrt{n}}, \bar{x}+z_{0.025} \frac{s}{\sqrt{n}})\]

\((l,u)=(0.4556198-1.96\frac{0.4335571}{\sqrt{30}}, 0.4556198+1.96\frac{0.4335571}{\sqrt{30}})\)

\[=(0.300,0.610)\]
or

\[\hat{\mu}=0.45 \pm 0.15\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{_main_files/figure-latex/unnamed-chunk-99-1.pdf} \includegraphics{_main_files/figure-latex/unnamed-chunk-99-2.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-for-proportions}{%
\section{Interval estimation for proportions}\label{interval-estimation-for-proportions}}

A random sample of \(400\) patients was selected for testing a new vaccine for the influenza virus, after \(6\) months of vaccination \(136\) were ill.~

\begin{itemize}
\tightlist
\item
  What is the expected efficacy of the vaccine?
\end{itemize}

We have \(136\) failures in \(400\) trials, each trial is a
Bernoulli trial

\[X \rightarrow Bernoulli(p)\]

with:

\begin{itemize}
\tightlist
\item
  the probability \(p\) of failure for one person (\(k=1\))
\item
  mean \(E (X) = p\)
\item
  variance \(V (X) = p (1-p)\)
\end{itemize}

We want to have a \(95\%\) CI for \(p\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-for-proportions-1}{%
\section{Interval estimation for proportions}\label{interval-estimation-for-proportions-1}}

If the distribution of a random experiment is

\[X \rightarrow Bernoulli (p)\]
Then \(\bar{X}\) has

\begin{itemize}
\tightlist
\item
  mean \(E(\bar{X})=E(X)=p\) (unbiased estimator of \(p\))\\
\item
  variance \(V(\bar{X})=\frac{V(X)}{n}=\frac{p(p-1)}{n}\) (consistent estimator of \(p\))
\end{itemize}

\[\hat{p}=\bar{x}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-for-proportions-2}{%
\section{Interval estimation for proportions}\label{interval-estimation-for-proportions-2}}

When \(\hat{p}n>5\) and \((\hat{p}-1)n>5\)

\begin{itemize}
\tightlist
\item
  The \textbf{standardized statistic} of \(\bar{X}\) can be approximated by a standard distribution
\end{itemize}

\[Z=\frac{\bar{X}-E(\bar{X})}{\sqrt{V(\bar{X})}}= \frac{\bar{X}-p}{\big[\frac{p(1-p)}{n} \big]^{1/2}}\rightarrow N(0,1)\]

\begin{itemize}
\tightlist
\item
  The \(95\%\) CI interval of \(p\) is:
\end{itemize}

\[CI=(l,u)=(\bar{x}-z_{0.025}\big[\frac{\bar{x}(1-\bar{x})}{n} \big]^{1/2},  \bar{x}+z_{0.025}\big[\frac{\bar{x}(1-\bar{x})}{n} \big]^{1/2})\]
Where we estimate the Bernoulli variance \(p(1-p)\) by \(\bar{x}(1-\bar{x})\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-for-proportions-3}{%
\section{Interval estimation for proportions}\label{interval-estimation-for-proportions-3}}

In our case, we are counting failures on vaccinations \(136\) in \(400\) trials

we know

\begin{itemize}
\tightlist
\item
  \(\bar{x}=134/400=0.34\)
\item
  \(z_{0.025}=1.96\)
\end{itemize}

\(CI=(l,u)=(\bar{x}-1.96 \big[\frac{\bar{x}(1-\bar{x})}{n} \big]^{1/2}, \bar{x}+1.96 \big[\frac{\bar{x}(1-\bar{x})}{n} \big]^{1/2})\)

\[=(0.29,0.39)\]

The probability of failure of the vaccine is

\[\hat{p}=0.34 \pm 0.05\]
Note: Polls for the intention to vote (Bernoulli trial) in a sample of \(n\) individuals report this type of estimate with its margin of error. It does not mean that the \textbf{true value} of \(p\) is within this interval with probability \(95\%\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-vs-confidence}{%
\section{Probability Vs Confidence}\label{probability-vs-confidence}}

There are two views of uncertainty:

\begin{itemize}
\tightlist
\item
  \textbf{Future}: probability on observations
\end{itemize}

When \textbf{we know} the probability distribution of our random experiment we ask:

What is the \textbf{probability} that a \textbf{new} value of \(\bar{X}\) is close to \(\mu\) ?

\includegraphics{_main_files/figure-latex/unnamed-chunk-100-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{probability-vs-confidence-1}{%
\section{Probability Vs Confidence}\label{probability-vs-confidence-1}}

\begin{itemize}
\tightlist
\item
  \textbf{Present}: confidence on parameters
\end{itemize}

When we have \textbf{observations} and do \textbf{not know} the parameter \(\mu\) we ask:

What is the range of values of \(\bar{X}\) where we \textbf{believe} that \(\mu\) \textbf{is} with \(95\%\) confidence?

We use the \textbf{margin of error} to compute the CI, but it does not mean we have calculated an error (we don't know where \(\mu\) is).

\includegraphics{_main_files/figure-latex/unnamed-chunk-101-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-for-the-variance}{%
\section{Interval estimation for the variance}\label{interval-estimation-for-the-variance}}

A metallic material is tested for impact to measure the energy required to cut it at a given temperature.

\begin{itemize}
\item
  Ten specimens of A238 steel were cut at 60ºC at the following impact energies (J)
\item
  \(64.1, 64.7, 64.5, 64.6, 64.5, 64.3, 64.6, 64.8, 64.2, 64.3\)
\end{itemize}

We know that the estimate for \(s^2=0.227^2=0.05\), but what is its confidence interval?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-for-the-variance-1}{%
\section{Interval estimation for the variance}\label{interval-estimation-for-the-variance-1}}

When \(X \hookrightarrow N(\mu, \sigma^2)\).

\[W=\frac{S^2(n-1)}{\sigma^2}\]
Captures the proportion in the error of \(\sigma^2\) and
follows a \(\chi^2\) distribution with \(n-1\) degrees of freedom

\[\frac{S^2}{\sigma^2}(n-1)\rightarrow \chi^2_{n-1}\]

\begin{itemize}
\tightlist
\item
  We look for confidence interval of \(\sigma^2\) at confidence \(95\%\) \((L,U)\) such that \[P(L \leq \sigma^2 \leq U)=0.95\]
\end{itemize}

We can use the \(\chi^2\) to determine the \(95\%\) of the distribution about \(W\)

\[P(\chi^2_{0.975,n-1} \leq W \leq \chi^2_{0.025,n-1})=0.95\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{chi2-statistic-2}{%
\section{\texorpdfstring{\(\chi^2\)-statistic}{\textbackslash chi\^{}2-statistic}}\label{chi2-statistic-2}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-102-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-for-the-variance-2}{%
\section{Interval estimation for the variance}\label{interval-estimation-for-the-variance-2}}

replacing the value of \(W\)

\[P(\chi^2_{0.975,n-1} \leq \frac{S^2}{\sigma_X^2}(n-1) \leq \chi^2_{0.025,n-1})=0.95\]

and solving for \(\sigma_X^2\)

\[P(\frac{S^2 (n-1)}{\chi^2_{0.025,n-1}}\leq \sigma_X^2 \leq \frac{S^2(n-1)}{\chi^2_{0.975,n-1}})=0.95\]

The random interval at \(95\%\) confidence

\[(L,U) = (\frac{S^2 (n-1)}{\chi^2_{0.025,n-1}},\frac{S^2(n-1)}{\chi^2_{0.975,n-1}})\]

and the \(95\%\) confidence interval (script size)

\[(l,u) = (\frac{s^2 (n-1)}{\chi^2_{0.025,n-1}},\frac{s^2(n-1)}{\chi^2_{0.975,n-1}})\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-for-the-variance-3}{%
\section{Interval estimation for the variance}\label{interval-estimation-for-the-variance-3}}

\(\chi^2_{0.975,n-1}=F^{-1}(0.025)\)
for \(n=10\) or \(df=n-1=9\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chi0}\FloatTok{.975} \OtherTok{\textless{}{-}} \FunctionTok{qchisq}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\AttributeTok{df=}\DecValTok{9}\NormalTok{)}
\NormalTok{chi0}\FloatTok{.975}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.700389
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chi0}\FloatTok{.025} \OtherTok{\textless{}{-}} \FunctionTok{qchisq}\NormalTok{(}\FloatTok{0.975}\NormalTok{, }\AttributeTok{df=}\DecValTok{9}\NormalTok{)}
\NormalTok{chi0}\FloatTok{.025}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 19.02277
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-9}{%
\section{Interval estimation}\label{interval-estimation-9}}

In our example

\begin{itemize}
\tightlist
\item
  \(s=0.227\)
\item
  \(n=10\)
\end{itemize}

\(\hat{\sigma}^2=(l,u) = (\frac{s^2 (n-1)}{\chi^2_{0.025,n-1}},\frac{s^2(n-1)}{\chi^2_{0.975,n-1}})\)

\[= (\frac{0.227^2 (10-1)}{19.02277},\frac{0.227^2(10-1)}{2.700389})=(0.02,0.17)\]

According to the data \(\sigma^2 \neq 1\) at \(95\%\) confidence: Had we made an error considering \(\sigma=1\) when we calculated the first CI for this data?

in R: library(Ecfun);
confint.var(0.05, 9)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-10}{%
\section{Interval estimation}\label{interval-estimation-10}}

The interval for the variance is \textbf{not symmetric} and we cannot formulate it as an estimate \(\pm\) margin of error.

\includegraphics{_main_files/figure-latex/unnamed-chunk-103-1.pdf}

\hypertarget{hypothesis-testing}{%
\chapter{Hypothesis testing}\label{hypothesis-testing}}

\hypertarget{objective-13}{%
\section{Objective}\label{objective-13}}

\begin{itemize}
\tightlist
\item
  Hypothesis testing of means and proportions
\item
  Hypothesis testing of variances
\item
  Errors in hypothesis testing
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis}{%
\section{Hypothesis}\label{hypothesis}}

When we make inferences about our process, we often want to test if the process satisfies a desired condition/property

\begin{itemize}
\item
  Measurements and their inferences provide \textbf{evidence} for that condition.
\item
  We can formulate the condition in terms of the values that some \textbf{parameters} of probability distributions can take.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-1}{%
\section{Hypothesis}\label{hypothesis-1}}

Examples:

\begin{itemize}
\tightlist
\item
  Tyre manufacturers want to know whether the half-life of the tires they produce is at least 20,000 km
\item
  Fertilizer developers want to test whether their new product has a real effect on the growth of plants
\item
  Pharmaceutical companies need to know if chemotherapy can cure 90\% of cancer patients
\end{itemize}

These questions can be translated into statements of probability distributions

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-2}{%
\section{Hypothesis}\label{hypothesis-2}}

\begin{itemize}
\tightlist
\item
  Tyre manufacturers want to know whether the half-life of the tires they produce is at least 20,000 km
\end{itemize}

Assuming that the life of tires follows a population probability distribution, we are interested in finding if the mean of the distribution is at least 20,000Km.

This can be done in two dichotomic statements

\begin{itemize}
\tightlist
\item
  The mean life of tires is \textbf{less} than 20,000km
\item
  The mean life of tires is \textbf{greater} than 20,000km
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-3}{%
\section{Hypothesis}\label{hypothesis-3}}

or being \(\mu\) the mean of the population distribution

\begin{itemize}
\tightlist
\item
  \(H_0: \mu \leq 20,000km\)
\item
  \(H_1: \mu > 20,000km\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-4}{%
\section{Hypothesis}\label{hypothesis-4}}

\textbf{Definition}

In statistics, a statement (conjecture) about the distribution of a random variable is called a \textbf{hypothesis}.

The hypothesis is usually written in two dichotomous statements

\begin{itemize}
\tightlist
\item
  The \textbf{null} hypothesis: \(H_0\) when the conjecture is False (usually refers to status quo)
\item
  The \textbf{alternative} hypothesis: \(H_1\) when the conjecture is True (usually refers to research hypothesis)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{null-hypothesis}{%
\section{Null hypothesis}\label{null-hypothesis}}

So what are the null and the alternative hypothesis for these situations?

\begin{itemize}
\item
  Tyre manufacturers want to know whether the half-life of the tires they produce is at least 20,000 km
\item
  Fertilizer developers want to test whether their new product has a real effect on the growth of plants
\item
  Pharmaceutical companies need to know if chemotherapy can cure 90\% of cancer patients
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{null-hypothesis-1}{%
\section{Null hypothesis}\label{null-hypothesis-1}}

\begin{itemize}
\tightlist
\item
  Fertilizer developers want to test whether their new product has a real effect on the growth of plants
\end{itemize}

Being \(\mu_0\) the mean growth of the plants \textbf{without} fertilizer (known) and \(\mu\) the mean growth of the plants with the fertilizer (unknown)

\begin{itemize}
\tightlist
\item
  \(H_0:\mu \leq \mu_0\) (The fertilizer does nothing: status quo)
\item
  \(H_1:\mu > \mu_0\) (The fertilizer has the desired effect: research interest)
\end{itemize}

What could be a suitable distribution of \(\mu\)?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Example:}

You perform \(8\) random experiments: Load a cable until it breaks and record the breaking load. These are the results.

\begin{verbatim}
## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The average of the data is \(\bar{x}=13.21\)
\item
  The standard deviation is \(s=0.3571565\)
\item
  We may want to use this data to show that our cables break \textbf{on average} at more than \(13\) Tons.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Example:}

If we \textbf{hypothesize} that our cables truly distribute as \[X \rightarrow N(\mu=13, \sigma^2=0.35^2)\] then

\[\bar{X} \rightarrow N(13, \frac{0.35^2}{8})\]
We ask:

\begin{itemize}
\tightlist
\item
  Are the measurements consistent with the null hypothesis \(H_0: \mu=13\)?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-test-with-acceptancerejection-zones}{%
\section{Hypothesis test with acceptance/rejection zones}\label{hypothesis-test-with-acceptancerejection-zones}}

\begin{itemize}
\tightlist
\item
  \(H_0:\mu = 13\) (cables break as usual: status quo)
\item
  \(H_1:\mu \neq 13\) (cables do not brake as usual: research interest)
\end{itemize}

To test the hypothesis contrast the standardized \textbf{observed error} with the standardized \textbf{margin of error} from the null hypothesis.

Since

\[\bar{X} \rightarrow N(13, \frac{0.35^2}{8})\]

Then the \textbf{standardized error} from the null hypothesis follows a standard distribution

\[Z=\frac{\bar{X}-13}{\frac{0.35}{\sqrt{8}}}  \rightarrow N(0,1)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standardized-margin-of-errors}{%
\section{standardized margin of errors}\label{standardized-margin-of-errors}}

\begin{itemize}
\tightlist
\item
  The \textbf{standardized margin of errors} are the quantiles of \(Z\)
\end{itemize}

\[P(-z_{0.025} \leq Z \leq z_{0.025})=0.95\]
The interval: \[(-z_{0.025}, z_{0.025})\] is called acceptance interval of \(H_0\) at \(95\%\) confidence level.

\begin{itemize}
\tightlist
\item
  \(\alpha=0.05=2\times 0.025=1-0.95\) is called the \textbf{significance} limit.
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-105-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standardized-observed-error}{%
\section{Standardized observed error}\label{standardized-observed-error}}

\begin{itemize}
\tightlist
\item
  The \textbf{standardized observed error} is
\end{itemize}

\[z_{obs}=\frac{\bar{x}-13}{\frac{0.35}{\sqrt{8}}}=1.697056 \in (-z_{0.025}, z_{0.025})\]
We conclude:

\begin{itemize}
\item
  Our observed error is consistent with \(95\%\) of the observations for the statistic \(Z\) when the null hypothesis is true
\item
  We accept that the \(H_0\) is true and give up on the idea that we have stronger cables than expected.
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-106-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-test-with-p-value}{%
\section{Hypothesis test with P-value}\label{hypothesis-test-with-p-value}}

We can also contrast the hypothesis by calculating the probability that the average of another sample will be rarer than the average we just observe.

\[pvalue = P(Z \leq -z_{obs}) + P(z_{obs} \leq Z) = 2 (1-\phi(|z_{obs}|))\]

\begin{itemize}
\tightlist
\item
  We reject \(H_0\) if \(pvalue \leq \alpha =0.05\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-107-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standardized-observed-error-1}{%
\section{Standardized observed error}\label{standardized-observed-error-1}}

\begin{itemize}
\tightlist
\item
  The \textbf{pvalue} is
\end{itemize}

\[pvalue=2 (1-\phi(1.697056))=0.089\]
R: 2*(1-pnorm(1.697056))

We conclude:

\begin{itemize}
\item
  If we performed a new sample is not unlikely that we can get a more extreme result for the average at limit \(\alpha=0.05\) if the null hypothesis is true.
\item
  We accept that \(H_0\) could have produced our data and give up on the idea that we have stronger cables than expected.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-test-confidence-interval}{%
\section{Hypothesis test Confidence Interval}\label{hypothesis-test-confidence-interval}}

From the point of view of the estimation, we can also contrast the hypothesis.

\begin{itemize}
\tightlist
\item
  We trust that our estimation of \(\mu\) is correct with \(95\%\) confidence
\end{itemize}

The CI is:

\[(l,u)=(\bar{x}-z_{0.025} \frac{\sigma}{\sqrt{n}}, \bar{x}+z_{0.025} \frac{\sigma}{\sqrt{n}})= (12.97,13.45)\]

\begin{itemize}
\item
  The CI tells us that we can be \(95\%\) confident that we have captured the true value of \(\mu\).
\item
  We don't know the true value of \(\mu\) but \(H_0: \mu=13\) tons could be it.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-test-confidence-interval-1}{%
\section{Hypothesis test Confidence Interval}\label{hypothesis-test-confidence-interval-1}}

\begin{itemize}
\tightlist
\item
  Since
\end{itemize}

\[H_0: \mu=13 \in (12.97,13.45)\]
We conclude:

\begin{itemize}
\item
  Our data is consistent with the fact that our estimate of \(\mu\) is the null hypothesis.
\item
  We accept that \(H_0\) could have produced our interval and give up on the idea that we have stronger cables than expected.
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-108-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-test-with-unknown-variance}{%
\section{Hypothesis test with unknown variance}\label{hypothesis-test-with-unknown-variance}}

It is common to \textbf{hypothesize} the values of the parameters we can contrast. Other nuisance parameters we may leave unknown.

We can \textbf{hypothesize} that our cables truly distribute as \[X \rightarrow N(\mu=13, \sigma^2)\] then

\[\bar{X} \rightarrow N(13, \frac{\sigma^2}{8})\]
We ask again:

\begin{itemize}
\tightlist
\item
  Are the measurements consistent with the null hypothesis \(H_0: \mu=13\)?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standardized-error-with-unknown-variance}{%
\section{Standardized error with unknown variance}\label{standardized-error-with-unknown-variance}}

If \(X\) is normal

\[X \rightarrow N(\mu, \sigma^2)\] then the \textbf{standardized errors} with respect to the \textbf{sample standard deviation} \(S\)

\[T=\frac{\bar{X}-\mu}{\frac{S}{\sqrt{n}}}\]

Follows a \(t\)-distribution with \(n-1\) degrees of freedom.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-testing-with-unknown-variance}{%
\section{Hypothesis testing with unknown variance}\label{hypothesis-testing-with-unknown-variance}}

we accept \(H_0\) because of any of following the equivalent contrasts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The acceptance region for \(H_0\) is:
\end{enumerate}

\[(-t_{0.025,7}, t_{0.025,7})=( -2.36,  2.36)\]

and the observed standardized error from \(H_0\) is
\[t_{obs} =  \frac{13.21268-13}{\frac{0.3571565}{\sqrt{8}}}=1.6843\]

within the acceptance region.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-testing-with-unknown-variance-1}{%
\section{Hypothesis testing with unknown variance}\label{hypothesis-testing-with-unknown-variance-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The \[pvalue=2(1-F^{-1}_{t,7}(1.6843))=0.136\]
\end{enumerate}

R: 2*(1-pt(1.6843,7))

is higher than \(\alpha=0.05\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The confidence interval
\end{enumerate}

\[(\bar{x}-t_{0.025, n-1} \frac{s}{\sqrt{n}}, \bar{x}+t_{0.025, n-1} \frac{s}{\sqrt{n}})=(12.91409, 13.51127)\]

contains \(H_0:\mu=13\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

in R:
t.test(c(13.34642, 13.32620, 13.01459, 13.10811,
12.96999, 13.55309, 13.75557, 12.62747), mu=13)

\includegraphics{_main_files/figure-latex/unnamed-chunk-109-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{one-tailed-test}{%
\section{One-tailed test}\label{one-tailed-test}}

We may be interested in only testing for the fact that our estimate is higher than the null hypothesis (we do not care if it is lower)

Upper-tailed test:

\begin{itemize}
\tightlist
\item
  \(H_0:\mu = 13\) (cables break as usual)
\item
  \(H_1:\mu > 13\) (cables break at a higher load) -We will test the higher tail of the distribution.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-testing-of-the-upper-tail}{%
\section{Hypothesis testing of the upper tail}\label{hypothesis-testing-of-the-upper-tail}}

In this example, we accept \(H_0\) because of any of the following equivalent contrasts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The acceptance region for \(H_0\) is:
\end{enumerate}

\[(-\infty, t_{0.05,7})=( -\infty,  1.894579)\]

and the observed standardized error from \(H_0\) is
\[t_{obs} =  \frac{13.21268-13}{\frac{0.3571565}{\sqrt{8}}}=1.6843\]

within the acceptance region.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-testing-with-unknown-variance-2}{%
\section{Hypothesis testing with unknown variance}\label{hypothesis-testing-with-unknown-variance-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  For the upper tail \[pvalue=1-F^{-1}_{t,7}(1.6843)=0.06799782\]
\end{enumerate}

R: 1-pt(1.6843,7)

is higher than \(\alpha=0.05\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The \textbf{upper tailed} confidence interval
\end{enumerate}

\[(\bar{x}-t_{0.05, n-1} \frac{s}{\sqrt{n}}, \infty)=(12.97344, \infty)\]

contains \(H_0:\mu=13\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

in R:
t.test(c(13.34642, 13.32620, 13.01459, 13.10811,
12.96999, 13.55309, 13.75557, 12.62747), mu=13, alternative=``greater'')

\includegraphics{_main_files/figure-latex/unnamed-chunk-110-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-1-4}{%
\section{Example 1:}\label{example-1-4}}

\(11.6\) g of NaCl is dissolved in \(100\) g of water and has a molar concentration of \(1.92 mol/L\)

We design a process to remove salt from this concentration and obtain the following results

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{concentration }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{1.716}\NormalTok{, }\FloatTok{1.889}\NormalTok{, }\FloatTok{1.783}\NormalTok{, }\FloatTok{1.849}\NormalTok{, }\FloatTok{1.891}\NormalTok{)}
\NormalTok{concentration}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.716 1.889 1.783 1.849 1.891
\end{verbatim}

\begin{itemize}
\tightlist
\item
  We want to test at \(0.05\) significant threshold if the process does remove salt from the concentration.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-1-5}{%
\section{Example 1:}\label{example-1-5}}

Two-tailed test:

\begin{itemize}
\tightlist
\item
  \(H_0:\mu=1.92\); \(H_1:\mu \neq 1.92\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{1.716}\NormalTok{, }\FloatTok{1.901}\NormalTok{, }\FloatTok{1.783}\NormalTok{, }\FloatTok{1.849}\NormalTok{, }\FloatTok{1.891}\NormalTok{), }
       \AttributeTok{mu=}\FloatTok{1.92}\NormalTok{, }\AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  c(1.716, 1.901, 1.783, 1.849, 1.891)
## t = -2.6389, df = 4, p-value = 0.05764
## alternative hypothesis: true mean is not equal to 1.92
## 95 percent confidence interval:
##  1.731206 1.924794
## sample estimates:
## mean of x 
##     1.828
\end{verbatim}

Lower-tailed test:

\begin{itemize}
\tightlist
\item
  \(H_0:\mu=1.92\); \(H_1:\mu < 1.92\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{1.716}\NormalTok{, }\FloatTok{1.901}\NormalTok{, }\FloatTok{1.783}\NormalTok{, }\FloatTok{1.849}\NormalTok{, }\FloatTok{1.891}\NormalTok{), }
       \AttributeTok{mu=}\FloatTok{1.92}\NormalTok{, }\AttributeTok{alternative =} \StringTok{"less"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  c(1.716, 1.901, 1.783, 1.849, 1.891)
## t = -2.6389, df = 4, p-value = 0.02882
## alternative hypothesis: true mean is less than 1.92
## 95 percent confidence interval:
##      -Inf 1.902322
## sample estimates:
## mean of x 
##     1.828
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-2-3}{%
\section{Example 2:}\label{example-2-3}}

In some cases, we are not sure about the numerical value of the hypothesis to test, but we know that we want to improve the value of a parameter in two different conditions.

In the original paper of Gosset, he analyzed the effect of two soporific medicines.

\begin{itemize}
\tightlist
\item
  10 individuals were given \textbf{soporific 1} and write down the additional hours slept under treatment, with a mean \(0.75\)
\end{itemize}

\begin{verbatim}
##  [1]  0.7 -1.6 -0.2 -1.2 -0.1  3.4  3.7  0.8  0.0  2.0
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The same 10 individuals were given \textbf{soporific 2} and write down the additional hours slept under treatment, with a mean \(2.33\)
\end{itemize}

\begin{verbatim}
##  [1]  1.9  0.8  1.1  0.1 -0.1  4.4  5.5  1.6  4.6  3.4
\end{verbatim}

Scientific hypothesis: Soporific 2 is better than soporific 1

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-2-4}{%
\section{Example 2:}\label{example-2-4}}

For each individual, Gosset made the difference between the treatments. Taking \(X\) as the difference between treatments, this was the sample observed for \(X\)

\begin{verbatim}
##  [1] 1.2 2.4 1.3 1.3 0.0 1.0 1.8 0.8 4.6 1.4
\end{verbatim}

finding an average of treatment gain from soporific 2 with respect to soporific 1 of \(1.58\), and \(s=1.229995\)

\textbf{Upper-tailed paired t-test}:

\begin{itemize}
\tightlist
\item
  \(H_0:\mu=0\); \(H_1:\mu >\)
\end{itemize}

Where \(\mu\) is the mean of the differences between treatments.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-2-5}{%
\section{Example 2:}\label{example-2-5}}

The \textbf{standardized error} is:

\[T=\frac{\bar{X}}{\frac{S}{\sqrt{n}}}\]

and its observation

\[t_{obs}=\frac{\bar{x}}{\frac{s}{\sqrt{n}}}\]
which is also known as the \textbf{noise} to \textbf{signal} ratio.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{1.9}\NormalTok{,}\FloatTok{0.8}\NormalTok{,}\FloatTok{1.1}\NormalTok{,}\FloatTok{0.1}\NormalTok{,}\SpecialCharTok{{-}}\FloatTok{0.1}\NormalTok{,}\FloatTok{4.4}\NormalTok{,}\FloatTok{5.5}\NormalTok{,}\FloatTok{1.6}\NormalTok{,}\FloatTok{4.6}\NormalTok{,}\FloatTok{3.4}\NormalTok{),}
       \FunctionTok{c}\NormalTok{(}\FloatTok{0.7}\NormalTok{,}\SpecialCharTok{{-}}\FloatTok{1.6}\NormalTok{,}\SpecialCharTok{{-}}\FloatTok{0.2}\NormalTok{,}\SpecialCharTok{{-}}\FloatTok{1.2}\NormalTok{,}\SpecialCharTok{{-}}\FloatTok{0.1}\NormalTok{,}\FloatTok{3.4}\NormalTok{,}\FloatTok{3.7}\NormalTok{,}\FloatTok{0.8}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{),}
       \AttributeTok{paired =} \ConstantTok{TRUE}\NormalTok{,}
       \AttributeTok{alternative=}\StringTok{"greater"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Paired t-test
## 
## data:  c(1.9, 0.8, 1.1, 0.1, -0.1, 4.4, 5.5, 1.6, 4.6, 3.4) and c(0.7, -1.6, -0.2, -1.2, -0.1, 3.4, 3.7, 0.8, 0, 2)
## t = 4.0621, df = 9, p-value = 0.001416
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  0.8669947       Inf
## sample estimates:
## mean of the differences 
##                    1.58
\end{verbatim}

\includegraphics{_main_files/figure-latex/unnamed-chunk-118-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-testing-with-large-n-and-any-distribution}{%
\section{Hypothesis testing with large n and any distribution}\label{hypothesis-testing-with-large-n-and-any-distribution}}

On many occasions, \(X\) is not normally distributed but we can take large samples \(n \ge 30\) then we can use the CLT:

Then the \textbf{standardized error} from the null hypothesis follows a standard distribution

\[Z=\frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}  \rightarrow N(0,1)\]

and proceed as before, and if \(\sigma\) is unknown we replace it with its estimate \(s\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-testing-for-proportions}{%
\section{Hypothesis testing for proportions}\label{hypothesis-testing-for-proportions}}

\textbf{Example:}

We may be satisfied with a new process if \(90\%\) of the times we improve the previous process.

\begin{itemize}
\tightlist
\item
  If we run a sample of \(200\) new processes and find that \(188\) times we improved the previous process, can we be satisfied with the new process at \(95\%\) confidence?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-for-proportions-4}{%
\section{Interval estimation for proportions}\label{interval-estimation-for-proportions-4}}

We hypothesize that the distribution of a random experiment is

\[X \rightarrow Bernoulli (p)\]
with an upper-tailed hypothesis contrast for \(p\):

\begin{itemize}
\tightlist
\item
  \(H_0: p=0.1\) (Not satisfactory)
\item
  \(H_1: p> 0.9\) (Satisfactory)
\end{itemize}

Then if the null hypothesis is true \(\bar{X}\) has

\begin{itemize}
\item
  mean \(E(\bar{X})=E(X)=p=0.9\) (unbiased estimator of \(p\))\\
\item
  variance \(V(\bar{X})=\frac{V(X)}{n}=\frac{p(p-1)}{n}=0.00045\) (consistent estimator of \(p\))
\item
  The observed \(\bar{X}\) was \(\bar{x}=188/200=0.94\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-for-proportions-5}{%
\section{Interval estimation for proportions}\label{interval-estimation-for-proportions-5}}

\begin{itemize}
\tightlist
\item
  By the CLT, the \textbf{standardized error} from the null hypothesis
\end{itemize}

\[Z=\frac{\bar{X}-E(\bar{X})}{\sqrt{V(\bar{X})}}= \frac{\bar{X}-p}{\big[\frac{p(1-p)}{n} \big]^{1/2}}\rightarrow N(0,1)\]
is a standard normal variable, when \(pn>5\) and \((p-1)n>5\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-for-proportions-6}{%
\section{Interval estimation for proportions}\label{interval-estimation-for-proportions-6}}

In this example, we \textbf{reject} \(H_0\) because of any of the following equivalent contrasts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The acceptance region for \(H_0\) is:
\end{enumerate}

\[(-\infty, z_{0.05})=( -\infty,  1.644854)\]

and the observed standardized error from \(H_0\) is
\[z_{obs} =  \frac{0.94-0.90}{\sqrt{0.00045}}=1.885618\]

outside the acceptance region (inside the rejection zone).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-for-proportions-7}{%
\section{Interval estimation for proportions}\label{interval-estimation-for-proportions-7}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  For the upper tail \[pvalue=1-\phi^{-1}(1.885618)=0.02967323\]
\end{enumerate}

R: 1-pnorm(1.885618)

is lower than \(\alpha=0.05\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interval-estimation-for-proportions-8}{%
\section{Interval estimation for proportions}\label{interval-estimation-for-proportions-8}}

In R: prop.test(188, 200, p=0.9, alternative = ``greater'', correct=FALSE)

\includegraphics{_main_files/figure-latex/unnamed-chunk-119-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{test-for-variances}{%
\section{Test for variances}\label{test-for-variances}}

In many cases, experiments are run to test specific values of the dispersion of data.

Such as

\begin{itemize}
\tightlist
\item
  for complying with strict design standards where measurements must be between certain values
\item
  when relative measurements are taken such as the reaction of a treatment on an individual (insulin administration on an individual's sugar levels)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{test-for-variances-1}{%
\section{Test for variances}\label{test-for-variances-1}}

For a random sample \(X_1,...X_n\) with a normal population distribution (\(X_i \rightarrow N(\mu, \sigma^2)\)) the statistics defined by

\[X=\frac{(n-1)S^2}{\sigma^2}\]

Has a \(\chi^2\) (chi-squared) distribution with n-1 degrees of freedom given by

\[f(x)=C_n  x^{\frac{n-3}{2}} e^{-\frac{x}{2}}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{test-for-variances-2}{%
\section{Test for variances}\label{test-for-variances-2}}

Suppose we want to test whether the variance of the population distribution is equal to a given value \(\sigma_0\)

\begin{itemize}
\tightlist
\item
  \(H_0:\sigma=\sigma_0\)
\end{itemize}

Alternative hypothesis

\begin{itemize}
\item
  two tailed: \(H_1:\sigma \neq \sigma_0\)
\item
  upper tailed: \(H_1:\sigma > \sigma_0\)
\item
  lower tailed: \(H_1:\sigma < \sigma_0\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{test-for-variances-3}{%
\section{Test for variances}\label{test-for-variances-3}}

\(S^2\) is an unbiased estimate of \(\sigma^2\): \(E(S^2)=\sigma^2\)

The \textbf{standardized error ratio}

\[W=\frac{(n-1)S^2}{\sigma_0^2} \rightarrow \chi^2(n-1)\]

Follows a \(\chi^2\) distribution with \(n-1\) degrees of freedom.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-21}{%
\section{Example}\label{example-21}}

\begin{itemize}
\item
  The production of a semiconductor chip is regulated by a process that requires that the thickness of a particular layer does not vary in more than \(\sigma_0=0.6mm\), from its mean of \(25mm\).
\item
  To keep control of the process every so often a sample of \(20\) specimens is taken.
\item
  If on one occasion the estimated standard deviation was \(s=0.8462188\) is the process out of control at \(0.01\) confidence and should be stopped?
\end{itemize}

This is the data:

\begin{verbatim}
##  [1] 24.51239 24.79975 26.35608 25.06134 25.11248 26.49211 25.40100 23.89940
##  [9] 24.40244 24.61227 26.06495 25.31304 25.34867 25.09629 24.51642 26.55461
## [17] 25.43313 23.28904 25.61018 24.58867
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{test-for-variances-4}{%
\section{Test for variances}\label{test-for-variances-4}}

We want to contrast the hypotheses

\begin{itemize}
\item
  \(H_0:\sigma=0.6\) (Process under control)
\item
  \(H_1:\sigma > 0.6\) (Process out of control)
\item
  Statistic: \(W=\frac{(n-1)S^2}{\sigma_0^2} \rightarrow \chi^2(n-1)\)
\item
  Threshold limit \(\alpha=0.01\)
\item
  The acceptance region for \(H_0\): \(P(W\leq \chi^2_{0.01,19})=0.9\)
\end{itemize}

\[(0, \chi^2_{0.01,19})=(0,36.19)\]

In R: \(\chi^2_{0.01,19}=\)qchisq(0.99,19)\(= 36.19\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{test-for-variances-5}{%
\section{Test for variances}\label{test-for-variances-5}}

In this example, we \textbf{reject} \(H_0\) because of any of the following equivalent contrasts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The observed \textbf{standardized error ratio} is:
\end{enumerate}

\[w_{obs}=\frac{19 (0.8462188)^2}{0.60^2}=37.79344\]

That falls outside the acceptance region (inside the rejection zone)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  For the upper tail \[pvalue=1-F_{\chi,19}^{-1}(37.24)= 0.006\]
\end{enumerate}

R: 1-pchisq(37.79344, 19)

is lower than \(\alpha=0.05\)

Therefore we need to conclude that yes! the process is out of control.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{chi2-statistic-3}{%
\section{\texorpdfstring{\(\chi^2\)-statistic}{\textbackslash chi\^{}2-statistic}}\label{chi2-statistic-3}}

in R: library(EnvStats);
varTest(thickness, sigma.squared = 0.6\^{}2, alternative = ``greater'')

\includegraphics{_main_files/figure-latex/unnamed-chunk-121-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{errors-in-hypothesis-testing}{%
\section{Errors in hypothesis testing}\label{errors-in-hypothesis-testing}}

When we infer a parameter with a statistic and then apply a criterion to decide on a hypothesis we have four possibilities

\begin{itemize}
\tightlist
\item
  \(H_0\) unknown reality: \textbf{true} or \textbf{false}
\item
  Result of \(H_0\) testing: \textbf{negative}, \textbf{positive}
\end{itemize}

We usually aim to reject \(H_0\) (prosecutor):

\textbf{positive}: expected research interest, rejection of \(H_0\), or the status quo (rejecting innocence).

\begin{longtable}[]{@{}ccc@{}}
\toprule
\(H_0\) & reality: true & reality: false \\
\midrule
\endhead
\textbf{test: positive} (negative result) & true negative & false negative \\
\textbf{test: negative} (positive result) & false positive & true positive \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{errors-in-hypothesis-testing-1}{%
\section{Errors in hypothesis testing}\label{errors-in-hypothesis-testing-1}}

You take a sample of \(5\) PCR tests to test infection; \(H_0\) you are **not infected*

\begin{itemize}
\tightlist
\item
  \(H_0\) unknown reality is: \textbf{true}, \textbf{false}
\item
  \(H_0\) test is: \textbf{negative} (accept), \textbf{positive} (reject)
\end{itemize}

\textbf{positive} for infection.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.33}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.33}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.33}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\centering
\(H_0\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
not infected: true
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
not infected: false (infected: true)
\end{minipage} \\
\midrule
\endhead
\textbf{negative} (accept) & true negative: \(P(accept|H_0:true)\) & false negative:\(P(accept|H_0:false)\) \\
\textbf{positive} (reject) & false positive: \(P(reject|H_0:true)\) & true positive: \(P(reject|H_0:false)\) \\
\textbf{sum} & 1 & 1 \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{errors-in-hypothesis-testing-2}{%
\section{Errors in hypothesis testing}\label{errors-in-hypothesis-testing-2}}

\hypertarget{errors-are-also-known-as}{%
\subsection{Errors are also known as}\label{errors-are-also-known-as}}

\begin{itemize}
\item
  Type I error: false positive (sending an innocent man to jail)
  \(P(reject|H_0:true)\)
\item
  Type II error: false negative (letting go of a criminal)
  \(P(accept|H_0:false)\)
\end{itemize}

\hypertarget{correct-contrasts-are-also-known-as}{%
\subsection{Correct contrasts are also known as}\label{correct-contrasts-are-also-known-as}}

\begin{itemize}
\item
  Sensitivity: true positive (sending a criminal to jail)
  \(P(reject|H_0:false)\)
\item
  Specificity: true negative (letting go of an innocent man)
  \(P(accept|H_0:true)\)
\end{itemize}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.33}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.33}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.33}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\centering
\(H_0\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
not infected: true
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
not infected: false (infected: true)
\end{minipage} \\
\midrule
\endhead
\textbf{negative} (accept) & Specificity: \(P(accept|H_0:true)\) & Type II error: \(P(accept|H_0:false)\) \\
\textbf{positive} (reject) & Type I error: \(P(reject|H_0:true)\) & Specificity: \(P(reject|H_0:false)\) \\
\textbf{sum} & 1 & 1 \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bayesian-statistics}{%
\section{Bayesian statistics}\label{bayesian-statistics}}

What happens if we apply the Bayes theorem to the previous table?

\[P(H_0|data)=\frac{P(data|H_0)P(H_0)}{P(data)}\]

We subvert the meaning of an event and apply it to a hypothesis.

Can we assign a probability to a hypothesis?

Bayesian interpretation of probability

\begin{itemize}
\tightlist
\item
  The probability is our \textbf{state of belief} on the veracity of a hypothesis given the data.
\end{itemize}

\hypertarget{contingency-tables-1}{%
\chapter{Contingency tables}\label{contingency-tables-1}}

\hypertarget{objective-14}{%
\section{Objective}\label{objective-14}}

\begin{itemize}
\tightlist
\item
  \(\chi^2\) test
\item
  Fisher exact test
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-proportions}{%
\section{Difference between proportions}\label{difference-between-proportions}}

For disease surveillance, we want to know if more hepatitis C patients are being observed in hospital \(A\) than in hospital \(B\)?

\begin{itemize}
\tightlist
\item
  We write down the status of hepatitis C of a patient who goes to \textbf{hospital A}. This is a Bernoulli variable \(K\) with outcomes (0:no hepatitis and 1:hepatitis) that has a probability mass function
\end{itemize}

\[K_A \rightarrow Bernoulli (p_A)\]
The parameter \(p_A\) is the probability of hepatitis at hospital \(A\)

\begin{itemize}
\tightlist
\item
  We also write down the hepatitis status of a patient who goes to \textbf{hospital B}.
\end{itemize}

\[K_B \rightarrow Bernoulli (p_B)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-proportions-1}{%
\section{Difference between proportions}\label{difference-between-proportions-1}}

One random experiment has two outcomes: \((disease, hospital)\).

Categorical variables:

\begin{itemize}
\tightlist
\item
  \(Disease \in\{no, yes\}\)
\item
  \(Hospital \in \{A,B\}\)
\end{itemize}

Repeating the experiment \(n\) times, the data for the first five repetitions look like

\begin{verbatim}
##   Hospital Disease
## 1        A     yes
## 2        A      no
## 3        B      no
## 4        A     yes
## 5        A      no
\end{verbatim}

\textbf{Question:} Are \(Disease\) and \(Hopital\) statistically independent variables?

Let's formulate the null hypothesis.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-proportions-2}{%
\section{Difference between proportions}\label{difference-between-proportions-2}}

Instead of taking random samples across hospitals, we take random samples \textbf{conditioned to} each hospital, for example:

\begin{itemize}
\item
  Hospital \(A\) included in the study total of \(n_A=200\) patients separately from hospital \(B\) that included \(n_B=400\).
\item
  Hospital \(A\) observed \(18\) patients with hepatitis C, Hospital \(B\) observed \(46\)
\end{itemize}

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Hospital: \(A\) & Hospital: \(B\) \\
\midrule
\endhead
\textbf{Hepatitis} (no) & \(n_{no|A}=182\) & \(n_{no|B}=354\) \\
\textbf{Hepatitis} (yes) & \(n_{yes|A}=18\) & \(n_{yes|B}=46\) \\
\textbf{sum} & \(n_A=200\) & \(n_B=400\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-proportions-3}{%
\section{Difference between proportions}\label{difference-between-proportions-3}}

For hospital \(A\) we have that \(\bar{K_A}\) is an estimator of \(p_A\)

\begin{itemize}
\tightlist
\item
  \(\bar{k}_A=\hat{p}_A=18/200=0.09\)
\end{itemize}

For hospital \(B\) we have that \(\bar{K_B}\) is an estimator of \(p_B\)

\begin{itemize}
\tightlist
\item
  \(\bar{k}_B=\hat{p}_B=46/400=0.115\)
\end{itemize}

These are the conditional frequencies:

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Hospital: \(A\) & Hospital: \(B\) \\
\midrule
\endhead
\textbf{Hepatitis} (no) & \(f_{no|A}=0.91\) & \(f_{no|B}=0.885\) \\
\textbf{Hepatitis} (yes) & \(f_{yes|A}=0.09\) & \(f_{yes|B}=0.115\) \\
\textbf{sum} & \(1\) & \(1\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-proportions-4}{%
\section{Difference between proportions}\label{difference-between-proportions-4}}

\hypertarget{null-hypothesis-2}{%
\subsection{Null hypothesis:}\label{null-hypothesis-2}}

\begin{itemize}
\item
  The null hypothesis (status quo) assumes that both hospitals have the same parameter (probability of hepatitis C) \(H_0: p=p_A=p_B\)
\item
  Therefore, the alternative hypothesis is that they are different \(H_1: p_A\neq p_B\)
\end{itemize}

We don't know \(p\), but we can take as the null hypothesis the value of \(p\) estimated from the two hospitals \textbf{taken together}, as if it was the same hospital:

\begin{itemize}
\tightlist
\item
  \(\hat{p}=\frac{n_{yes|A}+n_{yes|B}}{n_A+n_B}=\frac{18+46}{200+400}= f_{yes}=0.152381\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{chi2-test}{%
\section{\texorpdfstring{\(\chi^2\) test}{\textbackslash chi\^{}2 test}}\label{chi2-test}}

\begin{itemize}
\tightlist
\item
  By the CLT, the \textbf{standardized squared error} from the null hypothesis is
\end{itemize}

\[W= \frac{(\bar{K}_A-f_{yes})^2}{\frac{f_{yes}(1-f_{yes})}{n_A}} + \frac{(\bar{K}_B-f_{yes})^2}{\frac{f_{yes}(1-f_{yes})}{n_B}}\rightarrow \chi^2(1)\]

\includegraphics{_main_files/figure-latex/unnamed-chunk-123-1.pdf}

\includegraphics{_main_files/figure-latex/unnamed-chunk-124-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{chi2-test-1}{%
\section{\texorpdfstring{\(\chi^2\) test}{\textbackslash chi\^{}2 test}}\label{chi2-test-1}}

Under the \textbf{null hypothesis}, we consider that Hospital labeling, \(A\) or \(B\), was really a random choice from the same hospital, therefore our data is rather the contingency table with overall disease marginals

\begin{longtable}[]{@{}cccc@{}}
\toprule
& Hospital: \(A\) & Hospital: \(B\) & sum \\
\midrule
\endhead
\textbf{Hepatitis} (no) & \(f_{no,A}=182/600\) & \(f_{no,B}=354/600\) & \(f_{no}=536/600\) \\
\textbf{Hepatitis} (yes) & \(f_{yes,A}=18/600\) & \(f_{yes,B}=46/600\) & \(f_{yes}=64/600\) \\
\textbf{sum} & \(f_{A}=200/600\) & \(f_{B}=400/600\) & 1 \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{chi2-test-2}{%
\section{\texorpdfstring{\(\chi^2\) test}{\textbackslash chi\^{}2 test}}\label{chi2-test-2}}

In this context, the same \textbf{standardized squared error} of before can be re-written as:

\[W= \frac{(f_{no,A}-f_{no}f_{A})^2}{f_{no}f_{A}}+\frac{(f_{no,A}-f_{no}f_{B})^2}{f_{no}f_{B}}+\frac{(f_{yes,A}-f_{yes}f_{A})^2}{f_{yes}f_{A}}+\frac{(f_{yes,B}-f_{yes}f_{B})^2}{f_{yes}f_{B}}\]

Which are the squared differences between the

\begin{itemize}
\tightlist
\item
  \textbf{observed} frequencies \(f_{disease, hospital}\) and
\item
  the \textbf{expected} frequencies under statistical independence \(f_{disease}*f_{hospital}\) (multiplication of marginals)
\end{itemize}

Therefore, equivalently, we can state that:

\begin{itemize}
\tightlist
\item
  \(H_0:p_{disease, hospital}=p_{disease}*p_{hospital}\) (Disease and Hospital are statistically \textbf{independent})
\item
  \(H_1:p_{disease, hospital}\neq p_{disease}*p_{hospital}\)
  (Disease and Hospital are statistically \textbf{dependent})
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{chi2-test-3}{%
\section{\texorpdfstring{\(\chi^2\) test}{\textbackslash chi\^{}2 test}}\label{chi2-test-3}}

If the observed value for \(W\) is a rare error from the null hypothesis, for a \(\chi^2\) variable, we then reject the null hypothesis.

The observed value of \(W\) is

\[w_{obs}= 0.87453\]

And

\[pvalue=P(W \leq w_{obs}) =0.3497\]

in R: chisq.test(matrix(c(182, 18, 354, 46), ncol=2), correct = FALSE)

Which is not lower than the significance level \(\alpha=0.05\) and therefore we \textbf{do not} reject \(H_0\) and conclude:

\begin{itemize}
\item
  The frequencies of hepatitis C \textbf{are equal} between hospitals
\item
  or, equivalently, that the frequency of hepatitis C \textbf{is independent} from hospital
\item
  or, equivalently, that the frequency of hepatitis C \textbf{is not significantly associated} with the hospital.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{fishers-exact-test}{%
\section{Fisher's exact test}\label{fishers-exact-test}}

Another approach is \textbf{Fisher's exact test}

Take a ballot for each of the \(N=600\) patients of both studies into an urn:

From a population of \(N\):

\begin{itemize}
\tightlist
\item
  There are \(K=64\) that have hepatitis C
\item
  \(N-K=536\) do not have hepatitis C
\end{itemize}

Then, if we take a sample of \(n=200\) (similar in number to hospital \(A\))

\begin{itemize}
\tightlist
\item
  what is the probability of observing more than \(18\) patients with hepatitis C, as observed in hospital \(A\)?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{fishers-exact-test-1}{%
\section{Fisher's exact test}\label{fishers-exact-test-1}}

\begin{itemize}
\item
  The null hypothesis (status quo) assumes that hospital \(A\) has the same parameter of both hospitals together \(H_0: p=\hat{p}\)
\item
  The alternative hypothesis is the parameter of hospital \(A\) is lower than the parameter for both hospitals together \(H_1: p < \hat{p}\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypergeometric-distribution}{%
\section{Hypergeometric distribution}\label{hypergeometric-distribution}}

The probability of obtaining \(x\) hepatitis C cases in a sample of \(n\) drawn from a population of \(N\) where \(K\) have hepatitis C is

\(P(X=x)=P(one\,sample) \times (Number\, of\, ways\, of\, obtaining\, x)\)

\(=\frac{1}{\binom N n}\binom K x \binom {N-K} {n-x}\)

where \(k \in \{\max(0, n+K-N), ... \min(K, n) \}\)

\[X \rightarrow Hypergeometric(N,K,n)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypergeometric-distribution-1}{%
\section{Hypergeometric distribution}\label{hypergeometric-distribution-1}}

It has

\begin{itemize}
\item
  mean: \(E (X) = n * K / N = np\)
\item
  variance: \(V(X) = np (1-p)\frac{N-n}{N-1}\)
\end{itemize}

when \(p=K/N\) is the proportion of hepatitis C in a population of size \(N\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypergeometric-distribution-2}{%
\section{Hypergeometric distribution}\label{hypergeometric-distribution-2}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-125-1.pdf}

\includegraphics{_main_files/figure-latex/unnamed-chunk-126-1.pdf}

\includegraphics{_main_files/figure-latex/unnamed-chunk-127-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{fishers-exact-test-2}{%
\section{Fisher's exact test}\label{fishers-exact-test-2}}

If the observed value for \(x=18\) is a rare \textbf{observation} from the null hypothesis, for a hypergeometric variable, we then reject the null hypothesis.

The lower tail \(pvalue\) for an observation \(X=18\) is

\[pvalue=P_{hyper}(X \leq 18) =0.2147683\]
In R: phyper(18, 64, 536, 200)

Which is not lower than the significance level \(\alpha=0.05\) and therefore we \textbf{do not} reject \(H_0\) and conclude:

\begin{itemize}
\tightlist
\item
  that the frequency of hepatitis C \textbf{is not significantly associated} with the hospital.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{fishers-exact-test-3}{%
\section{Fisher's exact test}\label{fishers-exact-test-3}}

The odds ratio is defined as:

\[OR=\frac{f_{no,B}/f_{yes,B}}{f_{no,A}/f_{yes,A}}=1.31\]

Gives the strength of the \textbf{observed association} between hospital and disease.

This is how we talk:

\begin{itemize}
\tightlist
\item
  There was \textbf{an increase} in \(31\%\) in the risk of hepatitis C for hospital \(B\) but it was not statistically significant.
\end{itemize}

This is how we compute it:

fisher.test(matrix(c(182, 18, 354, 46), ncol=2), alternative=``greater'')

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-several-proportions}{%
\section{Difference between several proportions}\label{difference-between-several-proportions}}

Now, we want to know if the frequency of hepatitis C is different across \(5\) difference hospitals.

\begin{longtable}[]{@{}ccccccc@{}}
\toprule
& \(A\) & \(B\) & \(C\) & \(D\) & \(E\) & sum \\
\midrule
\endhead
\textbf{Hepatitis} (no) & \(182\) & \(354\) & \(375\) & \(85\) & \(90\) & \(90\) \\
\textbf{Hepatitis} (yes) & \(18\) & \(46\) & \(25\) & \(15\) & \(10\) & \(121\) \\
\textbf{sum} & \(200\) & \(400\) & \(400\) & \(100\) & \(100\) & \(1200\) \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{null-hypothesis-3}{%
\subsection{Null hypothesis:}\label{null-hypothesis-3}}

\begin{itemize}
\item
  The null hypothesis (status quo) assumes that hospital (\(i=\{A,B,C,D,E\}\)) and disease (\(j=\{yes, no\}\)) are all independent \(H_0: p_ip_j=p_{i,j}\)
\item
  The alternative hypothesis is that \textbf{at least one} \(p_ip_j\neq p_{i,j}\) is not independent.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-several-proportions-1}{%
\section{Difference between several proportions}\label{difference-between-several-proportions-1}}

Writing the relative frequencies

\begin{longtable}[]{@{}ccccccc@{}}
\toprule
& \(A\) & \(B\) & \(C\) & \(D\) & \(E\) & sum \\
\midrule
\endhead
\textbf{Hepatitis} (no) & \(0.1516667\) & \(0.29500000\) & \(0.31250000\) & \(0.07083333\) & \(0.075000000\) & \(0.905\) \\
\textbf{Hepatitis} (yes) & \(0.0150000\) & \(0.03833333\) & \(0.02083333\) & \(0.01250000\) & \(0.008333333\) & \(0.095\) \\
\textbf{sum} & \(0.16666667\) & \(0.33333333\) & \(0.33333333\) & \(0.08333333\) & \(0.08333333\) & \(1\) \\
\bottomrule
\end{longtable}

We have that the \textbf{standardized squared error} from the null hypothesis can be written as:

\[W= \sum_{i=A,B,C,D,E} \sum_{j=yes,no} \frac{(f_{j,i}-f_{j}f_{i})^2}{f_{j}f_{i}}\]
\(= \frac{(0.1516667 - 0.16666667*0.905)^2}{0.16666667*0.905} + ... \rightarrow \chi^2(4)\)

that follows a \(\chi^2\) distribution with \(4=5-1\) degrees of freedom (number of hospitals \(-1\)).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-several-proportions-2}{%
\section{Difference between several proportions}\label{difference-between-several-proportions-2}}

\begin{itemize}
\tightlist
\item
  The observed \textbf{standardized squared error} is
  \[w_obs=10.381\]
\end{itemize}

And

\[pvalue=P(W \leq w_{obs}) = 0.03448\]
in R:
chisq.test(matrix(c(182, 18, 354, 46, 375, 25 , 85, 15, 90, 10), nrow=2))

Which is lower than the significance level \(\alpha=0.05\) and therefore we \textbf{reject} \(H_0\) and conclude:

\begin{itemize}
\tightlist
\item
  that the frequency of hepatitis C \textbf{is significantly associated} with the hospital.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-several-proportions-3}{%
\section{Difference between several proportions}\label{difference-between-several-proportions-3}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-128-1.pdf}

\hypertarget{mean-differences-between-two-samples}{%
\chapter{Mean differences between two samples}\label{mean-differences-between-two-samples}}

\hypertarget{objective-15}{%
\section{Objective}\label{objective-15}}

\begin{itemize}
\tightlist
\item
  large \(n\): \(Z\) test
\item
  small \(n\) with equal and unequal variances: \(t\) test
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-means}{%
\section{Difference between means}\label{difference-between-means}}

Let's consider an outcome of interest \(Y\)

\[ Y \rightarrow N(\mu, \sigma^2)\]

we repeat the random experiment under two conditions \(A\) and \(B\), to determine if the means between conditions change.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-means-1}{%
\section{Difference between means}\label{difference-between-means-1}}

Leptin is an adipose tissue hormone that creates the sensation of satiety after eating. We want to study the serum leptin levels in obese children (PMID: 18755049) under different conditions, such as sex.

\begin{itemize}
\tightlist
\item
  We assume that the levels of leptin in girls have a probability density
\end{itemize}

\[Y_A \rightarrow N(\mu_A, \sigma_A^2)\]

\begin{itemize}
\tightlist
\item
  We assume a normal distribution of leptin in boys.
\end{itemize}

\[Y_B \rightarrow N(\mu_B, \sigma_B^2)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-means-2}{%
\section{Difference between means}\label{difference-between-means-2}}

One random experiment has two outcomes: \((leptin, sex)\).

Continuous variable (outcome of interest)

\begin{itemize}
\tightlist
\item
  \(leptin \in (0, 200)\)
\end{itemize}

Categorical variable:

\begin{itemize}
\tightlist
\item
  \(sex \in \{girl:A,boy:B\}\)
\end{itemize}

Repeating the experiment \(n\) times, the data for the first five repetitions look like

\begin{verbatim}
##   leptin sex
## 1   37.8   B
## 2   40.1   B
## 3   48.6   A
## 4   39.0   A
## 5   43.9   A
\end{verbatim}

\textbf{Question:} \(Sex\) and \(Leptin\) statistically independent variables?

Let's formulate the null hypothesis.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-means-3}{%
\section{Difference between means}\label{difference-between-means-3}}

We take leptin levels \textbf{conditioned to} each sex, and observed:

\begin{itemize}
\item
  \(n_A=190\) girls had a mean of \(\bar{y}_A=48.0\) and \(s=27.1\)
\item
  \(n_B=166\) boys has a mean of \(\bar{y}_B=34.4\) and \(s=22.4\)
\end{itemize}

Instead of a conditional table, we draw histograms of leptin for each condition

\includegraphics{_main_files/figure-latex/unnamed-chunk-130-1.pdf}

boxplots are also popular

\includegraphics{_main_files/figure-latex/unnamed-chunk-131-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-means-4}{%
\section{Difference between means}\label{difference-between-means-4}}

\hypertarget{null-hypothesis-4}{%
\subsection{Null hypothesis:}\label{null-hypothesis-4}}

\begin{itemize}
\item
  The null hypothesis (status quo) assumes that both sexes have the same mean \(H_0: \mu_A=\mu_B\) or \(H_0: \delta=\mu_A-\mu_B=0\)
\item
  Therefore, the alternative hypothesis is that they are different, that is \(H_1: \delta \neq 0\)
\end{itemize}

We need an estimator of \(\delta\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estiamtor-of-the-mean-differnce}{%
\section{Estiamtor of the mean differnce}\label{estiamtor-of-the-mean-differnce}}

The statistic \(D=\bar{Y}_A-\bar{Y}_B\) is an estimator of \(\delta\)

\begin{itemize}
\item
  \(E(D)=E(\bar{Y}_A-\bar{Y}_B)=\mu_A-\mu_B=\delta\) (unbiased estimator)
\item
  \(V(D)=V(\bar{Y}_A-\bar{Y}_B)=\frac{\sigma^2_A}{n_A}+\frac{\sigma^2_B}{n_B}\) (consistent estimator)
\end{itemize}

More over if \(n_A\) and \(n_B\) are large (by CLT) then

\[Z=\frac{E(D)-\delta}{\sqrt{V(D)}}=\frac{\bar{Y}_A-\bar{Y}_B -\delta}{\sqrt{\frac{\sigma^2_A}{n_A}+\frac{\sigma^2_B}{n_B}}} \rightarrow N(0,1)\]

is a normal standard variable.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standardized-error}{%
\section{Standardized error}\label{standardized-error}}

Then the \textbf{standardized error} from the null hypothesis (\(\delta=0\)) follows a standard distribution

\[Z=\frac{\bar{Y}_A-\bar{Y}_B}{\sqrt{\frac{\sigma^2_A}{n_A}+\frac{\sigma^2_B}{n_B}}}\]

\begin{itemize}
\tightlist
\item
  Is our observed \(\delta_{obs}\) within the acceptance region of the null hypothesis?
\end{itemize}

\[P(-z_{0.025} \leq Z \leq z_{0.025})=P(-1.96 \leq Z \leq 1.96)=0.95\]

\begin{itemize}
\tightlist
\item
  Is the of \(d_{obs}\) lower than \(\alpha=0.05\)?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-comparison}{%
\section{Mean comparison}\label{mean-comparison}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-132-1.pdf}

\includegraphics{_main_files/figure-latex/unnamed-chunk-133-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-testing-1}{%
\section{Hypothesis testing}\label{hypothesis-testing-1}}

\begin{itemize}
\tightlist
\item
  The \textbf{observed mean difference}
\end{itemize}

\[d_{obs}=\frac{\bar{y}_A-\bar{y}_B }{\sqrt{\frac{s^2_A}{n_A}+\frac{s^2_B}{n_B}}}=\frac{48-34.4}{\sqrt{\frac{27.1^2}{190}+\frac{22.4^2}{166}}}=5.181952\]
is outside of the acceptance region.

\begin{itemize}
\tightlist
\item
  The two-tailed \(pvalue\):
\end{itemize}

\[Pval=2*(1-\phi(5.181952))=2.195757 \times 10^{-7}\]

is lower than \(\alpha\).

Therefore, we reject the null hypothesis that the leptin levels in obese children are equal between boys and girls.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{reporting}{%
\section{Reporting}\label{reporting}}

\hypertarget{abstract}{%
\subsection{Abstract}\label{abstract}}

Obesity rates are different between boys and girls, suggesting that the physiopathology of the disease is different between the sexes.

In this study, we tested the hypothesis that the leptin levels in serum are different between boys and girls.

We analyzed data from 190 obese girls and 166 obese boys and found a significant difference in leptin between sexes (mean difference \(13.6\), \(P=2.195757 \times 10^{-7}\))

\includegraphics{_main_files/figure-latex/unnamed-chunk-134-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-difference-small-n}{%
\section{\texorpdfstring{Mean difference small \(n\)}{Mean difference small n}}\label{mean-difference-small-n}}

For performing the statistical test we computed

\begin{itemize}
\tightlist
\item
  The \textbf{observed mean difference}
\end{itemize}

\[d_{obs}=\frac{\bar{y}_A-\bar{y}_B }{\sqrt{\frac{s^2_A}{n_A}+\frac{s^2_B}{n_B}}}\]
where we replaced the values of \(\sigma_A\) and \(\sigma_B\) for their estimated values \(s_A\) and \(s_B\). The statistic \(D\) is approximately normal because \(n\ge 30\) (CLT).

What happens when \(n\) is small?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-difference-small-n-1}{%
\section{\texorpdfstring{Mean difference small \(n\)}{Mean difference small n}}\label{mean-difference-small-n-1}}

In a study that wanted to test the effect of leptin in neurodevelopment, 7 male mice had their leptin gene knocked out. While 16 mice were left with normal leptin function (PMID: 30694175). An initial question was to test the effect of leptin on the body weight of the animals.

\begin{itemize}
\tightlist
\item
  We assume that the weight of the control animals has a probability density
\end{itemize}

\[Y_A \rightarrow N(\mu_A, \sigma_A^2)\]

\begin{itemize}
\tightlist
\item
  We assume a normal distribution weight for the mice with no leptin.
\end{itemize}

\[Y_B \rightarrow N(\mu_B, \sigma_B^2)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-means-5}{%
\section{Difference between means}\label{difference-between-means-5}}

One random experiment has two outcomes: \((weight, leptin)\).

Continuous variable (outcome of interest)

\begin{itemize}
\tightlist
\item
  \(weigth \in (20, 60)\)
\end{itemize}

Categorical variable:

\begin{itemize}
\tightlist
\item
  \(leptin \in \{control:A,knockout:B\}\)
\end{itemize}

The data looks like

\begin{verbatim}
##    weight    group
## 1   27.67  Control
## 2   27.40  Control
## 3   25.77  Control
## 4   25.60  Control
## 5   25.03  Control
## 6   25.90  Control
## 7   26.67  Control
## 8   25.60  Control
## 9   28.93  Control
## 10  31.83  Control
## 11  25.90  Control
## 12  26.30  Control
## 13  27.90  Control
## 14  26.77  Control
## 15  25.83  Control
## 16  20.87  Control
## 17  46.57 leptinKO
## 18  40.43 leptinKO
## 19  41.97 leptinKO
## 20  41.17 leptinKO
## 21  41.57 leptinKO
## 22  46.17 leptinKO
## 23  53.83 leptinKO
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-means-6}{%
\section{Difference between means}\label{difference-between-means-6}}

We take weights \textbf{conditioned to} each lepting condition, and observed:

\begin{itemize}
\item
  \(n_A=16\) control mice had a weight mean of \(\bar{y}_A=26.49813\) and \(s_A=2.247577\)
\item
  \(n_B=7\) leptin KO mice had a weight mean of \(\bar{y}_B=44.53\) and \(s_B=4.774167\)
\end{itemize}

We can draw boxplots per group

\includegraphics{_main_files/figure-latex/unnamed-chunk-136-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-means-7}{%
\section{Difference between means}\label{difference-between-means-7}}

\hypertarget{null-hypothesis-5}{%
\subsection{Null hypothesis:}\label{null-hypothesis-5}}

\begin{itemize}
\item
  The null hypothesis (status quo) assumes that both mice (control, leptin KO) have the same mean \(H_0: \delta=\mu_A-\mu_B=0\)
\item
  Therefore, the alternative hypothesis is that they are different, that is \(H_1: \delta \neq 0\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimator-of-the-mean-difference}{%
\section{Estimator of the mean difference}\label{estimator-of-the-mean-difference}}

The statistic \(D=\bar{Y}_A-\bar{Y}_B\) is an estimator of \(\delta\)

\begin{itemize}
\tightlist
\item
  \(E(D)=\delta\) (unbiased estimator)
\end{itemize}

If \(Y_A\) and \(Y_B\) are normal variables with the same variance \[\sigma^2=\sigma^2_A=\sigma^2_B\]

The \textbf{standardized error}

\[T=\frac{\bar{Y}_A-\bar{Y}_B -\delta}{\sqrt{\frac{s_p^2}{n_A}+\frac{s_p^2}{n_B}}} \rightarrow T(n_A+n_B-2)\]

follows exactly a T-distribution with \(n_A+n_B-2\) degrees of freedom.

The \textbf{pooled variance} \(s_p^2\), is an estimator of \(\sigma^2\)

\[\hat{\sigma}^2=s_p^2= \frac{(n_A-1) s^2_A+(n_B-1) s^2_B}{n_A+n_B-2}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-testing-2}{%
\section{Hypothesis testing}\label{hypothesis-testing-2}}

\begin{itemize}
\tightlist
\item
  Is our observed \(d_{obs}\) within the acceptance region of the null hypothesis?
\end{itemize}

\[P(-t_{0.025, 21} \leq T \leq t_{0.025, 21})=P(-2.079614 \leq T \leq -2.079614)=0.95\]

\begin{itemize}
\tightlist
\item
  Is the of \(value\) lower than \(\alpha=0.05\)?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-testing-3}{%
\section{Hypothesis testing}\label{hypothesis-testing-3}}

\begin{itemize}
\tightlist
\item
  The \textbf{observed mean difference}
\end{itemize}

\[d_{obs}=\frac{\bar{y}_A-\bar{y}_B }{\sqrt{\frac{s^2_p}{n_A}+\frac{s^2_p}{n_B}}}=\frac{26.49813-44.53}{\sqrt{\frac{3.18127^2}{16}+\frac{3.18127^2}{7}}}=-12.508\]

is outside of the acceptance region.

\begin{itemize}
\tightlist
\item
  The two-tailed \(pvalue\):
\end{itemize}

\[pvalue=2*(1-F^{-1}_{t,21}(12.508))=3.376854 \times 10^{-11}\]

is lower than \(\alpha\).

Therefore, the data shows a very significant increase in \(18.03\)gr (\(P=3.376854 \times 10^{-11}\)) in weight between the wild-type mice and leptin knockouts. ``Absence of leptin signaling in early life alters the energy balance and predisposes the animals to obesity''

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-testing-4}{%
\section{Hypothesis testing}\label{hypothesis-testing-4}}

in R

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(control, leptinKO, }\AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Two Sample t-test
## 
## data:  control and leptinKO
## t = -12.508, df = 21, p-value = 3.377e-11
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -21.02992 -15.03383
## sample estimates:
## mean of x mean of y 
##  26.49813  44.53000
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{unequal-variances}{%
\section{Unequal variances}\label{unequal-variances}}

The boxplot suggests that the variances for each group are different.

The \textbf{standardized error}

\[T=\frac{\bar{Y}_A-\bar{Y}_B -\delta}{\sqrt{\frac{s_A^2}{n_A}+\frac{s_B^2}{n_B}}} \rightarrow_{aprox} T(\nu)\]
approximately follows a t-distribution with
\[\nu=\frac{(\frac{s_A^2}{n_A}+\frac{s_B^2}{n_B})^2}{\frac{(s_A^2/n_B)^2}{n_A-1}+\frac{(s_B^2/n_B)^2}{n_B-1}}\]
degrees of freedom

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-testing-5}{%
\section{Hypothesis testing}\label{hypothesis-testing-5}}

in R

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(control, leptinKO, }\AttributeTok{var.equal =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  control and leptinKO
## t = -9.541, df = 7.1929, p-value = 2.444e-05
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -22.47665 -13.58710
## sample estimates:
## mean of x mean of y 
##  26.49813  44.53000
\end{verbatim}

Therefore, the data \textbf{still} shows a very significant increase in \(18.03\)gr (\(P=2.444 \times 10^{-5}\)) in weight between the wild-type mice and leptin knockouts (under a more appropriate test).

\includegraphics{_main_files/figure-latex/unnamed-chunk-139-1.pdf}

\hypertarget{mean-differences-between-several-samples}{%
\chapter{Mean differences between several samples}\label{mean-differences-between-several-samples}}

\hypertarget{objective-16}{%
\section{Objective}\label{objective-16}}

\begin{itemize}
\tightlist
\item
  Two group ANOVA
\item
  Several groups ANOVA
\item
  Two-factor ANOVA
\item
  Two-factor ANOVA with interaction
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{analisis-of-variance}{%
\section{Analisis of variance}\label{analisis-of-variance}}

Let's analyze the leptin experiment from a different perspective: Fisher's analysis of variance.

If the \textbf{null hypothesis is true}, then mice with and without leptin are \textbf{identical} and then split into groups are two random samples of size \(A\) and \(B\) from the same population.

\begin{itemize}
\item
  The overall variance is equal to the \textbf{within group} variances \[\sigma^2=\sigma^2_A=\sigma^2_B\]
\item
  There is no difference between means \[\delta=(\mu_A-\mu)-(\mu_B-\mu)=0\]
\end{itemize}

\begin{verbatim}
##    weight    group
## 1   27.67  Control
## 2   27.40  Control
## 3   25.77  Control
## 4   25.60  Control
## 5   25.03  Control
## 6   25.90  Control
## 7   26.67  Control
## 8   25.60  Control
## 9   28.93  Control
## 10  31.83  Control
## 11  25.90  Control
## 12  26.30  Control
## 13  27.90  Control
## 14  26.77  Control
## 15  25.83  Control
## 16  20.87  Control
## 17  46.57 leptinKO
## 18  40.43 leptinKO
## 19  41.97 leptinKO
## 20  41.17 leptinKO
## 21  41.57 leptinKO
## 22  46.17 leptinKO
## 23  53.83 leptinKO
\end{verbatim}

\includegraphics{_main_files/figure-latex/unnamed-chunk-140-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{analisis-of-variance-1}{%
\section{Analisis of variance}\label{analisis-of-variance-1}}

If the \textbf{null hypothesis is not true}, then the mean weight of the mice is \textbf{different} with and without leptin and the split into groups are two random samples of size \(A\) and \(B\) from \textbf{different} populations.

\begin{itemize}
\item
  The overall variance is greater than the within-group variances \[\sigma^2> \sigma^2_A=\sigma^2_B\]
\item
  The group means are different \[\delta=(\mu_A-\mu)-(\mu_B-\mu)\neq 0\]
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-141-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{linear-model}{%
\section{Linear model}\label{linear-model}}

Under the \textbf{alternative hypothesis}, let's consider the observations of mice weight conditioned to the leptin groups

\(Y_{Aj}\) for \(i=1...16\). For example: \(Y_{A5}=25.03\),

\begin{verbatim}
##    weight group
## 1   27.67     A
## 2   27.40     A
## 3   25.77     A
## 4   25.60     A
## 5   25.03     A
## 6   25.90     A
## 7   26.67     A
## 8   25.60     A
## 9   28.93     A
## 10  31.83     A
## 11  25.90     A
## 12  26.30     A
## 13  27.90     A
## 14  26.77     A
## 15  25.83     A
## 16  20.87     A
\end{verbatim}

\(Y_{Bj}\) for \(i=1...7\). For example: \(Y_{B2}=40.43\),

\begin{verbatim}
##   weight group
## 1  46.57     B
## 2  40.43     B
## 3  41.97     B
## 4  41.17     B
## 5  41.57     B
## 6  46.17     B
## 7  53.83     B
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{linear-model-1}{%
\section{Linear model}\label{linear-model-1}}

Let's assume that for all observations we can extract a
\textbf{random error}

\[Y_{ij} = \mu + \alpha_i +\epsilon_{ij}\]

\textbf{Fixed parameters}:

\begin{itemize}
\tightlist
\item
  \(\mu\) is the overall mean
\item
  \(\alpha_i\) is the deviation of group \(i\) to the overall mean: \(i \in (A,B)\) and \(\alpha_A=\mu_A-\mu\), \(\alpha_B=\mu_B-\mu\).
\item
  \(j \in {1,...n}\) (all groups have the same number of observations \(n_A=n_B=n\) for simplicity with no loss of generality)
\end{itemize}

\textbf{Random error}:

\begin{itemize}
\tightlist
\item
  \(\epsilon_{ij}\) is a \textbf{random variable} with \(E(\epsilon_{ij})=0\), \(V(\epsilon_{ij})=\sigma^2\)
\end{itemize}

Then

\begin{itemize}
\tightlist
\item
  \(E(Y_{ij})=\) \[E(Y|i)=\mu + \alpha_i\]
\end{itemize}

for instance: \(E(Y|A)=\mu_A=\mu + \alpha_A\)

\begin{itemize}
\tightlist
\item
  \(V(Y_{ij})=\sigma^2\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{variance-components}{%
\section{Variance components}\label{variance-components}}

The squared deviations of the observations to the overall average is

\[\sum_{i=A,B}\sum_{j=1}^n(Y_{ij}-\bar{Y})^2= \sum_{i=A,B}\sum_{j=1}^n(Y_{ij}-\bar{Y}_i)^2+n\sum_{i=A,B}(\bar{Y}_{i}-\bar{Y})^2\]

\begin{itemize}
\tightlist
\item
  Sum of squares (total)
\end{itemize}

\[SS_T=\sum_{i=A,B}\sum_{j=1}^n(Y_{ij}-\bar{Y})^2\]

\begin{itemize}
\tightlist
\item
  Sum of squares (error)
\end{itemize}

\[SSE=\sum_{i=A,B}\sum_{j=1}^n(Y_{ij}-\bar{Y}_i)^2\]

\begin{itemize}
\tightlist
\item
  Sum of squares (treatment)
\end{itemize}

\[SS_{treatment}=n\sum_{i=A,B}(\bar{Y}_{i}-\bar{Y})^2\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{variance-components-1}{%
\section{Variance components}\label{variance-components-1}}

\begin{itemize}
\tightlist
\item
  The \textbf{mean square error} (MSE)
\end{itemize}

\(MSE=\frac{1}{2n-2} SSE\) \[= \frac{1}{2n-2} \sum_{i=A,B}\sum_{j=1}^n(Y_{ij}-\bar{Y}_i)^2= \frac{(n-1)S_A^2+(n-1)S_B^2}{2n-2}\]

is the \textbf{pooled variance estimator} \[E(MSE)=\sigma^2\]

\begin{itemize}
\tightlist
\item
  The \textbf{mean square of treatments} (MST)
\end{itemize}

\(MST=\frac{1}{2-1} SS_{treatment}\) \[ =  n\sum_{i=A,B}(\bar{Y}_{i}-\bar{Y})^2\]

is a \textbf{biased estimator} of the variance \[E(MST)=\sigma^2+n(\alpha_A^2+\alpha_B^2)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{linear-model-2}{%
\section{Linear model}\label{linear-model-2}}

In the linear model for the weight:

\(Y_{ij} = \mu + \alpha_i +E_{ij}\)

The null hypothesis is \(H_0=\mu_A=\mu_B=\mu\) therefore \(H_0:\alpha_i=0\)

\begin{itemize}
\item
  If the null hypothesis is true both \(MSE\) and \(MST\) are estimators of \(\sigma^2\).
\item
  If \(Y_i \rightarrow N(\mu_i, \sigma^2_i)\) then \(MSE \rightarrow \chi^2(2n-2)\) and \(MST \rightarrow \chi^2(n-1)\)
\end{itemize}

and the ratio of squares

\[\frac{MST}{MSE} \rightarrow F(2n-2, n-1)\]

follows a \(F\) distribution with \(2n-2\) and \(n-1\) degrees of freedom.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova}{%
\section{ANOVA}\label{anova}}

\begin{itemize}
\item
  \(H_0\): observed values of \(\frac{MST}{MSE}\) near \(1\) suggest that the means between groups \textbf{do not} differ.
\item
  \(H_1\): observed values of \(\frac{MST}{MSE}\) far from \(1\) suggest that the means between groups \textbf{differ}.
\end{itemize}

\[\frac{MST}{MSE}_{obs}=\frac{(\bar{y}_{A}-\bar{y}_{B})^2}{\frac{s^2_p}{n}}=d_{obs}^2\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-1}{%
\section{ANOVA}\label{anova-1}}

We have assumed the same number of observations in each group but when there are two groups the results above holds

\[f_{obs}=d_{obs}^2=(−12.508)^2=156.45\]
The upper tailed pvalue for \(f_{obs}\) is

\[Pval=1-F^{-1}_{F,1,21}(156.45)=3.377 \times 10^{-11}\]
in R: 1-pf(156.45, 1,21)

Which is the same as the one obtained with the t-test with equal variances.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-2}{%
\section{ANOVA}\label{anova-2}}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: weight
##           Df  Sum Sq Mean Sq F value    Pr(>F)    
## group      1 1583.33 1583.33  156.45 3.377e-11 ***
## Residuals 21  212.53   10.12                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{_main_files/figure-latex/unnamed-chunk-145-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-several-groups}{%
\section{ANOVA several groups}\label{anova-several-groups}}

The ANOVA approach allows for the analysis of many groups.

Consider the \textbf{linear model}

\[Y_{ij} = \mu + \alpha_i +\epsilon_{ij}\]

with \textbf{Random error}:

\begin{itemize}
\tightlist
\item
  \(\epsilon_{ij}\) is a \textbf{random variable} with \(E(\epsilon_{ij})=0\), \(V(\epsilon_{ij})=\sigma^2\)
\end{itemize}

and \(k\) groups.

\begin{itemize}
\tightlist
\item
  \(\alpha_i, i \in\{1...k\}\) such that \(\sum_i \alpha_i=0\) are the deviations of the group means to the the overall mean.
\end{itemize}

\[E(Y|i)=\mu + \alpha_i\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-several-groups-1}{%
\section{ANOVA several groups}\label{anova-several-groups-1}}

\begin{itemize}
\tightlist
\item
  \(H_0: \alpha_1=\alpha_2, ...=\alpha_k=0\) There are no difference between group means
\end{itemize}

Then, observed values of \(\frac{MST}{MSE}\) near \(1\) suggest that the means between groups \textbf{do not}.

\begin{itemize}
\tightlist
\item
  \(H_1\) at least one \(\alpha_i\) is different
\end{itemize}

Then, observed values of \(\frac{MST}{MSE}\) far from \(1\) suggest that the means between groups \textbf{differ}.

\[\frac{MST}{MSE} \rightarrow F(k-1, k(n-1))\]

\begin{itemize}
\tightlist
\item
  where MSE is the estimated variance within groups
\end{itemize}

\[MSE=\frac{1}{k(n-1)} \sum_{i=1}^k\sum_{j=1}^n(Y_{ij}-\bar{Y}_i)^2\]

\begin{itemize}
\tightlist
\item
  MST is the estimated variance between groups
\end{itemize}

\[MST= \frac{n}{k-1}\sum_{i=A,B}(\bar{Y}_{i}-\bar{Y})^2\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-several-groups-2}{%
\section{ANOVA several groups}\label{anova-several-groups-2}}

In a study that wanted to test the effect of leptin in neurodevelopment, 7 male mice had their leptin gene knocked out. While 16 mice were left with normal leptin function. In a third group, 10 mice with knocked out leptin were injected leptin (PMID: 30694175). An initial question was to test the effect of the leptin group on the body weight of the animals.

\begin{itemize}
\tightlist
\item
  We assume that the weight of the control animals has a probability density
\end{itemize}

\[Y_A \rightarrow N(\mu_A, \sigma_A^2)\]

\begin{itemize}
\tightlist
\item
  We assume a normal distribution weight for the mice with no leptin.
\end{itemize}

\[Y_B \rightarrow N(\mu_B, \sigma_B^2)\]

\begin{itemize}
\tightlist
\item
  We assume a normal distribution weight for the mice with no leptin gene but were injected leptin.
\end{itemize}

\[Y_C \rightarrow N(\mu_C, \sigma_C^2)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-means-8}{%
\section{Difference between means}\label{difference-between-means-8}}

One random experiment has two outcomes: \((weight, leptin)\).

Continuous variable (outcome of interest)

\begin{itemize}
\tightlist
\item
  \(weigth \in (20, 60)\)
\end{itemize}

Categorical variable:

\begin{itemize}
\tightlist
\item
  \(leptin \in \{control:A,knockout:B, replace:C\}\)
\end{itemize}

The data looks like

\begin{verbatim}
##    weight    group
## 1   27.67  Control
## 2   27.40  Control
## 3   25.77  Control
## 4   25.60  Control
## 5   25.03  Control
## 6   25.90  Control
## 7   26.67  Control
## 8   25.60  Control
## 9   28.93  Control
## 10  31.83  Control
## 11  25.90  Control
## 12  26.30  Control
## 13  27.90  Control
## 14  26.77  Control
## 15  25.83  Control
## 16  20.87  Control
## 17  46.57 leptinKO
## 18  40.43 leptinKO
## 19  41.97 leptinKO
## 20  41.17 leptinKO
## 21  41.57 leptinKO
## 22  46.17 leptinKO
## 23  53.83 leptinKO
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-means-9}{%
\section{Difference between means}\label{difference-between-means-9}}

We take weights \textbf{conditioned to} each leptin condition, and observed:

\begin{itemize}
\item
  \(n_A=16\) control mice had a weight mean of \(\bar{y}_A=26.49813\) and \(s_A=2.247577\)
\item
  \(n_B=10\) leptin KO mice with leptin replacement had a weight mean of \(\bar{y}_B=25.668\) and \(s_B=5.034161\)
  We can draw boxplots per group
\item
  \(n_C=7\) leptin KO mice had a weight mean of \(\bar{y}_C=44.53\) and \(s_C=4.774167\)
\end{itemize}

\begin{verbatim}
##    weight    group
## 1   27.67  Control
## 2   27.40  Control
## 3   25.77  Control
## 4   25.60  Control
## 5   25.03  Control
## 6   25.90  Control
## 7   26.67  Control
## 8   25.60  Control
## 9   28.93  Control
## 10  31.83  Control
## 11  25.90  Control
## 12  26.30  Control
## 13  27.90  Control
## 14  26.77  Control
## 15  25.83  Control
## 16  20.87  Control
## 17  46.57 leptinKO
## 18  40.43 leptinKO
## 19  41.97 leptinKO
## 20  41.17 leptinKO
## 21  41.57 leptinKO
## 22  46.17 leptinKO
## 23  53.83 leptinKO
## 24  24.33   KOplus
## 25  22.37   KOplus
## 26  26.10   KOplus
## 27  17.50   KOplus
## 28  35.17   KOplus
## 29  25.97   KOplus
## 30  27.67   KOplus
## 31  23.37   KOplus
## 32  31.83   KOplus
## 33  22.37   KOplus
\end{verbatim}

We can see the differences in distributions with a boxplot

\includegraphics{_main_files/figure-latex/unnamed-chunk-148-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-several-groups-3}{%
\section{ANOVA several groups}\label{anova-several-groups-3}}

The observed value of the statistics is

\[\frac{MST}{MSE}=63.373\]

which is still a \textbf{rare} observation of an \(F\) distribution with \(2\) and \(30=(16+7+10)/3-1\) degrees of freedom

\[pvalue=1-F^{-1}_{F,2,30}(63.373)=1.694 \times 10^{-11}\]

Suggesting significant differences in at least one group mean.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-several-groups-4}{%
\section{ANOVA several groups}\label{anova-several-groups-4}}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: weight
##           Df  Sum Sq Mean Sq F value    Pr(>F)    
## group      2 1861.55  930.77  63.373 1.694e-11 ***
## Residuals 30  440.62   14.69                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\includegraphics{_main_files/figure-latex/unnamed-chunk-150-1.pdf}

We observed a significant difference between groups (ANOVA test \(F(2,30)=63.373, P= 1.69 \times 10^{-11}\)), due to the higher gain in weight of the knockout mice. Note that knocked-out mice with replacement recovered wild-type weight (t-test difference between means \(-0.83\), \(P=0.63\))

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-two-factor}{%
\section{ANOVA two factor}\label{anova-two-factor}}

The ANOVA approach allows for the analysis of the joint effect of \textbf{two random} variables.

Let us include an additional sample of female mice in the leptin study and ask: Is the change in weight across different leptin groups that we observed in male mice the same in female mice?

\begin{itemize}
\item
  Is there an effect of \textbf{sex} on the weight of the mice?
\item
  Is that effect different between leptin groups?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{two-factor}{%
\section{Two factor}\label{two-factor}}

One random experiment has three outcomes: \((weight, leptin, sex)\).

Continuous variable (outcome of interest)

\begin{itemize}
\tightlist
\item
  \(weigth \in (20, 60)\)
\end{itemize}

Categorical variable:

\begin{itemize}
\tightlist
\item
  \(leptin \in \{control:A,knockout:B\}\)
\end{itemize}

Categorical variable:

\begin{itemize}
\tightlist
\item
  \(sex \in \{male:a,female:b\}\)
\end{itemize}

The data looks like

\begin{verbatim}
##    weight    group sex
## 1   27.67  Control   M
## 2   27.40  Control   M
## 3   25.77  Control   M
## 4   25.60  Control   M
## 5   25.03  Control   M
## 6   25.90  Control   M
## 7   26.67  Control   M
## 8   25.60  Control   M
## 9   28.93  Control   M
## 10  31.83  Control   M
## 11  25.90  Control   M
## 12  26.30  Control   M
## 13  27.90  Control   M
## 14  26.77  Control   M
## 15  25.83  Control   M
## 16  20.87  Control   M
## 17  46.57 leptinKO   M
## 18  40.43 leptinKO   M
## 19  41.97 leptinKO   M
## 20  41.17 leptinKO   M
## 21  41.57 leptinKO   M
## 22  46.17 leptinKO   M
## 23  53.83 leptinKO   M
## 34  22.30  Control   F
## 35  23.30  Control   F
## 36  23.10  Control   F
## 37  22.20  Control   F
## 38  22.30  Control   F
## 39  19.90  Control   F
## 40  22.20  Control   F
## 41  20.60  Control   F
## 42  22.00  Control   F
## 43  20.40  Control   F
## 44  21.00  Control   F
## 45  22.00  Control   F
## 46  23.20  Control   F
## 47  22.00  Control   F
## 48  23.30  Control   F
## 49  20.60  Control   F
## 50  22.80  Control   F
## 51  19.50  Control   F
## 52  20.80  Control   F
## 53  20.20  Control   F
## 54  20.00  Control   F
## 55  20.80  Control   F
## 56  17.60  Control   F
## 57  16.30  Control   F
## 58  65.80 leptinKO   F
## 59  51.40 leptinKO   F
## 60  54.60 leptinKO   F
## 61  48.30 leptinKO   F
## 62  50.60 leptinKO   F
## 63  48.90 leptinKO   F
## 64  51.20 leptinKO   F
## 65  46.80 leptinKO   F
## 66  50.90 leptinKO   F
## 67  42.70 leptinKO   F
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-means-10}{%
\section{Difference between means}\label{difference-between-means-10}}

We take weights \textbf{conditioned to} each leptin by sex condition, and observed:

\begin{itemize}
\item
  \(n_{Aa}=16\) control \textbf{male} mice had a weight mean of \(\bar{y}_{Aa}=26.49813\) and \(s_{Aa}=2.247577\)
\item
  \(n_{Ba}=7\) leptin KO \textbf{male} mice had a weight mean of \(\bar{y}_{Ba}=44.53\) and \(s_{Ba}=4.774167\)
\item
  \(n_{Ab}=24\) control \textbf{female} mice had a weight mean of \(\bar{y}_{Ab}=21.18333\) and \(s_{Ab}=1.757386\)
\item
  \(n_{Bb}=10\) leptin KO \textbf{female} mice had a weight mean of \(\bar{y}_{Bb}=51.12\) and \(s_{Bb}=6.059483\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-means-11}{%
\section{Difference between means}\label{difference-between-means-11}}

We draw the boxplot, grouping leptin conditions, and observe a strong overall effect of leptin

\includegraphics{_main_files/figure-latex/unnamed-chunk-152-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-between-means-12}{%
\section{Difference between means}\label{difference-between-means-12}}

If we draw the boxplot grouping by sex, we observe that sex is not have such a strong effect on weight

\includegraphics{_main_files/figure-latex/unnamed-chunk-153-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-two-factor-1}{%
\section{ANOVA two factor}\label{anova-two-factor-1}}

The ANOVA approach allows for the analysis of two factors (each with many groups-levels).

Consider the \textbf{linear model}

\[Y_{ijr} = \mu + \alpha_i + \beta_j + \epsilon_{ijr}\]

with \textbf{Random error}:

\begin{itemize}
\tightlist
\item
  \(\epsilon_{ijr}\) is a \textbf{random variable} with \(E(\epsilon_{ijr})=0\), \(V(\epsilon_{ijr})=\sigma^2\)
\end{itemize}

and \(k\) groups for factor 1 (leptin).

\begin{itemize}
\tightlist
\item
  \(\alpha_i, i \in\{1...k\}\) such that \(\sum_i \alpha_i=0\) are the deviations of the group means to the the overall mean.
\end{itemize}

\[E(Y|i)=\mu + \alpha_i\]

and \(m\) groups for factor 2 (sex).

\[E(Y|j)=\mu + \beta_j\]
Each experiment is defined by a given group in factor 1 and a given group in factor 2 (e.g \((control, male)\)) and repeated \(n\) times (for simplicity but not loss of generality)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{variance-components-2}{%
\section{Variance components}\label{variance-components-2}}

The squared deviations of the observations to the overall average can be decomposed into their variations within each experiment (\(SSE\)) and the variations between factor 1 \((SS_{Fac1})\) and factor 2 \((SS_{Fac2})\).

\[SS_T= SSE + SS_{Fac1} + SS_{Fac2}\]

That defines \(F\) statistics

\[\frac{MS1}{MSE} \rightarrow F(k-1,(m-1)(nk-1))\]

and \[\frac{MS2}{MSE}\rightarrow F(n-1,(m-1)(nk-1))\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-several-groups-5}{%
\section{ANOVA several groups}\label{anova-several-groups-5}}

ANOVA allows testing two null hypothesis

First

\begin{itemize}
\item
  \(H_0: \alpha_1=\alpha_2, ...=\alpha_k=0\) There are no difference between group means for the fist factor
\item
  \(H_1\) at least one \(\alpha_i\) is different
\end{itemize}

Then, observed values of \(\frac{MS1}{MSE}\) far from \(1\) suggest that the means between groups of the first factor \textbf{differ}.

Second

\begin{itemize}
\item
  \(H_0: \beta_1=\beta_2, ...=\beta_k\) There are no difference between group means for the second factor
\item
  \(H_1\) at least one \(\beta_i\) is different
\end{itemize}

Then, observed values of \(\frac{MS1}{MSE}\) far from \(1\) suggest that the means between groups of the second factor \textbf{differ}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: weight
##           Df Sum Sq Mean Sq  F value Pr(>F)    
## group      1 7514.2  7514.2 396.8721 <2e-16 ***
## sex        1   41.6    41.6   2.1968 0.1441    
## Residuals 54 1022.4    18.9                    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

As we observed from the boxplots, the statistical inference confirms that there are significant differences in weight between leptin conditions but no significant differences between sexes.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-interaction}{%
\section{ANOVA interaction}\label{anova-interaction}}

From the \textbf{linear model}

\[Y_{ijr} = \mu + \alpha_i + \beta_j + \epsilon_{ijr}\]

We have that the expected value of any observation

\[E(Y|ij)=\mu + \alpha_i+ \beta_j\]

is the sum of the overall mean and the means of each factor (the factors superimpose).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-interaction-1}{%
\section{ANOVA interaction}\label{anova-interaction-1}}

This is only true if he had observed for the condition (\(male, leptinKO\)) something like (white box)

\includegraphics{_main_files/figure-latex/unnamed-chunk-155-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-interaction-2}{%
\section{ANOVA interaction}\label{anova-interaction-2}}

The apparent less-than-expected gain in weight of leptin KO males seems like a specific interaction between these two conditions.

We then formulate the \textbf{linear model} with an \textbf{interaction term}

\[Y_{ijr} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \epsilon_{ijr}\]

Such that each observation in each experiment have a specific contribution from the conditions in each factor

\[E(Y|ij)=\mu + \alpha_i+ \beta_j + (\alpha\beta)_{ij}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-interaction-3}{%
\section{ANOVA interaction}\label{anova-interaction-3}}

The squared deviations of the observations to the overall average can be decomposed into their variations within each experiment (\(SSE\)) and the variations between factor 1 \((SS_{Fac1})\), factor 2 \((SS_{Fac2})\) and their interaction terms \((SS_{Int})\).

\[SS_T= SSE + SS_{Fac1} + SS_{Fac2}+ SS_{Int}\]

That defines \(F\) statistics

\[\frac{MS1}{MSE} \rightarrow F(k-1,(m-1)(nk-1))\]

\[\frac{MS2}{MSE}\rightarrow F(n-1,(m-1)(nk-1))\]
and
\[\frac{MSI}{MSE}\rightarrow F((n-1)(k-1),(m-1)(nk-1))\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-interaction-4}{%
\section{ANOVA interaction}\label{anova-interaction-4}}

ANOVA allows testing three null hypothesis

First

\begin{itemize}
\tightlist
\item
  \(H_0: \alpha_1=\alpha_2, ...=\alpha_k=0\) There are no difference between group means for the fist factor
\end{itemize}

Second

\begin{itemize}
\tightlist
\item
  \(H_0: \beta_1=\beta_2, ...=\beta_k=0\) There are no difference between group means for the second factor
\end{itemize}

Third

\begin{itemize}
\tightlist
\item
  \(H_0: (\alpha\beta)_{ij}=0\) There is no difference between group means for the second factor
\end{itemize}

And the alternatives are that at least one of the terms is different from \(0\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anova-interaction-5}{%
\section{ANOVA interaction}\label{anova-interaction-5}}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: weight
##           Df Sum Sq Mean Sq  F value    Pr(>F)    
## group      2 7939.7  3969.8 299.5748 < 2.2e-16 ***
## sex        1   31.0    31.0   2.3422    0.1302    
## group:sex  2  419.0   209.5  15.8095 1.975e-06 ***
## Residuals 73  967.4    13.3                       
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

As we observed from the boxplots, the statistical inference confirms that there are significant differences in weight between leptin conditions, no significant differences between sexes, and significant interactions between sex and leptin condition. In particular, the effect of leptin knockout in weight is higher in females than males, opposite to what was observed in controls.

\hypertarget{regression-and-correlation}{%
\chapter{Regression and correlation}\label{regression-and-correlation}}

\hypertarget{objective-17}{%
\section{Objective}\label{objective-17}}

\begin{itemize}
\tightlist
\item
  Bivariate normal distribution
\item
  Correlation
\item
  Regression
\item
  Multiple regression
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{regression}{%
\section{Regression}\label{regression}}

Leptin is a hormone produced by adipose tissue. We want to study the serum leptin levels in the adult population (PMID: 23628382) under a \textbf{continuous} condition, such as the amount in Kg of body fat.

\begin{itemize}
\tightlist
\item
  We assume that the levels of leptin have a probability density
\end{itemize}

\[Y \rightarrow N(\mu_y, \sigma_x^2)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{regression-1}{%
\section{Regression}\label{regression-1}}

One random experiment has two outcomes: \((leptin, sex)\).

Continuous variable (outcome of interest)

\begin{itemize}
\tightlist
\item
  \(lepting \in (0, 5)\)
\end{itemize}

Continuous variable:

\begin{itemize}
\tightlist
\item
  \(fatmass \in (20,80)\)
\end{itemize}

Repeating the experiment \(n\) times, the data for the first five repetitions look like

\begin{verbatim}
##     leptin fatmass
## 1 3.355677  45.721
## 2 2.272126  43.895
## 3 1.071584  47.871
## 4 3.921082  65.801
## 5 1.536867  56.644
## 6 1.177115  56.355
\end{verbatim}

\textbf{Question:} \(fatmass\) and \(leptin\) statistically independent variables?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{continous-variation-of-the-mean}{%
\section{Continous variation of the mean}\label{continous-variation-of-the-mean}}

Leptin levels are continuous

\[Y \rightarrow N(\mu_y, \sigma_{y}^2)\]

But fat mass is also a continuous variable

\[X \rightarrow N(\mu_x, \sigma_x^2)\]
To formulate the null hypothesis, we want to condition \(leptin\) on \(fatmass\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-bivariate}{%
\section{Normal bivariate}\label{normal-bivariate}}

A random 2D vector of two random variables \((Y, X)\) follows a bivariate normal distribution if its probability density is

\[f(y,x)=\frac{1}{2\pi \sigma_y\sigma_x \sqrt{1-\rho^2}}e^{\frac{(y-\mu_y)^2}{\sigma_y^2}-\frac{2\rho(y-\mu_y)(x-\mu_x)}{\sigma_y\sigma_x}+\frac{(x-\mu_x)^2}{\sigma_x^2}}\]

With parameters

on the marginals:

\begin{itemize}
\tightlist
\item
  \(E(Y)=\mu_y\), \(V(Y)=\sigma^2_y\)
\item
  \(E(X)=\mu_x\), \(V(X)=\sigma^2_x\)
\end{itemize}

on the correlation:

\begin{itemize}
\tightlist
\item
  \(\rho= \frac{E[(Y-\mu_y)(X-\mu_x)]}{\sigma_y\sigma_x}\)
\end{itemize}

\(\rho\) is called the correlation coefficient

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-bivariate-1}{%
\section{Normal bivariate}\label{normal-bivariate-1}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-159-1.pdf}

\includegraphics{_main_files/figure-latex/unnamed-chunk-160-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-bivariate-2}{%
\section{Normal bivariate}\label{normal-bivariate-2}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-161-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimators}{%
\section{Estimators}\label{estimators}}

If we formulate the likelihood function

\[L=\Pi_{i=1}^n f(y_i,x_i; \mu_x, \mu_y, \sigma^2_x, \sigma_y^2, \rho)\]
Maximizing the function, we can obtain estimators for each of the parameters, resulting in

Estimators for

\begin{itemize}
\tightlist
\item
  \(\mu_y\): \(\bar{Y}=\frac{1}{n}\sum_{i=1}^n y_i\)
\item
  \(\mu_x\): \(\bar{X}=\frac{1}{n}\sum_{i=1}^n x_i\)
\item
  \(\sigma^2_y\): \(S^2_y=\frac{1}{n}\sum_{i=1}^n (y_i-\bar{y})^2\)
\item
  \(\sigma^2_x\): \(S^2_x=\frac{1}{n}\sum_{i=1}^n (x_i-\bar{x})^2\)
\item
  \(\rho\): \[R=\frac{\sum_{i=0}^n(X_i-\bar{X})(Y_i-\bar{Y})}{\sqrt{\sum_{i=1}^n(X_i-\bar{X})^2}\sqrt{\sum_{i=1}^n(Y_i-\bar{Y})^2}}\]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{correlation-coefficient}{%
\section{Correlation coefficient}\label{correlation-coefficient}}

The transformation of \(R\) (Fisher's z transformation) has a distribution

\[\frac{1}{2}\ln (\frac{1+R}{1-R}) \rightarrow_{aprox} N(\frac{1}{2}\ln (\frac{1+\rho}{1-\rho}), \frac{1}{n-3})\]

\begin{itemize}
\item
  If \(R\) is \(0\) there is no direction in the relationship between \(y\) and \(x\) (the probability distribution is concentric)
\item
  If \(R\) is near \(1\) most of the observations fall close to a line
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-5}{%
\section{Hypothesis}\label{hypothesis-5}}

Null hypothesis:

\begin{itemize}
\tightlist
\item
  \(Y\) and \(X\) are statistically independent, therefore \(f(y,x)=f(x)f(y)\) and \(H_0: \rho=0\)
\end{itemize}

Alternative hypothesis:

\begin{itemize}
\tightlist
\item
  \(Y\) and \(X\) are statistically dependent, therefore \(H_1: \rho \neq 0\)
\end{itemize}

We, therefore, use the statistic \(R\) to test whether there is a dependency between \(y\) and \(x\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{regression-coefficient}{%
\section{Regression coefficient}\label{regression-coefficient}}

The observed value of \(R\) is

\[r=\hat{\rho}=\frac{\sum_{i=0}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}\]

For our data

\begin{itemize}
\item
  \(r_{obs}=-0.2766492\)
\item
  The transformed value \(ln((1-0.2766492)/(1-0.2766492))/2=-0.2840499\) and it is rare under the distribution of \(R\)
\end{itemize}

\[pvalue=2(1- \phi(|r_{obs}|)=0.0001214\]

Therefore, since it is lower than \(\alpha=0.05\), we reject the null hypothesis that leptin and fat mass are independent.

Leptin and fat mass are weakly correlated but highly significant.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{correlation-coefficient-1}{%
\section{Correlation coefficient}\label{correlation-coefficient-1}}

\begin{verbatim}
## 
##  Pearson's product-moment correlation
## 
## data:  data$leptin and data$fatmass
## t = -3.9262, df = 186, p-value = 0.0001214
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.4037736 -0.1390439
## sample estimates:
##        cor 
## -0.2766492
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{correlation-coefficient-2}{%
\section{Correlation coefficient}\label{correlation-coefficient-2}}

We should take this result with care:

\begin{itemize}
\tightlist
\item
  We see that the \textbf{marginals} are not quite normal distributions.
\item
  As we increase fat mass we should obtain more leptin, as it is released from adipose tissue.
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-163-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conditional-distribution}{%
\section{Conditional distribution}\label{conditional-distribution}}

We can rather ask: For a given value of fat mass, what is the probability density of leptin?

The conditional probability of \(Y\) (leptin) given \(X\) (fat mass) is

\(f(y|x)=\frac{f(y,x)}{f(y)}\)
\[=N(\mu_{y|x}, \sigma^2_{y|x})\]

with

\begin{itemize}
\item
  mean: \(E(Y|X)=\mu_{y|x}=\mu_y+\rho\frac{\sigma_y}{\sigma_x}(x-\mu_x)\)
\item
  variance: \(V(Y|X)=\sigma^2_{y|x}= \sigma_y^2(1-\rho^2)\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-164-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sums-of-squares}{%
\section{Sums of squares}\label{sums-of-squares}}

\includegraphics{_main_files/figure-latex/unnamed-chunk-165-1.pdf}

\begin{itemize}
\tightlist
\item
  The total sum of squares for the conditional distributions are
\end{itemize}

\[SS_{tot}=\sum_{i=1}^n(Y_i-\bar{Y})^2=\sum_{i=1}^n(Y_i -\hat{\mu}_y)^2\]

\begin{itemize}
\tightlist
\item
  The sum of squares for the error given \(x_i\) is
\end{itemize}

\[SSE=\sum_{i=1}^n(Y_i-\bar{Y_i})^2=\sum_{i=1}^n(Y_i-\hat{\mu}_{y|x_i})^2\]

\begin{itemize}
\tightlist
\item
  The sum of squares for the treatment (explained by variations of \(x\)) is
\end{itemize}

\[SS_{treatment}=\sum_{i=1}^n(\bar{Y_i}-\bar{Y})^2=\sum_{i=1}^n(\hat{\mu}_{y|x} -\hat{\mu}_y)^2\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{coefficient-of-determination}{%
\section{Coefficient of determination}\label{coefficient-of-determination}}

The coefficient of determination \(R^2\) is the percentage explained of the total variance

\[R^2=\frac{SS_{treatment}}{SS_{tot}}\]
It has mean:

\[E(R^2)=\rho^2=\frac{\sigma^2_{y}-\sigma^2_{y|x}}{\sigma^2_{y}}\]

\begin{itemize}
\item
  \(\sigma_y^2-\sigma^2_{y|x}\) is the variance associated to changes in \(x\) (\(SS_{treatment}=SS_{tot} - SSE\)).
\item
  \(R^2\) is the square of the correlation coefficient.
\item
  If \(R^2\) is near \(1\) most of the total variance is explained by the regression (the error is near zero)
\item
  If \(R^2\) is \(0\) no variance is explained (total variance is all error).
\item
  In our data \(r^2=0.07\) and therefore only \(7%
  \) of the variation of \(x\) explained the variation on \(y\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{linear-model-3}{%
\section{Linear model}\label{linear-model-3}}

Consider the \textbf{linear model}

\[Y_{x_i} = \alpha + \beta x_i +\epsilon_{i}\]

with \textbf{Random error}:

\begin{itemize}
\item
  \(\epsilon_{i}\) is a \textbf{random variable} with \(E(\epsilon_{i})=0\), \(V(\epsilon_{i})=\sigma_{y|x}^2\)
\item
  \(i\) is the index of the observation: \(1...n\) (typically one for every \(x_i\) as \(x_i\) is continuous)
\end{itemize}

and

\[E(Y|x_i)=\alpha + \beta x_i\]
This is called a \textbf{regression line} of \(Y\) on \(x\), it tells us how the mean of \(Y_x\) varies with \(x\).

Note that:

\[\alpha=\mu_y-\beta\mu_x\]

\[\beta=\rho\frac{\sigma_y}{\sigma_x}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-6}{%
\section{Hypothesis}\label{hypothesis-6}}

Null hypothesis:

\begin{itemize}
\tightlist
\item
  \(Y\) and \(X\) are statistically independent, therefore \(f(y|x)=f(y)\) and \(H_0: \beta=0\)
\end{itemize}

Alternative hypothesis:

\begin{itemize}
\tightlist
\item
  \(Y\) and \(X\) are statistically dependent, therefore \(H_1: \beta\neq 0\)
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-166-1.pdf}

\includegraphics{_main_files/figure-latex/unnamed-chunk-167-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimators-1}{%
\section{Estimators}\label{estimators-1}}

\begin{itemize}
\tightlist
\item
  \(\beta=\rho\frac{\sigma_y}{\sigma_x}\) suggest the estimator for \(\beta\)
\end{itemize}

\[B=\frac{\sum_{i=1}^m(x_i-\bar{x})(Y_i-\bar{Y})}{\sum_{i=1}^n(x_i-\bar{x})^2}\]

\begin{itemize}
\tightlist
\item
  \(\alpha=\mu_y-\beta\mu_x\) suggests the estimator for \(\alpha\)
\end{itemize}

\[A=\bar{Y}- \hat{\beta}\bar{x}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimators-2}{%
\section{Estimators}\label{estimators-2}}

The estimators \(A\) and \(B\) for \(\alpha\) and \(\beta\) can formally be derived from \textbf{minimizing the sum of squares} for the error given \(x_i\)

\[SSE=\sum_{i=1}^n(Y_i-\bar{Y_i})^2=\sum_{i=1}^n(Y_i-\alpha + \beta x_i)^2\]
with respect to \(\alpha\) and \(\beta\), leading to

\[B=\frac{\sum_{i=1}^m(x_i-\bar{x})(Y_i-\bar{Y})}{\sum_{i=1}^n(x_i-\bar{x})^2}\]

\begin{itemize}
\tightlist
\item
  with mean
\end{itemize}

\[E(B)=\beta\]

\begin{itemize}
\tightlist
\item
  and distribution
\end{itemize}

\[B \rightarrow N(\beta, \frac{n\sigma^2_y}{{(n-2)s^2_x}})\]
The estimator \(A\) is for \(\alpha\) is

\[A=\bar{x}-\hat{\beta} \bar{Y}\]
with mean \(E(A)=\mu_x-\beta\mu_y\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-testing-6}{%
\section{Hypothesis testing}\label{hypothesis-testing-6}}

Under the null hypothesis, \(\beta=0\) and the standardized error from the null hypothesis are

\[\frac{E(B)}{\sqrt{\frac{ns^2_y}{{(n-2)s^2_x}}}} \rightarrow T(n-2)\]
follows a t-distribution with \(n-2\) degrees of freedom.

Is our \(t_{obs}\) a rare observation from this distribution?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{model-fit}{%
\section{Model fit}\label{model-fit}}

\[\beta_{obs}= \frac{\sum_{i=1}^m(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}= -0.02262\]
\[\alpha_{obs}=\bar{x}-\beta_{obs}\bar{y}= 3.72012\]
\includegraphics{_main_files/figure-latex/unnamed-chunk-168-1.pdf}

But leptin should increase with fat mass \ldots{}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-test}{%
\section{Hypothesis test}\label{hypothesis-test}}

\begin{verbatim}
## 
## Call:
## lm(formula = leptin ~ fatmass, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.89723 -0.66914  0.04445  0.64555  1.81122 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  3.720119   0.295149  12.604  < 2e-16 ***
## fatmass     -0.022624   0.005762  -3.926 0.000121 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.8338 on 186 degrees of freedom
## Multiple R-squared:  0.07653,    Adjusted R-squared:  0.07157 
## F-statistic: 15.42 on 1 and 186 DF,  p-value: 0.0001214
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multiple-regression}{%
\section{Multiple Regression}\label{multiple-regression}}

We can include other conditions in the regression, such as sex or age.

Consider the \textbf{linear model}

\[Y_{ij} = \alpha + \beta x_i +\gamma z_j+\epsilon_{ij}\]
It is important to adjust for other factors that we believe are correlated with the outcome \(Y\) and the condition \(x\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multiple-regression-1}{%
\section{Multiple Regression}\label{multiple-regression-1}}

We can now have one outcome \(Y\) with multiple regressors

\begin{verbatim}
##     leptin fatmass sex age
## 1 3.355677  45.721   F  45
## 2 2.272126  43.895   F  77
## 3 1.071584  47.871   M  79
## 4 3.921082  65.801   F  58
## 5 1.536867  56.644   M  42
## 6 1.177115  56.355   M  75
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multiple-regression-2}{%
\section{Multiple Regression}\label{multiple-regression-2}}

Consider the previous regression and color the points according to their sex

\includegraphics{_main_files/figure-latex/unnamed-chunk-171-1.pdf}

We find that the negative association between leptin and fat mass is given by the effect of sex.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multiple-regression-3}{%
\section{Multiple Regression}\label{multiple-regression-3}}

If we run regression separating by sex we find a positive association between leptin and fat mass, as expected.

\includegraphics{_main_files/figure-latex/unnamed-chunk-172-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multiple-regression-4}{%
\section{Multiple Regression}\label{multiple-regression-4}}

When we include other factors in the regression, we observe that there is a positive increase in leptin for females and a positive relationship between leptin and fat mass within each sex.

\begin{verbatim}
## 
## Call:
## lm(formula = leptin ~ fatmass + sex + age, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.91314 -0.43523  0.07833  0.38451  1.49707 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  1.482557   0.304423   4.870 2.40e-06 ***
## fatmass      0.027585   0.006002   4.596 7.99e-06 ***
## sexM        -1.611965   0.135872 -11.864  < 2e-16 ***
## age          0.005353   0.002957   1.811   0.0718 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.6226 on 184 degrees of freedom
## Multiple R-squared:  0.4907, Adjusted R-squared:  0.4824 
## F-statistic: 59.09 on 3 and 184 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multiple-regression-interaction}{%
\section{Multiple Regression interaction}\label{multiple-regression-interaction}}

We can include interactions between conditions in the regression.

Consider the \textbf{linear model}

\[Y_{ij} = \alpha + \beta x_{i} +\gamma z_j + \delta x_{i}z_j +\epsilon_{ij}\]
The parameter \(\delta\) will add a contribution to \(\beta\) that is specific to the condition \(j\)

\begin{itemize}
\tightlist
\item
  if \(z_i \in (0,1)\) then when \(z=0\) the coefficient of \(x_i\) is \(\beta\), when \(z=1\) the coefficient of \(x_i\) is \(\beta+\gamma\)
\end{itemize}

\(\gamma\) will test the differences between \(\beta\)s in males and females.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multiple-regression-interaction-1}{%
\section{Multiple Regression interaction}\label{multiple-regression-interaction-1}}

Our data suggest that a steeper increase of leptin with body fat in males than in females (interaction: \(0.028427\), \(pvalue=0.03\))

\begin{verbatim}
## 
## Call:
## lm(formula = leptin ~ fatmass * sex + age, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.82098 -0.38692  0.03192  0.42065  1.43851 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   1.800052   0.337279   5.337 2.77e-07 ***
## fatmass       0.020022   0.006949   2.881  0.00443 ** 
## sexM         -3.218004   0.775217  -4.151 5.07e-05 ***
## age           0.005846   0.002939   1.989  0.04815 *  
## fatmass:sexM  0.028427   0.013513   2.104  0.03677 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.6169 on 183 degrees of freedom
## Multiple R-squared:  0.5027, Adjusted R-squared:  0.4918 
## F-statistic: 46.25 on 4 and 183 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{model-diagnostics}{%
\section{Model diagnostics}\label{model-diagnostics}}

All linear models have been made on the supposition that

\begin{itemize}
\item
  Errors are distributed normally
\item
  Errors have the same variance
\end{itemize}

There are a number of plots to check that at least the data is consistent with these suppositions

\includegraphics{_main_files/figure-latex/unnamed-chunk-175-1.pdf} \includegraphics{_main_files/figure-latex/unnamed-chunk-175-2.pdf} \includegraphics{_main_files/figure-latex/unnamed-chunk-175-3.pdf} \includegraphics{_main_files/figure-latex/unnamed-chunk-175-4.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{maximum-likelihood-3}{%
\section{Maximum likelihood}\label{maximum-likelihood-3}}

Let's look back at Gauss and study the maximum likelihood estimator of the regression.

\begin{itemize}
\tightlist
\item
  Gauss wanted to predict the position of Ceres in the summer of 1802 after it passed behind the sun. Depending on the position they could get decide whether Ceres was a new planet.
\end{itemize}

\[N(y_i;\mu_i=\alpha+\beta t_i,\sigma^2)= \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2\sigma^2} (y_i-\mu_i)^2}= \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2\sigma^2} (y_i-\alpha-\beta t_i)^2}\]

what are the maximum likelihood estimates for \(\alpha\) and \(\beta\)?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{maximum-likelihood-4}{%
\section{Maximum likelihood}\label{maximum-likelihood-4}}

The likelihood function, the probability of having observed \((x_1, ....x_n)\) at \(t_i, ...t_n\)

\(L(\mu_i, \sigma)=\Pi_{i=1..n} N(\alpha+\beta t_i;\mu,\sigma)\)
\[=\big( \frac{1}{\sigma \sqrt{2 \pi}}\big)^n e^{-\frac{1}{2\sigma^2} \sum_i(y_i-\alpha-\beta t_i)^2}\]

The log-likelihood is

\(\log(\Pi_{i=1..n} N(\alpha+\beta t_i;\mu,\sigma))=-\frac{n}{2}\log(2\pi)-n\log( \sigma) - \frac{1}{2\sigma^2} \sum_i(y_i-\alpha-\beta t_i)^2\)

that we differentiate with respect to \(\alpha\) and \(\beta\) and equate to 0 to find the maxima.

After some algebra (exercise) we have

\[\hat{\beta}=\frac{\sum_i (t_i-\bar{t})(y_i -\bar{y})}{\sum_i (t_i-\bar{t})^2}\]

\[\hat{\alpha} = \bar{y} - \hat{\beta} \bar{t}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{maximum-likelihood-5}{%
\section{Maximum likelihood}\label{maximum-likelihood-5}}

These are the values we obtained when we adjust a line to observations
\((x_1, y_1)...(x_n, y_n)\) by minimum squares (when we had \(x\) instead o \(t\)).

\[\hat{\beta}=\frac{\sum_i (x_i-\bar{x})(y_i -\bar{y})}{\sum_i (x_i-\bar{x})^2}\]

\[\hat{\alpha} = \bar{y} - \hat{\beta} \bar{x}\]

\(\beta\) is the realization of the statistic

\[B=\frac{\sum_i (x_i-\bar{x})(Y_i -\bar{Y})}{\sum_i (x_i-\bar{x})^2}\]

That is the sum of the normal variables \(Y_1, ... Y_n\), and therefore is normal.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{maximum-likelihood-6}{%
\section{Maximum likelihood}\label{maximum-likelihood-6}}

\begin{itemize}
\item
  We can then test the probability that \(pval=P(B>0)\) or that Ceres is moving in the sky.
\item
  As we fix the values of \(\alpha\) and \(\beta\) we can compute \(E(Y_{t_n})\) as the most likely prediction of the position of Ceres at time \(t_n\).
\end{itemize}

Gauss's story is one of the most important advancements in science. To predict where to find Ceres in the sky in 1802

\begin{itemize}
\tightlist
\item
  he discovered the \textbf{normal distribution}
\item
  lay the foundations of \textbf{maximum likelihood} method and \textbf{regression} analysis.
\item
  showed that the most likely value of Ceres was the \textbf{average}
\end{itemize}

Astronomers pointed their telescopes where Gauss told them, and there Ceres was!

\hypertarget{group-work-sessions}{%
\chapter{Group Work sessions}\label{group-work-sessions}}

\hypertarget{objectives}{%
\section{Objectives}\label{objectives}}

\begin{itemize}
\item
  The objective of the work sessions is to work together with a student of a \textbf{different background} to perform a full analysis of the \textbf{misophonia dataset}.
\item
  The analysis is \textbf{open}. You can formulate the analysis you consider interesting, trying to cover as much as possible the material we have seen in theory and bootcamps.
\item
  \textbf{Justify} your analysis and \textbf{discuss} them.
\item
  We will have \textbf{two sessions} to perform the report that will be done in colab and handed in through \textbf{google classroom}.
\item
  \emph{Work together, follow your interests and have fun!}
\end{itemize}

Next, we \textbf{describe} the data and show an \textbf{example} of the kind of analysis that can be performed in both group sessions.

\hypertarget{misophonia-dataset}{%
\section{Misophonia dataset}\label{misophonia-dataset}}

Misophonia is a recently described neurological condition whereby patients feel strong anxiety when hearing particular noises (someone blowing their nose, mobile ringing, trains passing, etc..). It is believed that 5\% of the population suffers from this condition without knowing it, likely blaming their anxiety on other causes.

The misophonia dataset is from a recent (unpublished) study that aimed to describe the relationships between misophonia and anxiety, depression, and cephalometric measures (shape of the jaw).

\begin{verbatim}
##   Misofonia Misofonia.dic     Estado Estado.dic ansiedad.rasgo
## 1        si             4 divorciado          2             99
## 2        si             2     casado          1             75
## 3        no             0 divorciado          2             77
## 4        si             3     casado          1             95
## 5        no             0     casado          1             30
## 6        no             0     casado          1             30
##   ansiedad.rasgo.dic ansiedad.estado ansiedad.estado.dic ansiedad.medicada
## 1                  1              99                   1                no
## 2                  1              75                   1                no
## 3                  1              55                   0                no
## 4                  1              99                   1                no
## 5                  0              40                   0                no
## 6                  0              30                   0                no
##   ansiedad.medicada.dic depresion depresion.dic Sexo Edad CLASE
## 1                     0     33.65             1    M   44   III
## 2                     0     19.77             0    M   43    II
## 3                     0     29.57             0    M   24     I
## 4                     0      1.40             0    M   33   III
## 5                     0      5.98             0    H   41     I
## 6                     0     13.87             0    H   35     I
##   Angulo_convexidad protusion.mandibular Angulo_cuelloYtercio Subnasal_H
## 1              7.97                 13.0                 89.6        1.5
## 2             18.23                 -5.0                107.2        7.3
## 3             12.27                 11.5                101.4        5.0
## 4              7.81                 16.8                 75.3        2.7
## 5              9.81                 33.0                105.5        6.0
## 6             13.50                  2.0                105.0        7.0
##   cambio.autoconcepto Misofonia.post Misofonia.pre ansiedad.dif
## 1                   1             21            14            0
## 2                   0             14            13            0
## 3                  NA             NA            NA          -22
## 4                   1             NA            NA            4
## 5                  NA             NA            NA           10
## 6                  NA             NA            NA            0
\end{verbatim}

Here is the description of the variables

{[}1{]} ``Misofonia'': Binary (si: misophinic, no: no misophinic)\\
{[}2{]} ``Misofonia.dic'': Categorical (0: no misophinic, 1: severity 1, 2: severity 2, 3: severity 3, 4: severity 4)\\
{[}3{]} ``Estado'': Marital status (casado: married, soltero: single, viuda: widow, divorciado:divorced)\\
{[}4{]} ``Estado.dic'': Numeric Marital status\\
{[}5{]} ``ansiedad.rasgo'': Score from 0-100 with anxiety personality trait\\
{[}6{]} ``ansiedad.rasgo.dic'': Binary score (0,1) of anxiety personality trait\\
{[}7{]} ``ansiedad.estado'': Score from 0-100 with current state of anxiety\\
{[}8{]} ``ansiedad.estado.dic'': Binary score (0,1) with current state of anxiety\\
{[}9{]} ``ansiedad.medicada'': Diagnosed with anxiety disorder (si, no)\\
{[}10{]} ``ansiedad.medicada.dic'': Diagnosed with anxiety disorder (1, 0)\\
{[}11{]} ``depresion'': Score from 0-50 with current state of depression\\
{[}12{]} ``depresion.dic'' : Binary score (0,1) with current state of depression\\
{[}13{]} ``Sexo'': Male=H, Female:M\\
{[}14{]} ``Edad'': Age\\
{[}15{]} ``CLASE'': Type of jaw\\
{[}16{]} ``Angulo\_convexidad'': convexity angle\\
{[}17{]} ``protusion.mandibular'': Projection of the jaw
{[}18{]} ``Angulo\_cuelloYtercio'': angle between jaw and neck
{[}19{]} ``Subnasal\_H'': Nasal angle\\
{[}20{]} ``cambio.autoconcepto'': Whether people changed their self-concept after treatment.\\
{[}21{]} ``Misofonia.post'': Misophionia diagnosed (A-MISO) after an educational program, where patients were made aware of a condition called misophonia.\\
{[}22{]} ``Misofonia.pre'': Misophionia diagnosed (A-MISO) before an educational program, where patients were made aware of a condition called misophonia\\
{[}23{]} ``ansiedad.dif'': Difference between anxiety state and anxiety trait scores

\hypertarget{group-work-session-1-data-description}{%
\section{Group Work session 1: Data description}\label{group-work-session-1-data-description}}

When reporting the results of a study, we first describe the variables of interest in tables and figures.

\begin{itemize}
\tightlist
\item
  We describe demographics (sex, age, marital status, etc..)
\item
  We describe outcome variables (misophonia)
\item
  We describe explanatory variables (cephalometric measures, anxiety, depression)
\end{itemize}

\textbf{Example:}

Imagine we want to study the anxiety of participants in the misophonia study

We load the data

\begin{verbatim}
##   Misofonia Misofonia.dic     Estado Estado.dic ansiedad.rasgo
## 1        si             4 divorciado          2             99
## 2        si             2     casado          1             75
## 3        no             0 divorciado          2             77
## 4        si             3     casado          1             95
## 5        no             0     casado          1             30
## 6        no             0     casado          1             30
##   ansiedad.rasgo.dic ansiedad.estado ansiedad.estado.dic ansiedad.medicada
## 1                  1              99                   1                no
## 2                  1              75                   1                no
## 3                  1              55                   0                no
## 4                  1              99                   1                no
## 5                  0              40                   0                no
## 6                  0              30                   0                no
##   ansiedad.medicada.dic depresion depresion.dic Sexo Edad CLASE
## 1                     0     33.65             1    M   44   III
## 2                     0     19.77             0    M   43    II
## 3                     0     29.57             0    M   24     I
## 4                     0      1.40             0    M   33   III
## 5                     0      5.98             0    H   41     I
## 6                     0     13.87             0    H   35     I
##   Angulo_convexidad protusion.mandibular Angulo_cuelloYtercio Subnasal_H
## 1              7.97                 13.0                 89.6        1.5
## 2             18.23                 -5.0                107.2        7.3
## 3             12.27                 11.5                101.4        5.0
## 4              7.81                 16.8                 75.3        2.7
## 5              9.81                 33.0                105.5        6.0
## 6             13.50                  2.0                105.0        7.0
##   cambio.autoconcepto Misofonia.post Misofonia.pre ansiedad.dif
## 1                   1             21            14            0
## 2                   0             14            13            0
## 3                  NA             NA            NA          -22
## 4                   1             NA            NA            4
## 5                  NA             NA            NA           10
## 6                  NA             NA            NA            0
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We describe the participants' sex, age, and marital status
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Sex
\end{enumerate}

\begin{verbatim}
## sex
##         H         M 
## 0.3658537 0.6341463
\end{verbatim}

\includegraphics{_main_files/figure-latex/unnamed-chunk-178-1.pdf}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Age
\end{enumerate}

\begin{verbatim}
## [1] 43.93496
\end{verbatim}

\begin{verbatim}
## [1] 14.18654
\end{verbatim}

\includegraphics{_main_files/figure-latex/unnamed-chunk-179-1.pdf} \includegraphics{_main_files/figure-latex/unnamed-chunk-179-2.pdf}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Age by sex
\end{enumerate}

\begin{verbatim}
## [1] 40.64444
\end{verbatim}

\begin{verbatim}
## [1] 10.75165
\end{verbatim}

\begin{verbatim}
## [1] 45.83333
\end{verbatim}

\begin{verbatim}
## [1] 15.58339
\end{verbatim}

\includegraphics{_main_files/figure-latex/unnamed-chunk-180-1.pdf}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Marital status
\end{enumerate}

\begin{verbatim}
## Mstate
##     casado divorciado    soltero      viuda 
## 0.52032520 0.21138211 0.23577236 0.03252033
\end{verbatim}

\includegraphics{_main_files/figure-latex/unnamed-chunk-181-1.pdf}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We describe the clinical outcome, for example, anxiety.
\end{enumerate}

We have four measures of anxiety:

\begin{itemize}
\tightlist
\item
  Trait: ansiedad.rasgo (are you an anxious person?) continuous:0-100
\item
  State: ansiedad.estado (are you currently feeling anxious?) continuous:0-100
\item
  Diagnosed: ansiedad.medicada (have you been diagnosed with an anxiety disorder?) binary (si, no)
\item
  Excess: ansiedad.dif (difference between State and Trait)
\end{itemize}

we describe these clinical outcomes

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Trait (min, max, quantiles, median)
\end{enumerate}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
##    1.00   60.00   80.00   68.77   89.00   99.00      15
\end{verbatim}

\includegraphics{_main_files/figure-latex/unnamed-chunk-183-1.pdf} \includegraphics{_main_files/figure-latex/unnamed-chunk-183-2.pdf}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State (min, max, quantiles, median)
\end{enumerate}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
##    1.00   45.00   77.00   67.85   90.00   99.00      15
\end{verbatim}

\includegraphics{_main_files/figure-latex/unnamed-chunk-184-1.pdf} \includegraphics{_main_files/figure-latex/unnamed-chunk-184-2.pdf}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Diagnosed
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-185-1.pdf}

We can look at relationships between outcomes

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Trait Vs Estate
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-186-1.pdf}

We can also look at the relationships between the clinical outcomes and the features of the participants

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Trait by sex
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-187-1.pdf}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  State by sex
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-188-1.pdf}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Diagnosed by sex
\end{enumerate}

\begin{verbatim}
##          sex
## diagnosed  H  M
##        no 42 71
##        si  3  7
\end{verbatim}

\includegraphics{_main_files/figure-latex/unnamed-chunk-189-1.pdf}

\begin{verbatim}
##          sex
## diagnosed          H          M
##        no 0.93333333 0.91025641
##        si 0.06666667 0.08974359
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Trait Vs age
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-190-1.pdf}

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  State Vs age
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-191-1.pdf}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  age by diagnosis
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-192-1.pdf}

\hypertarget{group-work-session-2-inference}{%
\section{Group Work session 2: Inference}\label{group-work-session-2-inference}}

When reporting the results of a study, we first describe the variables of interest in tables and figures.

\begin{itemize}
\tightlist
\item
  We describe demographics (sex, age, marital status, etc..)
\item
  We describe outcome variables (misophonia/axiety/depression/etc..)
\item
  We describe explanatory variables (cephalometric measures, anxiety, depression)
\end{itemize}

We then test the main hypotheses of the study.

\begin{itemize}
\item
  We state the main relationships we want to study and formulate the statistical hypothesis (Introduction)
\item
  We describe how the study was performed and the statistical methods to test the hypothesis (Methods)
\item
  We describe the results of the hypothesis tests with statistics, and significance measures.
\item
  We illustrate the results with figures.
\end{itemize}

\textbf{Example:}

Imagine we want to study the anxiety of participants in the misophonia study.

We formulate the following hypothesis:

Participants who enrolled in the study had an increased level of anxiety from their baseline (trait) that is related to their:

\begin{itemize}
\tightlist
\item
  age
\item
  sex
\item
  misophonia state.
\end{itemize}

We are interested in the variable misofonia.dif, that is the observed \textbf{excess} of anxiety from the trait

\(excess = state - trait\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Are the state and trait of anxiety correlated?
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-193-1.pdf}

\begin{verbatim}
## 
##  Pearson's product-moment correlation
## 
## data:  state and trait
## t = 23.282, df = 121, p-value < 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.8656964 0.9320106
## sample estimates:
##       cor 
## 0.9041609
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Is excess in anxiety higher than \(0\)?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  We describe the Excess variable with summary statistics and figures (histogram)
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-194-1.pdf}

\begin{verbatim}
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's 
## -45.0000  -8.0000   0.0000  -0.9187   8.0000  37.0000       15
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We then perform a hypothesis test for the mean of anxiety excess \(H_0:\mu=0\) against \(H_1:\mu \neq 0\).
\end{enumerate}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  excess
## t = -0.79192, df = 122, p-value = 0.4299
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -3.215212  1.377814
## sample estimates:
##  mean of x 
## -0.9186992
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  We conclude: We do not see significant large values of the difference in anxiety; Enrollment in the study does not seem to detect individuals with an excess of anxiety.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Is excess in anxiety higher than \(0\) for men and women separately?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  We first describe the conditional distributions
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-196-1.pdf}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We perform the hypothesis test for each sex separately
\end{enumerate}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  excess[sex == "M"]
## t = -1.6994, df = 77, p-value = 0.09328
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -5.5685793  0.4403741
## sample estimates:
## mean of x 
## -2.564103
\end{verbatim}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  excess[sex == "H"]
## t = 1.1158, df = 44, p-value = 0.2706
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -1.558796  5.425462
## sample estimates:
## mean of x 
##  1.933333
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  We conclude: We see that women (M) have a reduction in the excess of anxiety (almost significant), while men (H) had an increase (no significant). Why? perhaps because females tend to consult doctors before men do.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Is the excess of anxiety significantly different between the sexes?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  We test the hypothesis \(H_0:\mu_{men}=\mu_{women}\) against \(H_1:\mu_{men}\neq \mu_{women}\) using a group t.test
\end{enumerate}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  excess[sex == "M"] and excess[sex == "H"]
## t = -1.9574, df = 102.39, p-value = 0.05302
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -9.05452801  0.05965621
## sample estimates:
## mean of x mean of y 
## -2.564103  1.933333
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We conclude: we see that the difference between the group means is within the limit of significance with women having less excess anxiety than men.
\end{enumerate}

\begin{verbatim}
## 
## Call:
## lm(formula = excess ~ sex)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -42.436  -7.436   2.067   7.564  39.564 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)  
## (Intercept)    1.933      1.898   1.019   0.3105  
## sexM          -4.497      2.384  -1.887   0.0616 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 12.73 on 121 degrees of freedom
##   (15 observations deleted due to missingness)
## Multiple R-squared:  0.02858,    Adjusted R-squared:  0.02055 
## F-statistic:  3.56 on 1 and 121 DF,  p-value: 0.06158
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Is excess in anxiety higher in older people?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  We make a plot between anxiety and age
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-200-1.pdf}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We fit the regression model
\end{enumerate}

\[excess = \alpha + \beta * age + \epsilon\]

and test the hypothesis \(H_0: \beta=0\) against \(H_1: \beta\neq 0\)

\begin{verbatim}
## 
## Call:
## lm(formula = excess ~ age)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -43.151  -7.776   0.912   8.516  37.057 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)  -5.4917     3.7799  -1.453    0.149
## age           0.1041     0.0819   1.271    0.206
## 
## Residual standard error: 12.83 on 121 degrees of freedom
##   (15 observations deleted due to missingness)
## Multiple R-squared:  0.01317,    Adjusted R-squared:  0.005016 
## F-statistic: 1.615 on 1 and 121 DF,  p-value: 0.2062
\end{verbatim}

\includegraphics{_main_files/figure-latex/unnamed-chunk-201-1.pdf}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  We conclude: The association, while positive it is not significant. If we adjust by sex the association is a bit stronger but still not significant.
\end{enumerate}

\begin{verbatim}
## 
## Call:
## lm(formula = excess ~ age + sex)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.969  -6.849   0.781   8.019  34.124 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)  
## (Intercept) -3.57179    3.82807  -0.933   0.3527  
## age          0.13545    0.08198   1.652   0.1011  
## sexM        -5.20025    2.40467  -2.163   0.0326 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 12.64 on 120 degrees of freedom
##   (15 observations deleted due to missingness)
## Multiple R-squared:  0.05019,    Adjusted R-squared:  0.03436 
## F-statistic:  3.17 on 2 and 120 DF,  p-value: 0.04553
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Is excess in anxiety different between misophonic grades?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  We plot the excess anxiety across groups (boxplot)
  \includegraphics{_main_files/figure-latex/unnamed-chunk-203-1.pdf}
\item
  We test the hypothesizes \(H_0: \mu_{0}=\mu_{1} ... =\mu_{4}\) against \(H_1:\) at least one of them is different. We fit an ANOVA model.
\end{enumerate}

\begin{verbatim}
## 
## Call:
## lm(formula = excess ~ misophonic)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -39.902  -8.257   1.243   7.152  42.098 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)   
## (Intercept)   -5.098      1.944  -2.622  0.00988 **
## misophonic1   17.098      5.896   2.900  0.00445 **
## misophonic2    3.854      2.822   1.366  0.17464   
## misophonic3    6.904      2.962   2.331  0.02148 * 
## misophonic4    7.986      4.582   1.743  0.08391 . 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 12.45 on 118 degrees of freedom
##   (15 observations deleted due to missingness)
## Multiple R-squared:  0.09483,    Adjusted R-squared:  0.06414 
## F-statistic:  3.09 on 4 and 118 DF,  p-value: 0.01847
\end{verbatim}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: excess
##             Df Sum Sq Mean Sq F value  Pr(>F)  
## misophonic   4   1915  478.76  3.0904 0.01847 *
## Residuals  118  18280  154.92                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  We conclude: We see that anxiety excess of misophonia grade 1 is significantly higher than misophonia grade 0 (no misophonia), as it is grade 3. The ANOVA table shows that we accept the alternative hypothesis, where the differences between groups are significantly higher than within groups.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Are the differences in excess anxiety between monophonic grades modulated by sex?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  We plot excess anxiety for each misophonic group, for men and women separately
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-205-1.pdf}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We perform an ANOVA test for the interaction
\end{enumerate}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: excess
##                 Df  Sum Sq Mean Sq F value  Pr(>F)  
## misophonic       4  1915.0  478.76  3.0366 0.02026 *
## sex              1   179.7  179.74  1.1400 0.28792  
## misophonic:sex   4   284.5   71.13  0.4512 0.77137  
## Residuals      113 17815.9  157.66                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  We conclude: We do not see a significant interaction (modulation) of the effect of sex on the group differences. We cannot say that the profiles of anxiety excess across misophonia grades are different between sexes.
\end{enumerate}

\hypertarget{exercises}{%
\chapter{Exercises}\label{exercises}}

\hypertarget{data-description-1}{%
\section{Data description}\label{data-description-1}}

\hypertarget{exercise-1}{%
\subsubsection{Exercise 1}\label{exercise-1}}

We have performed an experiment 8 times with the following results

\begin{verbatim}
## [1]  3  3 10  2  6 11  5  4
\end{verbatim}

Answer the following questions:

\begin{itemize}
\tightlist
\item
  Compute the relative frequencies of each outcome.
\item
  Compute the cumulative frequencies of each outcome.
\item
  What is the average of the observations?
\item
  What is the median?
\item
  What is the third quartile?
\item
  What is the first quartile?
\end{itemize}

\hypertarget{exercise-2}{%
\subsubsection{Exercise 2}\label{exercise-2}}

We have performed an experiment 10 times with the following results

\begin{verbatim}
##  [1] 2.875775 7.883051 4.089769 8.830174 9.404673 0.455565 5.281055 8.924190
##  [9] 5.514350 4.566147
\end{verbatim}

Consider 10 bins of size 1: {[}0,1{]}, (1,2{]}\ldots(9,10{]}.

Answer the following questions:

\begin{itemize}
\item
  Compute the relative frequencies of each outcome and draw the histogram
\item
  Compute the cumulative frequencies of each outcome and sketch the cumulative plot.
\item
  Sketch a boxplot.
\end{itemize}

\hypertarget{probability-3}{%
\section{Probability}\label{probability-3}}

\hypertarget{exercise-1-1}{%
\subsubsection{Exercise 1}\label{exercise-1-1}}

The outcome of one random experiment is to measure the misophonia severity \textbf{and} depression status of one patient.

\begin{itemize}
\tightlist
\item
  Misophonia severity: \(x\in \{0,1,2,3,4\}\)
\item
  Depression: \(y\in \{0,1\}\) (no:\(0\), yes:\(1\))
\end{itemize}

\begin{verbatim}
##   Misofonia.dic depresion.dic
## 1             4             1
## 2             2             0
## 3             0             0
## 4             3             0
## 5             0             0
## 6             0             0
\end{verbatim}

A large study showed the relative frequencies \(f_i\) given in the contingency table:

\begin{verbatim}
##               
##                Depression:0 Depression:1
##   Misophonia:4   0.00000000   0.07317073
##   Misophonia:3   0.20325203   0.04878049
##   Misophonia:2   0.27642276   0.02439024
##   Misophonia:1   0.04065041   0.00000000
##   Misophonia:0   0.29268293   0.04065041
\end{verbatim}

Let's assume that \(N>>0\) and that the frequencies \textbf{estimate} the probabilities \(f_{i,j}=\hat{P}(x_i, y_j)\)

\begin{itemize}
\tightlist
\item
  What is the marginal probability of misophonia severity 3?
\item
  What is the probability of not being misophonic \textbf{and} not depressed?
\item
  What is the probability of being misophonic \textbf{or} depressed?
\item
  What is the probability of being misophonic \textbf{and} depressed?
\item
  Describe in English the outcomes with probability 0.
\end{itemize}

\hypertarget{exercise-2-1}{%
\subsubsection{Exercise 2}\label{exercise-2-1}}

We have performed an experiment 10 times with the following results

\begin{verbatim}
##         A     B
## 1    male  dead
## 2    male  dead
## 3    male  dead
## 4  female alive
## 5    male  dead
## 6  female alive
## 7  female  dead
## 8  female alive
## 9    male alive
## 10   male alive
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Create the contingency table for the number (\(n_{i,j}\)) of observations of each outcome (\(A,B\))
\item
  Create the contingency table for the relative frequency (\(f_{i,j}\)) of the outcomes
\item
  What is the marginal frequency of being male?
\item
  What is the marginal frequency of being alive?
\item
  What is the frequency of being alive \textbf{or} female?
\end{itemize}

\hypertarget{conditional-probability-3}{%
\section{Conditional Probability}\label{conditional-probability-3}}

\hypertarget{exercise-1-2}{%
\subsubsection{Exercise 1}\label{exercise-1-2}}

A machine is tested for its performance to produce high-quality turning rods. These are the results of the testing

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Rounded: Yes & Rounded: No \\
\midrule
\endhead
smooth surface: yes & 200 & 1 \\
smooth surface: no & 4 & 2 \\
\bottomrule
\end{longtable}

\begin{itemize}
\item
  What is the estimated probability that the machine produces a rod that does not satisfy any quality control?
\item
  What is the estimated probability that the machine produces a rod that does not satisfy at least one quality control?
\item
  What is the estimated probability that the machine produces rounded and smoothed surfaced rods?
\item
  what is the estimated probability that the rod is rounded if the rod is smooth?
\item
  what is the estimated probability that the rod is smooth if it is rounded?
\item
  Are smoothness and roundness independent events?
\end{itemize}

\hypertarget{exercise-2-2}{%
\subsubsection{Exercise 2}\label{exercise-2-2}}

We develop a test to detect the presence of bacteria in a lake. We find that if the lake contains the bacteria the test is positive 70\% of the time. If there are no bacteria then the test is negative 60\% of the time. We deploy the test in a region where we know that 20\% of the lakes have bacteria.

\begin{itemize}
\tightlist
\item
  What is the probability that one lake that tests positive is contaminated with bacteria?
\end{itemize}

\hypertarget{exercise-3}{%
\subsubsection{Exercise 3}\label{exercise-3}}

Two machines are tested for their performance to produce high-quality turning rods. These are the results of the testing

\textbf{Machine 1}

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Rounded: Yes & Rounded: No \\
\midrule
\endhead
smooth surface: yes & 200 & 1 \\
smooth surface: no & 4 & 2 \\
\bottomrule
\end{longtable}

\textbf{Machine 2}

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Rounded: Yes & Rounded: No \\
\midrule
\endhead
smooth surface: yes & 145 & 4 \\
smooth surface: no & 8 & 6 \\
\bottomrule
\end{longtable}

\begin{itemize}
\tightlist
\item
  what is the probability that the rod is rounded?
\item
  What is the probability that the rod has been produced by machine 1?
\item
  what is the probability that the rod is not smooth?
\item
  What is the probability that the rod is smooth or rounded or produced by machine 1?
\item
  what is the probability that the rod has come from machine 2 if it does not pass at least one of the quality controls?
\end{itemize}

\hypertarget{exercise-4}{%
\subsubsection{Exercise 4}\label{exercise-4}}

We want to cross an avenue with three traffic lights. The probability of finding one traffic light in red given that the previous one was in red is 0.15. Whereas, the probability of finding one traffic right in red given that the previous one was in green is 0.25. If the probability of finding the first traffic light in red is 0.6 then

\begin{itemize}
\tightlist
\item
  What is the probability of having to stop at each traffic light?
\item
  What is the probability of having to stop at at least one traffic light?
\item
  What is the probability of having to stop at only one traffic light?
\end{itemize}

Assume that the probability of one traffic light depends only on the previews one

hints:

\begin{itemize}
\item
  The joint probability of finding three traffic lights in red can be written as:
  \(P(R_1,R_2,R_3)=P(R_3|R_2,R_1)P(R_2|R_1)P(R_1)\)
\item
  If the probability of one traffic light depends only on the previous one then
  \(P(R_3|R_2,R_1)=P(R_3|R_2)\)
\end{itemize}

\hypertarget{exercise-5}{%
\subsubsection{Exercise 5}\label{exercise-5}}

A quality test on a random brick is defined by the events:

\begin{itemize}
\tightlist
\item
  Pass quality test: \(E\), do no pass quality test: \(\bar{E}\)
\item
  Defective: \(D\), non-defective: \(\bar{D}\)
\end{itemize}

If the diagnostic test has sensitivity \(P(E|\bar{D})=0.99\) and specificity \(P(\bar{E}|D)=0.98\), and the probability of passing a test is \(P(E)=0.893\) then

\begin{itemize}
\item
  what is the probability that a brick chosen at random is defective \(P(D)\)?
\item
  What is the probability that a brick that has passed the test is really defective?
\item
  The probability that a brick is not defective \textbf{and} that it does not pass the test
\item
  Are \(D\) and \(\bar{E}\) statistical independent?
\end{itemize}

\hypertarget{random-variables}{%
\section{Random variables}\label{random-variables}}

\hypertarget{exercise-1-3}{%
\subsubsection{Exercise 1}\label{exercise-1-3}}

Given the probability distribution for a discrete variable \(X\)

\[
    F(x)= 
\begin{cases}
0, & x \leq -1 \\
0.2,& x \in [-1,0)\\
0.35,& x \in [0,1)\\
0.45,& x \in [1,2)\\
1,& x \geq 2\\
\end{cases}
\]

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  find \(f(X)\)
\item
  find \(E(X)\) and \(V(X)\)
\item
  what is the expected value and variance of \(Y=2X+3\)
\item
  what is the median of \(X\)?
\end{enumerate}

\hypertarget{exercise-2-3}{%
\subsubsection{Exercise 2}\label{exercise-2-3}}

We have a system of transmission of pixels that is totally noisy. We are testing the system and have designed an experiment to transmit 3 pixels.

\begin{itemize}
\item
  What is the probability of receiving 0, 1, 2, or 3 errors in the transmission of 3 pixels?
\item
  Sketch the probability mass function
\item
  What is the expected value of the error?
\item
  What is its variance?
\item
  Sketch the probability distribution
\item
  What is the probability of transmitting at least 1 error?
\end{itemize}

hints:

\begin{itemize}
\item
  Sample space: \(\{(0,0,0), (1,0,0), (0,1,0), (0,0,1), (0,1,1), (1,0,1), (1,1,0), (1,1,1)\}\)
\item
  where, for example, the event \((0,1,1)\) is the event of receiving the first pixel with no error and the second and third pixels with errors.
\item
  All events are equally probable.
\end{itemize}

\hypertarget{exercise-3-1}{%
\subsubsection{Exercise 3}\label{exercise-3-1}}

\begin{itemize}
\tightlist
\item
  for the probability density
\end{itemize}

\[
    f(x)= 
\begin{cases}
    \frac{1}{100},& \text{if } x\in (0,100)\\
    0,& otherwise 
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  compute the mean
\item
  compute variance using \(E(X^2)=V(X)+E(X)^2\)
\item
  compute \(P(\mu-\sigma\leq X \leq \mu+\sigma)\)
\item
  What are the first and third quartiles?
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-212-1.pdf}

\hypertarget{exercise-4-1}{%
\subsubsection{Exercise 4}\label{exercise-4-1}}

For the probability density

\[
    f(x)= 
\begin{cases}
    \lambda e^{-\lambda x},& \text{if } 0 \leq\\
    0,& otherwise 
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  Confirm that this is a probability density
\item
  Find the probability distribution \(F(a)\)
\item
  Compute the mean
\item
  Compute variance using \(E(X^2)=V(X)+E(X)^2\)
\end{itemize}

\hypertarget{exercise-5-1}{%
\subsubsection{Exercise 5}\label{exercise-5-1}}

Given the cumulative distribution for a random variable X

\[
    F(x)= 
\begin{cases}
0, & x  < -1 \\
\frac{1}{80}(17+16x-x^2),& x \in [-1,7)\\
1,& x \geq 7\\
\end{cases}
\]

compute:

\begin{itemize}
\tightlist
\item
  \(P(X>0)\)
\item
  \(E(X)\)
\item
  \(P(X>0|X<2)\)
\end{itemize}

\hypertarget{probability-models}{%
\section{Probability Models}\label{probability-models}}

\hypertarget{exercise-1-4}{%
\subsubsection{Exercise 1}\label{exercise-1-4}}

A search engine fails to retrieve information with a probability \(0.1\)

\begin{itemize}
\item
  If we system receives \(50\) search requests, what is the probability that the system fails to answer three of them?
\item
  What is the probability that the engine successfully completes \(15\) searches before the first failure?
\item
  We consider that a search engine works sufficiently well when it is able to find information for \(10\) requests for every \(2\) failures. What is the probability that in a reliability trial our search engine is satisfactory?
\end{itemize}

\hypertarget{exercise-2-4}{%
\subsubsection{Exercise 2}\label{exercise-2-4}}

In a population, the probability that a baby boy is born is \(p=0.51\). Consider a family of 4 children

\begin{itemize}
\tightlist
\item
  What is the probability that a family has only one boy?
\item
  What is the probability that a family has only one girl?
\item
  What is the probability that a family has only one boy or only one girl?
\item
  What is the probability that the family has at least two boys?
\item
  What is the number of children that a family should have such that the probability of having at least a girl is more than 0.75?
\end{itemize}

\hypertarget{exercise-3-2}{%
\subsubsection{Exercise 3}\label{exercise-3-2}}

The average number of radioactive particles hitting a Geiger counter is \(2.3\) seconds.

\begin{itemize}
\item
  What is the probability of counting exactly 2 particles in a second?
\item
  What is the probability of detecting exactly \(10\) particles in \(5\) seconds?
\item
  What is the probability of at least one count in two seconds?
\item
  What is the probability of having to wait \(2.5\) seconds after we switch on the detector?
\end{itemize}

\hypertarget{exercise-4-2}{%
\subsubsection{Exercise 4}\label{exercise-4-2}}

\begin{itemize}
\item
  What is the probability that a man's height is at least
  \(165\)cm if the population mean is \(175\)cm y the standard deviation is \(10\)cm?
\item
  What is the probability that a man's height is between
  \(165\)cm and \(180\)cm.
\item
  What is the height that defines the \(5\%\) of the smallest men?
\end{itemize}

\hypertarget{point-estimators-4}{%
\section{Point Estimators}\label{point-estimators-4}}

\hypertarget{exercise-1-5}{%
\subsubsection{Exercise 1}\label{exercise-1-5}}

Consider the probability model

\[
    f(x)= 
\begin{cases}
    1/2-a,& \text{if } x=-1 \\ 
    1/2,& \text{if } x=0\\
    a,& 1 \text{if } x=1\\ 
\end{cases}
\]

where \(a\) is a parameter.

Compute the mean and variance of the statistic: \[T=\frac{\bar{X}}{2}+\frac{1}{4}\]

where \(\bar{X}=\frac{1}{N}\sum_{i=1}^N X_i\)

\begin{itemize}
\item
  is \(T\) a biased estimator of \(a\)?
\item
  is \(T\) consistent? i.e.~\(V(T) \rightarrow 0\) when \(N\rightarrow \infty\)
\end{itemize}

\hypertarget{exercise-2-5}{%
\subsubsection{Exercise 2}\label{exercise-2-5}}

\begin{itemize}
\tightlist
\item
  Is \(\bar{X}^2=(\frac{1}{N}\sum_{i=1}^N X_i)^2\) an unbiased estimator of \(E(X)^2\)?
\end{itemize}

\hypertarget{sampling-and-central-limit-theorem}{%
\section{Sampling and Central Limit Theorem}\label{sampling-and-central-limit-theorem}}

\hypertarget{exercise-1-6}{%
\subsubsection{Exercise 1}\label{exercise-1-6}}

A battery model charges up to \(75\%\) of its capacity within an hour with a standard deviation of \(15\%\).

\begin{itemize}
\item
  If we charge \(25\), what is the probability that the sample average is within a distance of \(5\%\) charge from the mean?
\item
  If we charge \(100\), what is that probability?
\item
  If, instead we only charge \(9\) batteries, what is the charge that is surpassed by the sample average with only \(0.015\) probability?
\end{itemize}

\hypertarget{exercise-2-6}{%
\subsubsection{Exercise 2}\label{exercise-2-6}}

An electronic component is needed for the correct functioning of a telescope. It needs to be replaced immediately when it wears out.

The mean life of the component (\(\mu\)) is \(100\) hours and its standard deviation \(\sigma\) is \(30\) hours.

\begin{itemize}
\item
  what is the probability that the average of the mean life of \(50\) components is within \(1\) hour from the mean life of a single component?
\item
  How many components do we need such that the telescope is operational \(2750\) consecutive hours with \(0.95\) probability?
\end{itemize}

\hypertarget{exercise-3-3}{%
\subsubsection{Exercise 3}\label{exercise-3-3}}

An automated machine fills test tubes with biological samples with mean \(\mu=130\)mg and a standard deviation of \(\sigma=5\)mg.

\begin{itemize}
\item
  for a random sample of size \(50\). What is the probability that
  the sample mean (average) is between \(128\) and \(132\)gr?
\item
  what should be the size of the sample (\(n\)) such that the sample mean \(\bar{X}\) is higher than \(131\)gr with a probability less or equal than \(0.025\)?
\end{itemize}

\hypertarget{exercise-4-3}{%
\subsubsection{Exercise 4}\label{exercise-4-3}}

In the Caribbean, there appears to be an average of \(6\) hurricanes per year. Considering that hurricane formation is a Poisson process, meteorologists plan to estimate the mean time between the formation of two hurricanes. They plan to collect a sample of size \(36\) for the times between two hurricanes.

\begin{itemize}
\item
  What is the probability that their sample average is between \(45\) and \(60\) days?
\item
  Which should be the sample size such that they have a probability of \(0.025\) that the sample mean is greater than \(70\) days?
\end{itemize}

\hypertarget{exercise-5-2}{%
\subsubsection{Exercise 5}\label{exercise-5-2}}

The probability that a particular mutation is found in the population is \(0.4\). If we test \(2000\) people for the mutation:

\begin{itemize}
\tightlist
\item
  What is the probability that the total number of people with the mutation is between \(791\) and \(809\)?
\end{itemize}

hint: Use the CLT with a sample of \(2000\) Bernoulli trials. This is known as the normal approximation of the binomial distribution.

\hypertarget{maximum-likelihood-7}{%
\section{Maximum likelihood}\label{maximum-likelihood-7}}

\hypertarget{exercise-1-7}{%
\subsubsection{Exercise 1}\label{exercise-1-7}}

For a random variable with a binomial probability function

\[f(x; p)=\binom n x p^x(1-p)^{n-x}\]

\begin{itemize}
\item
  What is the maximum-likelihood estimator of \(p\) for a sample of size \(1\) of this random variable?
\item
  In \textbf{one} exam of \(100\) students we observed \(x_1=68\) students that passed the exam. What is the estimate of the \(p\)?
\end{itemize}

\hypertarget{exercise-2-7}{%
\subsubsection{Exercise 2}\label{exercise-2-7}}

Take a random variable with the following probability density function

\[
f(x)=
\begin{cases}
    (1+\theta)x^\theta,& \text{if } x\in (0,1)\\
    0,&  x\notin (0,1)
\end{cases}
\]

\begin{itemize}
\item
  What is the maximum likelihood estimate for \(\theta\)?
\item
  If we take a \(5\)-sample with observations
  \(x_1 = 0.92; \qquad x_2 = 0.79; \qquad x_3 = 0.90; \qquad x_4 = 0.65; \qquad x_5 = 0.86\)
\end{itemize}

What is the estimated value of the parameter \(\theta\)?

\hypertarget{exercise-3-4}{%
\subsubsection{Exercise 3}\label{exercise-3-4}}

Take a random variable with the following probability density function

\[
    f(x)= 
\begin{cases}
    \lambda e^{-\lambda x},& \text{if } 0 \leq\\
    0,& otherwise 
\end{cases}
\]

\begin{itemize}
\item
  What is the maximum likelihood estimate for \(\lambda\)?
\item
  If we take a \(5\)-sample with observations
  \(x_1 = 0.223 \qquad x_2 = 0.681; \qquad x_3 = 0.117; \qquad x_4 = 0.150; \qquad x_5 = 0.520\)
\end{itemize}

What is the estimated value of the parameter \(\lambda\)?

\hypertarget{method-of-moments-11}{%
\section{Method of moments}\label{method-of-moments-11}}

\hypertarget{exercise-1-8}{%
\subsubsection{Exercise 1}\label{exercise-1-8}}

What are the estimators of the following parametric models given by the method of moments?

\begin{longtable}[]{@{}lll@{}}
\toprule
Model & f(x) & E(X) \\
\midrule
\endhead
Bernoulli & \(p^x(1-p)^{1-x}\) & \(p\) \\
Binomial & \(\binom n x p^x(1-p)^{n-x}\) & \(np\) \\
Shifted geometric & \(p(1-p)^{x-1}\) & \(\frac{1}{p}\) \\
Negative Binomial & \(\binom {x+r-1} x p^r(1-p)^x\) & \(r\frac{1-p}{p}\) \\
Poisson & \(\frac{e^{-\lambda}\lambda^x}{x!}\) & \(\lambda\) \\
Exponential & \(\lambda e^{-\lambda x}\) & \(\frac{1}{\lambda}\) \\
Normal & \(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\) & \(\mu\) \\
\bottomrule
\end{longtable}

\hypertarget{exercise-2-8}{%
\subsubsection{Exercise 2}\label{exercise-2-8}}

Take a random variable with the following probability density function

\[
f(x)=
\begin{cases}
    (1+\theta)x^\theta,& \text{if } x\in (0,1)\\
    0,& x\notin (0,1)
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  Compute \(E(X)\) as a function of \(\theta\)
\item
  What is the estimate for \(\theta\) using the method of moments?
\item
  If we take a \(5\)-sample with observations
  \(x_1 = 0.92; \qquad x_2 = 0.79; \qquad x_3 = 0.90; \qquad x_4 = 0.65; \qquad x_5 = 0.86\)
\end{itemize}

What is the estimated value of the parameter \(\theta\)?

\hypertarget{exercise-3-5}{%
\subsubsection{Exercise 3}\label{exercise-3-5}}

Consider a discrete random variable \(X\) that follows a negative binomial distribution with probability mass function:

\[f(x) = \binom{x+r-1}{x}p^r(1-p)^x\]

Given that

\begin{itemize}
\tightlist
\item
  \(E(X)=\dfrac{r(1-p)}{p}\)
\item
  \(V(X) =\dfrac{r(1-p)}{p^2}\)
\end{itemize}

compute:

\begin{itemize}
\item
  An estimate for the parameter \(r\) and an estimate for the parameter \(p\) obtained from a random sample of size \(n\) using the method of moments.
\item
  The values of the estimates of \(r\) y \(p\) for the folowing random sample:
\end{itemize}

\[x_1 = 27; \qquad x_2 = 8; \qquad  x_3 = 22; \qquad  x_4 = 29; \qquad  x_5 = 19; \qquad  x_5 = 32\]

  \bibliography{book.bib,packages.bib}

\end{document}
