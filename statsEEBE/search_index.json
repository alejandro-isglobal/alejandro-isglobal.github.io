[["index.html", "EEBE Estadística Chapter 1 Objetivo 1.1 Lectura recomendada", " EEBE Estadística Alejandro Cáceres (alejandro.caceres.dominguez@upc.edu) 2023-10-01 Chapter 1 Objetivo Este es el curso de introducción a la estadística de la EEBE (UPC). La estadística es un lenguaje que permite afrontar problemas nuevos, sobre los que no tenemos solución, y en donde interviene la aleatoridad. En este curso trataremos los conceptos fundamentales de estadística. 3 horas de teoría por semana: Explicaremos los conceptos, haremos ejercicios. 6 horas de estudio individual por semana: Notas notas de curso y los recursos en ATENEA. 2 horas de Solución de problemas con R: Sesiones presenciales con ordenador (Prácticas). Las fechas de exámenes y material de estudio adicional se pueden encontrar en ATENEA metacurso: Objetivos de evaluación: Q1 (10%): Prueba en ordenador duración 2h en las fechas indicadas. Dominio de comandos básicos en R (Prácticas) Capacidad de calcular estadísticos descriptivos y gráficos, en situaciones concretas (Teoría/Práctica) Conocimiento sobre la regresión lineal (Prácticas) EP1 (25%): Prueba escrita (2-3 problemas) Capacidad de interpretación de enunciados en fórmulas de probabilidad (Teoría). Conocimiento de las herramientas básicas para solucionar problemas de probabilidad conjunta y probabilidad condicional (Teoría). Dominio matemático de funciones de probabilidad para calcular sus propiedades básicas (Teoría). Q2 (20%): Prueba en ordenador duración 2h en horario de clase en las fechas indicadas Capacidad de identificación de modelos de probabilidad en problemas concretos (Teoría/Práctica). Uso de funciones de R para calcular probabilidades de modelos probabilísticos (Práctica/Teoría) Capacidad de identificación de un estadístico de muestreo y sus propiedades (Teoría/Práctica) Conocimiento de cómo calcular la probabilidad de los estadísticos de muestreo (Teoría/Práctica) Uso de comandos en R para calcular probabilidades y hacer simulaciones de muestras aleatorias (Prácticas) EP2 (40%): Prueba escrita (2-3 problemas) Capacidad matemática para determinar estimadores puntuales de modelos de probabilidad. Conociemiento de las propiedades de los estimadores puntuales. Conocimiento de los intervalos de confianza y sus propiedades (Teoría). Capacidad de identificar el tipo de intervalo de confianza en un problema concreto (Teoría). Capacidad de interpretación del tipo de hipótesis a usar en un problema concreto (Teoría). Uso de comandos en R para resolver problemas de intervalos de confianza y prueabas de hipótesis (Práctica). CG (5%): Prueba escrita (2 preguntas sobre un texto) Capacidad de expresión escrita sobre un tema relacionado a la estadística. coordinadores: Luis Mujica (luis.eduardo.mujica@upc.edu) Pablo Buenestado (pablo.buenestado@upc.edu) 1.1 Lectura recomendada Los apuntes de clase se nuestra sección estarán accesibles en ATENEA en pdf y en html. Douglas C. Montgomery and George C. Runger. “Applied Statistics and Probability for Engineers” 4th Edition. Wiley 2007. "],["descripción-de-datos.html", "Chapter 2 Descripción de datos 2.1 Método científico 2.2 Estadística 2.3 Datos 2.4 Tipos de resultado 2.5 Experimentos aleatorios 2.6 Frecuencias absolutas 2.7 Frecuencias relativas 2.8 Diagrama de barras 2.9 Gráfico de sectores (pie) 2.10 Variables categóricas ordinales 2.11 Frecuencias acumuladas absolutas y relativas 2.12 Gráfica de frecuencia acumulada 2.13 Variables numéricas 2.14 Transformando datos continuos 2.15 Tabla de frecuencias para una variable continua 2.16 Histograma 2.17 Gráfica de frecuencia acumulada 2.18 Estadísticas de resumen 2.19 Promedio (media muestral) 2.20 Promedio 2.21 mediana 2.22 Dispersión 2.23 Variación de la muestra 2.24 Rango intercuartílico (IQR) 2.25 Diagrama de caja 2.26 Preguntas 2.27 Ejercicios", " Chapter 2 Descripción de datos En este capítulo, presentaremos herramientas para describir datos. Lo haremos utilizando tablas, figuras y estadísticos descriptivos de tendencia central y dispersión. También presentaremos conceptos clave en estadística como experimentos aleatorios, observaciones, resultados y frecuencias absolutas y relativas. 2.1 Método científico Uno de los objetivos del método científico es proporcionar un marco para resolver los problemas que surgen en el estudio de los fenómenos naturales o en el diseño de nuevas tecnologías. Los humanos modernos han desarrollado un método durante miles de años que todavía está en desarrollo. El método tiene tres actividades humanas principales: Observación caracterizada por la adquisición de datos Razón caracterizada por el desarrollo de modelos matemáticos Acción caracterizada por el desarrollo de nuevos experimentos (tecnología) Su compleja interacción y resultados son la base de la actividad científica. 2.2 Estadística La estadística se ocupa de la interacción entre modelos y datos (la parte inferior de la figura). Las preguntas de tipo estadístico son: ¿Cuál es el mejor modelo para mis datos (inferencia)? ¿Cuáles son los datos que produciría un determinado modelo (predicción)? 2.3 Datos Los datos se presentan en forma de observaciones. Una Observación o Realización es la adquisición de un número o una característica de un experimento. Por ejemplo, tomemos la serie de números que se producen por la repetición de un experimento (1: éxito, 0: fracaso) … 1 0 0 1 0 1 0 1 1 … El número en negrita es una observación en una repetición del experimento Un resultado es una posible observación que es el resultado de un experimento. 1 es un resultado, 0 es el otro resultado del experimento. Recuerda que la observación es concreta es el número que obtienes un día en el laboratorio. El resultado abstracto es una de las características del tipo de experimento que estás realizando. 2.4 Tipos de resultado En estadística nos interesan principalmente dos tipos de resultados. Categóricos: Si el resultado de un experimento es una cualidad. Pueden ser nominales (binario: sí, no; múltiple: colores) u ordinales cuando las cualidades pueden jerarquizarse (gravedad de una enfermedad). Numéricos: Si el resultado de un experimento es un número. El número puede ser discreto (número de correos electrónicos recibidos en una hora, número de leucocitos en sangre) o continuo (estado de carga de la batería, temperatura del motor). 2.5 Experimentos aleatorios Se puede decir que el tema de estudio de la estadística son los experimentos aleatorios, el medio por el cual producimos datos. Definición: Un experimento aleatorio es un experimento que da diferentes resultados cuando se repite de la misma manera. Los experimentos aleatorios son de diferentes tipos, dependiendo de cómo se realicen: en el mismo objeto (persona): temperatura, niveles de azúcar. sobre objetos diferentes pero de la misma medida: el peso de un animal. sobre eventos: el número de huracanes por año. 2.6 Frecuencias absolutas Cuando repetimos un experimento aleatorio con resultados categóricos, registramos una lista de resultados. Resumimos las observaciones contando cuántas veces vimos un resultado particular. Frecuencia absoluta: \\[n_i\\] es el número de veces que observamos el resultado \\(i\\). Ejemplo (leucocitos) Extraigamos un leucocito de un donante y anotemos su tipo. Repitamos el experimento \\(N=119\\) veces. (célula T, célula T, neutrófilo, ..., célula B) La segunda célula T en negrita es la segunda observación. La última célula B es la observación número 119. Podemos listar los resultados (categorías) en una tabla de frecuencia: ## outcome ni ## 1 T Cell 34 ## 2 B cell 50 ## 3 basophil 20 ## 4 Monocyte 5 ## 5 Neutrophil 10 De la tabla, podemos decir que, por ejemplo, \\(n_1=34\\) es el número total de células T observadas en la repetición del experimento. También notamos que el número total de repeticiones \\(N=\\sum_i n_i=119\\). 2.7 Frecuencias relativas También podemos resumir las observaciones calculando la proporción de cuántas veces vimos un resultado en particular. \\[f_i=n_i/N\\] donde \\(N\\) es el número total de observaciones En nuestro ejemplo se registraron \\(n_1=34\\) células T, por lo que nos preguntamos por la proporción de células T del total de \\(119\\). Podemos agregar estas proporciones \\(f_i\\) en la tabla las frecuencias. ## outcome ni fi ## 1 T Cell 34 0.28571429 ## 2 B cell 50 0.42016807 ## 3 basophil 20 0.16806723 ## 4 Monocyte 5 0.04201681 ## 5 Neutrophil 10 0.08403361 Las frecuencias relativas son fundamentales en estadística. Dan la proporción de un resultado en relación con los otros resultados. Más adelante las entenderemos como las observaciones de las probabilidades. Para las frecuencias absolutas y relativas tenemos las propiedades \\(\\sum_{i=1..M} n_i = N\\) \\(\\sum_{i=1..M} f_i = 1\\) donde \\(M\\) es el número de resultados. 2.8 Diagrama de barras Cuando tenemos muchos resultados y queremos ver cuáles son los más probables, podemos usar un gráfico de barras que es una cifra de \\(n_i\\) Vs los resultados. 2.9 Gráfico de sectores (pie) También podemos visualizar las frecuencias relativas con un gráfico de sectores. El área del círculo representa el 100% de las observaciones (proporción = 1) y las secciones las frecuencias relativas de cada resultado. 2.10 Variables categóricas ordinales El tipo de leucocito de los ejemplos anteriores es una variable nominal categórica. Cada observación pertenece a una categoría (cualidad). Las categorías no siempre tienen un orden determindado. A veces, las variables categóricas se pueden ordenar cuando cumplen una clasificación natural. Esto permite introducir frecuencias acumulativas. Ejemplo (misofonía) Este es un estudio clínico en 123 pacientes que fueron examinados por su grado de misofonía. La misofnía es ansiedad/ira descontrolada producida por ciertos sonidos. Cada paciente fue evaluado con un cuestionario (AMISO) y se clasificaron en 4 grupos diferentes según la gravedad. Los resultados del estudio son ## [1] 4 2 0 3 0 0 2 3 0 3 0 2 2 0 2 0 0 3 3 0 3 3 2 0 0 0 4 2 2 0 2 0 0 0 3 0 2 ## [38] 3 2 2 0 2 3 0 0 2 2 3 3 0 0 4 3 3 2 0 2 0 0 0 2 2 0 0 2 3 0 1 3 2 4 3 2 3 ## [75] 0 2 3 2 4 1 2 0 2 0 2 0 2 2 4 3 0 3 0 0 0 2 2 1 3 0 0 3 2 1 3 0 4 4 2 3 3 ## [112] 3 0 3 2 1 2 3 3 4 2 3 2 Cada observación es el resultado de un experimento aleatorio: medición del nivel de misofonía en un paciente. Esta serie de datos se puede resumir en términos de los resultados en la tabla de frecuencia ## outcome ni fi ## 1 0 41 0.33333333 ## 2 1 5 0.04065041 ## 3 2 37 0.30081301 ## 4 3 31 0.25203252 ## 5 4 9 0.07317073 2.11 Frecuencias acumuladas absolutas y relativas La gravedad de la misofonía es categórica ordinal porque sus resultados pueden ordenarse en relación con su grado. Cuando los resultados se pueden ordenar, es útil preguntar cuántas observaciones se obtuvieron hasta un resultado dado. Llamamos a este número la frecuencia acumulada absoluta hasta el resultado \\(i\\): \\[N_i=\\sum_{k=1..i} n_k\\] También es útil para calcular la proporción de las observaciones que se obtuvo hasta un resultado dado \\[F_i=\\sum_{k=1..i} f_k\\] Podemos agregar estas frecuencias en la tabla de frecuencias ## outcome ni fi Ni Fi ## 0 0 41 0.33333333 41 0.3333333 ## 1 1 5 0.04065041 46 0.3739837 ## 2 2 37 0.30081301 83 0.6747967 ## 3 3 31 0.25203252 114 0.9268293 ## 4 4 9 0.07317073 123 1.0000000 Por lo tanto, el 67 % de los pacientes tenían misofonía hasta la gravedad 2 y el 37 % de los pacientes tenían una gravedad inferior o igual a 1. 2.12 Gráfica de frecuencia acumulada \\(F_i\\) es una cantidad importante porque nos permite definir la acumulación de probabilidades hasta niveles intermedios. La probabilidad de un nivel intermedio \\(x\\) (\\(i\\leq x&lt; i+1\\)) es solo la acumulación hasta el nivel inferior \\(F_x=F_i\\). \\(F_x\\) es por lo tanto una función de rango continuo. Podemos dibujarla con respecto a los resultados. Por lo tanto, podemos decir que el 67 % de los pacientes tenían misofonía hasta gravedad \\(2.3\\), aunque \\(2.3\\) no es un resultado observado. 2.13 Variables numéricas El resultado de un experimento aleatorio puede producir un número. Si el número es discreto, podemos generar una tabla de frecuencias, con frecuencias absolutas, relativas y acumulativas, e ilustrarlas con gráficos de barras, de sectores y acumulativos. Cuando el número es continuo las frecuencias no son útiles, lo más probable es que observemos o no un número contínuo en particular. Ejemplo (misofonía) Los investigadores se preguntaron si la convexidad de la mandíbula afectaría la gravedad de la misofonía. La hipótesis científica es que el ángulo de convexidad de la mandíbula puede influir en el oído y su sensibilidad. Estos son los resultados de la convexidad de la mandíbula (grados) para cada paciente: ## [1] 7.97 18.23 12.27 7.81 9.81 13.50 19.30 7.70 12.30 7.90 12.60 19.00 ## [13] 7.27 14.00 5.40 8.00 11.20 7.75 7.94 16.69 7.62 7.02 7.00 19.20 ## [25] 7.96 14.70 7.24 7.80 7.90 4.70 4.40 14.00 14.40 16.00 1.40 9.76 ## [37] 7.90 7.90 7.40 6.30 7.76 7.30 7.00 11.23 16.00 7.90 7.29 6.91 ## [49] 7.10 13.40 11.60 -1.00 6.00 7.82 4.80 11.00 9.00 11.50 16.00 15.00 ## [61] 1.40 16.80 7.70 16.14 7.12 -1.00 17.00 9.26 18.70 3.40 21.30 7.50 ## [73] 6.03 7.50 19.00 19.01 8.10 7.80 6.10 15.26 7.95 18.00 4.60 15.00 ## [85] 7.50 8.00 16.80 8.54 7.00 18.30 7.80 16.00 14.00 12.30 11.40 8.50 ## [97] 7.00 7.96 17.60 10.00 3.50 6.70 17.00 20.26 6.64 1.80 7.02 2.46 ## [109] 19.00 17.86 6.10 6.64 12.00 6.60 8.70 14.05 7.20 19.70 7.70 6.02 ## [121] 2.50 19.00 6.80 2.14 Transformando datos continuos Como los resultados continuos no se pueden contar (de manera informativa), los transformamos en variables categóricas ordenadas. Primero cubrimos el rango de las observaciones en intervalos regulares del mismo tamaño (contenedores) ## [1] &quot;[-1.02,3.46]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; &quot;(16.8,21.3]&quot; Luego mapeamos cada observación a su intervalo: creando una variable categórica ordenada; en este caso con 5 resultados posibles ## [1] &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; ## [6] &quot;(12.4,16.8]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; ## [11] &quot;(12.4,16.8]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; ## [16] &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; ## [21] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; ## [26] &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [31] &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; &quot;[-1.02,3.46]&quot; ## [36] &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [41] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; ## [46] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; ## [51] &quot;(7.92,12.4]&quot; &quot;[-1.02,3.46]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [56] &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; ## [61] &quot;[-1.02,3.46]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; ## [66] &quot;[-1.02,3.46]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;[-1.02,3.46]&quot; ## [71] &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; ## [76] &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; ## [81] &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; ## [86] &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; ## [91] &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; ## [96] &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; ## [101] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; ## [106] &quot;[-1.02,3.46]&quot; &quot;(3.46,7.92]&quot; &quot;[-1.02,3.46]&quot; &quot;(16.8,21.3]&quot; &quot;(16.8,21.3]&quot; ## [111] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; ## [116] &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [121] &quot;[-1.02,3.46]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; Por tanto, en lugar de decir que el primer paciente tenía un ángulo de convexidad de \\(7.97\\), decimos que su ángulo estaba entre el intervalo (o bin) \\((7.92,12.4]\\). Ningún otro paciente tenía un ángulo de \\(7.97\\), pero muchos tenían ángulos entre \\((7.92,12.4]\\). 2.15 Tabla de frecuencias para una variable continua Para una partición regular dada del intervalo de resultados en intervalos, podemos producir una tabla de frecuencias como antes ## outcome ni fi Ni Fi ## 1 [-1.02,3.46] 8 0.06504065 8 0.06504065 ## 2 (3.46,7.92] 51 0.41463415 59 0.47967480 ## 3 (7.92,12.4] 26 0.21138211 85 0.69105691 ## 4 (12.4,16.8] 20 0.16260163 105 0.85365854 ## 5 (16.8,21.3] 18 0.14634146 123 1.00000000 2.16 Histograma El histograma es la gráfica de \\(n_i\\) o \\(f_i\\) Vs los resultados en intervalos (bins). El histograma depende del tamaño de los bins. Este es un histograma con 20 bins. Vemos que la mayoría de las personas tienen ángulos dentro de \\((7, 8]\\) 2.17 Gráfica de frecuencia acumulada También podemos graficar \\(F_x\\) contra los resultados. Como \\(F_x\\) es de rango continuo, podemos ordenar las observaciones (\\(x_1 &lt;... x_j &lt; x_{j+1} &lt; x_n\\)) y por lo tanto \\[F_x = \\frac{k}{n}\\] para \\(x_{k} \\leq x &lt; x_{k+1}\\). \\(F_x\\) se conoce como la distribución de los datos. \\(F_x\\) no depende del tamaño del bin. Sin embargo, su resolución depende de la cantidad de datos. 2.18 Estadísticas de resumen Las estadísticas de resumen son números calculados a partir de los datos que nos dicen características importantes de las variables numéricas (discretas o continuas). Por ejemplo, tenemos estadísticas que describen los valores extremos: mínimo: el resultado mínimo observado máximo: el resultado máximo observado 2.19 Promedio (media muestral) Una estadística importante que describe el valor central de los resultados (dónde esperar la mayoría de las observaciones) es el promedio \\[\\bar{x}=\\frac{1}{N} \\sum_{j=1..N} x_j\\] donde \\(x_j\\) es la observación \\(j\\) de un total de \\(N\\). Ejemplo (Misofonía) La convexidad promedio se puede calcular directamente a partir de las observaciones \\(\\bar{x}= \\frac{1}{N}\\sum_j x_j\\) \\(= \\frac{1}{N}(7.97 + 18.23 + 12.27... + 6.80) = 10.19894\\) Para variables categóricamente ordenadas, podemos usar las frecuencias relativas para calcular el promedio \\(\\bar{x}=\\frac{1}{N}\\sum_{i=1...N} x_j=\\frac{1}{N}\\sum_{i=1...M} x_i*n_ {i}\\) \\[=\\sum_{i=1...M} x_i*f_{i}\\] donde pasamos de sumar \\(N\\) observaciones a sumar \\(M\\) resultados. La forma \\(\\bar{x}= \\sum_{i = 1...M} x_i f_i\\) muestra que el promedio es el centro de gravedad de los resultados. Como si cada resultado tuviera una densidad de masa dada por \\(f_i\\). Ejemplo (Misofonía) La severidad promedio de la misofonía en el estudio se puede calcular a partir de las frecuencias relativas de los resultados ## outcome ni fi ## 1 0 41 0.33333333 ## 2 1 5 0.04065041 ## 3 2 37 0.30081301 ## 4 3 31 0.25203252 ## 5 4 9 0.07317073 \\(\\bar{x}=0*f_{0}+1*f_{1}+2*f_{2}+3*f_{3}+4*f_{4}=1.691057\\) 2.20 Promedio El promedio es también el centro de gravedad de las variables continuas. Ese es el punto donde las frecuencias reativas se equilibran. 2.21 mediana Otra medida de centralidad es la mediana. La mediana \\(x_m\\), o \\(q_{0.5}\\), es el valor por debajo del cual encontramos la mitad de las observaciones. Cuando ordenamos las observaciones \\(x_1 &lt;... x_j &lt; x_{j+1} &lt; x_N\\), las contamos hasta encontrar la mitad de ellas. \\(x_m\\) es tal que \\[\\sum_{i\\leq m} 1 = \\frac{N}{2}\\] Ejemplo (Misofonía) Si ordenamos los ángulos de convexidad, vemos que \\(62\\) observaciones (individuos) (\\(N/2 \\sim 123/2\\)) están por debajo de \\(7.96\\). La convexidad mediana es por lo tanto \\(q_{0.5}=x_{62}=7.96\\) ## [1] -1.00 -1.00 1.40 1.40 1.80 2.46 2.50 3.40 3.50 4.40 4.60 4.70 ## [13] 4.80 5.40 6.00 6.02 6.03 6.10 6.10 6.30 6.60 6.64 6.64 6.70 ## [25] 6.80 6.91 7.00 7.00 7.00 7.00 7.02 7.02 7.10 7.12 7.20 7.24 ## [37] 7.27 7.29 7.30 7.40 7.50 7.50 7.50 7.62 7.70 7.70 7.70 7.75 ## [49] 7.76 7.80 7.80 7.80 7.81 7.82 7.90 7.90 7.90 7.90 7.90 7.94 ## [61] 7.95 7.96 ## [1] 7.96 7.97 8.00 8.00 8.10 8.50 8.54 8.70 9.00 9.26 9.76 9.81 ## [13] 10.00 11.00 11.20 11.23 11.40 11.50 11.60 12.00 12.27 12.30 12.30 12.60 ## [25] 13.40 13.50 14.00 14.00 14.00 14.05 14.40 14.70 15.00 15.00 15.26 16.00 ## [37] 16.00 16.00 16.00 16.14 16.69 16.80 16.80 17.00 17.00 17.60 17.86 18.00 ## [49] 18.23 18.30 18.70 19.00 19.00 19.00 19.00 19.01 19.20 19.30 19.70 20.26 ## [61] 21.30 ## [1] 7.96 En términos de frecuencias, \\(q_{0.5}\\) hace que la frecuencia acumulada \\(F_x\\) sea igual a \\(0.5\\) \\[\\sum_{i = 0, ... m} f_i =F_{q_{0.5}}=0.5\\] o \\[q_{0.5}=F^{-1}(0.5)\\] En el gráfico de distribución, la mediana es el valor de \\(x\\) en el que se encuentra la mitad del máximo de \\(F\\). El promedio y la mediana no siempre son iguales. 2.22 Dispersión Otras estadísticas de resumen importantes de las observaciones son las de dispersión. Muchos experimentos pueden compartir su media, pero difieren en cuán dispersos son los valores. La dispersión de las observaciones es una medida del ruido. 2.23 Variación de la muestra La dispersión sobre la media se mide con la varianza muestral \\[s^2=\\frac{1}{N-1} \\sum_{j=1..N} (x_j-\\bar{x})^2\\] Este número, mide la distancia cuadrada promedio de las observaciones al promedio. La razón de \\(N-1\\) se explicará cuando hablemos de inferencia, cuando estudiemos la dispersión de \\(\\bar{x}\\), además de la dispersión de las observaciones. En términos de las frecuencias de las variables categóricas y ordenadas \\[s^2=\\frac{N}{N-1} \\sum_{i=1... M} (x_i-\\bar{x})^2 f_i\\] \\(s^2\\) se puede considerar como el momento de inercia de las observaciones. La raíz cuadrada de la varianza de la muestra se denomina desviación estándar \\(s\\). Ejemplo (Misofonía) La desviación estándar del ángulo de convexidad es \\(s= [\\frac{1}{123-1}((7.97-10.19894)^2+ (18.23-10.19894)^2\\) \\(+ (12.27-10.19894)^2 + ...)]^{1/2} = 5.086707\\) La convexidad de la mandíbula se desvía de su media en \\(5.086707\\). 2.24 Rango intercuartílico (IQR) La dispersión de los datos también se puede medir con respecto a la mediana usando el rango intercuartílico: Definimos el primer cuartil como el valor \\(x_m\\) que hace que la frecuencia acumulada \\(F_{q_{0.25}}\\) sea igual a \\(0.25\\) (\\(x\\) donde hemos acumulado una cuarta parte de las observaciones) \\[F_{q_{0.25}}=0.25\\] Definimos el tercer cuartil como el valor \\(x_m\\) que hace que la frecuencia acumulada \\(F_{q_{0.75}}\\) sea igual a \\(0.75\\) (\\(x\\) donde hemos acumulado tres cuartos de observaciones) \\[F_{q_{0.75}}=0.75\\] El rango intercuartílico (IQR) es \\(IQR=q_{0.75} - q_{0.25}\\). Esa es la distancia entre el tercer y el primer cuartil y captura el \\(50\\%\\) central de las observaciones 2.25 Diagrama de caja El rango intercuartílico, la mediana y los \\(5\\%\\) y \\(95\\%\\) de los datos se pueden visualizar en un diagrama de caja. En el diagrama de caja, los valores de los resultados están en el eje y. El IQR es la caja, la mediana es la línea del medio y los bigotes marcan los \\(5\\%\\) y \\(95\\%\\) de los datos. 2.26 Preguntas 1) En el siguiente diagrama de caja, el primer cuartil y el segundo cuartil de los datos son: \\(\\qquad\\)a: \\((-1.00, 21.30)\\); \\(\\qquad\\)b: \\((-1.00, 7.02)\\); \\(\\qquad\\)c: \\((7.02, 7.96)\\); \\(\\qquad\\)d: \\((7.02, 14.22)\\) 2) La principal desventaja de un histograma es que: \\(\\qquad\\)a: Depende del tamaño del bin; \\(\\qquad\\)b: No se puede utilizar para variables categóricas; \\(\\qquad\\)c: No se puede usar cuando el tamaño del bin es pequeño; \\(\\qquad\\)d: Se usa solo para frecuencias relativas; 3) Si las frecuencias acumuladas relativas de un experimento aleatorio con resultados \\(\\{1,2,3,4\\}\\) son: \\(F(1)=0.15, \\qquad F(2)=0.60, \\qquad F(3)=0.85, \\qquad F(4)=1\\). Entonces la frecuencia relativa para el resultado \\(3\\) es \\(\\qquad\\)a: \\(0.15\\); \\(\\qquad\\)b: \\(0.85\\); \\(\\qquad\\)c: \\(0.45\\); \\(\\qquad\\)d: \\(0.25\\) 4) En una muestra de tamaño \\(10\\) de un experimento aleatorio obtuvimos los siguientes datos: \\(8,\\qquad 3,\\qquad 3,\\qquad 7,\\qquad 3,\\qquad 6,\\qquad 5,\\qquad 10,\\qquad 3,\\qquad 8\\). El primer cuartil de los datos es: \\(\\qquad\\)a: \\(3.5\\); \\(\\qquad\\)b: \\(4\\); \\(\\qquad\\)c: \\(5\\); \\(\\qquad\\)d: \\(3\\) 5) Imaginemos que recopilamos datos para dos cantidades que no son mutuamente excluyentes, por ejemplo, el sexo y la nacionalidad de los pasajeros de un vuelo. Si queremos hacer un solo gráfico circular para los datos, ¿cuál de estas afirmaciones es verdadera? \\(\\qquad\\)a: Solo podemos hacer un gráfico circular de nacionalidad porque tiene más de dos resultados posibles; \\(\\qquad\\)b: Podemos hacer un gráfico circular para una variable nueva que marca el sexo y la nacionalidad; \\(\\qquad\\)c: Podemos hacer un gráfico circular para la variale sexo o la variable nacionalidad; \\(\\qquad\\)d: Solo podemos elegir si hacemos un gráfico circular para el sexo o un gráfico circular para la nacionalidad. 2.27 Ejercicios 2.27.0.1 Ejercicio 1 Hemos realizado un experimento 8 veces con los siguientes resultados ## [1] 3 3 10 2 6 11 5 4 Responde las siguientes cuestiones: Calcula las frecuencias relativas de cada resultado. Calcula las frecuencias acumuladas de cada resultado. ¿Cuál es el promedio de las observaciones? ¿Cuál es la mediana? ¿Cuál es el tercer cuartil? ¿Cuál es el primer cuartil? 2.27.0.2 Ejercicio 2 Hemos realizado un experimento 10 veces con los siguientes resultados ## [1] 2.875775 7.883051 4.089769 8.830174 9.404673 0.455565 5.281055 8.924190 ## [9] 5.514350 4.566147 Considera 10 bins de tamaño 1: [0,1], (1,2]…(9,10). Responde las siguientes cuestiones: Calcula las frecuencias relativas de cada resultado y dibuja el histograma Calcula las frecuencias acumulativas de cada resultado y dibuja la gráfica acumulativa. Dibuja un diagrama de caja . "],["probabilidad.html", "Chapter 3 Probabilidad 3.1 Experimentos aleatorios 3.2 Probabilidad de medición 3.3 Probabilidad clásica 3.4 Frecuencias relativas 3.5 Frecuencias relativas en el infinito 3.6 Probabilidad frecuentista 3.7 Probabilidades clásicas y frecuentistas 3.8 Definición de probabilidad 3.9 Tabla de probabilidades 3.10 Espacio muestral 3.11 Eventos 3.12 Álgebra de eventos 3.13 Resultados mutuamente excluyentes 3.14 Probabilidades conjuntas 3.15 Tabla de contingencia 3.16 La regla de la suma: 3.17 Preguntas 3.18 Ejercicios", " Chapter 3 Probabilidad En este capítulo introduciremos el concepto de probabilidad a partir de frecuencias relativas. Definiremos los eventos como los elementos sobre los que se aplica la probabilidad. Los eventos compuestos se definirán usando álgebra de conjuntos. Luego discutiremos el concepto de probabilidad condicional derivado de la probabilidad conjunta de dos eventos. 3.1 Experimentos aleatorios Recordemos el objetivo básico de la estadística. La estadística se ocupa de los datos que se presentan en forma de observaciones. Una observación es la adquisición de un número o una característica de un experimento Las observaciones son realizaciones de resultados. Un resultado es una posible observación que es el resultado de un experimento. Al realizar experimentos, a menudo obtenemos resultados diferentes. La descripción de la variabilidad de los resultados es uno de los objetivos de la estadística. Un experimento aleatorio es un experimento que da diferentes resultados cuando se repite de la misma manera. La pregunta filosófica detrás es ¿Cómo podemos conocer algo si cada vez que lo miramos cambia? 3.2 Probabilidad de medición Nos gustaría tener una medida para el resultado de un experimento aleatorio que nos diga cuán seguros estamos de observar el resultado cuando realicemos un futuro experimento aleatorio. Llamaremos a esta medida la probabilidad del resultado y le asignaremos valores: 0, cuando estamos seguros de que la observación no ocurrirá. 1, cuando estamos seguros de que la observación sucederá. 3.3 Probabilidad clásica Siempre que un experimento aleatorio tenga \\(M\\) resultados posibles que son todos igualmente probables, la probabilidad de cada resultado \\(i\\) es \\[P_i=\\frac{1}{M}\\]. La probabilidad clásica fue defendida por Laplace (1814). Dado que cada resultado es igualmente probable en este tipo de experimento, declaramos una completa ignorancia y lo mejor que podemos hacer es distribuir equitativamente la misma probabilidad para cada resultado. No observamos \\(P_i\\) Deducimos \\(P_i\\) de nuestra razón y no necesitamos realizar ningún experimento para conocerla. Ejemplo (dado): ¿Cuál es la probabilidad de que obtengamos \\(2\\) en el lanzamiento de un dado? \\(P_2=1/6=0.166666\\). 3.4 Frecuencias relativas ¿Qué sucede con los experimentos aleatorios cuyos posibles resultados no son igualmente probables? ¿Cómo podemos entonces definir las probabilidades de los resultados? Ejemplo (experimento aleatorio) Imaginemos que repetimos un experimento aleatorio \\(8\\) veces y obtenemos las siguientes observaciones 8 4 12 7 10 7 9 12 ¿Qué tan seguro estamos de obtener el resultado \\(12\\) en la siguiente observación? La tabla de frecuencias es ## outcome ni fi ## 1 4 1 0.125 ## 2 7 2 0.250 ## 3 8 1 0.125 ## 4 9 1 0.125 ## 5 10 1 0.125 ## 6 12 2 0.250 La frecuencia relativa \\(f_i=\\frac{n_i}{N}\\) parece una medida de probabilidad razonable porque es un número entre \\(0\\) y \\(1\\). mide la proporción del total de observaciones que observamos de un resultado particular. Como \\(f_{12}=0.25\\) entonces estaríamos un cuarto seguros, una de cada 4 observaciones, de obtener \\(12\\). Pregunta: ¿Qué tan bueno es \\(f_i\\) como medida de certeza del resultado \\(i\\)? Ejemplo (experimento aleatorio con mas repeticiones) Digamos que repetimos el experimento 100000 veces más: La tabla de frecuencias es ahora ## outcome ni fi ## 1 2 2807 0.02807 ## 2 3 5607 0.05607 ## 3 4 8435 0.08435 ## 4 5 11070 0.11070 ## 5 6 13940 0.13940 ## 6 7 16613 0.16613 ## 7 8 13806 0.13806 ## 8 9 10962 0.10962 ## 9 10 8402 0.08402 ## 10 11 5581 0.05581 ## 11 12 2777 0.02777 y el gráfico de barras es Aparecieron nuevos resultados y \\(f_{12}\\) ahora es solo \\(0.027\\), y entonces estamos sólo un \\(\\sim 3\\%\\) seguros de obtener \\(12\\) en el próximo experimento. Las probabilidades medidas por \\(f_i\\) cambian con \\(N\\). 3.5 Frecuencias relativas en el infinito Una observación crucial es que si medimos las probabilidades de \\(f_i\\) en valores crecientes de \\(N\\) ¡convergen! En este gráfico cada sección vertical da la frecuencia relativa de cada observación. Vemos que después de \\(N=1000\\) (\\(log10(N)=3\\)) las proporciones apenas varían con mas \\(N\\). Encontramos que cada una de las frecuencias relativas \\(f_i\\) converge a un valor constante \\[lim_{N\\rightarrow \\infty} f_i = P_i\\] 3.6 Probabilidad frecuentista Llamamos Probabilidad \\(P_i\\) al límite cuando \\(N \\rightarrow \\infty\\) de la frecuencia relativa de observar el resultado \\(i\\) en un experimento aleatorio. Defendida por Venn (1876), la definición frecuentista de probabilidad se deriva de datos/experiencia (empírica). No observamos \\(P_i\\), observamos \\(f_i\\) Estimamos \\(P_i\\) con \\(f_i\\) (normalmente cuando \\(N\\) es grande), escribimos: \\[\\hat{P_i}=f_i\\] Similar a la relación entre observación y resultado, tenemos la relación entre frecuencia relativa y probabilidad como un valor concreto de una cantidad abstracta. 3.7 Probabilidades clásicas y frecuentistas Tenemos situaciones en las que se puede usar la probabilidad clásica para encontrar el límite de frecuencias relativas. Si los resultados son igualmente probables, la probabilidad clásica nos da el límite: \\[P_i=lim_{N\\rightarrow \\infty} \\frac{n_i}{N}=\\frac{1}{M}\\] Si los resultados en los que estamos interesados pueden derivarse de otros resultados igualmente probables; Veremos más sobre esto cuando estudiemos los modelos de probabilidad. Ejemplo (suma de dos dados) Nuestro ejemplo anterior se basa en la suma de dos dados. Si bien realizamos el experimento muchas veces, anotamos los resultados y calculamos las frecuencias relativas, podemos conocer el valor exacto de probabilidad. Esta probabilidad se deduce del hecho de que el resultado de cada dado es igualmente probable. A partir de esta suposición, podemos encontrar que (Ejercicio 1) \\[ P_i= \\begin{cases} \\frac{i-1}{36},&amp; i \\in \\{2,3,4,5,6, 7\\} \\\\ \\frac{13-i}{36},&amp; i \\in \\{8,9,10,11,12\\} \\\\ \\end{cases} \\] La motivación de la definición frecuentista es empírica (datos) mientras que la de la definición clásica es racional (modelos). A menudo combinamos ambos enfoques (inferencia y deducción) para conocer las probabilidades de nuestro experimento aleatorio. 3.8 Definición de probabilidad Una probabilidad es un número que se asigna a cada resultado posible de un experimento aleatorio y satisface las siguientes propiedades o axiomas: cuando los resultados \\(E_1\\) y \\(E_2\\) son mutuamente excluyentes; es decir, solo uno de ellos puede ocurrir, entonces la probabilidad de observar \\(E_1\\) o \\(E_1\\), escrito como \\(E_1\\cup E_2\\), es su suma: \\[P(E_1\\cup E_2) = P(E_1) + P(E_2)\\] cuando \\(S\\) es el conjunto de todos los resultados posibles, entonces su probabilidad es \\(1\\) (al menos se observa algo): \\[P(S)=1\\] La probabilidad de cualquier resultado está entre 0 y 1 \\[P(E) \\in [0,1]\\] Propuesto por Kolmogorov’s hace menos de 100 años (1933) 3.9 Tabla de probabilidades Las propiedades de Kolmogorov son las reglas básicas para construir una tabla de probabilidad, de manera similar a la tabla de frecuencia relativa. Ejemplo (Dado) La tabla de probabilidad para el lanzamiento de un dado resultado Probabilidad \\(1\\) 1/6 \\(2\\) 1/6 \\(3\\) 1/6 \\(4\\) 1/6 \\(5\\) 1/6 \\(6\\) 1/6 \\(P(1 \\cup 2\\cup ... \\cup 6)\\) 1 Verifiquemos los axiomas: Donde \\(1 \\cup 2\\) es, por ejemplo, el evento de lanzar un \\(1\\) o un \\(2\\). Entonces \\[P(1 \\cup 2)=P(1)+P(2)=2/6\\] Como \\(S=\\{1,2,3,4,5,6\\}\\) se compone de resultados mutuamente excluyentes, entonces \\[P(S)=P(1\\cup 2\\cup ... \\cup 6) = P(1)+P(2)+ ...+P(n)=1\\] Las probabilidades de cada uno de resultados están entre \\(0\\) y \\(1\\). 3.10 Espacio muestral El conjunto de todos los resultados posibles de un experimento aleatorio se denomina espacio muestral y se denota como \\(S\\). El espacio muestral puede estar formado por resultados categóricos o numéricos. Por ejemplo: temperatura humana: \\(S = (36, 42)\\) grados Celsius. niveles de azúcar en humanos: \\(S=(70-80) mg/dL\\) el tamaño de un tornillo de una línea de producción: \\(S=(70-72) mm\\) número de correos electrónicos recibidos en una hora: \\(S =\\{1, ...\\infty \\}\\) el lanzamiento de un dado: \\(S=\\{1, 2, 3, 4, 5, 6\\}\\) 3.11 Eventos Un evento \\(A\\) es un subconjunto del espacio muestral. Es una colección de resultados. Ejemplos de eventos: El evento de una temperatura saludable: \\(A=37-38\\) grados Celsius El evento de producir un tornillo con un tamaño: \\(A=71.5mm\\) El evento de recibir más de 4 emails en una hora: \\(A=\\{4, \\infty \\}\\) El evento de obtener un número menor o igual a 3 en la tirada de a dice: \\(A=\\{1,2,3\\}\\) Un evento se refiere a un posible conjunto de resultados. 3.12 Álgebra de eventos Para dos eventos \\(A\\) y \\(B\\), podemos construir los siguientes eventos derivados utilizando las operaciones básicas de conjuntos: Complemento \\(A&#39;\\): el evento de no \\(A\\) Unión \\(A \\cup B\\): el evento de \\(A\\) o \\(B\\) Intersección \\(A \\cap B\\): el evento de \\(A\\) y \\(B\\) Ejemplo (dado) Lancemos un dado y veamos los eventos (conjunto de resultados): un número menor o igual a tres \\(A:\\{1,2,3\\}\\) un número par \\(B:\\{2,4,6\\}\\) Veamos como podemos construir nuevos eventos con las operaciones de conjuntos: un número no menor de tres: \\(A&#39;:\\{4,5,6\\}\\) un número menor o igual a tres o par: \\(A \\cup B: \\{1,2,3,4,6\\}\\) un número menor o igual a tres y par \\(A \\cap B: \\{2\\}\\) 3.13 Resultados mutuamente excluyentes Los resultados como tirar \\(1\\) y \\(2\\) en un dado son eventos que no pueden ocurrir al mismo tiempo. Decimos que son mutuamente excluyentes. En general, dos eventos denotados como \\(E_1\\) y \\(E_2\\) son mutuamente excluyentes cuando \\[E_1\\cap E_2=\\emptyset\\] Ejemplos: El resultado de tener una gravedad de misofonía de \\(1\\) y una gravedad de \\(4\\). Los resultados de obtener \\(12\\) y \\(5\\) al sumar el lanzamiento de dos dados. De acuerdo con las propiedades de Kolmogorov, solo los resultados mutuamente excluyentes se pueden organizar en tablas de probabilidad, como en las tablas de frecuencias relativas. 3.14 Probabilidades conjuntas La probabilidad conjunta de \\(A\\) y \\(B\\) es la probabilidad de \\(A\\) y \\(B\\). Eso es \\[P(A \\cap B)\\] o \\(P(A,B)\\). Para escribir probabilidades conjuntas de eventos no mutuamente excluyentes (\\(A \\cap B \\neq \\emptyset\\)) en una tabla de probabilidad, notamos que siempre podemos descomponer el espacio muestral en conjuntos mutuamente excluyentes que involucran las intersecciones: \\(S=\\{A\\cap B, A \\cap B&#39;, A&#39;\\cap B, A&#39;\\cap B&#39;\\}\\) Consideremos el diagrama de Ven para el ejemplo donde \\(A\\) es el evento que corresponde a sacar número menor o igual que 3 y \\(B\\) corresponde a un número par: Las marginales de \\(A\\) y \\(B\\) son la probabilidad de \\(A\\) y la probabilidad de \\(B\\), respectivamente: \\(P(A)=P(A\\cap B&#39;) + P(A \\cap B)=2/6+1/6=3/6\\) \\(P(B)=P(A&#39;\\cap B) +P(A \\cap B)=2/6+1/6=3/6\\) Podemos ahora escribir la tabla de probabilidad para las probabilidades conjuntas Resultado Probabilidad \\((A \\cap B)\\) \\(P(A \\cap B)=1/6\\) \\((A\\cap B&#39;)\\) \\(P(A \\cap B&#39;)=2/6\\) \\((A&#39; \\cap B)\\) \\(P(A&#39; \\cap B)=2/6\\) \\((A&#39; \\cap B&#39;)\\) \\(P(A&#39; \\cap B&#39;)=1/6\\) suma \\(1\\) Cada resultado tiene \\(dos\\) valores (uno para la característica del tipo \\(A\\) y otro para el tipo \\(B\\)) 3.15 Tabla de contingencia La tabla de probabilidad conjunta también se puede escribir en una tabla de contingencia \\(B\\) \\(B&#39;\\) suma \\(A\\) \\(P(A \\cap B )\\) \\(P(A\\cap B&#39; )\\) \\(P(A)\\) \\(A&#39;\\) \\(P(A&#39;\\cap B )\\) \\(P(A&#39;\\cap B&#39; )\\) \\(P(A&#39;)\\) suma \\(P(B)\\) \\(P(B&#39;)\\) 1 Donde las marginales son las sumas en las márgenes de la tabla, por ejemplo: \\(P(A)=P(A \\cap B&#39;) + P(A \\cap B)\\) \\(P(B)=P(A&#39; \\cap B) +P(A \\cap B)\\) En nuestro ejemplo, la tabla de contingencia es \\(B\\) \\(B&#39;\\) suma \\(A\\) \\(1/6\\) \\(2/6\\) \\(3/6\\) \\(A&#39;\\) \\(2/6\\) \\(1/6\\) \\(3/6\\) suma \\(3/6\\) \\(3/6\\) 1 3.16 La regla de la suma: La regla de la suma nos permite calcular la probabilidad de \\(A\\) o \\(B\\), \\(P(A \\cup B)\\), en términos de la probabilidad de \\(A\\) y \\(B\\), \\(P(A \\cap B)\\). Podemos hacer esto de tres maneras equivalentes: Usando las marginales y la probabilidad conjunta \\[P(A \\cup B)=P(A) + P(B) - P(A\\cap B)\\] Usando solo probabilidades conjuntas \\[P(A \\cup B)=P(A \\cap B)+P(A\\cap B&#39;)+P(A&#39;\\cap B)\\] Usando el complemento de la probabilidad conjunta \\[P(A \\cup B)=1-P(A&#39;\\cap B&#39;)\\] Ejemplo (dado) Tomemos los eventos \\(A:\\{1,2,3\\}\\), sacar un número menor o igual que \\(3\\), y \\(B:\\{2,4,6\\}\\), sacar un número par en el lanzamiento de un dado. Por lo tanto: \\(P(A \\cup B)=P(A) + P(B) - P(A\\cap B)=3/6+3/6-1/6=5/6\\) \\(P(A \\cup B)=P(A \\cap B)+P(A\\cap B&#39;)+P(A&#39;\\cap B)=1/6+2/6+2/6=5/6\\) \\(P(A \\cup B)=1-P(A&#39;\\cap B&#39;)= 1-1/6=5/6\\) En la tabla de contingencia \\(P(A \\cup B)\\) corresponde a las casillas en negrita (método 2 arriba), o todas menos el 1/6 de abajo a la derecha (método 3). \\(B\\) \\(B&#39;\\) \\(A\\) 1/6 2/6 \\(A&#39;\\) 2/6 1/6 3.17 Preguntas Recopilamos la edad y categoría de 100 deportistas en una competición \\(edad:junior\\) \\(edad:senior\\) \\(categoria:1ra\\) \\(14\\) \\(12\\) \\(categoria:2a\\) \\(21\\) \\(18\\) \\(categoria:3a\\) \\(22\\) \\(13\\) 1) ¿Cuál es la probabilidad estimada de que un deportista sea de 2ª categoría y senior? \\(\\qquad\\)a: \\(18/100\\); \\(\\qquad\\)b: \\(18/43\\); \\(\\qquad\\)c: \\(18\\); \\(\\qquad\\)d: \\(18/39\\) 2) ¿Cuál es la probabilidad estimada de que el atleta no esté en la tercera categoría y sea senior? \\(\\qquad\\)a: \\(35/100\\); \\(\\qquad\\)b: \\(30/100\\); \\(\\qquad\\)c: \\(22/100\\); \\(\\qquad\\)d: \\(13/100\\) 3) ¿Cuál es la probabilidad marginal de la tercera categoría? \\(\\qquad\\)a: \\(13/100\\); \\(\\qquad\\)b: \\(35/100\\); \\(\\qquad\\)c: \\(22/100\\); \\(\\qquad\\)d: \\(13/22\\) 4) ¿Cuál es la probabilidad marginal de ser senior? \\(\\qquad\\)a: \\(13/100\\); \\(\\qquad\\)b: \\(43/100\\); \\(\\qquad\\)c: \\(43/57\\); \\(\\qquad\\)d: \\(57/100\\) 5) ¿Cuál es la probabilidad de ser senior o de tercera categoría? \\(\\qquad\\)a: \\(65/100\\); \\(\\qquad\\)b: \\(86/100\\); \\(\\qquad\\)c: \\(78/100\\); \\(\\qquad\\)d: \\(13/100\\) 3.18 Ejercicios 3.18.0.1 Probabilidad clásica: Ejercicio 1 Escribe la tabla de probabilidad conjunta para los resultados de lanzar dos dados; en las filas escribe los resultados del primer dado y en las columnas los resultados del segundo dado. ¿Cuál es la probabilidad de sacar \\((3,4)\\)? (R:1/36) ¿Cuál es la probabilidad de tirar \\(3\\) y \\(4\\) con cualquiera de los dos dados? (R:2/36) ¿Cuál es la probabilidad de tirar \\(3\\) en el primer dado o \\(4\\) en el segundo? (A:11/36) ¿Cuál es la probabilidad de tirar \\(3\\) o \\(4\\) con cualquier dado? (R:20/36) Escribe la tabla de probabilidad para el resultado de la suma de dos dados. Supon que el resultado de cada dado es igualmente probable. Verifica que es: \\[ P_i= \\begin{cases} \\frac{i-1}{36},&amp; i \\in \\{2,3,4,5,6, 7\\} \\\\ \\frac{13-i}{36},&amp; i \\in \\{8,9,10,11,12\\} \\\\ \\end{cases} \\] 3.18.0.2 Probabilidad frecuentista: Ejercicio 2 El resultado de un experimento aleatorio es medir la gravedad de la misofonía y el estado de depresión de un paciente. Gravedad de la misofonía: \\(S_M:\\{M_0,M_1,M_2,M_3,M_4\\}\\) Depresión: \\(S_D:\\{D&#39;, D\\}\\)) Escribe la tabla de contingencia para las frecuencias absolutas (\\(n_{M,D}\\)) para un estudio sobre un total de 123 pacientes en el que se observó 100 individuos no tuvieron depresión. Ningún individuo con misofonía 4 y sin depresión. 5 individuos con misofonía de grado 1 y sin depresión. El mismo número que el caso anterior para individuos con depresión y sin misofonía. 25 individuos sin depresión y grado 3 de misofonía. El número de misofónicos sin depresión para los grados 2 y 0 se repartieron a cantiaddes iguales. El número de individuos con depresión y misofonía incrementó progresivamente en múltiplos de tres, empezando en 0 individuos para grado 1. Reponde las siguientes preguntas: ¿Cuantos individuos tuvieron misofonía? (R:83) ¿Cuantos individuos tuvieron misofonía de grado 3? (R:31) ¿Cuantos individuos tuvieron misofonía de grado 2 sin depresión? (R:35) Escribe las tabla de consingencia para frecuencias relativas \\(f_{M,D}\\). Supongamos que \\(N\\) es grande y que las frecuencias absolutas estiman las probabilidades \\(f_{M,D}=\\hat{P}(M \\cap D)\\). Responde las siguientes preguntas: ¿Cuál es la probabilidad marginal de misofonía de gravedad 2? (R: 0.3) ¿Cuál es la probabilidad de no ser misofónico y no estar deprimido? (R:0.284) ¿Cuál es la probabilidad de ser misofónico o estar deprimido? (R: 0.715) ¿Cuál es la probabilidad de ser misofónico y estar deprimido? (R: 0.146) Describir en lenguaje hablado los resultados con probabilidad 0. 3.18.0.3 Ejercicio 3 Hemos realizado un experimento aleatorio \\(10\\) veces, que consiste en anotar el sexo y el estado vital de pacientes con algún tipo de cáncer después de 10 años del diagnóstico. Obtuvimos los siguientes resultados ## A B ## 1 male dead ## 2 male dead ## 3 male dead ## 4 female alive ## 5 male dead ## 6 female alive ## 7 female dead ## 8 female alive ## 9 male alive ## 10 male alive Crea la tabla de contingencia para el número (\\(n_{i,j}\\)) de observaciones de cada resultado (\\(A,B\\)) Crea la tabla de contingencia para la frecuencia relativa (\\(f_{i,j}\\)) de los resultados ¿Cuál es la frecuencia marginal de ser hombre? (R/0.6) ¿Cuál es la frecuencia marginal de estar vivo? (R/0.5) ¿Cuál es la frecuencia de estar vivo o ser mujer? (R/0.6) 3.18.0.4 Teoría: Ejercicio 4 De la segunda forma de la regla de la suma, obtener la primera y la tercera forma. ¿Cuál es la regla de la suma de la tercera forma para la probabilidad de tres eventos \\(P(A \\cup B \\cup C)\\)? "],["probabilidad-condicional.html", "Chapter 4 Probabilidad condicional 4.1 Probabilidad conjunta 4.2 Independencia estadística 4.3 La probabilidad condicional 4.4 Tabla de contingencia condicional 4.5 Independencia estadística 4.6 Dependencia estadística 4.7 Prueba de diagnóstico 4.8 Probabilidades inversas 4.9 Teorema de Bayes 4.10 Preguntas 4.11 Ejercicios", " Chapter 4 Probabilidad condicional En este capítulo, introduciremos la probabilidad condicional. Usaremos la probabilidad condicional para definir la independencia estadística. Discutiremos el teorema de Bayes y discutiremos una de sus principales aplicaciones, que es la eficacia de predicción de una herramienta de diagnóstico. 4.1 Probabilidad conjunta Recordemos que la probabilidad conjunta de dos eventos \\(A\\) y \\(B\\) se define como su intersección \\[P( A,B )=P(A \\cap B)\\] Ahora, imagina experimentos aleatorios que miden dos tipos diferentes de resultados. altura y peso de un individuo: \\((h, w)\\) tiempo y posición de una carga eléctrica: \\((p, t)\\) el lanzamiento de dos dados: (\\(n_1\\),\\(n_2\\)) cruzar dos semáforos en verde: (\\(\\bar{R_ 1}\\) , \\(\\bar{R_2}\\)) A menudo nos interesa saber si los valores de un resultado condicionan los valores del otro. 4.2 Independencia estadística En muchos casos, estamos interesados en saber si dos eventos a menudo tienden a ocurrir juntos. Queremos poder discernir entre dos casos. Independencia entre eventos. Por ejemplo, sacar un 1 en un dado no hace más probable sacar otro 1 en un segundo dado. Correlación entre eventos. Por ejemplo, si un hombre es alto, probablemente sea pesado. Ejemplo (conductor) Realizamos un experimento para averiguar si observar fallas estructurales en un material afecta su conductividad. Los datos se verían como Conductor Estructura Conductividad \\(c_1\\) con fallas defectuosa \\(c_2\\) sin fallas sin defectos \\(c_3\\) con fallas defectuosa … … … \\(c_i\\) sin fallas defectuosa* … … … … … … \\(c_n\\) con fallas sin defectos* Podemos esperar que la conductividad defectuosa ocurra más a menudo con fallas que sin fallas si las fallas afectan la conductividad. Imaginemos que a partir de los datos obtenemos la siguiente tabla de contingencia de probabilidades conjuntas estimadas con fallas (F) sin fallas (F’) suma defectuoso (D) \\(0.005\\) \\(0.045\\) \\(0.05\\) sin defectos (D’) \\(0.095\\) \\(0.855\\) \\(0.95\\) suma \\(0.1\\) \\(0.9\\) 1 donde, por ejemplo, la probabilidad conjunta de \\(F\\) y \\(D\\) es \\(P(D,F)=0.005\\) y las probabilidades marginales son \\(P(D)=P(D, F) + P(D, F&#39;)=0.05\\) \\(P(F)=P(D, F) + P(D&#39;, F)= 0.1\\). 4.3 La probabilidad condicional La conductividad defectuosa es independiente de tener fallas estructurales en el material si la probabilidad de tener conductividad defectuosa (\\(D\\)) es la misma ya sea que tenga fallas (\\(F\\)) o no (\\(F&#39;\\)) . Consideremos primero solamente los materiales que tienen fallas. Dentro de aquellos materiales que tienen fallas (\\(F\\)), ¿cuál es la probabilidad estimada de que sean defectuosos? \\(\\hat{P}(D|F)=\\frac{n_{F,D}}{n_{F}}=\\frac{n_{F,D}/n}{n_{F}/n}= \\frac{f_{F,D}}{f_{F}}\\) \\[= \\frac{\\hat{P}(F,D)}{\\hat{P}(F)}\\] Por lo tanto, en el límite cuando \\(N \\rightarrow \\infty\\), tenemos \\[P(D|F)=\\frac{P(F, D)}{P(D)}=\\frac{P(F \\cap D)}{P(D)}\\] Definición: La probabilidad condicional de un evento \\(B\\) dado un evento \\(A\\), indicado como \\(P(A|B)\\), es \\[P(A|B) = \\frac{P(A\\cap B)}{P(B)}\\] Podemos probar que la probabilidad condicional satisface los axiomas de probabilidad. La probabilidad condicional se puede entender como una probabilidad con un espacio muestral dado por \\(B\\): \\(S_B\\). En nuestro ejemplo, los materiales con fallas. 4.4 Tabla de contingencia condicional Si dividimos las columnas de la tabla de probabilidad conjunta por las probabilidades marginales de los efectos condicionantes (\\(F\\) y \\(F&#39;\\)), podemos escribir una tabla de contingencia condicional F F’ D P(D | F) P(D | F’) D’ P(D’ | F) P(D’ | F’) suma 1 1 Donde las probabilidades por columnas suman uno. La primera columna muestra las probabilidades de ser defectuoso o no solo de que los materiales que tienen fallas (primera condición: \\(F\\)). La segunda columna muestra las probabilidades solo para los materiales que no tienen fallas (segunda condición: \\(F&#39;\\)). Las probabilidades condicionales son las probabilidades del evento dentro de cada condición. Las leemos como: \\(P(D|F)\\): Probabilidad de tener conductividad defectuosa si tiene fallas \\(P(D&#39;|F)\\): Probabilidad de no tener conductividad defectuosa si tiene fallas \\(P(D|F&#39;)\\): Probabilidad de tener conductividad defectuosa si no tiene fallas \\(P(D&#39;|F&#39;)\\) Probabilidad de no tener conductividad defectuosa si no tiene fallas 4.5 Independencia estadística En nuestro ejemplo, la tabla de contingencia condicional es F F’ D P(D|F) = 0.05 P(D|F’)=0.05 D’ P(D’|F)=0.95 P(D’|F’)=0.95 suma 1 1 ¡Observamos que las probabilidades marginales y condicionales son las mismas! \\(P(D|F)=P(D|F&#39;)=P(D)\\) \\(P(D&#39;|F)=P(D&#39;|F&#39;)=P(D&#39;)\\) Esto quiere decir que la probabilidad de observar un conductor defectuoso no depende tener una falla estructural o no. Concluimos que la conductividad defectuosa no se ve afectada por tener una falla estructural. Definición Dos eventos \\(A\\) y \\(B\\) son estadísticamente independientes si ocurre cualquiera de los casos equivalentes \\(P(A|B)=P(A)\\); \\(A\\) es independiente de \\(B\\) \\(P(B|A)=P(B)\\); \\(B\\) es independiente de \\(A\\) y por la definición de probabilidad condicional \\(P(A\\cap B)=P(A|B)P(B)=P(A)P(B)\\) Esta tercera forma es un enunciado sobre las probabilidades conjuntas. Dice que podemos obtener probabilidades conjuntas por la multiplicación de las marginales. En nuestra tabla de probabilidad conjunta original F F’ suma D \\(0.005\\) \\(0.045\\) \\(0.05\\) D’ \\(0.095\\) \\(0.855\\) \\(0.95\\) suma \\(0.1\\) \\(0.9\\) 1 podemos confirmar que todas las entradas de la matriz son el producto de las marginales. Por ejemplo: \\(P(F)P(D)= P(D \\cap F)\\) y \\(P(D&#39;)P(F&#39;)=P(D&#39; \\cap F&#39;)\\). Por lo tanto, ser defectuoso es independiente de tener un defecto. Ejemplo (Monedas) Queremos confirmar que los resultados de lanzar dos monedas son independientes. Consideramos que todos los resultados son igualmente probables: resultado Probabilidad \\((H,T)\\) 1/4 \\((H,H)\\) 1/4 \\((T,T)\\) 1/4 \\((T,H)\\) 1/4 suma 1 donde \\((H,T)\\) es, por ejemplo, el evento de cara en la primera moneda y cruz en la segunda moneda. La tabla de contingencia para las probabilidades conjuntas es: H T suma H \\(1/4\\) \\(1/4\\) \\(1/2\\) T \\(1/4\\) \\(1/4\\) \\(1/2\\) suma \\(1/2\\) \\(1/2\\) 1 De esta tabla vemos que la probabilidad de obtener una cara y luego una cruz es el producto de las marginales \\(P(H, T)=P(H)*P(T)=1/4\\). Por lo tanto, el evento de cara en la primera moneda y cruz en la segunda son independientes. Si elaboramos la tabla de contingencia condicional sobre el lanzamiento de la primera moneda veremos que obtener cruz en la segunda moneda no está condicionado por haber obtenido cara en la primera moneda: \\(P(T|H)=P(T) =1/2\\) 4.6 Dependencia estadística Un ejemplo importante de dependencia estadística se encuentra en el desempeño de herramientas de diagnóstico, donde queremos determinar el estado de un sistema (s) con resultados inadecuado (si) adecuado (no) con una prueba (t) con resultados positivo negativo Por ejemplo, probamos una batería para saber cuánto tiempo puede durar. Tensamos un cable para saber si resiste llevar cierta carga. Realizamos una PCR para ver si alguien está infectado. 4.7 Prueba de diagnóstico Consideremos diagnosticar una infección con una nueva prueba. Estado de infección: si (infectado) no (no infectado) Prueba: positivo negativo La tabla de contingencia condicional es lo que obtenemos en un ambiente controlado (laboratorio) Infección: si Infección: No Test: positivo P(positivo | si) P(positivo | no) Test: negativo P(negativo | si) P(negativo | no) suma 1 1 Miremos las entradas de la tabla 1) Tasa de verdaderos positivos (Sensibilidad): La probabilidad de dar positivo si tiene la enfermedad \\(P(positivo|si)\\) Tasa de verdaderos negativos (Especificidad): La probabilidad de dar negativo si no tiene la enfermedad \\(P(negativo|no)\\) Tasa de falsos positivos: la probabilidad de dar positivo si no tiene la enfermedad \\(P(positivo|no)\\) Tasa de falsos negativos: la probabilidad de dar negativo si tiene la enfermedad \\(P(negativo|si)\\) Alta correlación (dependencia estadística) entre la prueba y la infección significa valores altos de las probabilidades 1 y 2 y valores bajos para las probabilidades 3 y 4. Ejemplo (COVID) Ahora consideremos una situación real. En los días iniciales de la pandemia de coronavirus no había una medida de la eficacia de las PCR para detectar el virus. Uno de los primeros estudios publicados (https://www.nejm.org/doi/full/10.1056/NEJMp2015897) encontró que Las PCR tuvieron una sensibilidad del 70%, en condición de infección. Las PCR tuvieron una especificidad del 94%, en condición de no infección. La tabla de contingencia condicional es Infección: si Infección: No Test: positivo P(positivo|si)=0.7 P(positivo|no)=0.06 Test: negativo P(negativo|si)=0.3 P(negativo|no)=0.94 suma 1 1 Por lo tanto, los errores en las pruebas de diagnóstico fueron: La tasa de falsos positivos es \\(P(positivo|no)=0.06\\) La tasa de falsos negativos es \\(P(negativo|si)=0.3\\) 4.8 Probabilidades inversas Nos interesa encontrar la probabilidad de estar infectado si la prueba da positivo: \\[P(si|positivo)\\] Para eso: Recuperamos la tabla de contingencia para probabilidades conjuntas, multiplicando por las marginales Infección: si Infección: No suma Test: positivo P(positivo | si)P(si) P(positivo | no)P(no) P(positivo) Test: negativo P(negativo | si)P(si) P(negativo | no) P(no) P(negativo) suma P(si) P(no) 1 Usamos la definición de probabilidades condicionales para filas en lugar de columnas (dividimos por la marginal de los resultados de la prueba) Infección: si Infección: No suma Test: positivo P(si|positivo) P(sin|positivo) 1 Test: negativo P(si|negativo) P(sin|negativo) 1 Por ejemplo: \\[P(si|positivo)=\\frac{P(positivo|si)P(si)}{P(positivo)}\\] Para aplicar esta fórmula necesitamos las marginales \\(P(si)\\) (incidencia) y \\(P(positivo)\\). Para encontrar \\(P(si)\\), necesitamos un nuevo estudio: el primer estudio de prevalencia en España mostró que durante el confinamiento \\(P(si)=0.05\\), \\(P(no)=0.95\\), antes del verano de 2020. Para encontrar \\(P(positivo)\\), podemos usar la definición de probabilidad marginal y condicional: \\(P(positivo)=P(positivo \\cap si) + P(positivo \\cap no)\\) \\[= P(positivo|si)P(si)+P(positivo|no)P(no)\\] Esta última relación de las marginales se llama regla de probabilidad total. 4.9 Teorema de Bayes Después de sustituir la regla de probabilidad total en \\(P(si|positivo)\\), tenemos \\[P(si|positivo)=\\frac{P(positivo|si)P(si)}{P(positivo|si)P(si)+P(positivo|no)P(no)}\\] Esta expresión se conoce como teorema de Bayes. Nos permite invertir los condicionales: \\[P(positivo|si) \\rightarrow P(si|positivo)\\] O evaluar una prueba en una condición controlada (infección) y luego usarla para inferir la probabilidad de la condición cuando la prueba es positiva. Ejemplo (COVID): El rendimiento de la prueba fue: Sensibilidad: \\(P(positivo|si)=0.70\\) Tasa de falsos positivos: \\(P(positivo|no)=1- P(negativo|no)=0.06\\) El estudio en población española dio: \\(P(si)=0.05\\) \\(P(no)=1-P(si)=0.95\\). Por lo tanto, la probabilidad de estar infectado en caso de dar positivo era: \\[P(si|positivo)=0.38\\] Concluimos que en ese momento las PCR no eran muy buenas para confirmar infecciones. Sin embargo, apliquemos ahora el teorema de Bayes a la probabilidad de no estar infectado si la prueba fue negativa. \\[P(no|negativo) = \\frac{P(negativo|no) P(no)}{P(negativo|no) P(no)+P(negativo|si)P(si)}\\] La sustitución de todos los valores da \\[P(no|negativo)=0.98\\] Por lo tanto, las pruebas eran buenas para descartar infecciones y un requisito justo para viajar. Teorema de Bayes En general, podemos tener más de dos eventos condicionantes. Por lo tanto, el teorema de Baye dice: Si \\(E1, E2, ..., Ek\\) son \\(k\\) eventos mutuamente excluyentes y exhaustivos y \\(B\\) es cualquier evento, entonces la probabilidad inversa \\(P(Ei|B)\\) es \\[P(Ei|B)=\\frac{P(B|Ei)P(Ei)}{P(B|E1)P(E1) +...+ P(B|Ek)P(Ek)} \\] El denominador es la regla de probabilidad total para la marginal \\(P(B)\\), en términos de las marginales \\(P(E1), P(E2), ... P(Ek)\\). \\[P(B)=P(B|E1)P(E1) +...+ P(B|Ek)P(Ek)\\] Árbol condicional La regla de probabilidad total también se puede ilustrar usando un árbol condicional. Regla de probabilidad total para la marginal de \\(B\\): ¿De cuántas maneras puedo obtener el resultado \\(B\\)? \\(P(B)=P(B|A)P(A)+P(B|A&#39;)P(A&#39;)\\) 4.10 Preguntas Recopilamos la edad y categoría de 100 deportistas en una competición \\(junior\\) \\(senior\\) \\(1er\\) \\(14\\) \\(12\\) \\(2do\\) \\(21\\) \\(18\\) \\(3er\\) \\(22\\) \\(13\\) 1) ¿Cuál es la probabilidad estimada de que el atleta esté en la tercera categoría si el atleta es junior? \\(\\qquad\\)a: \\(22\\); \\(\\qquad\\)b: \\(22/100\\); \\(\\qquad\\)c: \\(22/57\\); \\(\\qquad\\)d: \\(22/35\\); 2) ¿Cuál es la probabilidad estimada de que el atleta sea junior y esté en 1ra categoría si el atleta no está en 3ra categoría? \\(\\qquad\\)a: \\(14/35\\); \\(\\qquad\\)b: \\(14/65\\); \\(\\qquad\\)c: \\(14/100\\); \\(\\qquad\\)d: \\(14/26\\) 3) Una prueba diagnóstica tiene una probabilidad de \\(8/9\\) de detectar una enfermedad si los pacientes están enfermos y una probabilidad de \\(3/9\\) de detectar la enfermedad si los pacientes están sanos. Si la probabilidad de estar enfermo es \\(1/9\\). ¿Cuál es la probabilidad de que un paciente esté enfermo si una prueba detecta la enfermedad? \\(\\qquad\\)a: \\(\\frac{8/9}{8/9+3/9}*1/9\\); \\(\\qquad\\)b: \\(\\frac{3/9}{8/9+3/9}*1/9\\); \\(\\qquad\\)c: \\(\\frac{3/9*8/9}{8/9*1/9+3/9*8/9}\\); \\(\\qquad\\)d: \\(\\frac{8/9*1/9}{8/9*1/9+3/9*8/9}\\); 4) Como se comenta en las notas, una prueba PCR para coronavirus tenía una sensibilidad del 70 % y una especificidad del 94 % y en España durante el confinamiento hubo una incidencia del 5 %. Con estos datos, ¿cuál era la probabilidad de dar positivo en España (\\(P(positivo)\\)) \\(\\qquad\\)a: \\(0.035\\); \\(\\qquad\\)b: \\(0.092\\); \\(\\qquad\\)c: \\(0.908\\); \\(\\qquad\\)d: \\(0.95\\) 5) Con los mismos datos que en la pregunta 4, dar positivo en la PCR y estar infectado no son eventos independientes porque: \\(\\qquad\\)a: La sensibilidad es del 70%; \\(\\qquad\\)b: La sensibilidad y la tasa de falsos positivos son diferentes; \\(\\qquad\\)c: La tasa de falsos positivos es del 0.06%; \\(\\qquad\\)d: la especificidad es del 96% 4.11 Ejercicios 4.11.0.1 Ejercicio 1 Se prueba el rendimiento de una máquina para producir varillas de torneado de alta calidad. Estos son los resultados de las pruebas Redondeado: si Redondeado: No superficie lisa: si 200 1 superficie lisa: no 4 2 ¿Cuál es la probabilidad estimada de que la máquina produzca una varilla que no satisfaga ningún control de calidad? (R: 2/207) ¿Cuál es la probabilidad estimada de que la máquina produzca una varilla que no satisfaga al menos un control de calidad? (R: 7/207) ¿Cuál es la probabilidad estimada de que la máquina produzca varillas de superficie redondeada y alisada? (R: 200/207) ¿Cuál es la probabilidad estimada de que la barra sea redondeada si la barra es lisa? (R: 200/201) ¿Cuál es la probabilidad estimada de que la varilla sea lisa si es redondeada? (R: 200/204) ¿Cuál es la probabilidad estimada de que la varilla no sea ni lisa ni redondeada si no satisface al menos un control de calidad? (R: 2/7) ¿Son eventos independientes la lisa y la redondez? (No) 4.11.0.2 Ejercicio 2 Desarrollamos un test para detectar la presencia de bacterias en un lago. Encontramos que si el lago contiene la bacteria, la prueba es positiva el 70% de las veces. Si no hay bacterias, la prueba es negativa el 60% de las veces. Implementamos la prueba en una región donde sabemos que el 20% de los lagos tienen bacterias. ¿Cuál es la probabilidad de que un lago que dé positivo esté contaminado con bacterias? (R: 0.30) 4.11.0.3 Ejercicio 3 Se prueba el rendimiento de dos máquinas para producir varillas de torneado de alta calidad. Estos son los resultados de las pruebas Máquina 1 Redondeado: si Redondeado: No superficie lisa: si 200 1 superficie lisa: no 4 2 Máquina 2 Redondeado: si Redondeado: No superficie lisa: si 145 4 superficie lisa: no 8 6 ¿Cuál es la probabilidad de que la barra sea redondeada? (R: 357/370) ¿Cuál es la probabilidad de que la varilla haya sido producida por la máquina 1? (R: 207/370) ¿Cuál es la probabilidad de que la varilla no sea lisa? (R: 20/370) ¿Cuál es la probabilidad de que la varilla sea lisa o redondeada o producida por la máquina 1? (R: 364/370) ¿Cuál es la probabilidad de que la varilla quede redondeada si es alisada y de la máquina 1? (R: 200/201) ¿Cuál es la probabilidad de que la varilla no esté redondeada si no está alisada y es de la máquina 2? (R: 6/14) ¿Cuál es la probabilidad de que la varilla haya salido de la máquina 1 si está alisada y redondeada? (R: 200/345) ¿Cuál es la probabilidad de que la varilla haya venido de la máquina 2 si no pasa al menos uno de los controles de calidad? (R:0.72) 4.11.0.4 Ejercicio 4 Queremos cruzar una avenida con dos semáforos. La probabilidad de encontrar el primer semáforo en rojo es 0.6. Si paramos en el primer semáforo, la probabilidad de parar en el segundo es 0.15. Mientras que la probabilidad de detenernos en el segundo si no nos detenemos en el primero es 0.25. Cuando intentamos cruzar ambos semáforos: ¿Cuál es la probabilidad de tener que detenerse en cada semáforo? (R:0.09) ¿Cuál es la probabilidad de tener que parar en al menos un semáforo? (R:0.7) ¿Cuál es la probabilidad de tener que detenerse en un solo semáforo? (R:0.61) Si paré en el segundo semáforo, ¿cuál es la probabilidad de que tuviera que parar en el primero? (R: 0.47) Si tuviera que parar en cualquier semáforo, ¿cuál es la probabilidad de que tuviera que hacerlo dos veces? (R: 0.12) ¿Parar en el primer semáforo es un evento independiente de detenerse en el segundo semáforo? (No) Ahora, deseamos cruzar una avenida con tres semáforos. La probabilidad de encontrarnos con el primer semáforo en rojo es del 0.6, y la probabilidad de encontrar el segundo semáforo en rojo depende únicamente de la probabilidad del primer semáforo. De manera similar, la probabilidad de encontrar un semáforo en rojo en el tercer semáforo depende solo de las probabilidades del segundo. Como antes, la probabilidad de detenernos en un semáforo es del 0.15 si nos detuvimos en el semáforo anterior. Si no nos detuvimos en el semáforo anterior, la probabilidad de detenernos en el siguiente semáforo es del 0.25. ¿Cuál es la probabilidad de tener que parar en cada semáforo? (R:0.013) ¿Cuál es la probabilidad de tener que parar en al menos un semáforo? (R:0.775) ¿Cuál es la probabilidad de tener que detenerse en un solo semáforo? (R:0.5425) consejos: Si la probabilidad de que un semáforo esté en rojo depende únicamente del anterior, entonces \\(P(R_3|R_2,R_1)=P(R_3|R_2,\\bar{R}_1)=P(R_3|R_2)\\) y \\(P(R_3|\\bar{R}_2,R_1)=P(R_3 |\\bar{R}_2,\\bar{R}_1)=P(R_3|\\bar{R}_2)\\) La probabilidad conjunta de encontrar tres semáforos en rojo se puede escribir como: \\(P(R_1,R_2,R_3)=P(R_3|R_2)P(R_2|R_1)P(R_1)\\) 4.11.0.5 Ejercicio 5 Una prueba de calidad en un ladrillo aleatorio se define por los eventos: Pasar la prueba de calidad: \\(E\\), no pasar la prueba de calidad: \\(\\bar{E}\\) Defectuoso: \\(D\\), no defectuoso: \\(\\bar{D}\\) Si la prueba diagnóstica tiene sensibilidad \\(P(E|\\bar{D})=0.99\\) y especificidad \\(P(\\bar{E}|D)=0.98\\), y la probabilidad de pasar la prueba es \\(P(E) =0.893\\) entonces ¿Cuál es la probabilidad de que un ladrillo elegido al azar sea defectuoso \\(P(D)\\)? (R:0.1) ¿Cuál es la probabilidad de que un ladrillo que ha pasado la prueba sea realmente defectuoso? (R:0.022) La probabilidad de que un ladrillo no sea defectuoso y que no pase la prueba (R:0.009) ¿Son \\(D\\) y \\(\\bar{E}\\) estadísticamente independientes? (No) "],["variables-aleatorias-discretas.html", "Chapter 5 Variables aleatorias discretas 5.1 Objetivo 5.2 Frecuencias relativas 5.3 Variable aleatoria 5.4 Eventos de observar una variable aleatoria 5.5 Probabilidad de variables aleatorias 5.6 Funciones de probabilidad 5.7 Funciones de probabilidad 5.8 Probabilidades y frecuencias relativas 5.9 La media o el valor esperado 5.10 Varianza 5.11 Funciones de probabilidad para funciones de \\(X\\) 5.12 Distribución de probabilidad 5.13 Función de probabilidad y distribución de probabilidad 5.14 Cuantiles 5.15 Resumen 5.16 Preguntas 5.17 Ejercicios", " Chapter 5 Variables aleatorias discretas 5.1 Objetivo En este capítulo definiremos las variables aleatorias y estudiaremos variables aleatorias discretas. Definiremos la función de masa de probabilidad y sus principales propiedades de media y varianza. Siguiendo el proceso de abstracción de las frecuencias relativas en probabilidades, también definimos la distribución de probabilidad como el caso límite de la frecuencia relativa acumulada. 5.2 Frecuencias relativas Las frecuencias relativas de los resultados de un experimento aleatorio son una medida de su propensión. Podemos usarlos como estimadores de sus probabilidades, cuando repetimos el experimento aleatorio muchas veces (\\(n \\rightarrow \\infty\\)). Definimos tendencia central (promedio), dispersión (varianza muestral) y la distribución de frecuencias de los datos (\\(F_i\\)). En términos de probabilidades, ¿cómo se definen estas cantidades? 5.3 Variable aleatoria Definimos las frecuencias relativas sobre las observaciones de los experimentos. Ahora definimos las cantidades equivalentes para las probabilidades en términos de los resultados de los experimentos. Nos ocuparemos únicamente de resultados de tipo numérico. Una variable aleatoria es un símbolo que representa un resultado numérico de un experimento aleatorio. Escribimos la variable aleatoria en mayúsculas (es decir, \\(X\\)). Definición: Una variable aleatoria es una función que asigna un número real a un evento del espacio muestral de un experimento aleatorio. Recuerda que un evento puede ser un resultado o una colección de resultados. Cuando la variable aleatoria toma un valor, indica la realización de un evento de un experimento aleatorio. Ejemplo: Si \\(X \\in \\{0,1\\}\\), entonces decimos que \\(X\\) es una variable aleatoria que puede tomar los valores \\(0\\) o \\(1\\). 5.4 Eventos de observar una variable aleatoria Hacemos la distinción entre variables en el espacio modelo con letras mayúsculas, como entidades abstractas, y la realización de un evento o resultado particular. Por ejemplo: \\(X=1\\) es el evento de observar la variable aleatoria \\(X\\) con valor \\(1\\) \\(X=2\\) es el evento de observar la variable aleatoria \\(X\\) con valor \\(2\\) … En general: \\(X=x\\) es el evento de observar la variable aleatoria \\(X\\) (\\(X\\) mayúscula) con valor \\(x\\) (\\(x\\) pequeño). 5.5 Probabilidad de variables aleatorias Nos interesa asignar probabilidades a los eventos de observar un valor particular de una variable aleatoria. Por ejemplo, para los dados escribiremos la tabla de probabilidad como \\(X\\) Probabilidad \\(1\\) \\(P(X=1)=1/6\\) \\(2\\) \\(P(X=2)=1/6\\) \\(3\\) \\(P(X=3)=1/6\\) \\(4\\) \\(P(X=4)=1/6\\) \\(5\\) \\(P(X=5)=1/6\\) \\(6\\) \\(P(X=6)=1/6\\) donde hacemos explícitos los eventos de que la variable toma un resultado dado \\(X=x\\). 5.6 Funciones de probabilidad Debido a que \\(x\\) (minúscula) es una variable numérica, las probabilidades de la variable aleatoria se pueden dibujar o escrir como una función \\[f(x)=P(X=x)=1/6\\] 5.7 Funciones de probabilidad Podemos crear cualquier tipo de función de probabilidad si satisfacemos las reglas de probabilidad de Kolmogorov: Para una variable aleatoria discreta \\(X \\in \\{x_1 , x_2 , .. , x_M\\}\\), una función de masa de probabilidad que se usa para calcular probabilidades \\(f(x_i)=P(X=x_i)\\) siempre es positiva \\(f(x_i)\\geq 0\\) y su suma sobre todos los valores de la variable es \\(1\\): \\(\\sum_{i=1}^M f(x_i)=1\\) Donde \\(M\\) es el número de resultados posibles. Ten en cuenta que la definición de \\(X\\) y su función de masa de probabilidad es general sin referencia a ningún experimento. Las funciones viven en el espacio modelo (abstracto). Aquí tenemos un ejemplo \\(X\\) y \\(f(x)\\) son objetos abstractos que pueden corresponder o no a un experimento. Tenemos la libertad de construirlos como queramos siempre que respetemos su definición. Las funciones de masa de probabilidad tienen algunas propiedades que se derivan exclusivamente de su definición. 5.8 Probabilidades y frecuencias relativas Considera el ejemplo Haz el siguiente experimento: En una urna pon \\(8\\) bolas y: marca \\(1\\) bola con el número \\(-2\\) marca \\(2\\) bolas con el número \\(-1\\) marca \\(2\\) bolas con el número \\(0\\) marca \\(2\\) bolas con el número \\(1\\) marca \\(1\\) bolas con el número \\(2\\) Y considere realizar el siguiente experimento aleatorio: Tome una bola y lea el número. A partir de la probabilidad clásica, podemos escribir la tabla de probabilidades, para lo cual no necesitamos realizar ningún experimento \\(X\\) \\(P(X=x)\\) \\(-2\\) \\(1/8=0.125\\) \\(-1\\) \\(2/8=0.25\\) \\(0\\) \\(2/8=0.25\\) \\(1\\) \\(2/8=0.25\\) \\(2\\) \\(1/8=0.125\\) Ahora, realicemos el experimento \\(30\\) veces y escribamos la tabla de frecuencia \\(X\\) \\(f_i\\) \\(-2\\) \\(0.132\\) \\(-1\\) \\(0.262\\) \\(0\\) \\(0.240\\) \\(1\\) \\(0.248\\) \\(2\\) \\(0.118\\) La probabilidad frecuentista nos dice \\[lim_{N \\rightarrow \\infty} f_i = f(x_i)=P(X=x_i)\\] Entonces, si no conocíamos el montaje del experimento (caja negra), lo mejor que podemos hacer es estimar las probabilidades con las frecuencias, obtenidas de \\(N\\) repeticiones del experimento aleatorio: \\[f_i = \\hat{P}_i\\] Cada vez que estimamos las probabilidades, nuestras estimaciones \\(\\hat{P}_i=f_i\\) cambian. Pero \\(P_i\\) es una cantidad abstracta que nunca cambia. A medida que aumenta \\(N\\), nos acercamos más a ella. 5.9 La media o el valor esperado Cuando discutimos las estadísticas de resumen de los datos, definimos el centro de las observaciones como un valor alrededor del cual se concentran las frecuencias de los resultados. Usamos el promedio para medir el centro de gravedad de los datos. En términos de las frecuencias relativas de los valores de los resultados discretos, escribimos el promedio como \\(\\bar{x}= \\sum_{i=1}^M x_i \\frac{n_i}{N}=\\) \\[\\sum_{i=1}^M x_i f_i\\] Definición La media (\\(\\mu\\)) o valor esperado de una variable aleatoria discreta \\(X\\), \\(E(X)\\), con función de masa \\(f(x)\\) está dada por \\[ \\mu = E(X)= \\sum_{i=1}^M x_i f(x_i) \\] Es el centro de gravedad de las probabilidades: El punto donde se equilibran las cargas de probabilidad. De la definición tenemos \\[\\bar{x} \\rightarrow \\mu\\] en el límite cuando \\(N \\rightarrow \\infty\\) como la frecuencia tiende a la función de masa de probabilidad \\(f_i \\rightarrow f(x_i)\\). Ejemplo ¿Cuál es la media de \\(X\\) si su función de masa de probabilidad \\(f(x)\\) está dada por \\(X\\) \\(f(x)=P(X=x)\\) \\(0\\) \\(1/16\\) \\(1\\) \\(4/16\\) \\(2\\) \\(6/16\\) \\(3\\) \\(4/16\\) \\(4\\) \\(1/16\\) \\[ \\mu =E(X)=\\sum_{i=1}^m x_i f(x_i) \\] \\(E(X)=\\)0 * 1/16 + 1 * 4/16 + 2 * 6/16 + 3 * 4/16 + 4 * 1/16 =2 La media \\(\\mu\\) es el centro de gravedad de la función de masa de probabilidad y no cambia. Sin embargo, el promedio \\(\\bar{x}\\) es el centro de gravedad de las observaciones (frecuencias relativas) cambia con diferentes datos. 5.10 Varianza Cuando discutimos los estadísticos de resumen, también definimos la dispersión de las observaciones como una distancia promedio de los datos al promedio. Definición La varianza, escrita como \\(\\sigma^2\\) o \\(V(X)\\), de una variable aleatoria discreta \\(X\\) con función de masa \\(f(x)\\) viene dada por \\[\\sigma^2 = V(X)= \\sum_{i=1}^M (x_i-\\mu)^2 f(x_i)\\] \\(\\sigma=\\sqrt{V(X)}\\) se llama la desviación estándar de la variable aleatoria. La varianza es la dispersión de las probabilidades con respecto a la media: El momento de inercia de las probabilidades sobre la media. Ejemplo ¿Cuál es la varianza de \\(X\\) si su función de masa de probabilidad \\(f(x)\\) está dada por \\(X\\) \\(f(x)=P(X=x)\\) \\(0\\) \\(1/16\\) \\(1\\) \\(4/16\\) \\(2\\) \\(6/16\\) \\(3\\) \\(4/16\\) \\(4\\) \\(1/16\\) \\[\\sigma^2 =V(X)=\\sum_{i=1}^m (x_i-\\mu)^2 f(x_i)\\] \\(V(X)=\\)(0-2)\\(^2\\)* 1/16 + (1-2)\\(^2\\)* 4/16 + (2- 2)\\(^2\\)* 6/16 + (3-2)\\(^2\\)* 4/16 + (4-2)\\(^2\\)* 1/ 16 = 1 \\[V(X)=\\sigma^2=1\\] \\[\\sigma=1\\] 5.11 Funciones de probabilidad para funciones de \\(X\\) En muchas ocasiones, estaremos interesados en resultados que sean función de las variables aleatorias. Quizás nos interese el cuadrado del número de contagios de gripe, o la raíz cuadrada del número de correos electrónicos en una hora. Definición Para cualquier función \\(h\\) de una variable aleatoria \\(X\\), con función de masa \\(f(x)\\), su valor esperado viene dado por \\[ E[h(X)]= \\sum_{i=1}^M h(x_i) f(x_i) \\] Esta es una definición importante que nos permite probar tres propiedades de la media y la varianza que se usan con frecuencia: La media de una función lineal es la función lineal de la media: \\[E(a\\times X +b)= a\\times E(X) +b\\] para \\(a\\) y \\(b\\) escalares (números ). La varianza de una función lineal de \\(X\\) es:\\[V(a\\times X +b)= a^2\\times V(X)\\] La varianza con respecto al origen es la varianza con respecto a la media más la media al cuadrado: \\[E(X^2)=V(X)+E(X)^2\\] Ejemplo ¿Cuál es la varianza \\(X\\) con respecto al origen, \\(E(X^2)\\), si su función de masa de probabilidad \\(f(x)\\) está dada por \\(X\\) \\(f(x)=P(X=x)\\) \\(0\\) \\(1/16\\) \\(1\\) \\(4/16\\) \\(2\\) \\(6/16\\) \\(3\\) \\(4/16\\) \\(4\\) \\(1/16\\) \\[E(X^2) =\\sum_{i=1}^m x_i^2 f(x_i)\\] \\(E(X^2)=\\)(0)\\(^2\\)* 1/16 + (1)\\(^2\\)* 4/16 + (2) \\(^2\\)* 6/16 + (3)\\(^2\\)* 4/16 + (4)\\(^2\\)* 1/16 =5 También podemos verificar: \\[E(X^2)=V(X)+E(X)^2\\] \\(5=1+2^2\\) 5.12 Distribución de probabilidad Cuando discutimos las estadísticas de resumen, también definimos la distribución de frecuencias (o la frecuencia acumulada relativa) \\(F_i\\). \\(F_i\\) es una cantidad importante porque es una función continua \\(F_x\\) es por lo tanto una función de rango continuo, incluso si los resultados son discretos. Definición: La función de distribución de probabilidad se define como \\[F(x)=P(X\\leq x)=\\sum_{x_i\\leq x} f(x_i) \\] Esa es la probabilidad acumulada hasta un valor dado \\(x\\) \\(F(x)\\) satisface por lo tanto satisface: \\(0\\leq F(x) \\leq 1\\) Si \\(x \\leq y\\), entonces \\(F(x) \\leq F(y)\\) Para la función de masa de probabilidad: \\(X\\) \\(f(x)=P(X=x)\\) \\(0\\) \\(1/16\\) \\(1\\) \\(4/16\\) \\(2\\) \\(6/16\\) \\(3\\) \\(4/16\\) \\(4\\) \\(1/16\\) La distribución de probabilidad es: \\[ F(x)= \\begin{cases} 1/16,&amp; \\text{if } 0 \\leq x &lt; 1\\\\ 5/16,&amp; 1\\leq x &lt; 2\\\\ 11/16,&amp; 2\\leq x &lt; 3\\\\ 15/16,&amp; 4\\leq x &lt; 5\\\\ 16/16,&amp; x \\leq 5\\\\ \\end{cases} \\] Para \\(X \\in \\mathbb{Z}\\) 5.13 Función de probabilidad y distribución de probabilidad La función de probabilidad y la distribución son equivalentes. Podemos obtener uno del otro y viceversa. \\[f(x_i)=F(x_i)-F(x_{i-1})\\] con \\[f(x_1)=F(x_1)\\] para \\(X\\) tomando valores en \\(x_1 \\leq x_2 \\leq ... \\leq x_n\\) Ejemplo De la distribución de probabilidad: \\[ F(x)= \\begin{cases} 1/16,&amp; \\text{if } 0 \\leq x &lt; 1\\\\ 5/16,&amp; 1\\leq x &lt; 2\\\\ 11/16,&amp; 2\\leq x &lt; 3\\\\ 15/16,&amp; 4\\leq x &lt; 5\\\\ 16/16,&amp; x \\leq 5\\\\ \\end{cases} \\] Podemos obtener la función masa de probabilidad. \\(f(0)=F(0)=1/16\\) \\(f(1)=F(1)-f(0)=5/32-1/32=4/16\\) \\(f(2)=F(2)-f(1)-f(0)=F(2)-F(1)=6/16\\) \\(f(3)=F(3)-f(2)-f(1)-f(0)=F(3)-F(2)=4/16\\) \\(f(4)=F(4)-F(3)=1/16\\) 5.14 Cuantiles Finalmente, podemos usar la distribución de probabilidad \\(F(x)\\) para definir la mediana y los cuartiles de la variable aleatoria \\(X\\). En general, definimos el q-cuantil como el valor \\(x_{p}\\) bajo el cual hemos acumulado q*100% de la probabilidad \\[q=\\sum_{i=1}^pf(x_i) = F (x_p)\\] La mediana es valor \\(x_m\\) tal que \\(q=0.5\\) \\[F(x_{m})=0.5\\] El cuantil \\(0.05\\) es el valor \\(x_{r}\\) tal que \\(q=0.05\\) \\[F(x_{r})=0.05\\] El cuantil de \\(0,25\\) es el primer cuartil el valor \\(x_{s}\\) tal que \\(q=0.25\\) \\[F(x_{s})=0.25\\] 5.15 Resumen nombres de cantidades modelo (no observado) datos (observados) función de masa de probabilidad // frecuencia relativa \\(f(x_i)=P(X=x_i)\\) \\(f_i=\\frac{n_i}{N}\\) distribución de probabilidad // frecuencia relativa acumulada \\(F(x_i)=P(X \\leq x_i)\\) \\(F_i=\\sum_{k\\leq i} f_k\\) media // promedio \\(\\mu=E(X)=\\sum_{i=1}^M x_i f(x_i)\\) \\(\\bar{x}=\\sum_{j=1}^N x_j/N\\) varianza // varianza de la muestra \\(\\sigma^2=V(X)=\\sum_{i=1}^M (x_i-\\mu)^2 f(x_i)\\) \\(s^2=\\sum_{j=1}^N (x_j-\\bar{x})^2/(N-1)\\) desviación estándar // muestra sd \\(\\sigma=\\sqrt{V(X)}\\) \\(s\\) varianza con respecto al origen // 2º momento muestral \\(E(X^2)=\\sum_{i=1}^M x_i^2 f(x_i)\\) \\(m_2= \\sum_{j=1}^N x_j^2/n\\) Ten en cuenta: \\(i=1...M\\) es un resultado de la variable aleatoria \\(X\\). \\(j=1...N\\) es una observación de la variable aleatoria \\(X\\). Propiedades: \\(\\sum_{i=1...N} f(x_i)=1\\) \\(f(x_i)=F(x_i)-F(x_{i-1})\\) \\(E(a\\times X +b)= a\\times E(X) +b\\); for \\(a\\) and \\(b\\) scalars. \\(V(a\\times X +b)= a^2\\times V(X)\\) \\(E(X^2)=V(X)+E(X)^2\\) 5.16 Preguntas 1) Para una función de masa de probabilidad no es cierto que \\(\\qquad\\)a: la suma de los valores de su imagen es 1; \\(\\qquad\\)b: sus valores pueden interpretarse como probabilidades de eventos; \\(\\qquad\\)c: siempre es positiva; \\(\\qquad\\)d: no puede tomar el valor 1; 2) El valor de una variable aleatoria representa \\(\\qquad\\)a: una observación de un experimento aleatorio; \\(\\qquad\\)b: la frecuencia de un resultado de un experimento aleatorio; \\(\\qquad\\)c: un resultado de un experimento aleatorio; \\(\\qquad\\)d: una probabilidad de un resultado; 3) El valor estimado de una probabilidad \\(\\hat{P_i}\\) es igual a la probabilidad \\(P_i\\) cuando el número de repeticiones del experimento aleatorio es \\(\\qquad\\)a: grande; \\(\\qquad\\)b: infinito; \\(\\qquad\\)c: pequeño \\(\\qquad\\)d: cero; 4) Si una función de masa de probabilidad es simétrica alrededor de \\(x=0\\) \\(\\qquad\\)a: La media es menor que la mediana; \\(\\qquad\\)b: La media es mayor que la mediana; \\(\\qquad\\)c: La media y la mediana son iguales; \\(\\qquad\\)d: La media y la mediana son diferentes de 0; 5) La media y la varianza \\(\\qquad\\)a: son inversamente proporcionales; \\(\\qquad\\)b: son valores esperados de funciones de \\(X\\); \\(\\qquad\\)c: de una función lineal son la función lineal de la media y la función lineal de la varianza; \\(\\qquad\\)d: cambia cuando repetimos el experimento aleatorio; 5.17 Ejercicios 5.17.0.1 Ejercicio 1 Considera la siguiente variable aleatoria \\(X\\) sobre los resultados resultado \\(X\\) \\(a\\) 0 \\(b\\) 0 \\(c\\) 1.5 \\(d\\) 1.5 \\(e\\) 2 \\(f\\) 3 Si cada resultado es igualmente probable, ¿cuál es la función de masa de probabilidad de \\(x\\)? Encuentra: \\(P(X&gt;3)\\) \\(P(X=0\\, \\cup \\, X=2 )\\) \\(P(X \\leq 2)\\) 5.17.0.2 Ejercicio 2 Dada la función de masa de probabilidad \\(x\\) \\(f(x)=P(X=x)\\) 10 0.1 12 0.3 14 0.25 15 0.15 17 ? 20 0.15 ¿Cuál es su valor esperado y su desviación estándar? (R: 14,2; 2,95) 5.17.0.3 Ejercicio 3 Dada la distribución de probabilidad para una variable discreta \\(X\\) \\[ F(x)= \\begin{cases} 0, &amp; x &lt; -1 \\\\ 0.2,&amp; x \\in [-1,0)\\\\ 0.35,&amp; x \\in [0,1)\\\\ 0.45,&amp; x \\in [1,2)\\\\ 1,&amp; x \\geq 2\\\\ \\end{cases} \\] encuentra \\(f(x)\\) encuentra \\(E(X)\\) y \\(V(X)\\) (R:1; 1.5) cuál es el valor esperado y la varianza de \\(Y=2X+3\\) (R:5, 6) ¿Cuál es la mediana y el primer y tercer cuartil de \\(X\\)? (R:2,0,2) 5.17.0.4 Ejercicio 4 Estamos probando un sistema para transmitir imágenes digitales. Primero consideramos el experimento de enviar \\(3\\) píxeles y tener como resultados posibles eventos como \\((0,1,1)\\). Este es el evento de recibir el primer píxel sin error, el segundo con error y el tercero con error. Enumera en una columna el espacio muestral del experimento aleatorio. En la segunda columna asigna la variable aleatoria que cuenta el número de errores transmitidos para cada resultado Considera que tenemos un canal totalmente ruidoso, es decir, cualquier resultado de tres píxeles es igualmente probable. ¿Cuál es la probabilidad de recibir errores de \\(0\\), \\(1\\), \\(2\\) o \\(3\\) en la transmisión de \\(3\\) píxeles? (R: 1/8; 3/8; 3/8; 1/8) Dibuja la función de masa de probabilidad para el número de errores ¿Cuál es el valor esperado para el número de errores? (R:1.5) ¿Cuál es su varianza? (R: 0,75) Dibuja la distribución de probabilidad ¿Cuál es la probabilidad de transmitir al menos 1 error? (R:7/8) "]]
