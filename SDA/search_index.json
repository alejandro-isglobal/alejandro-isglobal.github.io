[["index.html", "Chapter 1 Introduction 1.1 How to read the book 1.2 Acknowledgements", " Statistical Data Analysis for Experimental Sciences Alejandro Cáceres Barcelona, 2025-08-25 Chapter 1 Introduction This book originated from course materials (2019/2024) for graduate students, enrolled in the Master of Multidisciplinary Research in Experimental Sciences program, at the Barcelona Institute of Science and Technology (BIST), and undergraduate engineering students at the Universitat Politècnica de Catalunya (UPC). The primary objective of this book is to provide readers from experimental sciences with a comprehensive and unified understanding of statistical thinking for scientific inquiry. Emphasis is placed on grasping both the concepts and practical know-how for asking meaningful questions with relevant data. Experimental scientists face several obstacles when trying to understand statistics, which must be addressed early to set their learning process on the right track. One often-overlooked challenge is the need to clearly distinguish between observation and abstraction—for example, between data and outcome, or between the average and the mean. A useful simile is to consider observation as the territory to be explored and abstraction as the map representing that territory. Mistaking the map for the territory not only confuses students but also limits the early understanding of scientific practice, as it is easily forgotten that abstractions ultimately rest on refined human experience (Frank, Gleiser, and Thompson 2024). In addition, concepts as deduction and inference often remain at an intuitive level rather than being made explicit. For example, why do I know that the probability of rolling a 1 on a die is exactly 1/6, but the probability of experiencing a hurricane in the Gulf of Mexico in a given day can only be approximately 0.05?. One of the greatest challenges teaching statistics arises when framing it as a method for understanding a population. Experimental scientists are primarily focused on gaining knowledge through measurements in experiments that, when repeated, yield varying results. The concept of a ‘population of experiments’ may seem unnecessary or even awkward, as this population often must be considered infinite in order to account for the likelihood of future outcomes from unperformed experiments, uncreated specimens, or yet-to-be-born individuals. A key insight is that the probabilities used in statistics are statements about future experiments, informed by past experiences. In this sense, statistics serves as a method to systematize our past experiences, allowing us to infer underlying properties of experiments and make informed predictions about future outcomes. By centering statistics as a method to analyze the outcomes of random experiments—characterized by knowable parameters and modifiable conditions—a unified perspective emerges, one that is more easily relatable to the empiricist. The parameters, here, appear as abstract properties of the random experiment in question that can be either known by design or inferred by the actual realization of the experiment a number of times. Think of the elasticity constant of a spring or the reproduction rate of a bacteria. When the experimenter willingly changes a condition of the experiment (e.g. temperature), she is expecting to influence the property described by the parameter and, therefore, the value of its inference. Since the outcomes of the experiment vary due to error, she needs arguments to demonstrate that increasing the temperature produces oscillation amplitudes, or higher bacterial yield, that cannot be achieved by the natural variation of the experiment but rather to her intervention. This is the explanatory approach that it is consistently taken throughout the book. Emphasis is placed on the analysis of real data. Concepts are developed from the need to explore and understand insightful experiments. Sometimes a formal definition precedes the example; other times, the example illustrates the need to develop a concept. Plots and formulas are used. This is statistical thinking as it is applied in experimental science research. From this perspective, the book focuses on the use of analytic tools to develop a rigorous thinking about the experiment, rather than the rigorous development of the tools. Great mathematical statistics books cover that field (Davison 2003; Miller, Miller, and Freund 2014; Casella and Berger 2002). Historical data and classical experiments are widely used to explain statistical concepts and to highlight their critical role in shaping scientific conclusions. Analyzing data from iconic experiments not only demonstrates the central importance of statistics in scientific discovery but also allows students to learn through meaningful, relevant examples from the greatest scientists. Statistics textbooks are often filled with unmemorable or artificial cases, whereas Mendel’s genetic data vividly illustrate the necessity of probability laws; Rutherford and Geiger elegantly show that alpha particles must have independent sources of radioactive emission (atoms) using a Poisson distribution; and the debate over the extinction of the dinosaurs by a meteorite can be explored through a simple boxplot of the ages of meteorite debris. Paradigm-shifting examples are discussed throughout the book with the original published data. We discuss scientific discoveries from diverse areas such as physics (e.g., the electron e/m ratio, Maxwell’s gas viscosity, Hubble’s law), genetics (Mendel’s laws, lactase gene selection, knockout experiments), epidemiology (John Snow’s cholera study, the Framingham cohort), and ecology and earth sciences (dinosaur extinction, Fisher’s diversity index, Keeling’s CO2 emissions). Applying modern analytical methods to hystorical datasets serves to demonstrate what would now be considered preliminary analysis. Our goal is not to reinterpret the original experiments—their conclusions stand as part of the historical record—but rather to use them as instructive tools. Any scientific inaccuracies should be understood in the context of pedagogical intent. Publicly accessible data are also downloaded to illustrate various hypothesis tests within a research inquiry, such as data on lithium batteries, electrocardiograms, and earthquake frequency. Data from a misophonia study, kindly lent by Dr. Antonia Ferrer, are used throughout the book for hands on exercises of basic concepts to encourage the reader to learn how to gain scientific insight form a real empirical study. When data for some examples are created, emphasis is placed on the plausibility of similar experiments with real-world parameters, such as in the designs of a conductivity resistor, a copper wire of high purity and a pacemaker. Readers are expected to have prior exposure to basic mathematical concepts at the undergraduate level. Statistics is expressed in mathematical language and implemented using programming languages. Put simply, we need a map and a compass. While both will be utilized, this is not a book about mathematics or computer programming. The primary goal is to understand the application of statistical thinking and its solutions in scientific inquiry. With that clarified, the book aims to help readers develop a proficient use of both mathematical concepts and programming functions, without focusing on theorem proofs or code compilation. We will primarily use the R and Python programming languages. Although prior knowledge of R and Python is not a prerequisite, readers are encouraged to consult an introductory reference (Matloff 2011; McKinney 2022). Our focus lies in using statistics to guide the exploration of natural phenomena through experimental data. As an analytical tool, statistics not only sharpens scientific understanding but also strengthens the integrity of our conclusions by helping us ask the right questions, strive for precise thinking, and use accurate language. In an age of data-driven science, statistical literacy is a cornerstone of credible and collaborative discovery. 1.1 How to read the book The book is meant to be read linearly, one chapter after another, as concepts build upon each other. Nonetheless, each chapter tackles a particular subject and is intended to be as self-contained as possible for future reference or study. The first half of the book (up to Chapter 14 on hypothesis testing) serves as one semester of course material for second-year engineering students. The course material for master’s students comprises all the chapters, with more emphasis on the second half. In this course, more technical chapters, such as those on discrete and continuous random variables (Chapters 4 and 5) and maximum likelihood (Chapter 12), can be quickly revised. 1.2 Acknowledgements This book is the result of years of work in many different scientific research fields (physics, neuroimaging, genomics and epidemiology). My journey has taken me from abstract experiments (electrons orbiting black holes) to concrete ones (development of health biomarkers). Statistics and probability underpin them all. Along the way, I have had the privilege of meeting and discussing ideas with many great scientists—thank you all. I have also had the pleasure of learning from many great teachers from diverse disciplines. To my wife and sons, parents and sisters, I owe you all. Special thanks to my colleagues at ISGlobal, UPC and BIST. References "],["data-description.html", "Chapter 2 Data description 2.1 Scientific method 2.2 Data 2.3 Types of outcomes 2.4 Random experiments 2.5 Absolute frequencies 2.6 Relative frequencies 2.7 Bar chart 2.8 Pie chart 2.9 Ordinal categorical outcomes 2.10 Absolute and relative cumulative frequencies 2.11 Cumulative frequency graph 2.12 Numerical outcomes 2.13 Transforming continuous data 2.14 Frequency table for a continuous variable 2.15 Histogram 2.16 Cumulative frequency graph 2.17 Summary Statistics 2.18 Average (sample mean) 2.19 Median 2.20 Dispersion 2.21 Sample variance 2.22 Interquartile range (IQR) 2.23 Boxplot 2.24 Questions 2.25 Exercises 2.26 Practice", " Chapter 2 Data description Data refers to the yield of observation through measurement. The experimental scientist’s interaction with nature, achieved through the meticulous collection of data, forms the foundation of scientific knowledge. However, data is often collected with a specific purpose in mind. We aim to understand the natural processes that generate the data and the extent to which these processes depend on the particular experimental setup. Observation and abstraction are thus intertwined in the scientific method, and statistics provides a systematic approach to addressing both. To explain statistical methods, we adopt the practical perspective that data are objects of observation, while models are objects of abstraction. In this chapter, we will introduce tools for describing data. These tools include tables, figures, and descriptive statistics that summarize central tendency and dispersion. Before any explanation can be drawn from data, the data itself must first be observed. The goal is to develop an initial intuition by displaying the data in various forms, which will help us determine whether the questions we intend to ask are appropriate or whether we should explore new ones. By exploring meaningful real data on Mendelian inheritance, misophonia and dinosaur extinction, we will also introduce key statistical concepts such as random experiments, observations, outcomes, and absolute and relative frequencies. 2.1 Scientific method One of the goals of the scientific method is to provide a framework for solving problems that arise in the study of natural phenomena or in the design of new technologies. Natural philosophers, and latter scientist, have developed a method over thousands of years to study natural phenomena. The method, which is continuously under development, can be associated with three different human activities: Observation characterized by the acquisition of data Reasoning characterized by the development of mathematical models Action characterized by design of new experiments or technologies Their complex interaction and results are the basis of the scientific activity. To our understanding, statistics deals with the interaction between models and data (the bottom part of the figure). Typical statistical questions are: What is the best model for my data (inference)? What are the data that a certain model would produce (prediction)? The substantive knowledge of scientific research—whether physical, chemical, biological, or otherwise—is derived from specific experiments conducted on phenomena of interest, most likely under a controlled lab environment. This represents the upper part of the triangle, where the research methods and technologies of various scientific disciplines can be situated. 2.2 Data The data is presented in the form of observations. An observation or realization is the acquisition of a number or characteristic from an experimental run. We may also think of the result of a measuring process. For example, let us take the series of numbers produced by repeating an experiment, such as trying to start a newly assembled engine (1: success of starting an engine, 0: failure of starting it). \\[1, 0, 0, 1, 0, \\mathbf{1}, 0, 1, 1, ...\\] The number one in bold is an observation from the sixth repeat of the experiment. That is, in that particular trial, we were able to start the engine.Remember that the observation is concrete, it is the number you get one day in the laboratory. An observation is the recording of a particular and concrete event, located in space and time. Observations can only be obtained by experiments. By contrast, an outcome is a possible observation that could result from the experiment. No need to run the experiment to know the possibilities. Before we even run the first trial, we know that either the engine starts or not. In the example, the number 1 marks one of the possible observations (success), 0 is the other one (failure). The outcome of an experiment is abstract. We can obtain them by reasoning, no need to go to the lab. As such, the outcomes are general characteristics of the experiments we run. An outcome is a universal abstract entity, with no location in space and time. 2.3 Types of outcomes In statistics we are mainly interested in two types of outcomes. Categorical: If the outcome of an experiment is a quality. They can be nominal (binary: yes, no; multiple: colors) or ordinal, when the qualities can be ranked (severity of a disease, emergency grades). Numeric: If the outcome of an experiment is a number. The number can be discrete (number of messages received in an hour, number of leukocytes in the blood) or continuous (battery charge status, engine temperature). 2.4 Random experiments We may boldly say that the subject matter of statistics is the gaining of knowledge from random experiments, the means by which we produce data. Definition: A random experiment is an experiment that gives a different result or outcome when repeated in the same way. Random experiments are of different types, depending on how they are conducted: on the same object (person): temperature, sugar levels. on different objects (animals): the weight of an animal. on events (climate phenomena): the number of hurricanes per year. 2.5 Absolute frequencies When we repeat a random experiment with categorical outcomes, we make a list of the results. We summarize the observations by counting how many times we saw a particular outcome. For instance we compute the absolute frequency: \\[n_i\\] which is simply the number of times we observe the result \\(i\\). Example (mouse coating) Consider one version of the timeless random experiment of obtaining varying offspring from the same reproducing couple. Let us breed one male and one female mouse, both with grey coats, and observe the resulting coat color of their offspring. A fine controlled experiment of a suitable couple was run by Crampe, which was latter reported by Allen in 1904 (Allen 1904). Crampe repeated the experiment \\(156\\) times, that is, he produced \\(156\\) mouse off-spring from the same pair and recorded their color. While the experiment appeared to follow the Mendel’s rules of recessive inheritance for albinism, the detailed explanation for the full series was never done, as it had to wait for Bateson’s definition of epistasis in 1909, and its application to mice colors by Little’s in 1914 (Little 1914). When we look at Crampe’s mice, while most of the offspring have grey coats, to our surprise, several siblings are also black and albino. These are the results of the repetition of the random experiment black, grey, black, albino, grey, grey, grey, grey, grey, black, grey, grey, grey, grey, grey, albino, grey, grey, grey, grey, grey, grey, black, grey, albino, grey, grey, albino, grey, black, black, grey, albino, albino, grey, albino, black, grey, grey, albino, grey, black, black, albino, albino, grey, albino, black, grey, black, black, black, grey, grey, grey, grey, black, black, albino, black, albino, black, black, albino, grey, grey, grey, grey, grey, albino, black, albino, grey, grey, grey, grey, grey, albino, albino, grey, grey, albino, albino, grey, grey, albino, black, black, grey, grey, black, albino, grey, albino, black, grey, grey, grey, grey, black, albino, grey, grey, grey, grey, black, grey, grey, grey, albino, black, albino, black, albino, grey, grey, grey, grey, grey, grey, grey, grey, albino, albino, grey, grey, grey, grey, grey, grey, black, grey, grey, grey, grey, albino, albino, black, albino, grey, albino, grey, albino, black, albino, grey, grey, grey, grey, albino, grey, grey, grey, grey, albino, black Each experiment was the production of a specimen from the same grey couple and its color being recorded. The first black in the list is the observation of the first run of the random experiment (the first offspring of the couple). The last black is the observation number \\(156\\). We can list the observations using a frequency table of the outcomes (categories), observing high variability in the color of the offspring \\[ \\begin{array}{cc} \\mathbf{outcome} &amp; \\mathbf{n_i} \\\\ \\text{grey} &amp; 88 \\\\ \\text{black} &amp; 31 \\\\ \\text{albino} &amp; 37 \\\\ \\end{array} \\] The table summarizes the outcomes (first column) and the corresponding observations (second column). From the table, we can see that, for example, \\(n_1=88\\) represents the total number of grey mice observed. Additionally, we note that the total number of offspring is given by \\[\\sum_{i=1}^m n_i= n = 156,\\] where \\(m=3\\) is the number of outcomes (the number of rows in the table), and \\(n=156\\) is the total number of observations (repetitions of the random experiment). The frequency table in the example illustrates the well-known inheritance pattern of hybrid grey parents, which carry two versions of the C gene—grey (dominant) and black (recessive)—and two versions of another gene, A, associated with normal pigmentation (dominant) and albinism (recessive). This phenomenon, known as recessive epistasis, occurs when the offspring inherits the albinism mutation in gene A from both parents, thereby interfering with—or masking—the expression of grey or black coloration determined by gene C (Little 1914). Mechanistically, the protein encoded by the albino gene inhibits pigmentation driven by the color gene. 2.6 Relative frequencies We can also summarize the observations by calculating the proportion of how many times we saw a particular result. \\[f_i = \\frac{n_i}{n}\\] In our example, \\(n_1=88\\) grey mice were recorded, so we can ask about the proportion of grey from the total number of offspring observed: \\(156\\). We can copy these proportions \\(f_i\\) in the frequency table. \\[ \\begin{array}{ccc} \\mathbf{outcome} &amp; \\mathbf{n_i} &amp; \\mathbf{f_i} \\\\ \\text{grey} &amp; 88 &amp; 0.56 \\\\ \\text{black} &amp; 31 &amp; 0.20 \\\\ \\text{albino} &amp; 37 &amp; 0.24 \\\\ \\hline \\mathbf{sum} &amp; 156 &amp; 1 \\\\ \\end{array} \\] \\(56\\%\\) of mice were grey. Relative frequencies are fundamental in statistics. They represent the proportion of one outcome relative to the other outcomes. Later, we will understand them as observations of quantities that provide empirical information about probabilities, which are abstract, unobserved but knowable properties of the random experiment. In the mouse coat color example, the probabilities are derived from an inheritance model of recessive epistasis for albinos that states that the ratio of color outcomes is 9:3:4 for grey, black, and albino, respectively. This ratio have ideal frequencies \\((0.5625, 0.1875, 0.25)\\) for each outcome. No need to go into further genetics theory. However, while we will formally define probabilities in the following chapter, the point here is to note that probabilities are expected unobserved frequencies derived through reasoning, whereas relative frequencies are their observed empirical counterparts, obtained only through experimentation. The numerical values of probabilities and relative frequencies often differ; expectation is not usually matched by observation. For instance, consider the relative frequencies obtained by Crampe: \\((0.56, 0.20, 0.24)\\). These are close to, but not identical with, those predicted by the model of recessive epistasis, where albinism and color are determined by different genes. However, an alternative interpretation is Mendelian recessive inheritance, in which two variants of a single gene lead to either albinism or black coloration, and grey color occurs when the two variants are inherited from one parent each. The expected Mendelian frequencies in this model are: \\((0.5, 0.25, 0.25)\\), also close to Crampe’s data, though slightly less so than the epistatic model proposed by Little. For absolute and relative frequencies, we have the important properties \\[\\sum_{i=1}^m n_i = n\\] and \\[\\sum_{i=1}^m f_i = 1\\] where \\(m\\) is the number of outcomes. 2.7 Bar chart When we have a lot of outcomes and want to see which ones are most likely, we can use a bar chart. This is a plot of the frequencies \\(n_i\\) Vs the outcomes \\(i\\). 2.8 Pie chart We can also visualize the relative frequencies \\(f_i\\) using a pie chart. In the pie chart, the area of the circle represents the 100% of the observations (proportion = 1) and the sections represent the relative frequencies of each outcome. 2.9 Ordinal categorical outcomes The color of a mouse in the example above is a categorical nominal variable. Each observation belongs to a category (quality). The categories do not always have a certain order. Sometimes categorical outcomes can be sorted when they have a natural ranking. This allows you to compute cumulative frequencies. Example (misophonia) A typical clinical study involves standardized measurements across multiple subjects or patients, often under controlled conditions. While individuals are clearly distinct, their observations are treated as statistically equivalent, meaning they are representative of the same random experiment that could be repeated for the same class of individuals. This concept can be generalized to other scientific fields where experiments are conducted on specimens representing their respective classes. For example, consider measuring the hardness of an alloy. If the underlying experiment differs—either because the specimens belong to different classes (e.g., different alloys) or the measurement conditions (e.g., temperature) have changed—data interpretation and description must be adjusted accordingly. In a clinical study run in Barcelona in \\(2019\\), \\(123\\) patients referred to a clinical psychologist for anxiety disorders were examined for their degree of misophonia. Misophonia is an involuntary experience of anxiety or anger triggered by specific sounds, and it may affect up to \\(5\\%\\) of the population, often without their awareness. The study aimed to evaluate the risk factors and onset mechanisms associated with misophonia, a poorly recognized disorder at the time. Each patient included in the study, was evaluated with the misophomia questionnaire AMISO, that scores patients into \\(4\\) different groups according to severity. The results of the AMISO tests are 4, 2, 0, 3, 0, 0, 2, 3, 0, 3, 0, 2, 2, 0, 2, 0, 0, 3, 3, 0, 3, 3, 2, 0, 0, 0, 4, 2, 2, 0, 2, 0, 0, 0, 3, 0, 2, 3, 2, 2, 0, 2, 3, 0, 0, 2, 2, 3, 3, 0, 0, 4, 3, 3, 2, 0, 2, 0, 0, 0, 2, 2, 0, 0, 2, 3, 0, 1, 3, 2, 4, 3, 2, 3, 0, 2, 3, 2, 4, 1, 2, 0, 2, 0, 2, 0, 2, 2, 4, 3, 0, 3, 0, 0, 0, 2, 2, 1, 3, 0, 0, 3, 2, 1, 3, 0, 4, 4, 2, 3, 3, 3, 0, 3, 2, 1, 2, 3, 3, 4, 2, 3, 2 Each observation is, therefore, the result of the run of one particular random experiment: the measurement of the level of misophonia in a patient. This data series can be summarized using the frequency table \\[ \\begin{array}{ccc} \\mathbf{outcome} &amp; \\mathbf{n_i} &amp; \\mathbf{f_i} \\\\ 0 &amp; 41 &amp; 0.33333333 \\\\ 1 &amp; 5 &amp; 0.04065041 \\\\ 2 &amp; 37 &amp; 0.30081301 \\\\ 3 &amp; 31 &amp; 0.25203252 \\\\ 4 &amp; 9 &amp; 0.07317073 \\\\ \\hline \\mathbf{sum} &amp; 123 &amp; 1 \\end{array} \\] We say that about \\(25\\%\\) of the patients in the study had misophonia grade \\(3\\), or that the outcome of grade \\(3\\) was observed with \\(25\\%\\) frequency. Note that the rows of table are now ordered by the severity of the disease. 2.10 Absolute and relative cumulative frequencies Misophonia severity is categorical and ordinal because its outcomes can be meaningfully ordered by their degree. When the outcomes of a random experiment can be ordered, it is useful to ask how many observations were obtained up to a given outcome. We call this number the absolute cumulative frequency up to the outcome \\(i\\), and compute it with sum the relative frequencies up to \\(i\\) \\[N_i =\\sum_{k= 1}^i n_k\\] It is also useful to calculate the proportion of observations up to a given result. \\[F_i =\\sum_{k= 1}^i f_k\\] This is called the relative cumulative frequency, or frequency distribution of the data. We can add these frequencies to the frequency table \\[ \\begin{array}{ccccc} \\mathbf{outcome} &amp; \\mathbf{n_i} &amp; \\mathbf{f_i} &amp; \\mathbf{N_i} &amp; \\mathbf{F_i}\\\\ 0 &amp; 41 &amp; 0.33333333 &amp; 41 &amp; 0.3333333\\\\ 1 &amp; 5 &amp; 0.04065041 &amp; 46 &amp; 0.3739837\\\\ 2 &amp; 37 &amp; 0.30081301 &amp; 83 &amp; 0.6747967\\\\ 3 &amp; 31 &amp; 0.25203252 &amp; 114&amp; 0.9268293\\\\ 4 &amp; 4 &amp; 0.07317073 &amp; \\mathbf{123} &amp; \\mathbf{1}\\\\ \\end{array} \\] Therefore, 67% of patients had misophonia up to severity 2, and 37% of patients had severity less than or equal to 1. 2.11 Cumulative frequency graph \\(F_i\\) is an important quantity because it allows us define the accumulation of frequencies up to intermediate levels of the outcomes. The frequency up to an intermediate level \\(x\\) (\\(i\\leq x&lt; i+1\\)) is just the accumulation up to the lower possible outcome of the experiment \\[F_x = F_i\\] \\(F_x\\) is therefore a function on a continuous range of values. We can draw it with respect to the outcomes; for instance, for the misophonia example Therefore, we can say that \\(67\\%\\) of the patients had misophonia up to severity \\(2.3\\), even though \\(2.3\\) is not an observed outcome. 2.12 Numerical outcomes The result of a random experiment can produce a number. If the number is discrete, we can generate a frequency table, with absolute, relative, and cumulative frequencies, and illustrate them with bar, pie, and cumulative charts. When the outcome is continuous the frequency table is not so useful, because it is unlikely to obtain a repetition of the same continuous number, to arbitrary small precision. Example (Dinosaur extinction) The Alvarez hypothesis states that the mass extinction in the Cretaceous-Paleocene boundary, \\(\\sim 66\\) million of years ago (Ma), was due to the impact of a large meteorite in Yucatan’s peninsula. Alvarez and son discovered a layer of extraterrestrial iridium, presumably from the meteorite, uniformly distributed around the globe at the depth close to the time of the extinction. A rival theory explained a gradual extinction by unprecedented volcanic activity, as the impact appeared to precede extinction in 300,000 years. After 30 years of dispute, Renne et al designed a method to precisely measure the age of collected geological samples (Renne et al. 2013), by assessing the quantity of radioactive potassium degradation into argon in impact melt droplets (tektites). These were the ages in Ma of \\(87\\) tektites specimens for one of the experiments reported by Renne and colleagues: 65.664, 64.492, 66.239, 66.677, 66.137, 65.406, 65.980, 64.979, 66.302, 65.975, 66.252, 67.492, 65.792, 66.328, 65.975, 65.740, 66.462, 66.287, 67.281, 54.619, 40.915, 66.444, 65.710, 55.046, 65.463, 66.378, 65.774, 67.044, 65.756, 66.754, 65.809, 66.386, 51.835, 65.624, 63.565, 65.179, 65.896, 65.825, 66.257, 65.990, 65.421, 53.258, 66.884, 65.912, 65.693, 66.087, 65.857, 65.992, 65.916, 65.723, 66.275, 65.441, 66.630, 66.097, 69.700, 66.011, 66.218, 66.737, 60.071, 65.800, 62.429, 66.308, 67.036, 59.836, 66.149, 65.928, 65.951, 66.077, 70.980, 66.060, 66.207, 61.905, 66.055, 65.205, 66.433, 66.085, 66.001, 62.098, 66.064, 66.368, 66.348, 65.693, 66.035, 66.040, 66.27, 65.739, 66.023 If we look in detail these set of observations, we see that only two outcomes were repeated at \\(65.693\\) and \\(65.975\\), each. These are the result of measurement rounding or limited instrument resolution. While there was one single event that give rise to similar ages of the tektites, potassium decay into argon depends on so many factors that it is unlikely that two specimens have the same estimated aging, if we had sufficient resolution. 2.13 Transforming continuous data Since observations of continuous outcomes cannot be counted in frequencies (at least by definition), we may transform them into ordered categorical outcomes. We do it in two steps: First, we cover the range of observations in regular intervals of the same size (bins) [40.9,43.9], (43.9,46.9], (46.9,49.9], (49.9,52.9], (52.9,55.9], (55.9,59], (59,62], (62,65], (65,68], (68,71] Then, we map each observation to its interval: creating a categorical ordered outcomes; in this case, we have \\(10\\) possible outcomes (65,68], (62,65], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (52.9,55.9], [40.9,43.9], (65,68], (65,68], (52.9,55.9], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (49.9,52.9], (65,68], (62,65], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (52.9,55.9], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (68,71], (65,68], (65,68], (65,68], (59,62], (65,68], (62,65], (65,68], (65,68], (59,62], (65,68], (65,68], (65,68], (65,68], (68,71], (65,68], (65,68], (59,62], (65,68], (65,68], (65,68], (65,68], (65,68], (62,65], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68], (65,68] Therefore, instead of saying that the first tektite had an age of \\(65.664\\) million of years, we say that its age is in the interval (or bin) between \\((65,68]\\). 2.14 Frequency table for a continuous variable For a given regular partition of the range of the continuous outcomes into intervals, we can produce a frequency table as before \\[ \\begin{array}{ccccc} \\mathbf{outcome} &amp; \\mathbf{n_i} &amp; \\mathbf{f_i} &amp; \\mathbf{N_i} &amp; \\mathbf{F_i}\\\\ \\mathrm{[40.9,43.9]} &amp; 1 &amp; 0.01149425 &amp; 1 &amp; 0.01149425 \\\\ \\mathrm{(43.9,46.9]} &amp; 0 &amp; 0.00000000 &amp; 1 &amp; 0.01149425 \\\\ \\mathrm{(46.9,49.9]} &amp; 0 &amp; 0.00000000 &amp; 1 &amp; 0.01149425 \\\\ \\mathrm{(49.9,52.9]} &amp; 1 &amp; 0.01149425 &amp; 2 &amp; 0.02298851 \\\\ \\mathrm{(52.9,55.9]} &amp; 3 &amp; 0.03448276 &amp; 5 &amp; 0.05747126 \\\\ \\mathrm{(55.9,59]} &amp; 0 &amp; 0.00000000 &amp; 5 &amp; 0.05747126 \\\\ \\mathrm{(59,62]} &amp; 3 &amp; 0.03448276 &amp; 8 &amp; 0.09195402 \\\\ \\mathrm{(62,65]} &amp; 4 &amp; 0.04597701 &amp; 12 &amp; 0.13793103 \\\\ \\mathrm{(65,68]} &amp; 73 &amp; 0.83908046 &amp; 85 &amp; 0.97701149 \\\\ \\mathrm{(68,71]} &amp; 2 &amp; 0.02298851 &amp; \\mathbf{87} &amp; \\mathbf{1} \\\\ \\end{array} \\] We see that \\(83\\%\\) of the tektites had an age at this interval, already overlapping with the age of extinction; estimated at \\(65.957\\)Ma by fossil records. We still need some conceptual and analytic tools to be able to compare the true aging of two unobserved events, the extinction onset and meteorite impact, but tektite data give us the strong intuition that they were simultaneous in geological scale, and therefore that the meteorite was one of the causes of extinction. 2.15 Histogram The histogram is the graph of the absolute (\\(n_i\\)) or relative \\(f_i\\) frequencies against the interval outcomes (bins). The histogram depends on the size of the bin. The following is a histogram with \\(50\\) bins We see, for instance, that the most frequent estimated impact age is close to \\(66\\)Ma, the age of extinction. The data already suggest that the meteorite impact and the dinosaur extinction are likely simultaneous events. 2.16 Cumulative frequency graph We can also plot \\(F_x\\) against the outcomes. Since \\(F_x\\) has a continuous range, we can order the observations (\\(x_1 &lt;... x_j &lt; x_{j+1} &lt; x_N\\)) and therefore \\[F_x = \\frac{k}{N}\\] for \\(x_{k} \\leq x &lt; x_{k+ 1}\\) . \\(F_x\\) is known as the distribution of the data. \\(F_x\\) does not depend on the size of the bin. However, its resolution depends on the amount of data. We therefore see a steep increase in the number of observations between \\(65\\) and \\(67\\)Ma. 2.17 Summary Statistics Summary statistics are numbers calculated from the data that tell us important characteristics of the numerical outcomes (discrete or continuous). For example, we have statistics that describe extreme values: minimum: the minimum observation. maximum: the maximum observation. 2.18 Average (sample mean) An important statistic that describes the central value of the observations (where to expect most observations) is the average \\[\\bar{x}=\\frac{1}{N} \\sum_{j=1}^N x_j\\] where \\(x_j\\) is the observation \\(j\\) from a total of \\(N\\) observations. We also call the average the sample mean. The sample is the set of observations, or the data obtained from the repetition of the random experiment. Example (Dinosaur extinction) The average estimated age for the deadly meteorite impact from tektites can be calculated directly from the observations in the usual way \\(\\bar{x}= \\frac{1}{ N}\\sum_j x_j\\) \\(= \\frac{1}{87}(65.664 + 64.492 + 66.239... + 66.023) = 64.99741\\) The average is nearly a million years more recent than the start of the extinction. We will see that, under reasonable models and conditions, the average is an estimation that we can trust. We will say that it is the maximum likelihood value for the true aging. In this example, we can readily make some objections to this, as some few estimated ages appear to be excessively recent (\\(40\\)Ma) or excessively old (\\(70\\)Ma). As these observations do not gather with most of them, we have reasons to believe that they are not representative of the same potassium decay process that can age the impact, or that they are not suitable repetitions of the random experiment in question. Collection of those samples, processing of the material or potassium decay conditions may have substantially affected those specimens. Now let us look as the average of categorically ordered outcomes. For these cases, it is insightful to show that we can also use the relative frequencies to calculate the average \\(\\bar{x}=\\frac{1}{ N}\\sum_{i=1}^N x_j =\\frac{1}{N}\\sum_{i=1}^M x_i n_ {i}=\\) \\[\\sum_{i=1}^M x_i f_{i}\\] where we went from adding \\(N\\) observations to adding \\(M\\) outcomes. The form \\(\\bar{x}= \\sum_{i = 1}^M x_i f_i\\) shows that the average is the center of mass of the data. As if each observation had a mass weight given by \\(f_i\\). Example (misophonia) The average severity of misophonia in the study can be calculated from the relative frequencies of the outcomes \\[ \\begin{array}{ccc} \\mathbf{outcome} &amp; \\mathbf{n_i} &amp; \\mathbf{f_i} \\\\ 0 &amp; 41 &amp; 0.33333333 \\\\ 1 &amp; 5 &amp; 0.04065041 \\\\ 2 &amp; 37 &amp; 0.30081301 \\\\ 3 &amp; 31 &amp; 0.25203252 \\\\ 4 &amp; 9 &amp; 0.07317073 \\end{array} \\] \\(\\bar{x}=0\\times f_ {0}+ 1\\times f_{1}+2 \\times f_{2}+3 \\times f_{3}+4 \\times f_{4}=1.691057\\) The average is also the center of mass for continuous outcomes; that is, the point where the relative frequencies of the bins balance. Clearly the average for a categorical ordered variables is not necessarily an outcome of the random experiment. There is no observable misophonia severity of \\(1.691057\\). Likewise, if the average birth rate in a country is \\(1.5\\) per family, there is no family with \\(1.5\\) children. For continuous data, the average may be a possible observation and can still be considered the point where observations balance. We can imagine the histogram as a balancing rod, where we place sticks of different heights, and the mean represents the balancing point. 2.19 Median Another measure of centrality is the median. The median \\(x_m\\), or \\(q_{0.5}\\), is the value below which we find half of the observations. When we order the observations \\(x_1 \\leq ... x_j \\leq x_{j+1} \\leq x_N\\), we count them until we find half of them, then we report the value \\(x_m\\) that splits the observations in two equal parts. If the number of observations is a pair number, we can be split then into two equal parts and \\(x_m\\) is the value between the observation \\(x_{N/2}\\) and \\(x_{N/2+1}\\). That is, if \\(N\\) is even \\[x_1 \\leq... \\leq x_{N/2} \\leq x_{N/2+1} \\leq x_N\\] then \\[x_1 \\leq ... x_{N/2} \\leq x_m \\leq x_{N/2+1} \\leq x_N\\] or \\(x_m \\in (x_{N/2}, x_{N/2+1})\\). In general any value \\(x_m\\) in between these two numbers is a median, conventionally, if we need to report a single value we take the middle point between them. In the case that \\(N\\) is odd, the observation in the middle \\(x_j\\) \\[x_1 \\leq ... x_{j-1} \\leq x_j \\leq x_{j+1} \\leq x_N\\] for which \\(j=(N+1)/2\\), splits the data in half and it is the median \\(x_m=x_j\\) Example (Dinosaur extinction) If we order the tektites estimated ages and cut them in half at position \\(j=(87+1)/2=44\\), we see that \\(43\\) observations (tektites) are below \\(65.992\\). The median age is therefore \\(q_{0.5}= x_{44}=65.992\\). Let us show the data explicitly. This is the fist half of the ordered data from the first to the \\(43\\)th position 40.915, 51.835, 53.258, 54.619, 55.046, 59.836, 60.071, 61.905, 62.098, 62.429, 63.565, 64.492, 64.979, 65.179, 65.205, 65.406, 65.421, 65.441, 65.463, 65.624, 65.664, 65.693, 65.693, 65.710, 65.723, 65.739, 65.740, 65.756, 65.774, 65.792, 65.800, 65.809, 65.825, 65.857, 65.896, 65.912, 65.916, 65.928, 65.951, 65.975, 65.975, 65.980, 65.99 The position \\(44\\)the position, or the median is 65.992 This is the second half of the data from the \\(45\\)th position to the last one. 66.001, 66.011, 66.023, 66.035, 66.040, 66.055, 66.06, 66.064, 66.077, 66.085, 66.087, 66.097, 66.137, 66.149, 66.207, 66.218, 66.239, 66.252, 66.257, 66.270, 66.275, 66.287, 66.302, 66.308, 66.328, 66.348, 66.368, 66.378, 66.386, 66.433, 66.444, 66.462, 66.630, 66.677, 66.737, 66.754, 66.884, 67.036, 67.044, 67.281, 67.492, 69.700, 70.98 In terms of frequencies, \\(q_{0.5}\\) makes the cumulative frequency \\(F_x\\) equal to \\(0.5\\) \\[\\sum_{i = 1}^m f_i =F_{q_{0.5}}=0.5\\] that is \\[q_{ 0.5}= F^{-1}(0.5)\\] This last equation means that, in the distribution graph, the median \\(q_{ 0.5}\\) is the value of \\(x\\) at which we have climbed half of the total height of \\(F_x\\). We have accumulated half of the observations. The average and median are not always the same. The point that divides half of the mass is not always the center of mass. In the tektite aging, it appears that the median is a better estimation of the meteorite impact age than the median because the median is robust to the presence of extreme observations or outliers. We see that the median is closer to the most frequent observations and to the expected time of extinction. The median will be close to the average for histograms that look symmetrical about the average. 2.20 Dispersion Other important summary statistics for observations are the statistics of dispersion or variability. Many experiments may share their average, but differ in how spread the observations are. The dispersion of the observations is a measure of the noise or randomness of the experiment. If observations do not vary, the random experiment gives always the same outcome and there is no need to learn statistics. 2.21 Sample variance The dispersion about the average is measured by the sample variance \\[s^2=\\frac{ 1}{ N-1} \\sum_{j=1}^N ( x_j -\\bar{x})^2\\] This number measures the average squared distance of the observations to the average. The reason for \\(N-1\\) will be explained when we talk about inference, when we study the spread of \\(\\bar{x}\\), as well as the spread of the observations. In terms of the frequencies of the outcomes that are categorical and ordered, we can also calculate the sample variance as \\[s^2=\\frac{N}{N-1} \\sum_{i=1}^M (x_i -\\bar{x})^2 f_i\\] Take many observations and then make \\(N/(N+1)\\) close to \\(1\\), then \\(s^2\\) can be considered as the moment of inertia about the average of the observations. The square root of the sample variance, \\(s\\), is called standard deviation of the sample. Example (Dinosaur extinction) The standard deviation of the tektite ages is \\(s= [\\frac{ 1}{87-1}((64.99741-65.664)^2+ (64.99741-64.492)^2\\) \\[+ ... + (64.99741-66.023)^ 2]^{1/2} = 3.939105\\] The tektites ages tend to differ from the age average in \\(3.939105\\)Ma. As this is a measure of how far the observations will be from the average then the summarized aging of the tektites can be written as \\[(\\bar{x}, s)= (average=64.99741, sd= 3.939105)\\] 2.22 Interquartile range (IQR) The spread of the data can also be measured with respect to the median using the interquartile range, as follows: We define the first quartile as the value \\(x\\) that makes the cumulative frequency \\(F_{q_{0.25}}\\) equal to \\(0.25\\), or the value of \\(x\\) where we have accumulated a quarter of the observations. This is the value that splits the first quarter of the observations. \\[q_{0.25}=F^{-1}(0.25)\\] We define the third quartile as the value \\(x\\) that makes the cumulative frequency \\(F_{q_{0.75}}\\) equal to \\(0.75\\), or the value of \\(x\\) where we have accumulated three quarters of observations. \\[q_{0.75}=F^{-1}(0.75)\\] The interquartile range (IQR) is \\[IQR=q_{0.75} - q_{0.25}\\] This is the distance between the third and first quartiles and captures the central \\(50\\%\\) of the observations Example (Dinosaur extinction) The first and third quartiles of the tektite data are \\[q_{0.25}=65.693\\] \\[q_{0.75}=66.281\\] and therefore the IQR is \\[IQR=66.281-65.693=0.588\\]. A usual argument for reducing the dispersion of the observations, when extreme observations are obtained, is to discard those that are very far from the median. In particular, we can consider that observations with distances grater than \\(1.5\\times\\)IQR are outliers, meaning that they are not representative of our random experiment. We may consider removing them form our data as a quality control measure. The outliers for the tektite data are: 40.915, 51.835, 53.258, 54.619, 55.046, 59.836, 60.071, 61.905, 62.098, 62.429, 63.565, 64.492, 64.979, 66.884, 67.036, 67.044, 67.281, 67.492, 69.700, 70.98 We thus see that the summarized aging for the tektites without those observations \\[(\\bar{x}, s)= (average=66.01955, sd=0.339907)\\] gets closer to the median \\[(q_{0.5}, IQR)= (median=66.023, iqr=0.4675)\\] and substantially reduces the dispersion of the data. We also see that the median changes less than the average when we remove the outliers. 2.23 Boxplot The interquartile range, median, and \\(5\\%\\) and \\(95\\%\\) of the data can be displayed in a box plot. In the boxplot, the values of the outcomes are on the y-axis. The IQR is the box, the median is the middle line, and the whiskers mark the \\(5\\%\\) and \\(95\\%\\) of the data. The estimated age of extinction, \\(65.957\\)Ma, is close to both the average (\\(66.01955\\)Ma) and the median (\\(66.023\\)Ma) ages of trusted tektite samples measured via potassium decay. This alignment suggests that the impact event and the onset of extinction could have been simultaneous on a geological timescale. Alternatively, if the impact occurred slightly later, it likely took place in an already ecologically stressed environment, amplifying its deadly consequences. However, the use of the average (or median) derived from tektite chemistry to estimate the age of the impact raises an important question: how confident can we be in these estimates? If we were to analyze a different set of \\(87\\) tektites, we would likely obtain a different average. What if we analyzed \\(200\\) tektites? How much would the result vary? In other words, we aim to assess the level of certainty in each figure of our estimate. Intuitively, we are more confident in the first \\(6\\) of the \\(60\\) million years than in the last \\(5\\) of the \\(50\\) years in the figure \\(66.01955\\)Ma. Could we achieve a precision small enough to rule out a temporal gap of \\(300,000\\) years between the impact and the extinction? These are the critical questions we will address in the following chapters as we justify the use of the average (or any other statistic) and formulate its probable error. In their comprehensive study using multiple sources of evidence, Renne and colleagues achieved an unprecedented precision of \\(11,000\\) years for the impact age estimate, encompassing the onset of extinction. Their findings provide compelling evidence for the simultaneity of these events. 2.24 Questions 1) In the following boxplot, the first quartile and second quartile of the data are: \\(\\qquad\\)a: \\((-1.00, 21.30)\\); \\(\\qquad\\)b: \\((-1.00, 7.02)\\); \\(\\qquad\\)c: \\((7.02, 7.96)\\); \\(\\qquad\\)d: \\((7.02, 14.22)\\) 2) The main disadvantage of a histogram is that: \\(\\qquad\\) a : Depends on the size of the bin ; \\(\\qquad\\)b : Cannot be used for categorical outcome; \\(\\qquad\\) c : Cannot be used when the bin size is small; \\(\\qquad\\) d : Used only for relative frequencies; 3) If the relative cumulative frequencies of a random experiment with outcomes \\(\\{1,2,3,4\\}\\) are: \\(F(1)=0.15, \\qquad F(2)=0.60, \\qquad F(3)=0.85, \\qquad F(4)=1\\). Then the relative frequency for the outcome \\(3\\) is \\(\\qquad\\)a: \\(0.15\\); \\(\\qquad\\)b: \\(0.85\\); \\(\\qquad\\)c: \\(0.45\\); \\(\\qquad\\)d: \\(0.25\\) 4) In a sample of size \\(10\\) from a random experiment we obtained the following data: 8, 3, 3, 7, 3, 6, 5, 10, 3, 8 The first quartile of the data is: \\(\\qquad\\)a: \\(3.5\\); \\(\\qquad\\)b: \\(4\\); \\(\\qquad\\)c: \\(5\\); \\(\\qquad\\)d: \\(3\\) 5) Imagine that we collect data for two quantities that are not mutually exclusive, for example, the gender and nationality of passengers on a flight. If we want to make a single pie chart for the data, which of these statements is true? \\(\\qquad\\)a : We can only make a nationality pie chart because it has more than two possible outcomes; \\(\\qquad\\)b : We can make a pie graph for a new variable marking gender and nationality; \\(\\qquad\\)c : We can make a pie chart for the variable sex or the variable nationality; \\(\\qquad\\)d : We can only choose whether to make a pie chart for gender or a pie chart for nationality. 2.25 Exercises 2.25.0.1 Exercise 1 We have performed an experiment 8 times with the following results 3, 3, 10, 2, 6, 11, 5, 4 Answer the following questions: Calculate the relative frequencies of each result. Calculate the cumulative frequencies of each result. What is the average of the observations? What is the median? What is the third quartile? What is the first quartile? 2.25.0.2 Exercise 2 We have performed an experiment 10 times with the following results 2.875775, 7.883051, 4.089769, 8.830174, 9.404673, 0.455565, 5.281055, 8.924190, 5.514350, 4.566147 Consider 10 bins of size 1: [0,1], (1,2] …( 9,10). Answer the following questions: Calculate the relative frequencies of each result and draw the histogram Calculate the cumulative frequencies of each result and draw the cumulative graph. Draw a box plot . 2.26 Practice In the misophonia study, the researchers wondered if the type of jaw given by cephalometric measures such as the convexity angle of the jaw would affect the severity of misophonia. The scientific hypothesis is that the angle of convexity of the jaw can influence hearing and its sensitivity, as the inner ear rests on top of the jaw. Summary of Python and R code can be found in the Apendix. Load misophonia data from https://alejandro-isglobal.github.io/SDA/data/Misophonia.txt Extract the misophonia severity variable Do a bar plot and a pie chart Extract the convexity angle variable Calculate its sample mean (average), standard deviation and make a histogram Calculate its median and inter-quartile range Draw a boxplot for all the patients Draw a boxplot for each misophonia severity Solutions References "],["probability.html", "Chapter 3 Probability 3.1 Probability mesurement 3.2 Classical probability 3.3 Relative frequencies 3.4 Frequentist probability 3.5 Classical and frequentist probabilities 3.6 Sample space 3.7 Events 3.8 Algebra of events 3.9 Mutually exclusive events 3.10 Definition of probability 3.11 Probability table 3.12 Joint probabilities 3.13 Contingency table 3.14 The addition rule 3.15 Questions 3.16 Exercises 3.17 Practice", " Chapter 3 Probability We may state that one of the primary objectives of experimental sciences is either to predict the outcomes of a random experiment or to gain control over the experiment to influence the frequencies of outcomes in a desired manner. We hold a strong belief that advances in our knowledge and understanding of the experiment, framed within a coherent body of theories, will shed light on these objectives. Statistics plays a pivotal role in this endeavor, as it seeks to account for the randomness inherent in experiments and helps us extract meaningful signals from noise. Paradoxically, statistics involves the systematic study of aspects the experimentalist is not directly interested in: randomness and noise. The central epistemic question behind statistics is this: How can we derive knowledge from a random experiment when its results vary each time it is conducted? There must be some invariant—a physical property of the experiment—that we can identify and, in certain cases, influence. A cornerstone of experimental sciences is the recognition that the propensities with which outcomes occur are intrinsic properties of the random experiment itself. When a random experiment is repeated, each outcome retains the same propensity of being observed. Sometimes a specific outcome will occur, and sometimes it will not, but as the experiment is repeated, the outcome will appear with a certain regularity: perhaps 2 out of 5 times, or 3 out of 7. While we can never predict with certainty for a finite number of repetitions, if the experiment were repeated an infinite number of times, the exact propensity of each outcome would be revealed. In this chapter, we introduce the concept of probability as the value approached by relative frequencies when a random experiment is repeated infinitely. For experiments in which all outcomes have equal probabilities, this frequentist definition aligns with the classical definition of probability: the ratio of the number of favorable outcomes to the total number of possible outcomes. The frequentist definition also allows for a propensity interpretation, which views probabilities as abstract defining properties of the random experiment. With the help of a dice, computer simulations and the inheritance of genetic traits, we will define and explain the fundamental rules of probability in alignment with relative frequency tables. Events will be introduced as the basic elements to which probabilities are assigned. Composite events will be constructed using the principles of set algebra. The concept of the joint probability of two events will be derived from these axioms and will serve as the foundation for incorporating multiple simultaneous measurements in a random experiment. This will form the foundation of conditional probability and statistical independence, which will be explored in the next chapter. 3.1 Probability mesurement We seek a measure of the likelihood that a particular outcome will occur in a future instance of a random experiment. These likelihoods will be referred to as probabilities of outcomes and will represent statements about their future occurrences. We define the probability of an outcome as a measure of its propensity or likelihood of occurring, assigning it specific values: 0, when the outcome has no possibility of occurring in the next run of the experiment. 1, when the outcome is certain to occur in the next run of the experiment. Intermediate values can be easily conceived for a class of experiments with many outcomes, each equally likely. 3.2 Classical probability As long as a random experiment has \\(m\\) possible outcomes that are all equally likely, the probability of each outcome \\(i\\) can be defined as: \\[P_i =\\frac{1}{m}\\]. Classical probability was explicitly defined by Laplace (1814). Since every outcome is equally likely in this type of experiment, we may declare complete ignorance and, consequently, the best we can do is to equally distribute the same probability to each outcome. In other words, there are some random experiments for which we have no reason to assign more likelihood to one outcome than another. Note that: \\(P_i\\) is an abstract defining property of the random experiment. We do not observe \\(P_i\\). We deduce \\(P_i\\) from the ratio above and have no need to carry out any experiment to know it. Example (dice): What is the probability that we will get \\(2\\) on the roll of a die? \\[P_2=1/6=0.166666\\] We reason that all other \\(5\\) numbers in the dice are equally likely to \\(2\\). The equal probability of the dice outcomes is a property of the random experiment of rolling it, which just states that the dice is fair and we should not expect one outcome rather than another. 3.3 Relative frequencies What about random experiments whose possible outcomes are not equally likely? How then can we define the probabilities of the outcomes? Example (mouse coating) Imagine that we obtain \\(20\\) mice from a couple, both of which are grey in color. We write down the colors of their progeny: grey, grey, grey, grey, grey, albino, grey, black, albino, grey, black, grey, grey, grey, grey, black, grey, albino, black, grey From this data, how sure are we of obtaining an albino offspring from a future breading of the couple? The frequency table is \\[ \\begin{array}{ccc} \\mathbf{outcome} &amp; \\mathbf{n_i} &amp; \\mathbf{f_i} \\\\ \\text{grey} &amp; 13 &amp; 0.65 \\\\ \\text{black} &amp; 4 &amp; 0.20 \\\\ \\text{albino} &amp; 3 &amp; 0.15 \\\\ \\hline \\mathbf{sum} &amp; 20 &amp; 1 \\end{array} \\] and the bar plot The relative frequency \\(f_i =\\frac{n_ i}{n}\\) seems like a reasonable probability measure because it is a number between \\(0\\) and \\(1\\); and it measures the proportion of the total number of observations that we obtained for a particular outcome. Since \\(f_{albino}=0.15\\) then we would be about one sixth sure or, more precisely, three out of every \\(20\\) observations, of getting albino. How good is \\(f_i\\) as a measure of the outcome’s \\(i\\) likelihood? Let us imagine, we repeat the previous experiment \\(100,000\\) more times. This is clearly impossible in practice but to achieve this, we will use the computer to simulate data based on a known inheritance model. Here is the approach: Genetic theory for the epistatic model (Exercise 5), where albinism inhibits the grey and black coat colors, tell us that the relative ratios are 9:3:4 for grey, black and albino respectively. This means that out of \\(16\\) possible mice, \\(4\\) can be albino. To simulate this, we instruct the computer to create an urn containing \\(9\\) grey balls, \\(3\\) black balls, and \\(4\\) albino balls. A ball is drawn at random, its label is recorded, and then it is returned to the urn. This simulates running one single random experiment of epistatic inheritance. We can repeat this process \\(20\\) times, as we did earlier, or \\(100,000\\) times, as we now show. The resulting frequency table is now as follows: \\[ \\begin{array}{ccc} \\mathbf{outcome} &amp; \\mathbf{n_i} &amp; \\mathbf{f_i} \\\\ \\text{grey} &amp; 56281 &amp; 0.56281 \\\\ \\text{black} &amp; 18542 &amp; 0.18542 \\\\ \\text{albino} &amp; 25177 &amp; 0.25177 \\\\ \\hline \\mathbf{sum} &amp; 100000 &amp; 1 \\end{array} \\] and the bar plot is We see that the frequency for \\(f_{albino}\\) is now \\(0.25145\\), which is closer to the likelihood of \\(4\\) in \\(16\\) (\\(0.25\\)) for producing an albino in the next offspring. Thus, we note that the probabilities measured by \\(f_i\\) vary with \\(n\\). A crucial fact is that when we compute \\(f_i\\) with increasing values of \\(n\\), \\(f_i\\) converges. In the following, graph each vertical section gives the relative frequency of each observation, for a given value of \\(n\\). We see that after \\(n=1000\\) (\\(log10(n)=3\\)) the sections’ proportions hardly change with more \\(n\\), and the relative frequencies stabilize. An important point needs to be made. The unequal frequencies \\(f_1\\), \\(f_2\\) and \\(f_3\\), at given \\(n\\) and shown in the figure, were obtained from an experiment with \\(16\\) equally likely outcomes. Each of the \\(16\\) balls in the urn has the same chance of being drawn. Four balls are labeled “albino,” giving them an accumulated probability of \\(0.25\\) of being drawn. In practice, after drawing a million balls, we observed a relative frequency of \\(0.249898\\) for albino. While this is not exactly \\(0.25\\), it is very close. As the number of draws increases, the relative frequency approaches the probability, becoming equal to it only at infinity. By drawing an analogy between the urn and the reproduction of mice, we can imagine that the mice could reproduce to arbitrary numbers. While this is clearly impossible, it represents a step toward abstract thinking. If the mice could reproduce indefinitely, just as balls can be drawn from an urn indefinitely, then the relative frequency would converge to the probability. Specifically, the relative frequency \\(f_i\\) at infinity becomes an abstract quantity \\(P_i\\), the constant value to which \\(f_i\\) converges: \\[\\lim_{n \\to \\infty} f_i = P_i\\] This quantity, \\(P_i\\), is what we call frequentist probability. It is important to note that \\(f_i\\) is derived from a finite number \\(n\\) of observations. In the lab, we can only perform a limited number of repetitions of a random experiment—always far from infinity. No amount of observations can definitively prove that the inheritance model is epistatic, especially with just \\(20\\) observations, or with real data from actual mice. However, as we will discuss in the following chapters, we can state with a certain level of confidence that the observed frequencies are consistent with an epistatic model. 3.4 Frequentist probability We call Probability \\(P_i\\) the limit as \\(n \\rightarrow \\infty\\) of the relative frequency of observing the outcome \\(i\\) in a random experiment. Defended by Venn (1876), the frequentist definition of probability is derived from (empirical) data/experience. Note that: \\(P_i\\) is a property of an infinite repetition of the random experiment. We do not observe \\(P_i\\), we observe \\(f_i\\) at a given value of \\(n\\). We estimate \\(P_i\\) with \\(f_i\\) (usually when \\(n\\) is large), and write: \\[\\hat {P_ i}= f_i\\] Similar to the relationship between observation and outcome, there is a relationship between relative frequency and probability: a concrete value corresponding to an abstract quantity. A suitable interpretation of probability in experimental sciences is as the propensity of a random experiment to produce a given outcome (Popper 2002). Probability is thus viewed as an abstract defining property of a random experiment, which can be either approximated as the experiment is repeated a large number of times, or known by experimental design. 3.5 Classical and frequentist probabilities We have situations where classical probability can be used to find the limit of relative frequencies: If the results are equally probable, the classical probability gives us the limit: \\[P_i=lim_{n\\rightarrow \\infty} \\frac{n_i}{n}=\\frac{1}{m}\\] If the results in which we are interested can be derived from other equally probable results, as in the mice coating example and the following one. Example (sum of two dice) Let us consider the sum of the outcomes when rolling two dice. We can determine the exact probabilities of the outcomes without actually rolling the dice. This probability follows from the fact that the outcome of each die is equally likely. From this assumption, we can find that (Exercise 1) \\[ P_i = \\begin{cases} \\frac{i-1}{36},&amp; i \\in \\{2,3,4,5,6, 7\\} \\\\ \\frac{13-i}{36},&amp; i \\in \\{8,9,10,11,12\\} \\\\ \\end{cases} \\] Imagine, however, that we have a black box where we do not know that two dice are being thrown, but we can see the result of their roll displayed on a screen \\[5, 3, 11, 9, 5, 7\\] We are then able to closely approach the probabilities of the random experiment using the relative frequencies if we repeat the experiment a million times (as simulated by a computer) \\[ \\begin{array}{cccc} \\mathbf{outcome} &amp; \\mathbf{n_i} &amp; \\mathbf{f_i} &amp; \\mathbf{P_i} \\\\ 2 &amp; 28070 &amp; 0.028070 &amp; 0.02777778 \\\\ 3 &amp; 55617 &amp; 0.055617 &amp; 0.05555556 \\\\ 4 &amp; 82964 &amp; 0.082964 &amp; 0.08333333 \\\\ 5 &amp; 111113 &amp; 0.111113 &amp; 0.11111111 \\\\ 6 &amp; 138749 &amp; 0.138749 &amp; 0.13888889 \\\\ 7 &amp; 166717 &amp; 0.166717 &amp; 0.16666667 \\\\ 8 &amp; 139011 &amp; 0.139011 &amp; 0.13888889 \\\\ 9 &amp; 111193 &amp; 0.111193 &amp; 0.11111111 \\\\ 10 &amp; 83504 &amp; 0.083504 &amp; 0.08333333 \\\\ 11 &amp; 55397 &amp; 0.055397 &amp; 0.05555556 \\\\ 12 &amp; 27665 &amp; 0.027665 &amp; 0.02777778 \\\\ \\hline \\mathbf{sum} &amp; 1000000 &amp; 1 &amp; 1 \\\\ \\end{array} \\] Exact knowledge can be obtained from a model without the need to conduct an experiment, using the formula for \\(P_i\\). In contrast, precise knowledge about outcome likelihoods can be derived from large amounts of data, even without understanding the underlying mechanisms. Models are free to describe any experiment, real or not, while data can yield accurate predictions without necessarily deepening our understanding. Kant observed, ‘Thoughts without content are empty; intuitions without concepts are blind.’ Thus, scientific content and insight are often achieved by contrasting a model’s predictions with experimental results. The motivation behind the frequentist definition of probability is empirical, while that of the classical definition is rational. Since both approaches are complementary, we often combine them with inference and deduction to determine the probabilities of our random experiment. 3.6 Sample space Probabilities are numbers assigned to each possible outcome of a random experiment. However, probabilities are also applied to composite outcomes or events. For instance, we can ask about the probability of drawing a ball labeled “albino” when there are four such balls out of a total of 16. Each “albino” ball represents a different outcome of the drawing, but the event of drawing an “albino” ball is the same. Therefore, before providing a formal definition of probabilities, we need a further characterization of the outcomes that a random experiment can produce. The set of all possible outcomes of a random experiment is called the sample space and is denoted by \\(S\\). The sample space can consist of categorical or numerical outcomes. It represents the range of outcomes that a random experiment can yield. For example: human temperature: \\(S = (36, 42)\\) degrees Celsius sugar levels in humans: \\(S =( 70,80) mg/ dL\\) the size of a production line screw: \\(S =(70,72) mm\\) number of emails received in an hour: \\(S = \\{0, ...\\infty \\}\\) coat colors of mice: \\(S= \\{grey, black, albino\\}\\) the throw of a dice: \\(S= \\{ 1, 2, 3, 4, 5, 6\\}\\) 3.7 Events An event \\(A\\) is a subset of the sample space. It is a collection of possible outcomes. Examples of events: The event of a healthy temperature: \\(A=(37,38)\\) degrees Celsius The event of producing a screw with a size range: \\(A = (71.5mm, 71.6mm)\\) The event of receiving more than \\(4\\) emails in an hour: \\(A= \\{ 4, \\infty \\}\\) The event of obtaining an albino mouse \\(A=\\{albino\\}\\) The event of obtaining a number less than or equal to 3 in the throw of a dice: \\(A= \\{ 1,2,3\\}\\) An event refers to a possible set of primary outcomes. 3.8 Algebra of events While it seems clear that the result of a random experiment can give us a specific outcome or a more general event, it is also expected that the result of the experiment can be a composite event. For two events \\(A\\) and \\(B\\), we can construct the following composite events using the basic set operations: Complement \\(A&#39;\\): the event of not \\(A\\) Union \\(A \\cup B\\): the event of \\(A\\) or \\(B\\) Intersection \\(A \\cap B\\): the event of \\(A\\) and \\(B\\) Example (dice) Let us imagine we intend to roll a die but first we want look at a range of interesting events (composite outcomes): a number less than or equal to three \\(A:\\{ 1,2,3\\}\\) an even number \\(B:\\{ 2,4,6\\}\\) Let us see how we can build new events with set operations: a number not less than three: \\(A&#39;:\\{4,5,6\\}\\) a number less than or equal to three or even: \\(A \\cup B: \\{ 1,2,3,4,6\\}\\) a number less than or equal to three and even \\(A \\cap B: \\{ 2\\}\\) Random experiments produce a rich set of outcomes that can be gather together in events and transformed into other more complex events. Probabilities will be, consequently, defined on events. 3.9 Mutually exclusive events Primary outcomes like rolling \\(1\\) and \\(2\\) on a die are events that cannot occur at the same run of the experiment, a single roll of the dice. We say that they are mutually exclusive. In general, two events denoted as \\(E_1\\) and \\(E_2\\) are mutually exclusive when they have no element in common \\[E_1\\cap E_2=\\emptyset\\] Examples: The following events are mutually exclusive: The result that a patient has a misophonia severity \\(1\\) and severity \\(4\\). Only one severity is possible. The results of obtaining \\(12\\) and \\(5\\) in the roll of two dice. If we get \\(12\\) we do not get \\(5\\). However, the events of rolling a number “less than or equal to three” and “even” in a dice are not mutually exclusive. They share the outcome \\(2\\). 3.10 Definition of probability The probability of an event, as an abstract quantity, is defined by a set of axioms. These axioms represent the minimum set of rules that relate to relative frequencies. The probabilities of a random experiment’s events are numbers satisfying the following properties or axioms: When the events \\(E_1\\) and \\(E_2\\) are mutually exclusive; that is, only one of them can occur, the probability of observing \\(E_1\\) or \\(E_1\\) is their sum: \\[ P( E_1\\cup E_2) = P(E_1) + P(E_2)\\] When \\(S\\) is the sample space, then its probability is \\(1\\) (at least something is observed): \\[P(S)=1\\] The probability of any event is between \\(0\\) and \\(1\\) (a continuous and bounded measure) \\[P(E) \\in [0,1]\\] Remarkably, the axioms were proposed by Kolmogorov in 1933 (Kolmogorov 2013), after statistics had already been formally established as a distinct discipline and following the formulation of statistical and quantum mechanics—two prominent physics theories based on probability concepts. Advancement in these theories was possible because probabilities were treated as frequencies of infinite populations. 3.11 Probability table Kolmogorov’s axioms are the fundamental rules for defining a measure on the sample space and, as such, are independent of any specific interpretation. Probabilities, as mathematical objects, do not necessarily need to represent any random experiment. However, our interest lies in probabilities as measures of the likelihood of outcomes in real random experiments. Kolmogorov’s axioms importantly support the close alignment of probabilities with relative frequencies, as they also form the foundational rules for constructing a probability table, similar to relative frequency tables. Therefore, we can assert that frequencies tend to probabilities as the random experiment is repeated indefinitely, or that probabilities may be regarded as abstract defining properties of the experiment, representing the likelihood of its outcomes in the next repetition of the experiment. Example (dice) The probability table for the roll of a dice is \\[ \\begin{array}{cc} \\mathbf{outcome} &amp; \\mathbf{P_i} \\\\ 1 &amp; 1/6 \\\\ 2 &amp; 1/6 \\\\ 3 &amp; 1/6 \\\\ 4 &amp; 1/6 \\\\ 5 &amp; 1/6 \\\\ 6 &amp; 1/6 \\\\ \\hline S=\\{1, 2, ... 6\\} &amp; 1 \\\\ \\end{array} \\] Let us verify the axioms: Where \\(1 \\cup 2\\) is, for example, the event of rolling a \\(1\\) or a \\(2\\). So \\[ P( 1 \\cup 2)=P(1)+P(2)=2/6\\] Since \\(S= \\{ 1,2,3,4,5,6\\}\\) is made up of mutually exclusive outcomes, then \\[P(S)=P(1\\cup 2 ... \\cup 6) = P(1)+P(2)+ ...+P(6)=1\\] The probabilities of each outcome are between \\(0\\) and \\(1\\). This can be seen in the table. According to Kolmogorov’s properties, only mutually exclusive outcomes can be organized into probability tables, similar to relative frequency tables. 3.12 Joint probabilities Mutually exclusive events whose overall union is the sample space can be considered primary events. They constitute all the possible values of a type of measurement of the experiment that can be listed in a table. However, we have seen that there are events that are not mutually exclusive. How are we to understand them and to list them on a probability table? Two different events with a common outcome may then constitute two different types of measurement, that sometimes coincide in that common outcome. Let us consider the probability table for the dice and the events: a number less than or equal to three \\(A:\\{ 1,2,3\\}\\) an even number \\(B:\\{ 2,4,6\\}\\) which are not mutually exclusive coinciding in the outcome \\(2\\): \\(A \\cap B = \\{2\\}\\). In the rolling of a dice, we can observe there types of outcomes, or three types of measurements: a number from 1 to 6 (outcome 1), the event of whether a number is \\(\\leq 3\\) (outcome 2) or the event of whether a number is pair (outcome 3). \\[ \\begin{array}{cccc} \\mathbf{outcome\\, 1} &amp; \\mathbf{outcome\\, 2} &amp; \\mathbf{outcome\\, 3} &amp; \\mathbf{P_i} \\\\ 1 &amp; A &amp; B&#39; &amp; 1/6 \\\\ \\mathbf{2} &amp; \\mathbf{A} &amp; \\mathbf{B} &amp; \\mathbf{1/6} \\\\ 3 &amp; A &amp; B&#39; &amp; 1/6 \\\\ 4 &amp; A&#39;&amp; B &amp; 1/6 \\\\ 5 &amp; A&#39;&amp; B&#39; &amp; 1/6 \\\\ 6 &amp; A&#39;&amp; B &amp; 1/6 \\\\ \\hline S=\\{1, 2, ... ,6\\} &amp; S=\\{A \\cup A&#39;\\} &amp; S=\\{B \\cup B&#39;\\} &amp; 1 \\\\ \\end{array} \\] Now, we can also observe the occurrence of the the crossed possibilities between outcomes 2 and 3. In particular, we can consider the joint event \\(A\\cap B\\) as the event with two different qualities: “being greater or equal to 3” and “pair”, namely, outcome 1 being \\(2\\). For non mutually exclusive events, those that share common primary outcomes, we note that we can always decompose the sample space into mutually exclusive sets involving the intersections of the crossed possibilities between the events and their complements: \\[S=\\{A\\cap B, A \\cap B&#39;, A&#39;\\cap B, A&#39;\\cap B&#39;\\}\\] If one of those events occur the others do not. We can now make a probability table of these events. The joint probability of \\(A\\) and \\(B\\) is the probability of \\(A\\) and \\(B\\). That is \\[P( A \\cap B)\\] or \\(P(A,B)\\). The probability table for the joint probabilities will be as follows \\[ \\begin{array}{ll} \\mathbf{outcome\\, 4} &amp; \\mathbf{joint\\,probabilities} \\\\ A\\cap B=\\{2\\} &amp; P(A\\cap B)=1/6 \\\\ A\\cap B&#39;=\\{1,3\\} &amp; P(A\\cap B&#39;)=2/6 \\\\ A&#39;\\cap B=\\{4,6\\} &amp; P(A&#39;\\cap B)=2/6 \\\\ A&#39;\\cap B&#39;=\\{5\\} &amp; P(A&#39;\\cap B&#39;)=1/6 \\\\ \\hline S=\\{A\\cap B, ... A&#39;\\cap B&#39;\\} &amp; P(S)=1 \\\\ \\end{array} \\] The outcomes of type 4 are, therefore, composed of 4 mutually exclusive events, namely a number in the dice that is: “\\(\\leq 3\\) and pair”; or “\\(\\leq 3\\) and not pair”; or “not \\(\\leq 3\\) and pair”; or “not \\(\\leq 3\\) and not pair”. The probabilities of the sum off all these composite events is \\(1\\). The marginals of \\(A\\) and \\(A&#39;\\) are the probabilities of each of those events. That is \\[ \\begin{array}{ll} \\mathbf{outcome\\, 2} &amp; \\mathbf{marginal\\,probabilities} \\\\ A=\\{1,2,3\\} &amp; P(A)=3/6 \\\\ A&#39;=\\{4,5,6\\} &amp; P(A&#39;)=3/6 \\\\ \\hline S=A\\cup A&#39; &amp; P(S)=1 \\\\ \\end{array} \\] Note that the marginal \\(P(A)\\) is the addition of the joint probabilities where \\(A\\) occurs \\[P(A)=P(A\\cap B&#39;) + P(A \\cap B)\\] \\[=2/6+1/6=3/6\\] . Similarly we have the marginals for \\(B\\) and \\(B&#39;\\) \\[ \\begin{array}{ll} \\mathbf{outcome\\, 3} &amp; \\mathbf{marginal\\,probabilities} \\\\ B=\\{2,4,6\\} &amp; P(B)=3/6 \\\\ B&#39;=\\{1,3,5\\} &amp; P(B&#39;)=3/6 \\\\ \\hline S=A\\cup A&#39; &amp; P(S)=1 \\\\ \\end{array} \\] And, the marginal \\(P(B)\\) is the addition of all the probabilities where event \\(B\\) occurs \\[P(B)=P(A&#39;\\cap B) +P(A \\cap B)\\] \\[=2/6+1/6=3/6\\] 3.13 Contingency table The joint and the marginal probability tables can be written in a single contingency table \\[ \\begin{array}{ccc|c} &amp; \\mathbf{B} &amp; \\mathbf{B&#39;} &amp; \\mathbf{marginals} \\\\ \\mathbf{A} &amp; P(A \\cap B) &amp; P(A \\cap B&#39;) &amp; P(A) \\\\ \\mathbf{A&#39;} &amp; P(A&#39; \\cap B) &amp; P(A&#39; \\cap B&#39;) &amp; P(A&#39;) \\\\ \\hline \\mathbf{marginals} &amp; P(B) &amp; P(B&#39;) &amp; 1 \\\\ \\end{array} \\] Where the marginals are the sums in the margins of the table, for example: \\(P(A)=P(A \\cap B&#39;) + P(A \\cap B)\\) \\(P(B)=P(A&#39; \\cap B) + P(A \\cap B)\\) In our example, the contingency table for the events \\(A\\) and \\(B\\) and their complements is \\[ \\begin{array}{ccc|c} &amp; \\mathbf{B} &amp; \\mathbf{B&#39;} &amp; \\mathbf{marginals} \\\\ \\mathbf{A} &amp; 1/6 &amp; 2/6 &amp; 3/6 \\\\ \\mathbf{A&#39;} &amp; 2/6 &amp; 1/6 &amp; 3/6 \\\\ \\hline \\mathbf{marginals} &amp; 3/6 &amp; 3/6 &amp; 1 \\\\ \\end{array} \\] 3.14 The addition rule The addition rule allows us to calculate the probability of \\(A\\) or \\(B\\), \\(P( A \\cup B)\\), in terms of the probability of \\(A\\) and \\(B\\), \\(P(A \\cap B )\\). We can do this in three equivalent ways: Using only joint probabilities \\[P( A \\cup B)=P(A \\cap B)+P(A\\cap B&#39;)+P(A&#39;\\cap B)\\] Using the complement of the joint probability \\[P(A \\cup B)=1-P(A&#39;\\cap B&#39;)\\] Using the marginals and the joint probability \\[P(A \\cup B)=P(A) + P(B) - P(A\\cap B)\\] Example (Mendel’s first law) Every organism inherits two versions of a gene that we call alleles, one from each parent (\\(X_m\\),\\(X_f\\)). This is the joint event (\\(X_m \\cap X_f\\)). The alleles can be identical or different, and sometimes can be linked to a characteristic of the organism. For the coat color of mice there is one allele for albino (\\(A\\)) and another for not-albino (\\(A&#39;\\)), which allows any color. Inheritance of the alleles is made at random with equal probabilities from the parental alleles. For one offspring, we define the event of inheriting albino allele from the mother \\(A_m\\) the event of inheriting albino allele from the father \\(A_f\\) If both parents are hybrids they both have alleles (\\(A \\cap A&#39;\\)) and their offspring can have four allele pairs \\[S =\\{A_m \\cap A_f, A_m \\cap A_f&#39;, A_m&#39; \\cap A_f, A_m&#39; \\cap A_f&#39;\\}\\] with equal probabilities of \\(1/4\\). The contingency table is \\[ \\begin{array}{ccc|c} &amp; \\mathbf{A_f} &amp; \\mathbf{A_f&#39;} &amp; \\mathbf{marginals} \\\\ \\mathbf{A_m} &amp; 1/4 &amp; \\mathbf{1/4} &amp; 1/2 \\\\ \\mathbf{A_m&#39;} &amp; \\mathbf{1/4} &amp; \\mathbf{1/4} &amp; 1/2 \\\\ \\hline \\mathbf{marginals} &amp; 1/2 &amp; 1/2 &amp; 1 \\\\ \\end{array} \\] Having a color is dominant trait, which means that a mouse is colored if at least one the alleles is not albino. That is the event \\((A&#39;_m \\cup A&#39;_f)\\). Therefore, the probability of producing a color mouse is \\(3/4\\) that can be computed in three different ways: \\(P( A&#39;_m \\cup A&#39;_f)=P(A&#39;_m \\cap A&#39;_f)+P(A_m\\cap A_f&#39;)+P(A_m&#39;\\cap A_f)=1/4+1/4+1/4=3/4\\) \\(P(A&#39;_m \\cup A&#39;_f)=1-P(A_m\\cap A_f)= 1-1/4=3/4\\) \\(P( A&#39;_m \\cup A&#39;_f)=P(A&#39;_m) + P(A&#39;_f) - P(A&#39;_m\\cap A&#39;_f)=1/2+1/2-1/4=3/4\\) In the contingency table, \\(P( A&#39;_m \\cup A&#39;_f)\\) corresponds to the addition of the three cells in bold (method 1 above). The probability is also given by the addition of all cells but the \\(1/4\\) from the top left (method 2), or by adding the marginals and subtracting \\(P(A&#39;_m\\cap A&#39;_f)\\) that has been added twice (method 3). Color is a dominant trait because it masks albinism when present in only one allele. We can also say that albino is a recessive trait. Therefore, the probabilities for color and albino are given by the ratio of 3 to 1 (3:1), or probability \\(3/4=0.75\\), satisfying the Mendel’s first law (Mendel 1901). We can consider more alleles at other genes that are not mutually exclusive with the inheritance of the albino alleles. For example, if we observe an additional trait (outcome type) with two alleles for the length of the ears (short or long), there will be \\(16\\) possible allele combinations, for all the possible crossings of paternal alleles. According to Mendel’s second law, if the traits are independent, there will be four possible types of mice with different albino and tail length status, having ratios of 9:3:3:1 (see Exercise 4); with only one possibility in 16 for the albino and long ear mice. William Bateson observed that some traits interfere with each other; for instance, being albino eliminates coat pigmentation. In such cases, inheritance deviates from Mendelian patterns and is considered epistatic, resulting in ratios like 9:3:4 (see Exercise 5), with only three observed colors. After collecting data from several offspring, our scientific interest may lie in determining whether two traits are independent or if one trait suppresses the other, potentially to gain control over the first if harmful. While exact probabilities cannot be derived from relative frequencies, sufficient evidence may still be obtained to decide which scenario is more likely. 3.15 Questions The following data is part of John Snow’s study on the London 1854 cholera epidemic, which was foundational for modern epidemiology and early germ theory (Snow 1855). He famously showed that cholera was transmitted by water and that identified Southwark and Vauxall water supply was the likely source of the epidemic. The following data record the date of death and the water supply source used by 3920 deceased patients. While more deaths were recorded Southwark and Vauxall the number of houses supplied by each company needs to be taken into account for correct conclusions. \\[ \\begin{array}{|l|r|r|r|r|r|r|} \\hline \\textbf{Week Ending} &amp; \\textbf{Total} &amp; \\textbf{S and V} &amp; \\textbf{Lambeth} &amp; \\textbf{Kent} &amp; \\textbf{Other} &amp; \\textbf{Not Ascertained} \\\\ \\hline \\text{Sept 2, 1854} &amp; 670 &amp; 399 &amp; 45 &amp; 38 &amp; 72 &amp; 116 \\\\ \\text{Sept 9, 1854} &amp; 972 &amp; 580 &amp; 72 &amp; 45 &amp; 62 &amp; 213 \\\\ \\text{Sept 16, 1854} &amp; 856 &amp; 524 &amp; 66 &amp; 48 &amp; 44 &amp; 174 \\\\ \\text{Sept 23, 1854} &amp; 724 &amp; 432 &amp; 72 &amp; 28 &amp; 62 &amp; 130 \\\\ \\text{Sept 30, 1854} &amp; 383 &amp; 228 &amp; 25 &amp; 19 &amp; 24 &amp; 87 \\\\ \\text{Oct 7, 1854} &amp; 200 &amp; 121 &amp; 14 &amp; 10 &amp; 9 &amp; 46 \\\\ \\text{Oct 14, 1854} &amp; 115 &amp; 69 &amp; 8 &amp; 3 &amp; 6 &amp; 29 \\\\ \\hline \\textbf{Total} &amp; \\textbf{3920} &amp; \\textbf{2353} &amp; \\textbf{302} &amp; \\textbf{191} &amp; \\textbf{279} &amp; \\textbf{795} \\\\ \\hline \\end{array} \\] 1) What is the estimated probability that a patient died on the 16th of September and likely drank from Southwark and Vauxhall’s water supply? \\(\\qquad\\)a: \\(524/3920\\); \\(\\qquad\\)b: \\(524/2353\\); \\(\\qquad\\)c: \\(524\\); \\(\\qquad\\)d: \\(524/856\\) 2) What is the estimated probability that the deceased patient was not from the borough of Lambeth? \\(\\qquad\\)a: \\(302/3920\\); \\(\\qquad\\)b: \\(3618/3920\\); \\(\\qquad\\)c: \\(795/3920\\); \\(\\qquad\\)d: \\(279/3920\\) 3) What is the marginal probability of dying in October? \\(\\qquad\\)a: \\(3605/100\\); \\(\\qquad\\)b: \\(315/3920\\); \\(\\qquad\\)c: \\(315/100\\); \\(\\qquad\\)d: \\(3605/3920\\) 4) What is the marginal probability that Kent water company supplied a deceased patient? \\(\\qquad\\)a: \\(13/191\\); \\(\\qquad\\)b: \\(191\\); \\(\\qquad\\)c: \\(191/3920\\); \\(\\qquad\\)d: \\(191/100\\) 5) What is the probability of dying in September or from an unknown water source? \\(\\qquad\\)a: \\(3680/3920\\); \\(\\qquad\\)b: \\(3634/3920\\); \\(\\qquad\\)c: \\(3620/3920\\); \\(\\qquad\\)d: \\(3611/3920\\) 3.16 Exercises 3.16.0.1 Classical probability: Exercise 1 Write the table of joint probability for the results of rolling two dice; In the rows write the results of the first die and in the columns the results of the second die. What is the probability of drawing \\((3, 4)\\) ? (A:1/36) What is the probability of rolling \\(3\\) and \\(4\\) with any of the two dice? (A:2/36) What is the probability of rolling \\(3\\) on the first die or \\(4\\) on the second? (To:11/36) What is the probability of rolling \\(3\\) or \\(4\\) with any dice? (A:20/36) Write the probability table for the result of the add of two dice. Assume that the outcome of each die is equally likely. Verify that it is: \\[ P_i= \\begin{cases} \\frac{i-1}{36},&amp; i \\in \\{2,3,4,5,6, 7\\} \\\\ \\frac{13-i}{36},&amp; i \\in \\{8,9,10,11,12\\} \\\\ \\end{cases} \\] 3.16.0.2 Frequentist probability: Exercise 2 The result of a randomized experiment is to measure the severity of misophonia and the state of depression of a patient. Misophonia severity: \\(S_M:\\{M_ 0,M _1,M_2,M_3,M_4\\}\\) Depression: \\(S_ D:\\{ D&#39;, D\\}\\)) Write the contingency table for the absolute frequencies (\\(n_{ M,D }\\)) for a study on a total of 123 patients in which it was observed 100 individuals did not have depression. No individual with misophonia 4 and without depression. 5 individuals with grade 1 misophonia and no depression. The same number as the previous case for individuals with depression and without misophonia . 25 individuals without depression and grade 3 misophonia . The number of misophonics without depression for grades 2 and 0 were distributed equally . The number of individuals with depression and misophonia increased progressively in multiples of three, starting at 0 individuals for grade 1. Answer the following questions: How many individuals had misophonia ? (A:83) How many individuals had grade 3 misophonia ? (A:31) How many individuals had grade 2 misophonia without depression? (A:35) Write down the contingency table for relative frequencies \\(f_{ M,D }\\). Suppose \\(N\\) is large and the absolute frequencies estimate the probabilities \\(f_{ M,D }=\\hat {P}(M \\cap D)\\). Answer the following questions: What is the marginal probability of severity 2 misophonia ? (A: 0.3) What is the probability of not being misophonic and not being depressed? (A:0.284) What is the probability of being misophonic or depressed? (A: 0.715) What is the probability of being misophonic and being depressed? (A: 0.146) Describe in spoken language the results with probability 0. 3.16.0.3 Exercise 3 We have carried out a randomized experiment \\(10\\) times, which consists of recording the sex and vital status of patients with some type of cancer after 10 years of diagnosis. We got the following results \\[ \\begin{array}{ccc} \\mathbf{Patient} &amp;\\mathbf{Sex} &amp; \\mathbf{Status} \\\\ 1 &amp; male &amp; dead \\\\ 2 &amp;male &amp; dead \\\\ 3 &amp;male &amp; dead \\\\ 4 &amp;female&amp; alive \\\\ 5 &amp;male &amp; dead \\\\ 6 &amp;female&amp; alive \\\\ 7 &amp;female&amp; dead \\\\ 8 &amp;female&amp; alive \\\\ 9 &amp;male &amp; alive \\\\ 10 &amp;male &amp; alive \\\\ \\end{array} \\] Create the contingency table for the number (\\(n_{ i,j }\\)) of observations of each result (\\(A,B\\)) Create the contingency table for the relative frequency (\\(f_{ i,j }\\)) of the results What is the marginal frequency of being a man? (R/0.6) What is the marginal frequency of being alive? (R/0.5) What is the frequency of being alive or being a woman? (R/0.6) 3.16.0.4 Exercise 4 Consider an additional gene in mice that determines ear length, with short ears being the dominant trait. Each mouse inherits alleles (\\(X_m\\), \\(X_f\\), \\(Y_m\\), \\(Y_f\\)), where \\(X\\) represents the alleles for albinism and \\(Y\\) the alleles for ear length, inherited from the mother (subscript m) and the father (subscript f). Suppose both parents are dihybrid—i.e., (\\(A,A&#39;,B,B&#39;\\))—and therefore express the dominant phenotypes: non-albino with short ears. Show that if the traits for albinism and ear length assort independently, then there are four distinct phenotypic outcomes among the offspring that appear in a 9:3:3:1 ratio, illustrating Mendel’s second law of independent assortment. 3.16.0.5 Exercise 5 Consider an additional gene in mice that determines black or grey color, with grey the dominant trait. Each mouse inherits alleles (\\(X_m\\), \\(X_f\\), \\(Y_m\\), \\(Y_f\\)), where \\(X\\) represents the alleles for albinism and \\(Y\\) the alleles for grey and black colors, inherited from the mother (subscript m) and the father (subscript f). Suppose both parents are dihybrid—i.e., (\\(A,A&#39;,B,B&#39;\\))—and therefore express the dominant phenotype: non-albino with grey color. Show that if the albinism removes pigmentation, then there are three distinct phenotypic outcomes among the offspring that appear in a 9:3:4 ratio, illustrating an apistatic model of inheritance. 3.16.0.6 Exercise 6 From the second form of the addition rule, obtain the first and the third form. What is the third form addition rule for the probability of three events \\(P(A \\cup B \\cup C)\\)? 3.17 Practice Load misophonia data https://alejandro-isglobal.github.io/SDA/data/Misophonia.txt Compute the contingency table of absolute frequencies for misophonia diagnosis (Misophonia severity) and depression (Depression) Compute the contingency table of relative frequencies for misophonia diagnosis and depression Compare the differences with exercise 2. Solutions References "],["conditional-probability.html", "Chapter 4 Conditional probability 4.1 Joint probability 4.2 Statistical independence and correlation 4.3 Conditional probability 4.4 Conditional contingency table 4.5 Statistical independence 4.6 Statistical dependency 4.7 Diagnostic test 4.8 Inverse probabilities 4.9 Bayes’ Theorem 4.10 Questions 4.11 Exercises 4.12 Practice", " Chapter 4 Conditional probability The final result of a random experiment can involve the observation of two or more types of outcomes. In a single realization of a random experiment, we may observe a list of observations of different types, each forming a column in an observation matrix. Repetitions of the random experiment add rows to the matrix. Random experiments thus increase in complexity. Events that are not mutually exclusive naturally support random experiments where multiple types of outcomes can be measured. Joint probabilities of these events extend probability tables into contingency tables for two types of outcomes. This allows us to determine the likelihood of a specific pair of values occurring for the two outcome types. In some experiments, the measurement of one type outcome may provide information about the other, which may be, for instance, technically challenging to ascertain. For example, consider diagnostic tools. In other experiments, one outcome may be easier to control or may provide insights into the probabilities of the other outcome; take for instance the relationship between the temperature of an engine and the power it delivers. In this chapter, we will introduce the concept of conditional probability. Conditional probability will be used to define statistical independence between two types of outcomes. Evaluating statistical independence is crucial for understanding and controlling random experiments. This topic will be explored further in subsequent chapters on hypothesis testing, where we will examine different ways of conditioning experiments and outcomes. Additionally, we will discuss Bayes’ theorem and one of its primary applications: assessing the predictive efficiency of diagnostic tools. 4.1 Joint probability Recall that the joint probability of two events \\(A\\) and \\(B\\) is defined as the probability of their intersection \\[P( A ,B )=P(A \\cap B)\\] Now imagine random experiments that measure simultaneously different types of outcomes: the throw of two dice: (\\(n_1, n_2\\)) height and weight of an individual: \\((h, w)\\) position and speed of a molecule in a gas: \\((x, v)\\) speed and distance of a galaxy: \\((v, d)\\) in glycolysis (braking down of glucose), the time the process takes in the first two reactions (phosphorylation of glucose and isomerization of G6P): (\\(t_1, t_2\\)) In the first four cases both measurements may appear to be simultaneous characteristics of the experiment, and it is clear that we can report one and then the other, or the other way round. In this last case, while the gycolysis reactions always occur in the same order, when running the experiment, we may also report the times of reaction as (\\(t_1 \\cap t_2\\)) or (\\(t_ 2 \\cap t_1\\)). That is, the joint events are commutative, or in the observation matrix, the columns can be reordered. We are now interested in whether the values of one type of outcome conditions the values of the other. 4.2 Statistical independence and correlation In many cases, we are interested in whether certain values of one type of outcome are more likely to occur alongside specific values of another. Our goal is to distinguish between these two scenarios: Independence between events. For example, rolling a 1 on one die does not make it more likely to roll another 1 on a second die. Correlation between events. For example, if a man is tall, he is probably heavy. While independence between two events refers to the probability of one event being unaffected by the other event, dependence implies that the probability of the one event is higher when the other event occurs. Correlation thus refers to the likelihood of observing the two events together, or their joint event, or their constant conjunction. As joint events are commutative, correlation does not imply a temporal or causal relationship between the events. However, if there is a causal relationship then correlation is expected. Example (conductor) Imagine we conducted an experiment to find out if structural flaws in a material affects its electrical conductivity. We repeated the experiment in \\(n\\) specimens and measured both quantities. The observation matrix would look like \\[ \\begin{array}{ccc} \\mathbf{Conductor} &amp; \\mathbf{Structure} &amp; \\mathbf{Conductivity} \\\\ c_1 &amp; \\text{flaws} &amp; \\text{low} \\\\ c_2 &amp; \\text{no flaws} &amp; \\text{high} \\\\ c_3 &amp; \\text{flaws} &amp; \\text{low} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ c_i &amp; \\text{no flaws} &amp; \\text{low*} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ c_n &amp; \\text{flaws} &amp; \\text{high*} \\\\ \\end{array} \\] We may expect that low conductivity occurs more often with flaws than without flaws if the flaws affect conductivity. That is a correlation between the flaws and low conductivity where there is more likelihood for the structure-conductivity pairs with no star. However, we need a tool to assess it. Let us imagine that from the data we obtain the following joint probability table \\[ \\begin{array}{ccc|c} &amp; \\mathbf{Flaws: F} &amp; \\mathbf{No\\, flaws: F&#39;} &amp; \\mathbf{sum} \\\\ \\mathbf{Low\\, conductivity: L} &amp; 0.005 &amp; 0.045 &amp; 0.05 \\\\ \\mathbf{High\\, Conductivity: L&#39;} &amp; 0.095 &amp; 0.855 &amp; 0.95 \\\\ \\hline \\mathbf{sum} &amp; 0.1 &amp; 0.9 &amp; 1 \\\\ \\end{array} \\] where, for example, the joint probability that one conductor has low conductivity (\\(L\\)) and flaws (\\(F\\)) is \\(P(L,F)=0.005\\) and the marginal probabilities are \\(P(L)=P(L, F) + P(L, F&#39;)=0.05\\) \\(P(F)=P(L, F) + P(L&#39;, F)= 0.1\\). 4.3 Conditional probability We will say that low conductivity is independent of having structural flaws if the probability of having low conductivity (\\(L\\)) is the same whether it has flaws (\\(F\\)) or not (\\(F&#39;\\) ) . Let us first consider only the materials that have flaws. Among those materials that have flaws (\\(F\\)), what is the estimated probability that they have low conductivity? Think of the observation matrix, and consider counting the number of specimens with low conductivity and flaws (\\(n_{L,F}\\)), and all the specimens with flaws \\(n_{F}\\). Then the fraction of materials with flaws that have low conductivity is \\(\\frac{n_{L,F}}{n_{F}}=\\frac{n_{L,F}/n}{n_{F}/n}= \\frac{f_{L,F}}{f_{F}}\\) \\[= \\frac{\\hat{P}( L,F )}{\\hat{P}(F)}\\] where \\(f_{L,F}\\) are \\(f_{F}\\) the relative frequencies for \\(n_{L,F}\\) and \\(n_{F}\\); respectively, and \\(n\\) the total number of specimens tested. The last term is the estimated probability that an specimen has low conductivity if it has flaws. In the limit when \\(n \\rightarrow \\infty\\) is \\[P(L| F)= \\frac{P(L,F)}{P(F)}=\\frac{P(L\\cap F)}{P(F)}\\] where the symbol \\(|\\) states that the random experiment of measuring conductivity is now run only on the specimens that we know they have flaws. Note that in \\(P(L| F)\\), \\(F\\) is not considered as an outcome but as a condition of the experiment. However, this new probability is computed from a random experiment where both \\(L\\) and \\(F\\) are outcomes \\(P(L\\cap F)\\), by reducing the sample space to the specimens with the condition \\(F\\). Definition: The conditional probability of an event \\(A\\) given an event \\(B\\), denoted \\(P(B| A)\\) , is \\[P(A|B) = \\frac{P(A\\cap B)}{P(B)}.\\] We can prove that conditional probability satisfies the axioms of probability. The point here is that conditional probability can be understood as a probability where the conditional event \\(B\\) is fixed. That is, reconsidering the random experiment where the event \\(B\\) is a condition and part of its design. In our example, the new experiment includes only specimens with structural flaws. 4.4 Conditional contingency table If we divide the columns of the joint probability table by their marginal probabilities \\(P(F)\\) and \\(P(F&#39;)\\), we obtain a conditional contingency table \\[ \\begin{array}{cc|c} &amp; \\mathbf{Flaws: F} &amp; \\mathbf{No\\, flaws: F&#39;} \\\\ \\mathbf{Low\\, conductivity: L} &amp; P(L \\mid F) &amp; P(L \\mid F&#39;) \\\\ \\mathbf{High\\, conductivity: L} &amp; P(L&#39; \\mid F) &amp; P(L&#39; \\mid F&#39;) \\\\ \\mathbf{sum} &amp; 1 &amp; 1 \\end{array} \\] where the column probabilities sum to one. The first column shows the probabilities of low conductivity or not, only of the materials that have flaws (first condition: \\(F\\)). The second column shows the probabilities only for the materials that have no flaws (second condition: \\(F&#39;\\)). This table refers to two different random experiments, one under the first condition and another under the second one. Conditional probabilities are the probabilities of the joint events within each condition. We read them as: \\(P(L| F)\\): Probability of having low conductivity if it has flaws \\(P(L&#39;| F)\\): Probability of not having low conductivity if it has flaws \\(P(L|F &#39;)\\): Probability of having low conductivity if it has no flaws \\(P(L&#39;|F &#39;)\\): Probability of not having low conductivity if it has no flaws 4.5 Statistical independence In our example, the conditional contingency table is \\[ \\begin{array}{cc|c} &amp; \\mathbf{Flaws: F} &amp; \\mathbf{No\\, flaws: F&#39;} \\\\ \\mathbf{Low\\, conductivity:L} &amp; 0.05 &amp; 0.05 \\\\ \\mathbf{High\\, conductivity: L&#39;} &amp; 0.95 &amp; 0.95 \\\\ \\mathbf{sum} &amp; 1 &amp; 1 \\end{array} \\] We note that the conditional probabilities in this table are equal to the marginals \\(P(L)\\) and \\(P(L&#39;)\\) in the joint probability table (Section 4.2) \\(P(L)=P(L| F)= P(L|F&#39;)\\) \\(P(L&#39;)=P(L&#39;| F)= P(L&#39;|F&#39;)\\) This means that the probability of observing low conductivity is not dependent on whether a structural flaw is present. Note that in the joint probability table, the most probable outcome was high conductivity with no flaws, \\(P(L&#39; \\cap F&#39;) = 0.855\\). At first glance, this might suggest a relationship between these two events. However, this apparent relationship arises because both low conductivity and flaws are relatively uncommon events (\\(P(L) = 0.05\\), \\(P(F) = 0.1\\)) compared to their complements. Conditioning on flaws eliminates the disparity between the marginal probabilities and reveals the actual independence of the events. We conclude that, for the physical situation represented by this experiment, low conductivity in the material is not influenced by the presence of structural flaws. Equivalently, high conductivity is independent of the absence of flaws. Definition Two events \\(A\\) and \\(B\\) are statistically independent if either of the equivalent cases occurs: \\(P(A| B)= P(A)\\); \\(A\\) is independent of \\(B\\) \\(P(B| A)= P(B)\\); \\(B\\) is independent of \\(A\\) and by the definition of conditional probability \\(P(A\\cap B)=P(A|B)P(B)=P(A)P(B)\\) This third form is a statement about joint probabilities. It says that we can obtain joint probabilities by multiplying the marginal probabilities. In our original joint probability table (Section 4.2), we can confirm that all the entries of the matrix are indeed the product of the marginal probabilities. For example: \\(P( L \\cap F)=P(F)P(L)\\) and \\(P(L&#39; \\cap F&#39;)=P(L&#39;)P(F&#39;)\\). Therefore, in our experiment, low conductivity is independent of having a structural flaw because their joint probability is the product of the marginals. Example (Mendel´s first law) We aim to confirm that the contingency table for Mendel’s first law represents an independent inheritance model of parental alleles. For mice, the contingency table for the albinism allele \\(A\\) of an offspring from parents, each with alleles (\\(A, A&#39;\\)), is provided in Section 3.14. \\[ \\begin{array}{ccc|c} &amp; \\mathbf{A_p} &amp; \\mathbf{A_p&#39;} &amp; \\mathbf{sum} \\\\ \\mathbf{A_m} &amp; \\mathbf{\\frac{1}{4}} &amp; \\mathbf{\\frac{1}{4}} &amp; \\frac{1}{2} \\\\ \\mathbf{A_m&#39;} &amp; \\mathbf{\\frac{1}{4}} &amp; \\frac{1}{4} &amp; \\frac{1}{2} \\\\ \\hline \\mathbf{sum} &amp; \\frac{1}{2} &amp; \\frac{1}{2} &amp; 1 \\\\ \\end{array} \\] From this table, we see that the probability of getting the albinism allele \\(A\\) from the mother and from the father is the product of the marginals \\(P(A_m, A_p)=P(A_m)P(A_p)=1/4\\). Therefore, the inheritance of the maternal allele is independent from the paternal allele. If we build the conditional contingency table, we will see that inheriting a maternal allele is not conditioned by having inherited a paternal allele: \\(P(A_m| A_p)=P(A_m)=1/2\\). 4.6 Statistical dependency An important example of statistical dependency or correlation is found in the performance of diagnostic tools, where we want to determine the state of a system with possible outcomes satisfactory (yes) unsatisfactory (not) using a test with results positive negative For example, we test a battery to see how long it can last. We perform a stress-strain test on a material to test its elasticity. We perform a PCR (polymerase chain reaction) to see if someone has an infection. 4.7 Diagnostic test Let us consider diagnosing an infection with a new test. The infection status has two possible outcomes: yes (the patient is infected) no (the patient is not infected) The test has two possible outcomes: positive (the test detects the infection) negative (the test does not detect the infection) In the laboratory we run performance studies for the testing. In a control environment, where we know whether a patient has the infection, we run the random experiment of testing for the infection. If the testing has high performance, then its results will give mostly positive outcomes. However, we also expect some negatives as the test is not perfect. We also run second experiment where we test patients without the disease. The conditional probability table describes the likelihoods of each testing outcome for each condition \\[ \\begin{array}{cc|c} &amp; \\mathbf{Infection: yes} &amp; \\mathbf{Infection: no} \\\\ \\mathbf{Test: positive} &amp; P(pos \\mid yes) &amp; P(pos \\mid no) \\\\ \\mathbf{Test: negative} &amp; P(neg \\mid yes) &amp; P(neg \\mid no) \\\\ \\mathbf{sum} &amp; 1 &amp; 1 \\end{array} \\] The conditional table tells us that we are running an experiment where we know that the patient either has the disease or not. These are the controlled conditions of the experiment. Think for instance that you test the diagnostic tool in the hospital where you know the patients are infected. You also test the tool in healthy individuals who you know they are not infected. Let us look at the table entries \\(P(pos| yes)\\) is called the sensitivity of the tool or the probability of a true positive: The probability of testing positive if a patient has the disease. \\(P(neg| no)\\) is called the specificity of the tool or the probability of a true negative: The probability of testing negative if a patient does not have the disease. \\(P(pos| no)\\) is called the probability of a false positive: the probability of testing positive if the patient does not have the disease. \\(P(neg| yes)\\) is called the probability of a false negative: the probability of testing negative if the patient has the disease. High correlation (statistical dependence) between test and infection means high values for probabilities 1 and 2 in the diagonal (successes) and low values for probabilities 3 and 4 off the diagonal (errors). Example (COVID) Now, let us consider a real situation. In the early days of the sars-cov-2 pandemic (2019/2020), there was no measure of the effectiveness of PCRs in detecting the virus. One of the first published studies (Woloshin, Patel, and Kesselheim 2020) found that The PCR had a sensitivity of 70%, in infection condition. The PCR had a specificity of 94%, in non-infected condition. The conditional probability table for this study was \\[ \\begin{array}{cc|c} &amp; \\mathbf{Infection: yes} &amp; \\mathbf{Infection: no} \\\\ \\mathbf{Test: positive} &amp; 0.7 &amp; 0.06 \\\\ \\mathbf{Test: negative} &amp; 0.3 &amp; 0.94 \\\\ \\mathbf{sum} &amp; 1 &amp; 1 \\end{array} \\] Therefore, the errors in the diagnostic tests had probabilities: \\(P(pos| no)= 0.06\\), for a false positive. \\(P(neg| yes)= 0.3\\), for a false negative. Can we say with this data that the test is useful to detect the infection? 4.8 Inverse probabilities We are really interested in finding the probability of being infected if the test is positive \\[P(yes| pos)\\] instead of the reported sensitivity \\(P(pos|yes)\\). In other words, you may want to run an experiment on a patient that you know is positive for the test and determine the probability that he is infected. Note that this experiment is not in a controlled environment of the lab, it is rather a surveying campaign on the population, where you collect all the positives and determine the fraction that were infected. To compute these inverse probability, where the condition is now on the other event (positive), we follow the steps: Recover the contingency table for joint probabilities, multiplying by the marginal \\(P(yes)\\) and \\(P(no)\\) that we need to know or be given. \\[ \\begin{array}{ccc|c} &amp; \\mathbf{Infection: yes} &amp; \\mathbf{Infection: no} &amp; \\mathbf{sum} \\\\ \\mathbf{Test: positive} &amp; P(pos \\mid yes)P(yes) &amp; P(pos \\mid no)P(no) &amp; P(pos) \\\\ \\mathbf{Test: negative} &amp; P(neg \\mid yes)P(yes) &amp; P(neg \\mid no)P(no) &amp; P(neg) \\\\ \\hline \\mathbf{sum} &amp; P(yes) &amp; P(no) &amp; 1 \\\\ \\end{array} \\] Obtain the conditional probability table for rows. \\[ \\begin{array}{cccc} &amp; \\mathbf{Infection: yes} &amp; \\mathbf{Infection: no} &amp; \\mathbf{sum} \\\\ \\mathbf{Test: positive} &amp; P(yes \\mid pos) &amp; P(no \\mid pos ) &amp; 1 \\\\ \\hline \\mathbf{Test: negative} &amp; P(yes \\mid neg) &amp; P(no \\mid neg) &amp; 1\\\\ \\end{array} \\] To compute these probabilities, we use the definition of conditional probabilities for rows instead of columns. We divide the rows of the joint probability table in step 1 by the marginals of the test outcomes: \\(P(pos)\\) and \\(P(neg)\\). For example, for the first cell of the conditional probability table we obtain: \\[P(yes| pos)= \\frac{P(pos|yes)P(yes)}{P(pos)}\\] Note that these is a new random experiment, in which all the individuals tested positive for the PCR fixed and we ask for the probability of infection. While \\(P(pos|yes)\\) was the result of the study (\\(0.7\\)), we still need to know the the marginals \\(P(yes)\\) (prevalence) and \\(P(pos)\\). The prevalence \\(P(yes)\\) needs to be given. In real life, it is obtained from another study. The first prevalence study in Spain, before the summer of 2020, showed that during lock down the estimated probability of infection was \\(P(yes)=0.05\\), and of no infection \\(P(no)=0.95\\). To find the marginal of positive tests \\(P(pos)\\), we can use the definition of marginal and conditional probabilities: \\(P(pos)= P(pos \\cap yes) + P(pos \\cap no)\\) \\[= P(pos| yes)P (yes)+P(pos|no)P(no)\\] This last relation of the marginals is called total probability rule. 4.9 Bayes’ Theorem After substituting the total probability rule into \\(P(yes| pos)\\) , we have \\[P(yes| pos)= \\frac{P(pos|yes)P(yes)}{P(pos|yes)P(yes)+P(pos|no)P(no)}\\] This expression is known as Bayes’ theorem. It allows us to reverse the conditionals: \\[P(pos|yes) \\rightarrow P(yes| pos)\\] This result is important. It allows us to assess a diagnostic tool in a controlled condition (infection status is a lab) and then use it to infer the probability of the condition (infection) when the test is positive. Example (COVID): The test performance was: Sensitivity: \\(P(positive| yes)= 0.70\\) False positive: \\(P(positive| no)= 1- P(neg|no)=0.06\\) The study in the Spanish population gave: \\(P(yes)=0.05\\) \\(P(no)=1-P(yes)=0.95\\). Therefore, the probability of being infected in case of testing positive was: \\[P(yes| pos)= 0.38\\] We conclude that at that time PCR was not very good at diagnosing infections. However, let us now apply Bayes’ theorem to the probability of not being infected if the test was negative. \\[P(no|neg) = \\frac{P(neg|no) P(no )}{ P(neg|no) P(no)+P(neg|yes)P(yes)}\\] Substituting all values gives \\[P(no| neg)= 0.98\\] Therefore, the tests were good for ruling out infections and a fair requirement for travelling. In general, we can have more than two conditioning events, or controlled environments of our experiment. Therefore, Bayes’ theorem says that the probability of an event \\(E_i\\) given the condition \\(B\\): \\[P(E_i| B)= \\frac{P(B|E_i)P(E_i)}{P(B|E_0)P(E_0) +...+ P(B|E_k)P(E_k)}\\] when \\(E_0, E_1, ..., E_k\\) are \\(k\\) mutually exclusive and exhaustive events and \\(B\\) is a condition of interest. Example (Mosophonia) Consider the diagnosis of misophonia. An interesting question is whether misophonia severity \\(4\\) is more likely in depressed patients (\\(D\\)). This corresponds to the conditional probability \\(P(4|D)\\). In a clinical study of misophonia, patients were evaluated for four different severity levels and tested for signs of clinical depression. The probability of severity \\(4\\) in depressed patients, \\(P(4|D)\\), can be expressed using Bayes’ theorem in terms of the probability of depression in severe cases (\\(P(D|4)\\)) and the marginal probabilities \\(P(4)\\) and \\(P(D)\\). The prevalence of depression (\\(P(D)\\)) can be calculated using the total probability rule: \\[ P(D) = P(D|0)P(0) + \\ldots + P(D|4)P(4) \\] We do not know whether depression or misophonia occurs first, but reverse conditional probabilities (e.g., \\(P(D|4)\\)) can be useful as an initial step toward identifying potential causal relationships. Depression relief could then, in some cases, be delivered by misophonia treatment. It is important to note that conditional probability reflects correlation, not causation. However, if misophonia is indeed a cause of depression, we would expect the probability of depression to increase with misophonia severity, following a dose-response relationship. Additionally, establishing causation requires biological or physical plausibility, ruling out reverse causation (e.g., depression causing misophonia), and other supporting criteria. Even so, this may not be sufficient, as confounding factors could influence the observed relationship. For example, if misophonia severity and depression are both higher in women, the association may be due to their shared dependence on gender rather than a direct causal link. Adjustments and further conditioning of the experiment may be necessary, requiring additional control variables or the measurement of more outcome types to disentangle these effects. Conditional tree The terms in the total probability rule can also be organized in a conditional tree. The total probability rule tells us in how many ways I can get the result \\(B\\) from the outcomes \\(A\\) or \\(A&#39;\\) \\[P(B)=P(B|A)P(A)+P(B|A&#39;)P(A&#39;)\\] Trees are an important tool to determine possible ways of causation as they can incorporate several types of conditions. Example (Monty Hall) Monty Hall, the host of a 1970s television show, asked a contestant to choose one of three doors. Behind one door was a car; behind the other two, goats. After the contestant made an initial choice, Monty revealed one of the two remaining doors that hid a goat. He then asked whether the contestant wanted to stick with their original choice or switch to the other unopened door. Once the final decision was made, the chosen door was opened to reveal the prize. The game gained widespread attention after a 1990 column by Marilyn vos Savant in Parade magazine, in which many academics incorrectly argued that switching made no difference. The controversy highlighted the common difficulty of understanding conditional probabilities and sparked a lasting debate within the statistical community: Should the player stick with their choice or switch doors? Consider now the events of choosing a car (\\(c\\)), the first goat (\\(g_1\\)), or the second goat (\\(g_2\\)) in the first door choice: \\(S_1=\\{c, g_1, g_2\\}\\) The probabilities of these events are equally likely \\(P(c)=P(g_1)=P(g_2)=\\frac{1}{3}\\). After Monty Hall opening a door that reveals either goat 1 or goat 2, consider the events that the unopened door has the car, the first goat, or the second one: \\(S_2=\\{C, G_1, G_2\\}\\) The probabilities on these events are conditional probabilities, as they depend on what we have initially chosen: \\[ \\begin{array}{cc|c|c} &amp; \\mathbf{c} &amp; \\mathbf{g_1} &amp; \\mathbf{g_2}\\\\ \\mathbf{C} &amp; 0 &amp; 1 &amp; 1\\\\ \\mathbf{G_1} &amp; \\frac{1}{2} &amp; 0 &amp; 0\\\\ \\mathbf{G_2} &amp; \\frac{1}{2} &amp; 0 &amp; 0 \\\\ \\mathbf{Sum} &amp; 1 &amp; 1 &amp; 1 \\end{array} \\] For instance, the probability that Hall’s unopened door has goat 1 if the car is in the player’s first door choice \\(P(G_1|c)=\\frac{1}{2}\\). Or, the probability that the unopened door has goat 1 if goat 2 is in the first door \\(P(G_1|g_2)=0\\), since Monty Hall should reveal goat 1 in the second door. What is the probability that in the remaining door there is a car? The total probability rule for \\(P(C)\\) is: \\[ P(C)=P(C|g_1)P(g_1)+P(C|g_2)P(g_2)=\\frac{2}{3} \\] Therefore, there is more chance of winning a car if we always switch doors and select the remaining door, as correctly indicated Selvin (Selvin 1975). Note that in the case Monty Hall does not reveal a door, the player remains with two doors to switch from. Then, the player has, for instance, half chance to win a car if they selected goat 1 in the first door \\(P(C|g_1)=P(G_2|g_1)=\\frac{1}{2}\\). Consequently, their overall chances remain the same \\(P(c)=P(C)=\\frac{1}{3}\\) if they decide to switch doors or not. 4.10 Questions The following data is part of John Snow’s study on the London 1854 cholera epidemic (Snow 1855). He famously showed that the lack of water supply hygiene was key in the spread of cholera. The data record the date of death and the water supply source used by 3920 deceased patients. \\[ \\begin{array}{|l|r|r|r|r|r|r|} \\hline \\textbf{Week Ending} &amp; \\textbf{Total} &amp; \\textbf{S and V} &amp; \\textbf{Lambeth} &amp; \\textbf{Kent} &amp; \\textbf{Other} &amp; \\textbf{Not Ascertained} \\\\ \\hline \\text{Sept 2, 1854} &amp; 670 &amp; 399 &amp; 45 &amp; 38 &amp; 72 &amp; 116 \\\\ \\text{Sept 9, 1854} &amp; 972 &amp; 580 &amp; 72 &amp; 45 &amp; 62 &amp; 213 \\\\ \\text{Sept 16, 1854} &amp; 856 &amp; 524 &amp; 66 &amp; 48 &amp; 44 &amp; 174 \\\\ \\text{Sept 23, 1854} &amp; 724 &amp; 432 &amp; 72 &amp; 28 &amp; 62 &amp; 130 \\\\ \\text{Sept 30, 1854} &amp; 383 &amp; 228 &amp; 25 &amp; 19 &amp; 24 &amp; 87 \\\\ \\text{Oct 7, 1854} &amp; 200 &amp; 121 &amp; 14 &amp; 10 &amp; 9 &amp; 46 \\\\ \\text{Oct 14, 1854} &amp; 115 &amp; 69 &amp; 8 &amp; 3 &amp; 6 &amp; 29 \\\\ \\hline \\textbf{Total} &amp; \\textbf{3920} &amp; \\textbf{2353} &amp; \\textbf{302} &amp; \\textbf{191} &amp; \\textbf{279} &amp; \\textbf{795} \\\\ \\hline \\end{array} \\] 1) What is the estimated probability that patient died at the on September the 2nd if use water from Southwark and Vauxhall supplier? \\(\\qquad\\)a: \\(399/3920\\); \\(\\qquad\\)b: \\(399/100\\); \\(\\qquad\\)c: \\(399/2353\\); \\(\\qquad\\)d: \\(399\\); 2) John Snow reports a total of 40046 houses supplied by Southwark and Vauxhall Company and 26107 houses by Lambeth Company. Build a contingency table for deaths and no-deaths in households by company. What is the probability of dying in Southwark and Vauxhall? (compare it with that of Lambeth) \\(\\qquad\\)a: \\(0.011\\); \\(\\qquad\\)b: \\(0.058\\); \\(\\qquad\\)c: \\(0.035\\); \\(\\qquad\\)d: \\(0.062\\) 3) A diagnostic test has a probability of \\(8/9\\) of detecting a disease if the patients are sick and a probability of \\(3/9\\) of detecting the disease if the patients are healthy. If the probability of being sick is \\(1/9\\). What is the probability that a patient is sick if a test detects the disease? \\(\\qquad\\)a: \\(\\frac{8/9}{8/9+3/9}\\times1/9\\); \\(\\qquad\\)b: \\(\\frac{3/9}{8/9+3/9}\\times1/9\\); \\(\\qquad\\)c: \\(\\frac{3/9\\times8/9}{8/9\\times1/9+3/9\\times8/9}\\); \\(\\qquad\\)d: \\(\\frac{8/9\\times1/9}{8/9\\times1/9+3/9\\times8/9}\\); 4) As discussed in the notes, a PCR test for coronavirus had a sensitivity of 70% and a specificity of 94% and in Spain during confinement there was an incidence of 5%. With these data, what was the probability of testing positive in Spain (\\(P(positive)\\)) \\(\\qquad\\)a: \\(0.035\\); \\(\\qquad\\)b: \\(0.092\\); \\(\\qquad\\)c: \\(0.908\\); \\(\\qquad\\)d: \\(0.95\\) 5) With the same data as in question 4, testing positive in the PCR and being infected are not independent events because: \\(\\qquad\\) a: Sensitivity is 70%; \\(\\qquad\\)b: Sensitivity and false positive rate are different; \\(\\qquad\\)c: The false positive rate is 0.06%; \\(\\qquad\\)d: the specificity is 96% 4.11 Exercises 4.11.0.1 Exercise 1 A machine is tested for its performance in producing high-quality turning rods. These are the test results \\[ \\begin{array}{ccc} &amp; \\textbf{Rounded: Yes} &amp; \\textbf{Rounded: No} \\\\ \\textbf{Smooth Surface: Yes} &amp; 200 &amp; 1 \\\\ \\textbf{Smooth Surface: No} &amp; 4 &amp; 2 \\\\ \\end{array} \\] What is the estimated probability that the machine will produce a rod that does not satisfy any quality control? (A: 2/207) What is the estimated probability that the machine will produce a rod that fails at least one quality check? (A: 7/207) What is the estimated probability that the machine will produce rods with a rounded and smooth surface? (A: 200/207) What is the estimated probability that the bar is rounded if the bar is smooth? (A: 200/201) What is the estimated probability that the rod is smooth if it is rounded? (A: 200/204) What is the estimated probability that the rod is neither smooth nor rounded if it does not satisfy at least one quality check? (A: 2/7) Are smoothness and roundness independent events? (No) 4.11.0.2 Exercise 2 Consider a probabilistic version of the quantum bomb tester using a double-slit experiment with a photon (Elitzur and Vaidman 1993). A photon is sent toward a barrier with two slits, and two detectors are placed at positions a and b, as shown in the figure below. Each time a detector captures the photon, it registers a “tick.” Detector a is faulty, with a probability of being “on” of \\(4/5\\). When a is “on,” it detects which slit the photon passed through, and there are three possible outcomes in this random experiment: I) The photon is detected at a, and thus not at b, II) The photon is detected at b with probability \\(2/3\\), III) The photon is not detected at either detector. If a is “off,” the photon passes through both slits as a wave, and interference occurs. In that case, there is only one possible outcome: IV) No detection occurs at b, if we cleverly placed it at a location where, in the absence of which-path detection (i.e., when a is off), destructive interference occurs — meaning the photon will never be detected at b in that case. Outcome 2 is the most curious: the photon is detected at b, yet we know that a was “on” — meaning we obtain information about a without any interaction. Elitzur and Vaidman attached a bomb to detector a to make the effect more dramatic: we may infer that a bomb is “on” without even interacting with it. Remarkably, this outcome occurs with nonzero probability. What is the probability that detector b ticks? (A:\\(4/15\\)) What is the probability that the bomb is “on” if the photon was detected at b? (A:\\(1\\)) What is the probability that the bomb is “on” if the photon was not detected at a (A:\\(2/3\\)) 4.11.0.3 Exercise 3 We developed a test to detect the presence of bacteria in a lake. We found that if the lake contains the bacteria, the test is positive 70% of the time. If there are no bacteria, the test is negative 60% of the time. We implemented the test in a region where we know that 20% of the lakes have bacteria. What is the probability that a lake that tests positive is contaminated with bacteria? (A: 0.30) 4.11.0.4 Exercise 4 A quality test on a random brick is defined by the events: Pass the quality test: \\(E\\), fail the quality test: \\(E&#39;\\) Defective: \\(D\\), non-defective: \\(D&#39;\\) If the diagnostic test has sensitivity \\(P(E|D&#39;)= 0.99\\) and specificity \\(P(E&#39;|D)=0.98\\), and the probability of passing the test is \\(P(E) =0.893\\) then What is the probability that a randomly chosen brick is defective \\(P(D)\\)? (A: 0.1) What is the probability that a brick that has passed the test is actually defective? (A: 0.022) The probability that a brick is not defective and that it fails the test (A: 0.009) Are \\(D\\) and \\(E&#39;\\) statistically independent? (No) 4.12 Practice Load misophonia data from https://alejandro-isglobal.github.io/SDA/data/Misophonia.txt Compute the conditional probability table of misophophonia (Misophonia) given marital status (Marital Status). What is the estimated probability of having misophonia if the patient is married? Compute the conditional probability table of marital status (Marital Status) given misophonia (Misophonia). What is the estimated probability of being married if the patient is misophonic? Solutions References "],["discrete-random-variables.html", "Chapter 5 Discrete Random Variables 5.1 Definition of a Random Variable 5.2 The value of a random variable 5.3 Probability of random variables 5.4 Probability functions 5.5 Probability mass functions 5.6 Mean or expected value 5.7 Variance 5.8 Probability functions for functions of \\(X\\) 5.9 Probability distribution 5.10 Probability function and probability distribution 5.11 Quantiles 5.12 Summary 5.13 Questions 5.14 Exercises", " Chapter 5 Discrete Random Variables Random experiments are better understood as a two-stage processes. The first stage involves the selection of a subject, the production of a specimen, or the execution of a process, which we refer to as the observation unit. The second stage is the observation or measurement process performed on the observation unit. As a result, we acquire a characteristic or quantity from a single run of the experiment. If we perform more than one type of measurement, the observation becomes the joint result of all the measurements. For instance, the pressure, volume, and temperature of a gas. Randomness arises in both stages. The production of an observation unit is typically subject to uncontrolled conditions, making each unit not identical to another. The experimental conditions are expected to remain consistent every time we perform the experiment so that the randomness originates from the same sources. However, if the selection of a subject is not genuinely representative of the experimental conditions, the repetition of the experiment becomes biased, and the relative frequencies may fail to converge to the expected probabilities. For example, selecting a depressed patient from a clinic introduces bias when generalizing the misophonia condition to the broader population. Randomness also arises from measurement error. Even if the production of observation units occurs under identical conditions, additional variation in experimental outcomes may stem from the limitations of the measuring instrument. Consider, for example, measuring the position of a star in the sky using an old sextant. While the observation unit, the star, remains the same and the same sextant is used in each measurement repetition, the outcomes vary due to the instrument’s precision. In this chapter, we focus on the final outcome of a random experiment, specifically the acquisition of a numerical value from an observation unit. We define a random variable as a representation of the entire observation process, encompassing the production of the observation unit, the performance of the measurement, and the acquisition of the final numerical value. We begin with discrete numerical outcomes, defining the probability mass function along with its main properties, such as mean and variance. Building on the abstraction of relative frequencies into probabilities, we further define the probability distribution as the limiting case of the relative cumulative frequency. 5.1 Definition of a Random Variable A random variable is a symbol that represents the process of acquiring a numerical outcome from the execution of a random experiment. We write the random variable in capitals (i.e. \\(X\\)). Think of it as the recipe to obtain an observation from the experiment. It is not the observation, it is the procedure to obtain it. A random variable can be the weight of an animal, that is the process of measuring its mass. \\(10.5Kg\\) is one observation. Definition: A random variable is a function that assigns a real number to an event from the sample space of a random experiment. Remember than an event can be an outcome or a collection of outcomes. When the random variable takes a value, it indicates the realization of an event and the production of a number from the random experiment. Example (Switch) If \\(X \\in \\{0,1\\}\\), we then say \\(X\\) is a random variable that can take the values \\(0\\) or \\(1\\). \\(X\\) may represent the process to determine the state of a switch and encoding it as a number. If we see light in the room, the switch is on and then we assign \\(X=1\\) to the state of the switch. If we see no light, the switch is off and and the variable takes the value \\(X=0\\). 5.2 The value of a random variable We make the distinction between variables in the model space with capital letters, as abstract entities (the state of a switch, the color of a car, the sex of a patient), and the realization of a particular outcome (whether the light of my office is on right now, the red color of my car, the first patient on the hospital today was male). For instance, we say that \\(X=1\\) is the event of observing the random variable \\(X\\) with the value of \\(1\\) \\(X=2\\) is the event of observing the random variable \\(X\\) with the value of \\(2\\) … In general: \\(X=x\\) is the event of observing the random variable \\(X\\) (big \\(X\\)) with value \\(x\\) (little \\(x\\)). We use lower case letters to indicate the numerical outcome of an experiment that is obtained one day in the lab. 5.3 Probability of random variables We are interested in assigning probabilities to the values that random variable can take. For instance, for the dice, we will write the probability table as \\[ \\begin{array}{cc} \\mathbf{x} &amp; \\mathbf{P_i} \\\\ 1 &amp; P(X=1) = 1/6 \\\\ 2 &amp; P(X=2) = 1/6 \\\\ 3 &amp; P(X=3) = 1/6 \\\\ 4 &amp; P(X=4) = 1/6 \\\\ 5 &amp; P(X=5) = 1/6 \\\\ 6 &amp; P(X=6) = 1/6 \\\\ \\end{array} \\] where we explicitly write down the events that the variable takes at given outcome value \\(X=x\\). If \\(X\\) is observing the number after rolling a dice, then \\(P(X=1)\\) is the probability that the roll of a dice gives an outcome value of \\(1\\). 5.4 Probability functions Because (little) \\(x\\) is a numerical quantity, the probabilities of the random variable can be plotted against the outcomes \\(x\\) or written as the mathematical function \\[f(x)=P(X=x)=1/6\\] Therefore, the probabilities of a random experiment can be given in table, in a plot or as a function. 5.5 Probability mass functions We can create any type of probability function if we satisfy Kolmogorov’s probability rules. For a discrete random variable \\(X \\in \\{x_1 , x_2 , .. , x_m\\}\\), a probability mass function that is used to compute probabilities \\(f(x_i)=P(X=x_i)\\) is always positive \\(f(x_i)\\geq 0\\) and its sum over all the values of the variable is \\(1\\): \\(\\sum_{i=1}^m f(x_i)=1\\) Where \\(m\\) is the number of possible outcomes. Note that the definition of \\(X\\) and its probability mass function is general without reference to any experiment. The functions live in the model (abstract) space. Example (Physical simulator) In one urn put \\(8\\) balls following the instructions: mark \\(1\\) ball with number \\(-2\\) mark \\(2\\) balls with number \\(-1\\) mark \\(2\\) balls with number \\(0\\) mark \\(2\\) balls with number \\(1\\) mark \\(1\\) ball with number \\(2\\) And consider performing the following random experiment: Take one ball and read the number. From the classical probability, we can write the probability table, for which we do not need to run any experiment \\[ \\begin{array}{cc} \\mathbf{x} &amp; f(x)=P(X=x) \\\\ -2 &amp; 1/8 \\\\ -1 &amp; 2/8\\\\ 0 &amp; 2/8 \\\\ 1 &amp; 2/8 \\\\ 2 &amp; 1/8 \\\\ \\end{array} \\] The plot is \\(X\\) and \\(f(x)\\) are mathematical objects that may or may not describe the probabilities of real random experiments. We have the freedom to construct them as we want as long as we respect their definition. The urn experiment is an important concept. It is a simple and flexible random experiment that allows us to recreate experiments almost any kind of probability mass function. We refer to it as a physical simulator. By adjusting the number of balls, we can match the propensity of an outcome to a given precision. For example, if the probability of an outcome “black” is \\(3/5 = 0.6\\), then placing \\(3\\) black balls in an urn of \\(5\\) total balls achieves a perfect match. However, if the probability is \\(2/\\pi \\approx 0.6366198\\), a reasonable approximation could involve placing \\(636\\) black balls in a total of \\(1000\\). Clearly balls can be labeled with more than one outcome type (“black” and “spotted”), recreating the measurement of two different quantities on an observation unit. A key feature of simulators is that the probabilities of the experiment are determined by its design. Once we conceive the simulator, there is no need to run the experiment to know the propensities of the outcomes. The usefulness of the simulator is to recreate the observations of the experiment. If the probabilities of the outcomes are known, the simulator allows us to inspect the likely observations of a single run or suitable finite runs of the random experiment. Clearly, a simulator is apt if we run it a large number of times (replacing each time the extracted ball back in the urn) and find that the relative frequencies it produces are close to the probabilities on which it is defined. The simulator works from the abstract to the concrete. An essential point, however, is that only physical simulators can recreate true randomness. The production of randomness is a feature of the physical world. Mathematical functions, on the other hand, cannot produce true randomness. Instead, we use computing software to create simulators based on complex mathematical functions whose outcomes are difficult to predict. These are pseudo-random simulators, which effectively mimic a real physical simulator when the generating functions are sufficiently obscured. 5.6 Mean or expected value Le us start again from a random experiment. From its relative frequencies, we defined the central tendency (average), dispersion (sample variance) and the frequency distribution (\\(F_i\\)) the experiment data. How are these quantities defined for probabilities, understood as frequencies at \\(n=\\infty\\)?. For a discrete random variable, we have \\[lim_{n\\rightarrow \\infty} f_i=f(x_i)\\] as \\(f(x_i)=P_i\\). The relative frequencies thus approach to the probability mass function. If we think in terms of plots, then the heights of bar plot (grey buildings) approach to the heights of the probability mass function (red needles), as we collect more data. For the urn (pseudo-random) simulator example above, we observe that the frequencies for four simulations with \\(n=1000\\) draws of the balls are consistently closer to the probabilities than the simulations with \\(n=30\\) draws. When we discussed summary statistics of data, we defined the center of the observations as a value around which the observations are concentrated. We used the average to measure the center of mass of the data. The average can be computed from the relative frequencies for discrete random variables as \\(\\bar{x}= \\frac{1}{n} \\sum_{i=1}^n x_i = \\sum_{i=1}^m x_i \\frac{n_i}{n}=\\) \\[\\sum_{i=1}^m x_i f_i\\] Note the change from the number of observations \\(n\\) to the number of outcomes \\(m\\) in the summation. Definition The mean (\\(\\mu\\)) or expected value of a discrete random variable \\(X\\), \\(E(X)\\), with mass function \\(f(x)\\) is given by \\[ \\mu = E(X)= \\sum_{i=1}^m x_i f(x_i) \\] It is the center of mass of the probabilities: The point where the probability loading on a road are balanced. From the definition we have \\[\\bar{x} \\rightarrow \\mu\\] in the limit when \\(n \\rightarrow \\infty\\) as the frequency tends to the probability mass function \\(f_i \\rightarrow f(x_i)\\). Note that the expected value is an abstract property of the random experiment. If the probabilities of the random variable are known for all its values, then \\(\\mu\\) is fully determined with no need to run the experiment. While the probabilities are the propensity of the outcomes, the mean is the value at which the outcomes will gather around. Note that the expression “expected value” is apt, as it is a statement about the future run of the experiment. In the next run of the random experiment, we expect the observation to be close to \\(\\mu\\). Example Find the mean of \\(X\\) if its probability mass function \\(f(x)\\) is given by \\[ \\begin{array}{cc} \\mathbf{x} &amp; f(x) \\\\ 0 &amp; 1/16 \\\\ 1 &amp; 4/16 \\\\ 2 &amp; 6/16 \\\\ 3 &amp; 4/16 \\\\ 4 &amp; 1/16 \\\\ \\end{array} \\] \\[ \\mu =E(X)=\\sum_{i=1}^m x_i f(x_i) \\] \\(E(X)=0 \\times \\frac{1}{16} + 1 \\times \\frac{4}{16} + 2 \\times \\frac{6}{16} + 3 \\times \\frac{4}{16} + 4 \\times \\frac{1}{16} =2\\) The mean \\(\\mu\\) is the center of mass of the probability mass function. It does not change. It is a property of the random experiment. However, the average \\(\\bar{x}\\) is the center of mass of the observations (relative frequencies). It changes with different data. While the mean is the value at which the observations will gather, the average is the value at which the observations gathered in the past. 5.7 Variance While we expect the observations to gather about \\(\\mu\\), we also expect them to distance from it due to their random behavior. When we discussed summary statistics of data, we defined the spread of the observations as an average distance from the data average. Definition The variance, written as \\(\\sigma^2\\) or \\(V(X)\\), of a discrete random variable \\(X\\) with mass function \\(f(x)\\) is given by \\[\\sigma^2 = V(X)= \\sum_{i=1}^m (x_i-\\mu)^2 f(x_i)\\] \\(\\sigma=\\sqrt{V(X)}\\) is called the standard deviation of the random variable and it is the expected distance from the mean of the next observation of the random experiment. The variance is the spread of the probabilities about the mean. That is the moment of inertia of probability loadings about the center of mass. Example What is the variance of \\(X\\) if its probability mass function \\(f(x)\\) is given by the previous example? \\[ \\begin{array}{cc} \\mathbf{x} &amp; f(x) \\\\ 0 &amp; 1/16 \\\\ 1 &amp; 4/16 \\\\ 2 &amp; 6/16 \\\\ 3 &amp; 4/16 \\\\ 4 &amp; 1/16 \\\\ \\end{array} \\] \\[\\sigma^2 =V(X)=\\sum_{i=1}^m (x_i-\\mu)^2 f(x_i)\\] \\(V(X)=(0-2)^2 \\times \\frac{1}{16} + (1-0)^2 \\times \\frac{4}{16} + (2-2)^2 \\times \\frac{6}{16} + (3-2)^2 \\times \\frac{4}{16} + (4-2)^2 \\times \\frac{1}{16} =1\\) \\[V(X)=\\sigma^2=1\\] \\[\\sigma=1\\] 5.8 Probability functions for functions of \\(X\\) In many occasions, we will be interested in outcomes that are function of the random variables. Perhaps, we are interested in the square of the number of flu infections, or on the square root of the number of emails in an hour. Definition For any function \\(h\\) of a random variable \\(X\\), with mass function \\(f(x)\\), its expected value is given by \\[ E[h(X)]= \\sum_{i=1}^M h(x_i) f(x_i) \\] This is an important definition that allows us to prove three frequently used properties of the mean and variance: The mean of a linear function is the linear function fo the mean: \\[E(a\\times X +b)= a\\times E(X) +b\\] for \\(a\\) and \\(b\\) scalars (numbers). The variance of a linear function of \\(X\\) is:\\[V(a\\times X +b)= a^2\\times V(X)\\] The variance about the origin is the variance about the mean plus the mean squared: \\[E(X^2)=E(X)^2 + V(X)\\] The last equation is fundamental in mathematical statistics. It is the Pythagoras theorem for statistics where the expected total variation of the data from \\(0\\) (hypotenuse=\\(E(X^2)=E[(X-0)^2]\\)) is the sum of the variation of the mean (\\(E(X)^2\\)) and the variation about the mean (\\(V(X)=E[(X-\\mu)^2]\\)). These last two variations are orthogonal, introducing important geometrical properties into statistical analysis. For more practical applications, the property is useful to compute the variance of a random variable. Example What is the variation of \\(X\\) about the origin, \\(E(X^2)\\), if its probability mass function \\(f(x)\\) is given by \\[ \\begin{array}{ccc} \\mathbf{x} &amp;\\mathbf{x^2} &amp; f(x) \\\\ 0 &amp; 0 &amp;1/16 \\\\ 1 &amp; 1 &amp;4/16 \\\\ 2 &amp; 4 &amp;6/16 \\\\ 3 &amp; 9 &amp;4/16 \\\\ 4 &amp; 16 &amp;1/16 \\\\ \\end{array} \\] \\[E(X^2) =\\sum_{i=1}^m x_i^2 f(x_i)\\] \\(E(X^2)=0^2 \\times \\frac{1}{16} + 1^2 \\times \\frac{4}{16} + 2^2 \\times \\frac{6}{16} + 3^2 \\times \\frac{4}{16} + 4^2 \\times \\frac{1}{16} =5\\) We can also verify: \\[E(X^2)=V(X)+E(X)^2\\] \\(5=1+2^2\\) 5.9 Probability distribution When we discussed summary statistics of data, we also defined the frequency distribution (or the relative cumulative frequency) \\(F_x\\). \\(F_x\\) is an important quantity because it is a function on the continuous range of \\(x\\), even if the outcomes are discrete. Definition: The probability distribution function of a random variable \\(X\\) is defined as \\[F(x)=P(X\\leq x)=\\sum_{x_i\\leq x} f(x_i) \\] That is the accumulated probability up to a given value \\(x\\) Therefore, \\(F(x)\\) is bounded: \\(0\\leq F(x) \\leq 1\\); and always increases: If \\(x \\leq y\\), then \\(F(x) \\leq F(y)\\) Example For the probability mass function: \\[ \\begin{array}{cc} \\mathbf{x} &amp; f(x) \\\\ 0 &amp; 1/16 \\\\ 1 &amp; 4/16 \\\\ 2 &amp; 6/16 \\\\ 3 &amp; 4/16 \\\\ 4 &amp; 1/16 \\\\ \\end{array} \\] The probability distribution is: \\[ F(x)= \\begin{cases} 0, &amp; x \\leq 0\\\\ 1/16,&amp; 0 \\leq x &lt; 1\\\\ 5/16,&amp; 1\\leq x &lt; 2\\\\ 11/16,&amp; 2\\leq x &lt; 3\\\\ 15/16,&amp; 3 \\leq x &lt; 4\\\\ 16/16,&amp; 4 \\leq x\\\\ \\end{cases} \\] For\\(X \\in \\mathbb{R}\\) 5.10 Probability function and probability distribution The probability function and distribution are equivalent. They encode the same information. We can get one from the other and vice-versa \\[f(x_i)=F(x_i)-F(x_{i-1})\\] with \\[f(x_1)=F(x_1)\\] for \\(X\\) taking values in \\(x_1 \\leq x_2 \\leq ... \\leq x_n\\) Example From probability distribution: \\[ F(x)= \\begin{cases} 1/16,&amp; \\text{if } 0 \\leq x &lt; 1\\\\ 5/16,&amp; 1\\leq x &lt; 2\\\\ 11/16,&amp; 2\\leq x &lt; 3\\\\ 15/16,&amp; 4\\leq x &lt; 5\\\\ 16/16,&amp; x \\leq 5\\\\ \\end{cases} \\] We can obtain the probability mas function. \\(f(0)=F(0)=1/16\\) \\(f(1)=F(1)-f(0)=5/32-1/32=4/16\\) \\(f(2)=F(2)-f(1)-f(0)=F(2)-F(1)=6/16\\) \\(f(3)=F(3)-f(2)-f(1)-f(0)=F(3)-F(2)=4/16\\) \\(f(4)=F(4)-F(3)=1/16\\) 5.11 Quantiles Finally, we can use the probability distribution \\(F(x)\\) to define the median and the quartiles of the random variable \\(X\\). In general, we define the q-quantile as the value \\(x_{q}\\) under which we have accumulated \\(q\\times 100\\%\\) of the probability \\[q=\\sum_{x_i\\leq x_q} f(x_i) = F (x_q)\\] The median is the value \\(x_{0.5}\\) such that \\(q=0.5\\) \\[F(x_{0.5})=0.5\\] or \\[x_{0.5}=F^{-1}(0.5)\\] The \\(0.05\\)-quantile is the value \\(x_{0.05}\\) such that \\(q=0.05\\) \\[x_{0.05}=F^{-1}(0.05)\\] The \\(0.25\\)-quantile is first quartile the value \\(x_{0.25}\\) such that \\(q=0.25\\) \\[x_{0.25}=F^{-1}(0.25)\\] In the plots for the probability distribution \\(F(X)\\), the median is the distance \\(x\\) at which \\(F(x)\\) has gone up \\(50\\%\\) of the height. And the first quartile is the distance at which \\(F(x)\\) has gone up \\(25\\%\\) of the height. 5.12 Summary This is a graphical summary of the correspondence between abstract quantities in the model space, defined on the outcomes, and observed quantities in the data space, defined on the observations. \\[ \\begin{array}{|c|c|} \\hline \\textbf{Data (Observed)} &amp; \\textbf{Model (Abstract)} \\\\ \\hline f_i=\\frac{n_i}{n} &amp; f(x_i)=P(X=x_i) \\\\ \\hline F_i=\\sum_{k \\leq i} f_k &amp; F(x_i)=P(X \\leq x_i) \\\\ \\hline \\bar{x}=\\frac{\\sum_{j=1}^n x_j}{n} &amp; \\mu= \\sum_{i=1}^m x_i f(x_i)\\\\ \\hline s^2=\\frac{\\sum_{j=1}^n (x_j-\\bar{x})^2}{N-1} &amp; \\sigma^2=\\sum_{i=1}^m (x_i-\\mu)^2 f(x_i) \\\\ \\hline s &amp; \\sigma \\\\ \\hline m_2=\\frac{\\sum_{j=1}^n x_j^2}{n} &amp; E(X^2)=\\sum_{i=1}^m x_i^2 f(x_i) \\\\ \\hline \\end{array} \\] Note that: \\(j=1...n\\) index the observations of the random variable \\(X\\) \\(i=1...m\\) index the outcomes of the random variable \\(X\\). Here is a useful list of properties \\(\\sum_{i=1}^m f(x_i)=1\\) \\(f(x_i)=F(x_i)-F(x_{i-1})\\) \\(E(a\\times X +b)= a\\times E(X) +b\\); for \\(a\\) and \\(b\\) scalars. \\(V(a\\times X +b)= a^2\\times V(X)\\) \\(E(X^2)=V(X)+E(X)^2\\) 5.13 Questions 1) For a probability mass function is not true that \\(\\qquad\\)a: the addition of their image values is 1; \\(\\qquad\\)b: its values can be interpreted as probabilities of events; \\(\\qquad\\)c: it is always positive; \\(\\qquad\\)d: cannot take value 1; 2) A value of a random variable is \\(\\qquad\\)a: an observation of a random experiment; \\(\\qquad\\)b: the frequency of an outcome of a random experiment; \\(\\qquad\\)c: an outcome of a random experiment; \\(\\qquad\\)d: a probability of an outcome; 3) The estimated value of a probability \\(\\hat{P_i}\\) is equal to the probability \\(P_i\\) when the number of repetitions of the random experiment is \\(\\qquad\\)a: large; \\(\\qquad\\)b: infinite; \\(\\qquad\\)c: small \\(\\qquad\\)d: zero; 4) If a probability mass function is symmetric around \\(x=0\\) \\(\\qquad\\)a: The mean is lower than the median; \\(\\qquad\\)b: The mean is greater than the median; \\(\\qquad\\)c: The mean and the median are equal; \\(\\qquad\\)d: The mean and the median are different from 0; 5) The mean and variance \\(\\qquad\\)a: are inversely proportional; \\(\\qquad\\)b: are expected values of functions of \\(X\\); \\(\\qquad\\)c: of a linear function are the linear function of the mean and the linear function of the variance; \\(\\qquad\\)d: change when we repeat the random experiment; 5.14 Exercises 5.14.0.1 Exercise 1 Consider the following random variable \\(X\\) over the outcomes \\[ \\begin{array}{cc} \\textbf{Outcome} &amp; x \\\\ a &amp; 0 \\\\ b &amp; 0 \\\\ c &amp; 1.5 \\\\ d &amp; 1.5 \\\\ e &amp; 2 \\\\ f &amp; 3 \\\\ \\end{array} \\] If each outcome is equally probable then what is the probability mass function of \\(x\\)? Find: \\(P(X&gt;3)\\) \\(P(X=0\\, \\cup \\, X=2 )\\) \\(P(X \\leq 2)\\) 5.14.0.2 Exercise 2 Given the probability mass function \\[ \\begin{array}{cc} x &amp; f(x) \\\\ 10 &amp; 0.1 \\\\ 12 &amp; 0.3 \\\\ 14 &amp; 0.25 \\\\ 15 &amp; 0.15 \\\\ 17 &amp; ? \\\\ 20 &amp; 0.15 \\\\ \\end{array} \\] what is its expected value and standard deviation? (A: 14.2; 2.95) 5.14.0.3 Exercise 3 Given the probability distribution for a discrete variable \\(X\\) \\[ F(x)= \\begin{cases} 0, &amp; x &lt; -1 \\\\ 0.2,&amp; x \\in [-1,0)\\\\ 0.35,&amp; x \\in [0,1)\\\\ 0.45,&amp; x \\in [1,2)\\\\ 1,&amp; x \\geq 2\\\\ \\end{cases} \\] find \\(f(x)\\) find \\(E(X)\\) and \\(V(X)\\) (A:1; 1.5) what is the expected value and variance of \\(Y=2X+3\\) (A: 6) what is the median and the first and third quartiles of \\(X\\)? (A:2,0,2) 5.14.0.4 Exercise 4 We are testing a system to transmit digital pictures. We first consider the experiment of sending \\(3\\) pixels and having as possible outcomes events such like \\((0,1,1)\\). This is the event of receiving the first pixel with no error, the second with error and third with error. List in one column the sample space of the random experiment. In the a second column assign the random variable that counts the number of errors transmitted for each outcome Consider that we have a totally noisy channel, that is any outcome of three pixels is equally likely. What is the probability of receiving \\(0\\), \\(1\\), \\(2\\), or \\(3\\) errors in the transmission of \\(3\\) pixels? (A: 1/8; 3/8; 3/8; 1/8) Sketch the probability mass function for the number of errors What is the expected value for the number of errors? (A:1.5) What is its variance? (A: 0.75) Sketch the probability distribution What is the probability of transmitting at least 1 error? (A:7/8) "],["continous-random-variables.html", "Chapter 6 Continous Random Variables 6.1 Probabilities of continuous random variables 6.2 Relative frequencies 6.3 Probability Density Function 6.4 Total area under the curve 6.5 Probabilities of continous variables 6.6 Probability distribution 6.7 Probability plots 6.8 Mean 6.9 Variance 6.10 Functions of \\(X\\) 6.11 Exercises", " Chapter 6 Continous Random Variables Most experiments involve the measurement of continuous quantities. Probabilities associated with continuous outcomes require special treatment. While continuous observations are constrained by the precision of the instrument, the outcomes themselves are not limited by experimental constraints. Instead, they represent the possible values a random variable can take, as if determined by an instrument with infinite precision. Continuous random variables can take values with any number of decimal places—even infinitely many. Moreover, these digits may be irregular but computable, allowing for outcomes such as \\(\\pi\\) or \\(\\sqrt{2}\\). In practice, experiments with infinite precision are impossible. Real experiments introduce randomness, either through measurement error or in the process of generating the observation unit. Small changes in the conditions can generate large outcome differences. As a result, decimal digits will inevitably become blurred beyond a certain point. The extent of this blurring depends on the limitations imposed by technology or nature, which set the boundaries for how finely we can measure. If randomness is allowed to occur at any level of precision due to differences in observation methods, continuous outcomes can be viewed as representing the ultimate scale: the real line. In this chapter, we will study continuous random variables. Continuity is addressed through calculus. Although we will not formally define derivatives or integrals, we will use these tools to generalize the concept of probability and provide a solid foundation for understanding random experiments with continuous outcomes. Specifically, we will define the probability density function, along with its mean and variance. Additionally, similar to discrete random variables, we will introduce the cumulative distribution function. 6.1 Probabilities of continuous random variables When we talked about continuous data, we saw that we had to transform them into discrete variables (bins) to produce relative frequency tables or histograms. Let us see how to define the probabilities of continuous variables taking these partitions into account. Example (misophonia) Let us reconsider the angle of convexity of patients with misophonia (Section 2.21). The angle of convexity of 123 patients was measured. We understood each measurement as the result of a random experiment that we repeated 123 times and that we could describe in a frequency table or in a histogram. To do this, we redefine the results as small regular intervals (bins) and calculate the relative frequency of each interval. \\[ \\begin{array}{ccc} \\mathbf{outcome} &amp; \\mathbf{n_i} &amp; \\mathbf{f_i} \\\\ \\mathrm{[-1.02,3.46]} &amp; 8 &amp; 0.06504065 \\\\ \\mathrm{(3.46,7.92]} &amp; 51 &amp; 0.41463415 \\\\ \\mathrm{(7.92,12.4]} &amp; 26 &amp; 0.21138211 \\\\ \\mathrm{(12.4,16.8]} &amp; 20 &amp; 0.16260163 \\\\ \\mathrm{(16.8,21.3]} &amp; 18 &amp; 0.14634146 \\\\ \\hline \\mathbf{sum} &amp; 123 &amp; 1 \\end{array} \\] 6.2 Relative frequencies Following the frequentist definition of probability. The relative frequency of bin \\((x_i, x_i + \\Delta x)\\) when \\(n \\rightarrow \\infty\\) is the probability that the random variable takes a value between \\(x_i\\) and \\(x_i + \\Delta x\\) \\[lim_{n\\rightarrow\\infty}f_i=lim_{n\\rightarrow\\infty}\\frac{n_i}{n} = P(x_i \\leq X \\leq x_i + \\Delta x)\\] The probability depends now on the length of the bins \\(\\Delta x\\). If we make the bins smaller and smaller then the frequencies get smaller and smaller because it is unlikely to find any observation is such a small bin; that is, \\(n_i \\rightarrow 0\\). As a consequence, the probability also vanishes \\[P(x_i \\leq X \\leq x_i + \\Delta x) \\rightarrow 0\\] when \\(\\Delta x \\rightarrow 0\\). We now see how the frequencies get smaller when we divide the range of \\(X\\) into \\(20\\) bins \\[ \\begin{array}{ccc} \\mathbf{outcome} &amp; \\mathbf{n_i} &amp; \\mathbf{f_i} \\\\ \\mathrm{[-1.02,0.115]} &amp; 2 &amp; 0.01626016 \\\\ \\mathrm{(0.115,1.23]} &amp; 0 &amp; 0.00000000 \\\\ \\mathrm{(1.23,2.34]} &amp; 3 &amp; 0.02439024 \\\\ \\mathrm{(2.34,3.46]} &amp; 3 &amp; 0.02439024 \\\\ \\mathrm{(3.46,4.58]} &amp; 2 &amp; 0.01626016 \\\\ \\mathrm{(4.58,5.69]} &amp; 4 &amp; 0.03252033 \\\\ \\mathrm{(5.69,6.8]} &amp; 11 &amp; 0.08943089 \\\\ \\mathrm{(6.8,7.92]} &amp; 34 &amp; 0.27642276 \\\\ \\mathrm{(7.92,9.04]} &amp; 12 &amp; 0.09756098 \\\\ \\mathrm{(9.04,10.2]} &amp; 4 &amp; 0.03252033 \\\\ \\mathrm{(10.2,11.3]} &amp; 3 &amp; 0.02439024 \\\\ \\mathrm{(11.3,12.4]} &amp; 7 &amp; 0.05691057 \\\\ \\mathrm{(12.4,13.5]} &amp; 2 &amp; 0.01626016 \\\\ \\mathrm{(13.5,14.6]} &amp; 6 &amp; 0.04878049 \\\\ \\mathrm{(14.6,15.7]} &amp; 4 &amp; 0.03252033 \\\\ \\mathrm{(15.7,16.8]} &amp; 8 &amp; 0.06504065 \\\\ \\mathrm{(16.8,18]} &amp; 4 &amp; 0.03252033 \\\\ \\mathrm{(18,19.1]} &amp; 9 &amp; 0.07317073 \\\\ \\mathrm{(19.1,20.2]} &amp; 3 &amp; 0.02439024 \\\\ \\mathrm{(20.2,21.3]} &amp; 2 &amp; 0.01626016 \\\\ \\hline \\mathbf{sum} &amp; 123 &amp; 1 \\end{array} \\] Let us just go to the limit and take \\(\\Delta =0\\) then \\(P(x_i \\leq X \\leq x_i)=P(X=x_i)=0\\). This means that probability that a continuous random variable takes one single value is \\(0\\). While this may sound puzzling, think of the continuous real line. Imagine a random variable with possible values between \\(0\\) and \\(5\\). What is the chance that the random experiment gives the exact value of \\(\\pi\\), with its infinite decimal places? 6.3 Probability Density Function We define a quantity at a point \\(x\\) that is the amount of probability per unit distance that we would find in an infinitesimal bin \\(dx\\) at \\(x\\) \\[f(x)= \\frac{P(x\\leq X \\leq x+dx)}{dx}\\] \\(f(x)\\) is called the probability density function. Density functions are how we deal with continuity. When we make the bin really small \\(dx\\) is small and the probability also. The ratio of these two small values, however, is constant at a given \\(x\\). What is the amount of mass in a particular point?. Zero. But the mass density at that point takes a finite value that can be different from zero. We can talk about probability densities as we talk about mass densities. The probability density at one point takes a finite value that can be different from zero. Therefore, the probability that the random variable \\(X\\) takes a value between \\(x\\) and \\(x+dx\\) is given by \\[P(x\\leq X \\leq x+dx)= f(x) dx\\] Definition For a continuous random variable \\(X\\), a probability density function is such that The function is positive: \\[f(x) \\geq 0\\] The probability that \\(X\\) takes any value is 1: \\[\\int_{-\\infty}^{\\infty} f(x) dx = 1\\] 3) The probability that \\(X\\) is within an interval is the area under the curve: \\[P(a\\leq X \\leq b)=\\int_{a}^{b} f(x) dx\\] The properties make sure that \\(f(x)dx\\) satisfies Kolmogorov’s axioms of a probability measure. The probability density function is a step forward in the abstraction of probabilities: we add the continuous limit \\[dx \\rightarrow 0\\] to the already limit of infinite repetitions \\(n \\rightarrow \\infty\\) of the random experiment. All the properties of probabilities are translated in terms of densities. From descrete random variables, we change summaions by integrations \\[\\sum \\rightarrow \\int\\] and we change point probabilities by probabilities within an interval \\[P(X=x_i)=f(x_i) \\rightarrow P(x\\leq X \\leq x+dx)= f(x)dx\\] Similar to probability mass functions for discrete variables, probability densities are mathematical quantities that do not necessarily represent random experiments. We are free to define them as long as we respect their properties. However, a fundamental interest in statistics is to select the densities that better describe our particular random experiment. 6.4 Total area under the curve Example (Needle drop) Consider the following random experiment: Draw a line in a sheet of paper, hold a needle vertical to the paper and let it drop. Measure the angle at which the needle drops with respect to the line. Different angles are derived from very small variations in the conditions of the experiment. Infinite small variations from the vertical line can give large differences in the angles. This random variation of external conditions is one of the ways that randomness can arise in an experiment. Without performing the experiment, we have no reason to believe that a range of angles is more likely that another, when we aim to let it drop from the vertical line. Therefore the probability density is constant for every angle \\(X\\) is given by \\[ f(x)= \\begin{cases} \\frac{1}{360},&amp; \\text{if } x\\in (0,360)\\\\ 0,&amp; otherwise \\end{cases} \\] Let us verify that the function satisfies the three properties of a probability density. It is evident from the definition that \\(f(x) \\geq 0\\) The probability of observing any angle; that is, that \\(X\\) takes any value, is summing up all the possible probabilities. That is the total area under the curve \\(P(-\\infty\\leq X \\leq \\infty)= \\int_{-\\infty}^{\\infty} f(x) dx = 360\\times \\frac{1}{360}= 1\\) The probability that \\(X\\) takes a value between \\(90\\) and \\(180\\) degrees is area under the curve within the interval \\(P(90 \\leq X \\leq 180) = \\int_{90}^{180} f(x) dx = (180-90)\\times \\frac{1}{360}=0.25\\) 6.5 Probabilities of continous variables For continuous variables, we compute the probability that the variable is between \\(a\\) and \\(b\\). That is \\[P(a \\leq X \\leq b)\\] We saw that for continuous variables, the probability that the experiment gives us a particular real number is zero: \\(P(X=a)=0\\). The probability \\(P(a \\leq X \\leq b)\\) is the area under the curve of \\(f(x)\\) between \\(a\\) and \\(b\\) \\(P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) dx\\) 6.6 Probability distribution The probability distribution \\(F(c)\\) is defined as the accumulation of probability up to the outcome \\(c\\) \\[F(c) = P(X \\leq c)\\] That is the probability that the random variable is less or equal to \\(c\\). We can use \\(F(c)\\) to compute \\(P(a \\leq X \\leq b)\\). Consider that: The probability accumulated up to \\(b\\) is given by \\(F(b) = P(X \\leq b)=\\int_{-\\infty}^bf(x)dx\\) That the probability accumulated up to \\(a\\) is \\(F(a) = P(X \\leq a)\\) Then the probability that the random variable is between \\(a\\) and \\(b\\) is given by the difference in the probability distribution between \\(a\\) and \\(b\\) \\(P(a\\leq X \\leq b) = \\int_a^b f(x)dx=F(b)-F(a)\\) Definition The probability distribution of a continuous random variable is defined as the area under the curve up to \\(a\\) \\[F(a)=P(X\\leq a) =\\int_{-\\infty} ^a f(x)dx\\] \\(F(a)\\) has the properties: It is between \\(0\\) and \\(1\\): \\[F(-\\infty)= 0 \\,\\, and \\,\\,F(\\infty)=1\\] It always increases: \\[F(a)\\leq F(b)\\] if \\(a\\leq b\\) It can be used to compute probabilities: \\[P(a \\leq X \\leq b)=F(b)-F(a)\\] It recovers the probability density: \\[f(x)=\\frac{dF(x)}{dx}\\] We use probability distributions to compute probabilities of a random variable within intervals. \\(F(x)\\) is derivative is the probability density function \\(f(x)\\). Example (Needle drop) For the uniform density function: \\[ f(x)= \\begin{cases} \\frac{1}{360},&amp; \\text{if } x\\in (0,360)\\\\ 0,&amp; otherwise \\end{cases} \\] We find that the probability distribution is \\[ F(x)= \\begin{cases} 0,&amp; x \\leq 0 \\\\ \\frac{x}{360},&amp; \\text{if } x\\in (0,360)\\\\ 1, &amp; 360 \\leq x \\\\ \\\\ \\end{cases} \\] 6.7 Probability plots We can plot the the probability of a random variable in an interval as the area under the density curve. For instance \\[P(90\\leq X \\leq 180)\\] Or, equivalently, we can plot the probability \\(P(90 \\leq X \\leq 180)\\) as the difference in distribution values 6.8 Mean As in the discrete case, the mean is the center of mass of probabilities and tells us the value around which we can expect the next observation of the random experiment. Definition Suppose \\(X\\) is a continuous random variable with probability density function \\(f(x)\\). The mean or expected value of \\(X\\), denoted as \\(\\mu\\) or \\(E(X)\\), is \\[\\mu=E(X)=\\int_{-\\infty}^\\infty x f(x) dx\\] It is the continuous version of the center of mass. Example (Needle drop) The random variable with probability density \\[ f(x)= \\begin{cases} \\frac{1}{360},&amp; \\text{if } x\\in (0,360)\\\\ 0,&amp; otherwise \\end{cases} \\] Has an expected value at \\[E(X)=\\int_{0}^{360} x\\frac{1}{360}dx= \\frac{1}{2}\\frac{x^2}{360} |_0^{360}= 180\\] 6.9 Variance As in the discrete case, the variance measures the dispersion of probabilities around the mean. It is the expected squared distance from the mean, indicating how far we expect the next observation of the random experiment to be. Definition Suppose \\(X\\) is a continuous random variable with probability density function \\(f(x)\\). The variance of \\(X\\), denoted as \\(\\sigma^2\\) or \\(V(X)\\), is \\[\\sigma^2=V(X)=\\int_{-\\infty}^\\infty (x-\\mu)^2 f(x) dx\\] It is the continuous version of the moment of inertia. 6.10 Functions of \\(X\\) In many occasions, we will be interested in outcomes that are functions of the random variables. Perhaps, we are interested in the square of the elongation of a spring, or on the square root of the temperature of an engine. Definition For any function \\(h\\) of a random variable \\(X\\), with mass function \\(f(x)\\), its expected value is given by \\[E[h(X)]= \\int_{-\\infty}^{\\infty} h(x) f(x)dx\\] From this definition we recover the same properties as in the discrete case: The mean of a linear function is the linear function fo the mean: \\[E(a\\times X +b)= a\\times E(X) +b\\] for \\(a\\) and \\(b\\) scalars. The variance of a linear function of \\(X\\) is:\\[V(a\\times X +b)= a^2\\times V(X)\\] The expected squared distance about the origin is the mean squared plus the variance about the mean: \\[E(X^2)=E(X)^2+V(X)\\] 6.11 Exercises 6.11.0.1 Exercise 1 For the probability density \\[ f(x)= \\begin{cases} \\frac{1}{100},&amp; \\text{if } x\\in (0,100)\\\\ 0,&amp; otherwise \\end{cases} \\] compute the mean (A:\\(50\\)) compute the variance using \\(E(X^2)=V(X)+E(X)^2\\) (A:\\(100^2/12\\)) compute \\(P(\\mu-\\sigma\\leq X \\leq \\mu+\\sigma)\\) (A: \\(0.57\\)) What are the first and third quartiles? (A: \\(25\\); \\(75\\)) 6.11.0.2 Exercise 2 Given \\[ f(x)= \\begin{cases} 0, &amp; x &lt; 0 \\\\ ax, &amp; x \\in [0,3] \\\\ b, &amp; x \\in (3,5) \\\\ \\frac{b}{3}(8-x),&amp; x \\in [5,8]\\\\ 0, &amp; x &gt; 8 \\\\ \\end{cases} \\] What are the values of \\(a\\) and \\(b\\) such that \\(f(x)\\) is a continous probability density function? (A: 1/15; 1/5) what is the mean of \\(X\\)? (A:4) 6.11.0.3 Exercise 3 For the probability density \\[ f(x)= \\begin{cases} \\lambda e^{-\\lambda x},&amp; \\text{if } x \\geq 0\\\\ 0,&amp; otherwise \\end{cases} \\] Confirm that this is a probability density Compute the mean (A: 1/\\(\\lambda\\)) Compute the expected value of \\(X^2\\) (A: 2/\\(\\lambda^2\\)) Compute variance (A: 1/\\(\\lambda^2\\)) Find the probability distribution \\(F(a)\\) (A: \\(1-exp(-\\lambda a)\\)) Find the median (A: \\(\\log{2}\\)/\\(\\lambda\\)) 6.11.0.4 Exercise 4 Given the cumulative distribution for a random variable X \\[ F(x)= \\begin{cases} 0, &amp; x &lt; -1 \\\\ \\frac{1}{80}(17+16x-x^2),&amp; x \\in [-1,7)\\\\ 1,&amp; x \\geq 7\\\\ \\end{cases} \\] compute: \\(P(X&gt;0)\\) (A:\\(63/80\\)) \\(E(X)\\) (A:\\(1.93\\)) \\(P(X&gt;0|X&lt;2)\\) (\\(A:28/45\\)) "],["discrete-probability-models.html", "Chapter 7 Discrete Probability Models 7.1 Probability model 7.2 Parametric models 7.3 Uniform probability mass function (one parameter) 7.4 Uniform probability mass function (two parameters) 7.5 Bernoulli trial 7.6 Binomial experiment 7.7 Binomial probability function 7.8 Negative binomial 7.9 Geometric distribution 7.10 Hypergeometric model 7.11 Questions 7.12 Exercises", " Chapter 7 Discrete Probability Models We seek to develop models to describe the probabilities of random experiments. Models are the first elements elements to consider when constructing a theory to explain such experiments. A suitable model not only allows us to describe the likelihoods of outcomes more accurately but also provides valuable insights into the experiment’s properties. These properties often include intrinsic quantities that characterize the experiment, which we may attempt to influence or relate to other natural phenomena. Consider, for instance, the distribution of deaths in London’s Soho during the cholera outbreak of 1854. John Snow observed that the probability of dying from cholera was significantly higher near the Broad Street water pump supplied by the Southwark and Vauxhull Company. By identifying the location as a probabilistic property of the outbreak, he hypothesized that the disease was transmitted through contaminated water. Acting on this reasoning, he had the pump handle removed, which led to a reduction in mortality. While he ruled out airborne transmission, the germ theory of cholera was only achieved by the critical knowledge from the additional experiments of Robert Koch (Lippi and Gotuzzo 2014). In this chapter we will see some probability mass functions that are used to describe common random experiments. When we propose a model and we give the probability of an outcome, we make a prediction for the outcome. That is a statement about the likelihood of its future observation. We will introduce the concept of parameter, as an intrinsic characteristic of the experiment, and of parametric models, as a set of similar experiments that differ only on that characteristic. In particular, we will discuss the uniform and Bernoulli probability functions and how they are used to derive the binomial and negative binomial probability models. We will show how to use Python and R functions to compute the probabilities of these models. 7.1 Probability model A probability model is a probability mass function that may represent the probabilities of a random experiment. Example (Physical simulator) The probability mass function defined by the table \\[ \\begin{array}{cc} x &amp; f(x) \\\\ -2 &amp; \\frac{1}{8} \\\\ -1 &amp; \\frac{2}{8} \\\\ 0 &amp; \\frac{2}{8} \\\\ 1 &amp; \\frac{2}{8} \\\\ 2 &amp; \\frac{1}{8} \\\\ \\end{array} \\] represents the experiment of drawing one ball from an urn where there are two balls with labels: \\(-1, 0, 1\\) and one ball fore each label: \\(-2, 2\\). Example (Dice) The probability mass function \\(f(x)=1/6\\), for \\(x\\in\\{1,2,3,4,5,6\\}\\), represents the throw of a dice. 7.2 Parametric models When we have a random experiment with \\(m\\) possible outcomes, we need to find \\(m\\) numbers to determine the probability mass function. As in the first example above, we needed \\(5\\) values in the column \\(f(x)\\) of the probability table. However, in many cases, we can formulate probability functions \\(f(x)\\) that depend on very few numbers only. As in the second example above, we only needed to know how many possible outcomes the throw of a dice has. Example (Classical probability) A random experiment with \\(m\\) equally likely outcomes has a probability mass function: \\[f(x)=\\frac{1}{m}\\] We only need to know \\(m\\). The numbers we need to know to fully determine a probability function are called parameters. 7.3 Uniform probability mass function (one parameter) The previous example is the classical interpretation of probability, and defines our first parametric model. Definition A random variable \\(X\\) with outcomes \\(\\{1,...m\\}\\) has a discrete uniform distribution if all its \\(m\\) outcomes have the same probability \\[f(x)=\\frac{1}{m}\\] \\(m\\) is the natural parameter of the model. Once we define \\(m\\) for an experiment, we choose a particular probability mass function. The function above is really a family of probability mass functions that depend on \\(m\\): \\(f(x; m)\\). The mean and variance of a variable that follows a uniform distribution are: \\[\\mu= \\frac{m+1}{2}\\] and \\[\\sigma^2= \\frac{m^2-1}{12}\\] which are derived from the definitions. Note that \\(\\mu\\) and \\(\\sigma^2\\) are also parameters. Remember that they are expected value, and the expected squared distance from the expected value. If we know any of them then we can fully determine the distribution. Using the equations above, we can have three different parametrizations of the uniform distribution \\[f(x)=\\frac{1}{m}=\\frac{1}{2\\mu-1}=\\frac{1}{\\sqrt{12\\sigma^2+1}}\\] The two last are cumbersome. The first one is natural, as the parameter \\(m\\) has the simple interpretation of the number of possible outcomes. Let us look at some probability mass functions in the family of uniform parametric models. Here are four members of the family, each characterized by a different \\(m\\) Example (Normality of \\(\\pi\\)) A normal number is a real number with infinitely many decimal places, where each digit appears with equal probability. In 1909, Borel hypothesized that \\(\\pi\\) is a normal number, yet after more than a 100 years mathematicians have not been able to prove it. \\(\\pi\\) has infinite digits and their values can be computed sequentially but the value at any given position is difficult to predict. If \\(\\pi\\) is normal, the numbers from \\(0\\) to \\(9\\) should each appear exactly one-tenth of the time in the infinite sequence of \\(\\pi\\). Borel’s conjecture is that the probability mass function for the digits of \\(\\pi\\) is \\[ f(x) = \\frac{1}{m} \\] where \\(m\\) is the base of the numerical system used to express \\(\\pi\\). If \\(\\pi\\) is expressed in binary, the probability of each digit (0 or 1) should be \\(f(x) = \\frac{1}{2}\\) for \\(x \\in \\{0,1\\}\\). Bailey an colleagues computed four trillion hexadecimal digits of in hexadecimal, where 16 digits are used (from 0 to F), and obtained relative frequencies near \\(\\frac{1}{16}\\) (Bailey et al. 2012). They obtained empirical evidence of what we believe is a truth that could be derived from pure mathematical reasoning. 7.4 Uniform probability mass function (two parameters) Let us introduce a new uniform probability model with two parameters: The minimum and maximum outcomes. If the random variable takes values in \\(\\{a, a+1, ...b\\}\\), where \\(a\\) and \\(b\\) are integers and all the outcomes are equally probable then \\[f(x)=\\frac{1}{b-a+1}\\] as the total number of outcomes is \\(m=b-a+1\\). If the random variable has the probability mass function \\(f(x)\\) above, we then say that \\(X\\) distributes uniformly between \\(a\\) and \\(b\\) and write \\[X \\sim Unif(a,b)\\] Properties The mean and variance of a variable that follows a two parameter uniform distribution are: \\[\\mu= \\frac{b+a}{2}\\] and \\[\\sigma^2= \\frac{(b-a+1)^2-1}{12}\\] To prove this, change variables \\(X=Y+a-1\\), \\(y \\in \\{1,...m\\}\\), and apply the properties of the mean and variance of a linear function. Probability mass functions Let us look at some probability mass functions in the family of uniform parametric models: Example (Children age) Consider the following prediction: What is the probability that a child is 9 years old if chosen at random from a primary school? We propose a model. Based on the setup of the experiment, and depending on the country, the minimum age in primary school is \\(a = 6\\) and the maximum is \\(b = 11\\). If all grades have the same number of students and each grade contains the same number of classes, then the appropriate probabilistic model for this random experiment is a discrete uniform distribution: \\[ X \\sim \\text{Unif}(a = 6, b = 11) \\] That is, \\[ f(x) = \\frac{1}{6} \\quad \\text{for } x \\in \\{6, 7, 8, 9, 10, 11\\}. \\] Therefore, our prediction is that \\(1\\) in \\(6\\) children are 9 years old. The mean and variance of this probability mass function are: \\[ \\mu = 8.5 \\quad \\text{and} \\quad \\sigma^2 = 2.916\\overline{6}. \\] These values are also predictions of the model: they indicate that we expect to observe ages of about \\(\\mu = 8.5\\), with a typical deviations of approximately \\(\\sigma = 1.70\\). Parametric models A model is one particular function \\(f(x)\\) that describes the probabilities of our experiment. If the function depends only on a few parameters to be fully determined then by changing the values of the parameters, we recreate a family of models \\(f(x; a,b)\\). A family of models are models that differ only on those parameters. Ideally, the model and the parameters are interpretable. In our example, \\(a\\) represents the the minimum age at school and \\(b\\) the maximum age.The family represents experiments that differ on those intrinsic characteristics, say the family of different school systems with different levels. Knowledge of the model that represents our experiment is then reduced to knowing the value of the parameters \\(a\\) and \\(b\\). They can be considered as the physical properties or intrinsic characteristics of the experiment. Note that in cases like this, we can fully specify the model parameters by deduction based on the design of the experiment and reasonable assumptions, without the need to run an actual experiment. Here, complete knowledge can be attained through pure reasoning alone, and predictions be made from the armchair. 7.5 Bernoulli trial We now move beyond the equal probability case. Suppose we have a simple model with only two possible outcomes, \\(A\\) and \\(A&#39;\\), which have unequal probabilities. Examples: Recording the sex of a patient who enters a hospital emergency room (\\(A: \\text{male}\\), \\(A&#39;: \\text{female}\\)). Checking whether a manufactured machine is defective or not (\\(A: \\text{defective}\\), \\(A&#39;: \\text{not defective}\\)). Hitting a target (\\(A: \\text{success}\\), \\(A&#39;: \\text{failure}\\)). Transmitting one pixel correctly (\\(A: \\text{yes}\\), \\(A&#39;: \\text{no}\\)). Note that in all these examples, the probability of outcome \\(A\\) is typically unknown and cannot be deduced from the experimental set up. Probability model We will introduce the probability of an outcome (\\(A\\)) as the parameter of the model, yet to be determined. The model can be expressed in three equivalent forms: As a probability table \\[ \\begin{array}{cc} \\textbf{Outcome} &amp; \\textbf{Probability} \\\\ A&#39; &amp; 1 - p \\\\ A &amp; p \\\\ \\end{array} \\] Outcome \\(A&#39;\\) (failure/outcome of no interest) has probability \\(1 - p\\). Outcome \\(A\\) (success/outcome of interest) has probability \\(p\\) (the parameter). As a probability mass function of a random variable \\(K\\) taking values \\(\\{0, 1\\}\\) for \\(A&#39;\\) and \\(A\\), respectively \\[ f(k) = \\begin{cases} 1 - p, &amp; k = 0 \\quad (event \\, A&#39;) \\\\ p, &amp; k = 1 \\quad (event \\, A) \\end{cases} \\] As a concise probability mass function \\[ f(k; p) = p^k (1 - p)^{1 - k} \\] for \\(k \\in \\{0,1\\}\\). We then say that \\(X\\) follows a Bernoulli probability mass function with parameter \\(p\\) \\[X \\sim Bernoulli(p)\\] We call these types of experiments Bernoulli trials. A Bernoulli trial is the simplest form of a random experiment, designed to produce either the observation of a single event or its absence. The defining characteristic of the experiment is the probability \\(p\\) of the event, represented as a parameter that is yet to be determined. The simplicity of the trial should not be underestimated. It is the “atom” of probabilistic models, forming the foundation for more complex models. Moreover, it provides the insight that probabilities are intrinsic properties of the random experiment itself, existing independently of any repetition. Repetition of the experiment is simply the way we reveal these probabilities. Understanding probabilities as abstract constructs (parameters), and demonstrating how they are revealed through the Law of Large Numbers, was Jacob Bernoulli’s great achievement (Bernoulli 2006)—one that laid the foundation for mathematical statistics. As we will see in an example further below. Properties: The mean and variance of a variable that follows a Bernoulli distribution are: \\[\\mu=p\\] and \\[\\sigma^2=p(1-p)\\] Note that the parameter \\(p\\) is the probability of the outcome \\(A\\), which is the same as the value of the probability mass function at \\(1\\) \\[p=P(A)=P(K=1)=f(1)\\] The parameter fully determines the probability mass function, including its mean and variance. Let us look at some probability mass functions in the family of Bernoulli distributions 7.6 Binomial experiment Now we consider the repetitions of the random experiment. When we repeat the random experiment \\(n\\) times, we first count the number of times an outcome was observed (\\(n_A\\)), we then divide by \\(n\\) to obtain the relative frequencies (\\(f_A=n_A/n\\)). In the repetition of the experiment, we obtain one single value for \\(n_A\\). However, if we perform other \\(n\\) repetitions of the experiment \\(n_A\\) changes its value. Consequently, \\(n_A\\) is the observation of a new random variable \\(X=N_A\\). This random variable is a measurement of an outcome of a new random experiment: The repetition of the original experiment \\(n\\) times. When the outcome \\(A\\) is obtained from a Bernoulli trial with probability \\(p\\) then the repetition of the trial \\(n\\) times produces an outcome \\(n_A=x\\) with some probability. We are interested in describing the probability that \\(X\\) takes a given value \\(x\\). Examples (Repetitions of Bernoulli trials): Writing down the sex of \\(n=10\\) patients (\\(A:man\\) and \\(A&#39;:woman\\)) who go into an emergency room of a hospital. What is the probability that at a given time \\(X=9\\) patients are men, when the probability that a patient in the room is a man is \\(p=0.8\\)? Trying \\(n=5\\) times to hit a target (\\(A:success\\) and \\(A&#39;:failure\\)). What is the probability that I hit the target \\(X=5\\) times when I usually hit it \\(25\\%\\) of the times (\\(p=0.25\\))? Transmitting a \\(100\\)-pixel picture with errors (\\(A:error\\) and \\(A&#39;:correct\\)). What is the probability that \\(X=2\\) pixels are errors, when the probability of one error is \\(p=0.1\\)? 7.7 Binomial probability function In what follows, we will assume that the true value of the Bernoulli trial parameter \\(p\\) is known. The question of how to determine \\(p\\) is central to statistics and will be addressed later. When we repeat a Bernoulli trial and stop at \\(n\\), is the value \\(x\\) that gives the total number of observed events \\(A\\) common or rare? what is its probability mass function \\(P(X=x)=f(x)\\)? Example (Transmission of pixels) What is the probability of transmitting \\(X=2\\) errors in a \\(4\\)-pixel picture, if the probability of a single error is \\(p=0.1\\)? Regard the transmission of a \\(4\\)-pixel picture as a random experiment where The transmission of each single pixel is a Bernoulli trial \\[K_i \\sim Bernoulli(p)\\] were \\(k_i \\in \\{no\\,error:0, error:1\\}\\) and, \\(i\\) is the pixel transmitted \\(i=\\{1,2,3,4\\}\\). For instance \\(K_3=0\\) is the event of no error in transmitting pixel \\(3\\). The random variable of the picture is: \\[(K_1, K_2, K_3, K_4)\\] For instance, we may obtain the outcome \\((K_1=0, K_2=1, K_3=0, K_4=1)\\) or \\((0, 1, 0, 1)\\) as a picture with an error in the second and in the fourth pixels, and no errors in the first and third pixels. If we send a new picture, will obtain another 4-size vector of zeros and ones. The random variable for the number of errors \\[X=\\sum_{i=1}^4 K_i\\] counts the number of errors in the picture. Therefore, the possible number of errors in the transmission are values \\(x\\in \\{0,1,2,3,4\\}\\). For example \\(X\\) takes the value \\(2\\) (\\(X=2\\)) for the outcome \\((0, 1, 0, 1)\\) because \\(x=0+1+0+1\\). Before we give the general formula of the probabilities, it is illustrative to first inspect the probabilities of particular number of errors: What is the probability of transmitting a picture with \\(4\\) errors? The probability of \\(4\\) errors is the joint probability of transmitting an error in the \\(1^{st}\\) and \\(2^{nd}\\) and \\(3^{rd}\\) and \\(4^{th}\\) pixel: \\[P(X=4)=P(1,1,1,1)=p\\times p\\times p\\times p=p^4\\] because \\(K_i\\) are independent the probabilities of the errors at each pixel multiply. What is the probability of transmitting a picture with no errors? The probability of \\(0\\) errors is the joint probability of transmitting no error in any pixel: \\[P(X=0)=P(0,0,0,0)=(1-p)(1-p)(1-p)(1-p)=(1-p)^4\\] What is the probability of tranmitting \\(3\\) errors? The probability of \\(3\\) errors is the addition of the probabilities of all possible pictures with \\(3\\) errors: \\[P(X=3)=P(0,1,1,1)+P(1,0,1,1)+P(1,1,0,1)+P(1,1,1,0)=4p^3(1-p)^1\\] because all off these outcomes (pictures) are mutually exclusive. Therefore, the probability of \\(x\\) errors is \\[ f(x)= \\begin{cases} 1\\times p^0(1-p)^4,&amp; x=0 \\\\ 4\\times p^1(1-p)^3,&amp; x=1 \\\\ 6\\times p^2(1-p)^2,&amp; x=2 \\\\ 4\\times p^3(1-p)^1,&amp; x=3 \\\\ 1\\times p^4(1-p)^0,&amp; x=4 \\\\ \\end{cases} \\] or more shortly \\[f(x)=\\binom 4 x p^x(1-p)^{4-x}\\] for \\(x=0,1,2,3,4\\) where \\(\\binom 4 x\\) is the number of possible 4-pixel pictures with \\(x\\) errors. For instance, \\(\\binom 4 2=6\\) in the formula above. Definition: The binomial probability function is the probability mass function of observing \\(x\\) outcomes of type \\(A\\) in \\(n\\) independent Bernoulli trials, where \\(A\\) has the same probability \\(p\\) in each trial. The function is given by \\[f(x)=\\binom n x p^x(1-p)^{n-x}\\] for \\(x=0,1,...n\\) and \\(\\binom n x= \\frac{n!}{x!(n-x)!}\\) is called the binomial coefficient and gives the number of ways one can obtain \\(x\\) outcomes of type \\(A\\) in a set of \\(n\\). When a variable \\(X\\) has a binomial probability function, we say it distributes binomially and write \\[X\\sim Binomial(n,p)\\] where \\(n\\) and \\(p\\) are parameters. Let us look at some probability mass functions in the family of binomial parametric models: Properties: The mean and variance of a variable that follows a Binomial distribution are \\[\\mu=np\\] \\[\\sigma^2=np(1-p)\\] Note that the mean and variance of a binomial variable is \\(n\\) times the mean and variance of a Bernoulli variable. These properties can be demonstrated by the fact that \\(X\\) is the sum of \\(n\\) independent Bernoulli variables. Therefore, \\(E(X)=E(\\sum_{i=1}^n K_i)=np\\) and \\(V(X)=V(\\sum_{i=1}^n K_i)=n(1-p)p\\) The last equation requires independence of the Bernoulli trials. Example (Transmission of pixels) The expected value for the number of errors in the transmission of \\(4\\) pixels is \\(np=4\\times 0.1=0.4\\) when the probability of an error is \\(0.1\\). The variance is \\(n(1-p)p=0.36\\) What is the probability of observing \\(4\\) errors? Since we are repeating a Bernoulli trial \\(n=4\\) times and counting the number of events of type \\(A\\) (errors), when \\(P(A)=p=0.1\\) then \\[X \\sim Binomial(n=4, p=0.1)\\] That is from \\[f(x)=\\binom 4 x 0.1^x(1-0.1)^{4-x}\\] We compte \\(P(X=4)=f(4)=\\binom 4 4 0.1^4 0.9^{0}=0.1^4=10^{-4}\\) which can be done in Python and R with the following code \\[ \\begin{array}{l|l} \\textbf{Python} &amp; \\textbf{R} \\\\ \\texttt{from scipy.stats import binom} &amp; \\texttt{dbinom(4,4,0.1)} \\\\ \\texttt{binom.pmf(4,4,0.1)}&amp; \\\\ \\end{array} \\] What is the probability of observing \\(2\\) errors? \\(P(X=2)=\\binom 4 2 0.1^2 0.9^2=0.0486\\) Computed with the functions: Python: binom.pmf(2,4,0.1) R: dbinom(2,4,0.1) Example (Earth’s rings) Tomkins and colleagues presented evidence suggesting that during the Ordovician period, the Earth may have had rings similar to those of Saturn. These rings were likely formed when a large asteroid became trapped in Earth’s orbit and was subsequently pulverized by tidal forces. Fourteen meteorite impact craters were identified within the time window between 466 and 450 million years ago (Tomkins et al. 2024), all located within a band of \\(\\pm30^\\circ\\) latitude around the equator. To assess the significance of this pattern, Tomkins et al. calculated the probability of observing at least 14 meteorite impacts within that equatorial band, assuming that meteorites fall uniformly over the Earth’s surface. The equatorial band covers approximately \\(29.4\\%\\) of the Earth’s surface area, so the probability of a single impact falling within it is \\(p = 0.294\\). What is the probability that all 14 meteorites fell in the band, when the probability of a single hit is about \\(3\\) in \\(10\\)? Let \\(X\\) be the random variable denoting the number of impacts (out of 14) that fall within the equatorial band. Then, \\[ X \\sim \\text{Binomial}(n=14, p=0.294) \\] with probability mass function: \\[ f(x) = \\binom{14}{x} (0.294)^x (0.706)^{14 - x} \\] The probability of observing all 14 impacts in the band is: \\[ P(X = 14) = f(14) = \\binom{14}{14} (0.294)^{14} \\approx 3.96 \\times 10^{-8} \\] which we can compute with the functions Python: 1-binom.cdf(13, 14, 0.9) R: 1-pbinom(13, 14, 0.296) As reported by Tomkins and colleagues, this probability is extremely low, suggesting that the assumption that the meteorites came from every direction with the same probability is unlikely. This supports the alternative explanation that the meteorites originated from the same direction—possibly from a disintegrating ring that once surrounded the Earth during that time. The presence of such a ring coincides with a global icehouse period, likely caused by the cooling effect of the ring’s shadows. Quantiles We can ask for the values of the binomial variable \\(X\\) that accumulate up to a quarter, half, or three-quarters of the probability. These values correspond to the first quartile, median, and third quartile. More generally, we can determine any level of accumulation: \\[x_q = F^{-1}(q)\\] where \\(q\\) is a quantile, a value between \\(0\\) and \\(1\\) that refers to the proportion of probability accumulated up to it. The interquartile range is \\[\\text{IQR} = x_{0.75}-x_{0.25}\\] For the 4-pixel transmission \\(x_{0.25}=0\\), \\(x_{0.75}=1\\) and \\(\\text{IQR}=1\\), or \\[P(0 \\leq X \\leq 1)=0.5\\] which means that half of the probability is captured between pictures with \\(0\\) and \\(1\\) errors. The inter quartile range can be computed as Python: binom.ppf(0.75, 4, 0.1)- binom.ppf(0.25, 4, 0.1) R: qbinom(0.75, 4, 0.1)- qbinom(0.25, 4, 0.1) Example (Bernoulli’s insight) Bernoulli’s insight is that the probability \\(p\\) of an event \\(A\\) can be experimentally inferred by repeating the experiment \\(n\\) times and computing the relative frequency \\(f_A = \\frac{n_A}{n}\\), where \\(n_A\\) is the number of times \\(A\\) occurs. To formalize this idea, Bernoulli used the properties of the binomial function to compute the interquartile range (IQR) of the relative frequency \\(f_A = \\frac{X}{n}\\). Here, \\(X\\) is the random variable that counts how many times we observe \\(A\\); its realization is \\(n_A\\). Since \\(X \\sim \\text{Binomial}(n, p)\\), we can define its interquartile range as the interval between the 25th and 75th percentiles: \\[ P(x_{0.25} \\leq X \\leq x_{0.75}) = 0.5 \\] Dividing the inequality by \\(n\\), we obtain: \\[ P\\left(\\frac{x_{0.25}}{n} \\leq \\frac{X}{n} \\leq \\frac{x_{0.75}}{n}\\right) = 0.5 \\] Therefore, we can use the binomial distribution to compute the interquartile range of the relative frequency as: \\[ \\text{IQR}(f_A) = \\frac{x_{0.75}}{n} - \\frac{x_{0.25}}{n} \\] For the 4-pixel picture this is simply \\(IQR(f_A)=1/4-0/4=1/4\\). Now, with the help of the binomial function, Bernoulli saw that if we compute this range for larger and larger values of \\(n\\), \\(\\text{IQR}(f_A)\\) gets smaller and smaller Therefore, the two values that capture the middle 50% of the relative frequencies get closer. As \\(n\\) grows, the distribution of \\(f_A\\) becomes more concentrated around \\(p\\) (Bernoulli 2006), meaning the frequency stabilizes near the true probability. In short, he provided a formal justification of \\[ \\lim_{n \\rightarrow \\infty} f_A = p \\] 7.8 Negative binomial We can now repeat the Bernoulli trial in different ways. Imagine that we are interested in counting the well-transmitted pixels (\\(A&#39;\\)) before a given number of errors occur. Say, we can tolerate \\(r\\) errors in the transmission. Our random experiment is now: Repeat Bernoulli trials until we observe the outcome \\(A\\) appears \\(r\\) times. Stop and count how many \\(A&#39;\\) you have got. The outcome of the experiment is the number of \\(A&#39;\\) events before \\(r\\) \\(A\\)’s occur, that is the frequency \\(n_{A&#39;}=y\\). We are interested in finding the probability of observing a particular number of events \\(A&#39;\\), \\(P(Y=y)\\), where \\(Y=N_{A&#39;}\\) is the random variable. Example (Transmission of pixels) What is the probability of observing \\(y\\) well-transmitted (\\(A&#39;\\)) pixels before \\(r\\) errors (\\(A\\))? Let us first find the probability of one particular picture with \\(y\\) number of correct pixels (\\(A&#39;\\)) and \\(r\\) number of errors (\\(A\\)). One observation of the experiment can be schematically reresented as \\[(0,0,1,., 0,1,...0,1)\\] where we consider that there are \\(y\\) zeros and \\(r\\) ones. Therefore, we observe \\(y\\) correct pixels in a total of \\(y + r\\) pixels. The probability of this picture is: \\[P(0,0,1,., 0,1,...0,1)=p^r(1-p)^y\\] Remember that \\(p\\) is the probability for a single pixel error (\\(A\\)). How many pictures can have \\(y\\) correct pixels (\\(0\\)’s) before \\(r\\) errors (\\(1\\)’s)? Note that The last pixel is fixed (marks the end of transmission) The total number of ways in which \\(y\\) number of zeros can be allocated in \\(y + r-1\\) pixels (the last pixel is fixed with value 1) is: \\(\\binom {y + r-1} y\\) Therefore, the probability of observing \\(y\\) \\(0\\)’s before \\(r\\) \\(1\\)’s (each 1 with probability \\(p\\)) is \\[P(Y=y)=f(y)=\\binom {y+r-1} y p^r(1-p)^y\\] for \\(y=0,1,...\\) We then say that \\(Y\\) follows a negative binomial distribution and we write \\[Y\\sim NB(r,p)\\] where \\(r\\) and \\(p\\) are parameters representing the tolerance and the probability of a single error (event \\(A\\)). Properties: The mean and variance of a variable that follows a negative binomial distribution are \\[\\mu = r\\frac{1-p}{p}\\] and \\[\\sigma^2= r\\frac{1-p}{p^2}\\] Let us look at some probability mass functions in the family of negative binomial parametric models: Example (Website) A website has three servers. One server operates at a time and, only when a request fails, another server is used. If the probability of failure for a request is known to be \\(p=0.0005\\) then What is the expected number of successful requests before the three computers fail? Since we are repeating a Bernoulli trial until \\(r=3\\) events of type \\(A\\) (failure) are observed (each with \\(P(A)=p=0.0005\\)) and are counting the number of events of type \\(A&#39;\\) (successful requests) then \\[Y \\sim NB(r=3, p=0.0005)\\] Therefore, the expected number of requests before the system fails is: \\(\\mu=r\\frac{1-p}{p}=3\\frac{1-0.0005}{0.0005}=5997\\) Note that there are actually \\(6000\\) trials. What is the probability of dealing with at most \\(5\\) successful requests before the system fails? We, therefore, want to compute the probability distribution at \\(5\\): \\(F(5)=P(Y\\leq 5)=\\Sigma_{y=0}^5 f(y)\\) \\(=\\sum_{y=0}^5\\binom {y+2} y 0.0005^r0.9995^y\\) \\(=\\binom {2} 0 0.0005^3 0.9995^0 +\\binom {3} 1 0.0005^3 0.9995^1\\) \\(+\\binom {4} 2 0.0005^3 0.9995^2 +\\binom {5} 3 0.0005^3 0.9995^3\\) \\(+\\binom {6} 4 0.0005^3 0.9995^4 +\\binom {7} 5 0.0005^3 0.9995^5\\) \\(= 6.9\\times 10^{-9}\\) which can be done with Pthyon: nbinom.cdf(5,3,0.0005) R: pnbinom(5,3,0.0005) Example (Sex differences in COVID mortality) Men were more than three times more likely to die during the COVID-19 pandemic than women. The fatality ratio was reported as \\(FR = 3.5\\) (Dehingia and Raj 2021). The fatality ratio is defined as the ratio of death probabilities between men and women: \\[ FR = \\frac{P(D \\mid M)}{P(D \\mid W)} \\] From this, we can compute the probability that a deceased patient was a woman using Bayes’ theorem: \\[ P(W \\mid D) = \\frac{P(D \\mid W) \\cdot P(W)}{P(D \\mid W) \\cdot P(W) + P(D \\mid M) \\cdot P(M)} \\] Substituting the fatality ratio and assuming an equal gender distribution in the population (\\(P(W) = P(M) = 0.5\\)): \\[ P(W \\mid D) = \\frac{(1/3.5) \\cdot 0.5}{(1/3.5) \\cdot 0.5 + 0.5} = 0.22 \\] How likely is it that at most 175 men died before 50 women did? We model this using the negative binomial distribution, which gives the number of male deaths required before 50 women have died, assuming each deceased individual is female with probability \\(p = 0.22\\). We want to compute the probability that at most 175 men (\\(X\\)) died before 50 women (\\(r = 50\\)) did: \\[ P(X \\leq 175) = F(175; r = 50, p = 0.22) \\] This gives approximately: \\[ P(X \\leq 175) \\approx 0.49 \\] That is, 175 male deaths is around the median number expected before observing 50 female deaths. Pthyon: nbinom.pmf(175, 50, 0.22) R: pnbinom(175, 50, 0.22) Note that the ratio of deaths at the median, 175 men to 50 women, gives \\(175/50=3.5\\). This matches the fatality ratio, reinforcing the model’s consistency. 7.9 Geometric distribution We call geometric distribution to the negative binomial distribution with \\(r=1\\) The probability of observing \\(A&#39;\\) events before observing the first event of type \\(A\\) is \\[P(Y=y)=f(y)= p(1-p)^y\\] \\[Y\\sim Geom(p)\\] The mean and variance of a variable that follows a geometric distribution are \\[\\mu= \\frac{1-p}{p}\\] and \\[\\sigma^2= \\frac{1-p}{p^2}\\] Note that the mean and variance of a negative binomial variable is \\(r\\) times the mean and variance of a geometric variable. 7.10 Hypergeometric model The hypergeometric model arises from the urn experiment, with a total number fo \\(N\\) balls and, we draw \\(n\\) balls without replacing them back in the urn. If the balls are labeled with \\(A\\) and \\(A&#39;\\), what is the probability of drawing \\(x\\) balls of type \\(A\\) in a total of \\(n\\) draws? The general model is to consider \\(N\\) total balls in a urn. Mark \\(K\\) with label \\(A\\) and \\(N-K\\) with label \\(A&#39;\\). Take out \\(n\\) balls one by one with no replacement in the urn, and then count how many \\(A\\)’s you obtained. The Binomial model can be derived from the Hypergeometric model when we consider that either \\(N\\) is infinite, or that every time we draw a ball we replace it back in the urn. Example (measles) By July 2025, the U.S. reported \\(N=1288\\) confirmed measles cases. Over \\(90\\%\\) of cases involved unvaccinated individuals or those with unknown vaccination status, that is \\(K=1159\\) patients. Of those reported cases, imagine we ask \\(n=200\\) and find that \\(x=183\\) did not vaccinate. What is the probability of the observing at least \\(183\\) unvaccinated people in our survey? Definition: The probability of obtaining \\(x\\) cases (type \\(A\\)) in a sample of \\(n\\), drawn from a population of \\(N\\) where \\(K\\) are cases (type \\(A\\)) is \\(P(X=x)=P(one\\,sample) \\times (Number\\, of\\, ways\\, of\\, obtaining\\, x)\\) \\[=\\frac{1}{\\binom N n}\\binom K x \\binom {N-K} {n-x}\\] where \\(x \\in \\{\\max(0, n-(N-K), ... \\min(K, n) \\}\\). The first limit includes the case where we draw more balls than \\(A&#39;\\)-labeled balls, and the second limit when we draw all he \\(A\\)-labeled balls. We then say that \\(X\\) follows a hypergeometric distribution and write \\[X \\sim Hypergeometric(N,K,n)\\] The hypergeometric model has three parameters. Properties: The mean and variance of a variable that follows a hypergeometric distribution are: \\[\\mu = n \\frac{K}{N} = np\\] and \\[\\sigma^2 = np(1-p)\\frac{N-n}{N-1}\\] \\(p=\\frac{K}{N}\\) is the proportion of chickenpox in a population of size \\(N\\). Note that when \\(N \\rightarrow \\infty\\) or we replace the ball, \\(K=n_A\\) and then we recover the binomial properties. Let us look at some probability mass functions in the family of hypergeometric parametric models: Example (measles) what is the probability that at least \\(183\\) measles patients are unvaccinated in a particular survey of \\(200\\) patients, drawn from a population of \\(1288\\) patients where \\(1159\\) did not vaccinate? Given \\(X \\sim Hypergeometric(N=1159,K=1159,n=200)\\), the probability that we need to compute is \\(1-P(X \\leq 183)=F(183)=0.18\\) That is about \\(18\\%\\) of the 200-patient surveys will report more than \\(187\\) unvaccinated people. Python: 1- hypergeom.cdf(183, 1288, 200, 1159) R: 1- phyper(183, 1159, 1288 - 1159, 200) The solution is the addition of the blue needles in the plot. 7.11 Questions 1) What is the expected value and the variance of the number of failures in \\(100\\) prototypes, when the probability of a failure is \\(0.25\\) \\(\\qquad\\)a: \\(0.25\\), \\(0.1875\\); \\(\\qquad\\)b: \\(25\\), \\(0.1875\\); \\(\\qquad\\)c: \\(0.25\\), \\(18.75\\); \\(\\qquad\\)d: \\(25\\), \\(18.75\\) 2) The number of available tables at lunch time in a restaurant is better described by which probability model \\(\\qquad\\)a: Binomial; \\(\\qquad\\)b: Uniform; \\(\\qquad\\)c: Negative Binomial; \\(\\qquad\\)d: Hypergeometric 3) The expected value of a Binomial distribution is not \\(\\qquad\\)a: \\(n\\) times the expected value of a Bernoulli; \\(\\qquad\\)b: the expected value of a Hypergeometric, when the population is very big; \\(\\qquad\\)c: \\(np\\); \\(\\qquad\\)d: the limit of the relative frequency when the number of repetitions is large 4) Opinion polls for the USA 2022 election give a probability of \\(0.55\\) that a voter favors the republican party. If we conduct our own poll and ask 100 random people on the street, How would you compute the probability that in our poll democrats win the election? \\(\\qquad\\)a:binom.cdf(49, 100, 0.55)=0.13; \\(\\qquad\\)b:1-binom.cdf(49, 100, 0.55)=0.86; \\(\\qquad\\)c:binom.cdf(51, 100, 0.45)=0.90; \\(\\qquad\\)d:1-binom.cdf(51, 100, 0.45)=0.095 5) In an exam a student chooses at random one of the four answers that he does not know. If he doesn’t know \\(10\\) questions what is the probability that more than \\(5\\) (\\(&gt;5\\)) questions are correct? \\(\\qquad\\)a:binom.pmf(5, 10, 0.25)~ 0.05; \\(\\qquad\\)b:binom.cdf(5, 10, 0.75)~ 0.07; \\(\\qquad\\)c:binom.pmf(5, 10, 0.75)~ 0.05; \\(\\qquad\\)d:1-binom.cdf(5, 10, 0.25)~ 0.01 7.12 Exercises 7.12.0.1 Exercise 1 If 20% of the bolts produced by a machine are defective, determine the probability that, out of 4 bolts chosen at random 1 bolts will be defective (A:0:4096) 0 bolts will be defective (A::4096) at most 2 bolts will be defective (A:0:9728) 7.12.0.2 Exercise 2 In a population, the probability that a baby boy is born is \\(p=0.51\\). Consider a family of 4 children What is the probability that a family has only one boy?(A: 0.240) What is the probability that a family has only one girl?(A: 0.259) What is the probability that a family has only one boy or only one girl?(A: 0.4999) What is the probability that the family has at least two boys?(A: 0.7023) What is the number of children that a family should have such that the probability of having at least one girl is more than \\(0.75\\)?(A:\\(n=3&gt;\\log(0.25)/\\log(0.51)\\)) 7.12.0.3 Exercise 3 A search engine fails to retrieve information with a probability \\(0.1\\) If we system receives \\(50\\) search requests, what is the probability that the system fails to answer three of them?(A:0.1385651) What is the probability that the engine successfully completes \\(15\\) searches before the first failure?(A:0.020) We consider that a search engine works sufficiently well when it is able to find information for moe than \\(10\\) requests for every \\(2\\) failures. What is the probability that in a reliability trial our search engine is satisfactory?(A: 0.697) References "],["poisson-and-exponential-models.html", "Chapter 8 Poisson and Exponential Models 8.1 Discrete probability models 8.2 Poissson experiment 8.3 Poisson probability mass function 8.4 Continuous probability models 8.5 Exponential process 8.6 Exponential probability density 8.7 Exponential Distribution 8.8 Questions 8.9 Exercises", " Chapter 8 Poisson and Exponential Models A single natural or artificial process may support different random experiments. Random experiments are often designed to extract information about the process, which is conveniently encoded as a parameter. They may also be designed to predict a particular type of outcome. For instance, if we are interested in knowing the probability of a Bernoulli trial, we can estimate it using the relative frequency of a Binomial experiment. The experiment with the most repetitions will provide a better estimate of the probability. If our goal is to determine the adverse effects of a drug, we might opt for a Binomial experiment and conduct a survey of 1,000 patients undergoing treatment. However, if we are interested in assessing the drug’s safety, we might choose a Negative Binomial experiment, testing patients sequentially and calculating the probability of observing no adverse effects before the first occurrence. The key point, however, is that different experiments can be designed from a similar process, with preferences based on efficiency or the type of outcomes we seek to predict. An important example of experimental models linked to similar phenomena is the relationship between the Poisson and exponential models. The Poisson model describes a discrete random variable that counts the number of events in an interval, while the exponential model describes a continuous random variable that measures the distance between two consecutive events. As we will see, the same underlying phenomenon can lead to different questions, which in turn result in either the Poisson or the exponential model being more appropriate. 8.1 Discrete probability models In the previous chapter, we built complex models from simple ones. At each stage, we introduced a novel concept. Uniform distribution: Classical interpretation of probability. Bernoulli trial: Introduction of the probability \\(p\\) as a parameter (family of models) Binomial distribution: Introduction of the repetition of a random experiment (\\(n\\)-times Bernoulli trials) Poisson distribution: Repetition of a random experiment within a continuous interval, with no control over when or where the Bernoulli trial occurs. The final stage is the Poisson process, which describes the repetition of a random experiment with additional randomness in the timing of occurrences. 8.2 Poissson experiment Imagine that we are observing events that depend on time or distance intervals. for example: cars arriving at a traffic light getting messages on a mobile phone impurities occurring at random in a copper wire Suppose that the events are outcomes of independent Bernoulli trials each appearing randomly on a continuous interval, and we want to count them. What is the probability of observing \\(X\\) events in an interval’s unit (time or distance)? Example (Impurities in a copper wire) Imagine that, for a biomedical sensor, we need to design a wire of high purity copper of level \\(5N\\), defined by \\(10^{15}\\) impurities per \\(cm^3\\). The wire is \\(0.1\\mu m\\) of diameter and \\(1\\mu m\\) long, and at \\(5N\\), \\(78\\) impurities are expected in the wire. Impurities will deposit randomly along a copper wire and we expect that the specimens we produce will change in the number of impurities. We want to determine how likely the number of impurities is higher than some quality control tolerance of \\(100\\) impurities. What is the probability that the the next wire we produce has more than \\(100\\) impurities? 8.3 Poisson probability mass function To calculate the probability mass function \\(f(x)=P(X=x)\\) of the previous example we divide the length of the wire of \\(10\\mu n\\) into \\(10,000nm\\). Nanometers are small enough so either there is or there is not an impurity in each nanometer. each nanometer can be considered as a Bernoulli trial. From the Binomial to the Poisson probability function The probability of observing \\(X\\) impurities in \\(n=10000nm\\) (\\(10\\mu m\\)) approximately follows a binomial distribution \\[f(x) \\sim \\binom n x p^x(1-p)^{n-x}\\] where \\(p\\) is the probability of finding an impurity in a micrometer. The expected value of a Binomial variable is: \\(E(X)=np\\). The expected number of impurities in \\(10\\mu\\) is a number \\(\\lambda=np\\) that we will assume to know. \\(\\lambda\\) is a property of the material. If we substitute \\(p=\\lambda/n\\) in the binomial function we obtain \\[f(x) \\sim \\binom n x \\big(\\frac{\\lambda}{n}\\big)^x(1-\\frac{\\lambda}{n})^{n-x}\\] Now, there could still be two impurities in a micrometer. Therefore, we need to increase the partition of the wire and take \\(n \\rightarrow \\infty\\). In the limit: \\[f(x)= \\frac{e^{-\\lambda}\\lambda^x}{x!}\\] where \\(\\lambda\\) is constant. It is the density of impurities per centimeter, a physical property of the system. \\(\\lambda\\) is therefore the parameter of the probability model. Derivation details: For \\(f(x) \\sim \\binom n x \\big(\\frac{\\lambda}{n}\\big)^x(1-\\frac{\\lambda}{n})^{n-x}\\) in the limit (\\(n \\rightarrow \\infty\\)) \\(\\frac{1}{n^x}\\binom n x =\\frac{1}{n^x}\\frac{n!}{x! (n-x)!}=\\frac{(n-x)!(n-x+1)...(n-1)n}{n^x x! (n-x)!}=\\frac{n(n-1)..(n-x+1)}{n^x x!} \\rightarrow \\frac{1}{x!}\\) \\((1-\\frac{\\lambda}{n})^{n} \\rightarrow e^{-\\lambda}\\) (definition of exponential) \\((1-\\frac{\\lambda}{n})^{-x} \\rightarrow 1\\) Putting everything together then: \\(f(x)= \\frac{e^{-\\lambda}\\lambda^x}{x!}\\) Definition Given an interval in the real numbers events occur at random in the interval the average number of events on the interval is known (\\(\\lambda\\)) if one can find a small regular partition of the interval such that each of them can be considered a Bernoulli trial. Then the random variable \\(X\\) that counts events across the interval is a Poisson variable with probability mass function \\[f(x)= \\frac{e^{-\\lambda}\\lambda^x}{x!}, \\lambda&gt;0\\] which we write as \\[X \\sim Poiss(\\lambda)\\] Properties: When \\(X\\) follows a Poisson model, it has mean and variance \\[\\mu= \\lambda\\] and \\[\\sigma^2= \\lambda\\] Example (Patient hospitalization) What is the probability that four patients are hospitalized with influenza in a day, when the average number of patients hospitalized is \\(1\\)? We have that the random variable that counts the number of influenza patients hospitalized follows a Poisson model \\[X \\sim Poiss(\\lambda)\\] with \\(\\lambda=1\\). That is; it has probability mass function \\[f(x)= \\frac{e^{-1}1^x}{x!}\\] Therefore, the probability that the variable takes value 4 is \\(P(X=4)\\) is \\[f(4; \\lambda=1)= \\frac{e^{-1}1^4}{4!}=0.01532831\\] Which can be computed with Python: poisson.pmf(4,1) R: dpois(4,1) What is the probability of hospitalizing four patients in three days, when the average number of hospitalized patients with influenza is \\(1\\) per day? The unit in which we do the counts has changed from \\(1\\) day to \\(3\\) days, so we have to re-scale \\(\\lambda\\). If before the average number of patients in one day was \\(\\lambda=1\\), the average number of patients in three days is now 3: \\(\\lambda_{3h}=3\\lambda_{1h}=3\\times 1=3\\) We have that the variable is Poisson: \\(X \\sim Poiss(\\lambda_{3h})\\) with \\(\\lambda_{3h}=3\\) and its probability mass function is: \\[f(x)= \\frac{e^{-3}3^x}{x!}\\] Therefore the probability that the variable takes value 4 is \\(P(X=4)\\): \\[f(4; \\lambda=3)= \\frac{e^{-3}3^4}{4!}=0.1680314\\] computed as: Python: poisson.pmf(4,3) R: dpois(4,3) Example (Impurities in a copper wire) What is the probability of counting at least \\(100\\) impurities in a \\(5N\\) copper wire with \\(0.1\\mu m\\) of diameter and \\(1\\mu m\\) long, for which \\(78\\) impurities are expected. We have that the variable is Poisson: \\(X \\sim Poiss(\\lambda)\\) with \\(\\lambda=78\\) and its probability mass function is: \\[f(x)= \\frac{e^{-78}78^x}{x!}\\] and then, we want to compute \\(P(X\\geq 100)=1-P(X &lt; 100)=1-P(X \\leq 99)=1-F(99; \\lambda=78)=0.00934\\) where we use the probability distribution \\(F(x,\\lambda)\\) of the Poisson model. We then likely to discard \\(0.9\\%\\) our production of cables that do not pass the quality control measure. Python: 1-poisson.cdf(99,78) R: 1-ppois(99, 78) Let us look at some probability mass functions in the family of parametric Poisson models: 8.4 Continuous probability models Continuous probability models are probability density functions \\(f(x)\\) of a continuous random variables that we believe describe real random experiments. A probability density function \\(f(x)\\) is positive \\[f(x) \\geq 0\\] allows us to compute probabilities using the area under the curve \\[P(a\\leq X \\leq b)=\\int_{a}^{b} f(x) dx\\] is such that the probability that the outcome is in sample space is \\(1\\) \\[\\int_{-\\infty}^{\\infty} f(x) dx = 1\\] 8.5 Exponential process Let us go back to a Poisson process defined by the probability \\[f(k)=\\frac{e^{-\\lambda}\\lambda^k}{k!}, \\lambda&gt;0\\] for the number of events (\\(k\\)) in an interval. Consider now that we are interested in the length/time we should wait for the first event to occur. We can ask about the probability that the first event occurs after length/time \\(X\\). Therefore, since \\(X\\) is a continuous random variable, we are looking for its probability density function \\(f(x)\\). 8.6 Exponential probability density Consider a Poisson variable that counts the number of events in an interval. The probability of no events (\\(0\\) counts) if an interval has unit \\(x\\) is \\[f(0|x)=\\frac{e^{-x\\lambda}x\\lambda^0}{0!}\\] where we re-scale the units of \\(\\lambda\\) to \\(x\\). Simplifying, we obtain \\[f(0|x)=e^{-x\\lambda}\\] We can treat this as the conditional probability of \\(0\\) events given a distance \\(x\\): \\(f(K=0|X=x)\\). We can apply the Bayes theorem to reverse the probability: \\[f(x|0)=C f(0|x)=C e^{-x\\lambda}\\] \\(C\\) is a constant that collects the marginal divided by total probability rule. With this density and using the probability distribution function, we can calculate the probability that a distance/time of at most \\(x\\) has \\(0\\) counts. This is the probability that we measure a maximum length/time \\(x\\) until the first event. In the copper wire example, this is the probability that we can produce a cable of maximum length \\(x\\) tha is totally pure, with no impurities. Since we can cut the wire at any point and measure the distance until the next impurity, if we decide to cut just after one impurity, this probability also gives the distance between any two consecutive events. We say that the model has no memory. We can start the clock at any time or measure the distance from anywhere, the exponential model does not consider what occurred before. Similar to tossing a coin, probabilities on future heads are not conditioned by previous ones. Definition In a Poisson process with parameter \\(\\lambda\\) the probability of measuring a length/time \\(X\\) between any two consecutive events is given by the probability density \\[f(x)= C e^{-x\\lambda}\\] where \\(C\\) is a constant that ensures \\(\\int_{-\\infty}^{\\infty} f(x) dx =1\\). By integration we find \\(C=\\lambda\\), and therefore \\[f(x)=\\lambda e^{-\\lambda x}, x\\geq 0\\] \\(\\lambda\\) is the parameter of the model also known as the decay rate. Properties: When \\(X\\) follows an exponential model we write \\[X \\sim Exp(\\lambda)\\] and its mean and variance is \\[\\mu=\\frac{1}{\\lambda}\\] and \\[\\sigma^2=\\frac{1}{\\lambda^2}\\] Let us look at a couple of the probability densities in the exponential family 8.7 Exponential Distribution Consider the following questions: In a Poisson process ¿What is the probability that an interval is maximum of size \\(a\\) until the first event? If \\(K \\sim Poisson(\\lambda)\\) counts independent events in an interval then the distance/time until the first event is \\(X \\sim Exp(\\lambda)\\). Note that the same parameter \\(\\lambda\\) is used for both models because they represent different random experiments on the same process. The first one measures a discrete variable and the second a continuous one. The probability until the first event is computed with the probability distribution of the exponential model \\(F(a)=P(X \\leq a)\\) \\[F(a)=\\lambda \\int_\\infty^a e^{-x\\lambda}dx=1-e^{-a\\lambda}\\] 2) In a Poisson process ¿What is the probability of observing an interval minimum of size \\(a\\) until the first event? Using the probability distribution this can be computed as: \\[P(X &gt; a)=1- P(X \\leq a)= 1- F(a) = e^{-a\\lambda}\\] Let us look at a couple of exponential distributions from the exponential family The median \\(x_m\\) is such that \\(F(x_m)=0.5\\). That is \\(x_m=\\frac{\\log(2)}{\\lambda}\\). Example (Radioactive decay) What is the probability of having to wait less than \\(2\\) seconds to detect one radioactive particle from Americium-241 (used in smoke detectors) using a Geiger counter, assuming the decay rate is \\(\\lambda = 3.33\\) particles per second? This is modeled using the exponential distribution, which describes the waiting time between events in a Poisson process. The probability is given by: \\[P(X \\le 2)=F(3.33,\\lambda=2)=0.999\\] computed as Python: expon.cdf(3.33,0,1/2) R: pexp(3.33,2) So, there is approximately a \\(99.9\\%\\) chance that you will detect a particle within 2 seconds from the radioactive material of a smoke detector. Note than Python uses the half life \\(1/\\lambda\\) as argument, rather than \\(\\lambda\\). Example (Earthquake) According to data from VolcanoDiscovery, Japan experiences approximately \\(12.7\\) earthquakes of magnitude \\(6\\) or higher per year. This translates to about \\(1\\) earthquakes per month. What is the probability that we have to wait less than \\(1\\) month for an earthquake of magnitude \\(6-7\\) in Japan? \\(X\\) is the time in years for the next earthquake and therefore \\(X \\sim Exp(\\lambda=1)\\). Therefore, we can compute \\[P(X &lt; 1)= P(X \\le 1) = F(1,\\lambda=1)=0.63\\] Where \\(F(1)\\) is the exponential distribution function above and computed as Python: expon.cdf(1,0,1) R: pexp(1,1) There is about a \\(63\\%\\) chance of an earthquake of magnitude \\(6-7\\) in Japan within a month. 8.8 Questions 1) During WWII in London, the expected number of bombs that hit an area of \\(3km^2\\) was \\(0.92\\). The probability that, in one day, one area received two bombs was \\(\\qquad\\)a:1-poisson.cdf(x=2, lambda=0.92); \\(\\qquad\\)b:poisson.cdf(x=2, lambda=0.92); \\(\\qquad\\)c:1-poisson.pmf(x=2, lambda=0.92); \\(\\qquad\\)d:poisson.pmf(x=2, lambda=0.92) 2) The probability that a passenger has to wait less than 20 minutes until the next bus arrives at her stop is better described by \\(\\qquad\\)a: A poisson model on the number of buses per 20 minutes; \\(\\qquad\\)b: An exponential distribution at 20 minutes with a given expectation of buses per minute; \\(\\qquad\\)c: A binomial model that counts the number of buses per 20 minutes \\(\\qquad\\)d: A normal distribution with an average of buses per 20 minutes and a given standard deviation; 3) From the exponential probability distribution in the figure below, what is the most likely value of the median? \\(\\qquad\\)a: \\(2\\); \\(\\qquad\\)b: \\(3\\); \\(\\qquad\\)c: \\(4\\); \\(\\qquad\\)d: \\(5\\) 8.9 Exercises 8.9.0.1 Exercise 1 The average number of phone calls per hour coming into the switchboard of a company is \\(150\\). Find the probability that during one particular minute there will be 0 phone calls (A:0.082) 1 phone call (A:0.205) 4 or fewer calls (A:0.891) more than 6 phone calls (A:0.0141) 8.9.0.2 Exercise 2 The average number of radioactive particles hitting a Geiger counter in a nuclear energy plant under control is \\(2.3\\) per minute. What is the probability of counting exactly \\(2\\) particles in a minute? (A:0.265) What is the probability of detecting exactly \\(10\\) particles in \\(5\\) minutes? (A:0.112) What is the probability of at least one count in two minutes? (A:0.9899) What is the probability of having to wait less than \\(1\\) second to detect a radioactive particle, after we switch on the detector? (A:0.037) We suspect that a nuclear plant has a radioactive leak if we wait less than \\(1\\) second to detect a radioactive particle, after we switch on the detector. What is the probability that when we visit in \\(5\\) plants that are under control, we suspect that at least one has a leak? (A:0.1744). "],["normal-distribution.html", "Chapter 9 Normal Distribution 9.1 History 9.2 normal density 9.3 Definition 9.4 Probability distribution 9.5 Standard normal density 9.6 Standard distribution 9.7 Standardization 9.8 Questions 9.9 Exercises", " Chapter 9 Normal Distribution In random experiments involving continuous outcomes, randomness can arise from changes in the units of measurement or from measurement error. In the first case, randomness can be understood as variability in the production of the unit. Under similar conditions, production may be influenced by uncontrolled factors. In the second case, the unit can be separated from the measuring process, and randomness can be understood as measurement error. Randomness can be described in a probabilistic model by a parameter, with the variance being the natural choice. Simple assumptions about how outcomes should deviate from the mean led Gauss to discover the normal distribution. While studying the trajectory of a celestial body, he realized that the randomness in measuring its position was due to instrumental error. The normal distribution was therefore first conceived as an error distribution. Later, when applied to anthropometric measurements, it was shown to effectively describe variations among individuals, paving the way for its applications in the life sciences. The importance of the normal distribution stems from the fact that it is the continuous limit of the binomial distribution. Coupled with Bernoulli’s insight that relative frequency approaches the expected value in a Bernoulli trial, the normal distribution supports this behavior for any type of distribution. This principle forms the basis of the central limit theorem, which we will cover in the following chapters. In this chapter, we will introduce the normal probability distribution as originally discovered by Gauss to represent errors in astronomical data and later applied by Galton to represent variations in human height. 9.1 History In 1801 Gauss analyzed the data that obtained by Giuseppe Piazzi for the position of the Ceres, a large asteroid between Mars and Jupiter. At the time Piazzi suspected it was a new planet, as it moved day to day against the fixed stars. In January, it could be seen at the horizon just before dawn. However, as days passed Ceres will rise latter and latter until it could not longer be seen because of the Sun rise. The Sun caught up with it. Gauss understood that the measurements for the position of Ceres had errors. He was therefore interested in finding how the observations were distributed so he could find the most likely orbit. With the orbit, we could derive the mass of the object and then decide whether it was massive as a planet or just a big asteroid. Data was available only for the month of January. After which Ceres would disappear. He wanted to predict six moths latter, once it had slowly transited behind the Sun, where astronomers should point their telescopes just after at dusk. Gauss had to account for the errors in the position of Ceres at a given day due to measurement Gauss supposed that small errors were more likely than large errors. error were symmetrical. The error at a distance \\(-\\epsilon\\) from the real position of Ceres was equally probable than the error at \\(+\\epsilon\\). the most likely position (the one that we believe the most) of Ceres at any given time in the sky was the average of multiple measurements. That was enough to show that the deviations of the observations \\(y\\) from the orbit satisfied the equation (Stahl 2006) \\[\\frac{df(y)}{dy}=-Cyf(y)\\] with \\(C\\) a positive constant. The solution of this differential equation is: \\[f(y)=\\frac{\\sqrt{C}}{\\sqrt{2\\pi}}e^{-\\frac{Cy^2}{2}}\\] 9.2 normal density Gaussian probability density gives the distribution of measurement errors from the actual but unknown position of Ceres in the sky. Let us make a couple of changes to the function. We write the error density from the horizon using the random variable \\(X\\), that is, \\(y=x-\\mu\\). \\(\\mu\\) is the actual but unknown position of Ceres from the horizon. After a change of variable we find the probability density function: \\[f(x)=\\frac{\\sqrt{C}}{\\sqrt{2\\pi}}e^{-C(x-\\mu)^2}\\] We rename the variable \\(C\\) to \\(\\frac{1}{\\sigma^2}\\) We then arrive at the following definition. 9.3 Definition A random variable \\(X\\) defined on the real numbers has a Normal probability density function if it takes the form \\[f(x; \\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, x \\in {\\mathbb R}\\] A normal variable has mean \\[E(X) = \\mu\\] which for Gauss represented the real position of Ceres from the horizon; and variance \\[V(X) = \\sigma^2\\] which represented the dispersion of the error in the observations from the mean that depended on the quality of the telescope and the skill of Piazzi. \\(\\mu\\) and \\(\\sigma\\) are the natural parameters of the model, which completely describe the normal density function. Their interpretation depends on the nature and design of the random experiment. It is important underline right away that the parameters are not observable, they are abstract quantities. This is true not only for the normal model but for every model. The real unique position of Ceres (\\(\\mu\\)) was unknown, Gauss instead had repetitions of a random experiment that measured the position of Ceres \\((x_1, x_2, ...x_n)\\). How can we get to know, or at least learn \\(\\mu\\) from \\((x_1, x_2, ...x_n)\\)? Following Bernouilli’s insight, the best we can do is to take the expected value of \\(X\\) as the average \\(\\bar{x}\\). However, this is the central question from chapter \\(10\\) on wards. Before we arrive there, here, we are going to assume that we know the parameters \\(\\mu\\) and \\(\\sigma\\), so we can use the normal model to make predictions. Let me just point out how deep the problem is. For empiricists, any knowledge comes from experience thus \\(\\mu\\) is an abstraction from data. For idealists, knowledge comes from intellectual insight or reasoning, so \\(\\mu\\) represents a fundamental aspect of reality and the observations are imperfect representations of it. The experimental scientist finds relief knowing that \\(\\bar{x}\\) approaches to \\(\\mu\\) when \\(n\\) is large. When \\(X\\) follows normal probability density function, we say that \\(X\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\) and write \\[X \\sim N(\\mu, \\sigma^2) \\] Let us look at some probability densities in the normal parametric model For Gauss, the change from the red to the yellow curve means that Ceres moved, and the change from the red to the blue curve means that the telescope was less accurate. 9.4 Probability distribution The probability distribution of the normal density: \\[F(a)=P(Z \\leq a)\\] is the error function defined by the area under the curve from \\(-\\infty\\) to \\(a\\) \\[F(a)=\\int_{-\\infty}^{a}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} dx\\] The function is found in most computer programs, and it does not have a closed form of known functions. Example (female height) Adolphe Quetelet applied the normal distribution to anthropometric data, proposing that the average of body measurements represented the “l’homme moyen” (the average man), which he saw as an idealized standard. Francis Galton, influenced by Quetelet, developed the concept of human variability as a defining property of populations. However, he also believed that selective breeding and eugenics could be used to shape populations toward an ideal. While both views are now considered ethically and scientifically incorrect and outdated, the introduction of normal distributions of traits was fundamental to the development of quantitative genetics. In Galton’s famous study on human height (Galton 1886), he estimated that the mean of female height was \\(163cm\\) (\\(64inch\\)) with standard deviation of \\(5.8cm\\) (\\(2.30inch\\)). What is the probability that a woman in Galton’s population is at most \\(150cm\\)? \\(P(X\\le 150)=F(150, \\mu=165, \\sigma=8)=0.012\\) wich can be computed with the code Python: norm.cdf(150, 163, 5.8) R: pnorm(150, 163, 5.8) Therefore, the type of woman that Galton measured, presumably 19th century middle-class English, had only \\(1.2\\%\\) chance to be smaller than \\(150cm\\). What is the probability that a woman’s height in the Galton’s population is between \\(165cm\\) and \\(170cm\\)? \\(P(165 \\le X \\le 170)=F(170, \\mu=165, \\sigma=8)-F(165, \\mu=165, \\sigma=8)=0.2513\\) Python: norm.cdf(170, 163, 5.8)-norm.cdf(165, 163, 5.8) R: pnorm(170, 163, 5.8)-pnorm(165, 163, 5.8) Let us look at the probability distribution function The red bar in the left is the probability in example above. What is the first quartile for female height? The first quartile is defined as: \\[F(x_{0.25}, \\mu=163, \\sigma=5.8)=0.25\\] or \\[x_{0.25}=F^{-1}(0.25, µ=165, \\sigma=8)=159.088\\] Python: norm.ppf(0.25, 163, 5.8) R: qnorm(0.25, 163, 5.8) Let us make the following prediction: If we measure many women in Galton’s population, then about \\(25\\%\\) of them would be at most \\(160cm\\) tall. Note that, while we cannot perform this experiment, as Galton’s population is no longer available, the probabilities, as the likelihood of the outcomes, reasonably describe the intrinsic properties of that experiment. Example (temperature) The statistical theory of gases provides another key example where the variance is interpreted as an intrinsic property of the system, rather than as measurement error. Maxwell proposed that the velocity \\(V\\) of a particle in an ideal gas at equilibrium follows a normal distribution (in one dimension): \\[ f(v; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi} \\, \\sigma} \\, e^{- \\frac{(v - \\mu)^2}{2\\sigma^2}} \\] Assuming the gas is at rest, the mean velocity is \\(\\mu = 0\\). For simplicity, we consider only one spatial dimension. Earlier, Rudolf Clausius had related temperature to the average kinetic energy of gas particles, using the ideal gas law. He showed that: \\[ \\frac{1}{2} k_B T = E\\left(\\frac{1}{2} m V^2\\right) \\] where \\(k_B\\) is Boltzmann’s constant, and \\(m\\) is the mass of a particle. Given Maxwell’s statistical description of molecular velocities, the kinetic energy becomes: \\[ E\\left(\\frac{1}{2} m V^2\\right) = \\frac{1}{2} m \\, E(V^2) = \\frac{1}{2} m \\, \\text{Var}(V) = \\frac{1}{2} m \\, \\sigma^2 \\] Solving for \\(T\\), we obtain: \\[ T = \\frac{m}{k_B} \\sigma^2 \\] Thus, temperature is directly proportional to the variance of particle velocities. This gives a powerful statistical interpretation of thermodynamic temperature: a measure of the spread in molecular motion. Although Maxwell could not directly measure particle speeds, his probabilistic velocity model made accurate predictions about macroscopic properties of gases. For instance, it predicted that the viscosity of a dilute gas should be independent of pressure, a result he later confirmed experimentally. Properties of the Normal distribution the mean \\(\\mu\\) is also the median as it splits the measurements in two. \\(x\\) values that fall farther than 2\\(\\sigma\\) are considered rare (less than \\(5\\%\\)) \\(x\\) values that fall farther than 3\\(\\sigma\\) are considered extremely rare (less than \\(0.2\\%\\)). Example (female height) We can define the limits of common outcomes for the distribution of female height in the population. at a distance of one standard deviation from the mean, we find \\(68\\%\\) of the population \\[P(165-8 \\leq X \\leq 165+8)=P(157 \\leq X \\leq 173)=F(173)-F(157)=0.68\\] at a distance of two standard deviations from the mean, we find \\(95\\%\\) of the population \\[P(165-2 \\times 8 \\leq X \\leq 165+2\\times 8)=F(181)-F(149)=0.95\\] at a distance of three standard deviations from the mean, we find \\(99.7\\%\\) of the population \\[P(165-3 \\times 8 \\leq X \\leq 165+3\\times 8)=F(189)-F(141)=0.997\\] It is customary to say that the distribution of the random variable \\(X\\) is the population distribution. When we talk about a population, we really mean the repetition of the random experiment many, many times with the idea that we exhaust the measurement units; like when we made the prediction for the heights in Galton’s population. If we could measure the entire population of women, at a given time, the histogram of heights from the population would be very close to \\(f(x)\\). This is again the difference between observation (histogram) and abstraction (probability density). The population distribution is then close in the limit to the random variable distribution. Consider, however, that it still makes sense to talk about the probability that a woman in a century’s time has a given height, even if she has not been born and the population is not yet available. Probabilities can then be taken as statements about future or hypothetical events. 9.5 Standard normal density The standard normal density is one member of the normal family, such that \\[f(x; \\mu=0, \\sigma^2=1)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(x)^2}{2}}, x \\in {\\mathbb R}\\] The mean and variance are \\[E (X)= \\mu = 0\\] and \\[V (X)= \\sigma^2 =1\\] When a random variable follows a standard normal probability density, we say that the variable is standard normal and write \\[X \\sim N(0,1)\\] 9.6 Standard distribution The probability distribution of a standard normal variable is \\[\\Phi(x)=F(x)=P(X \\leq a)\\] Because the standard distribution is special and will appear often, we use the letter \\(\\Phi\\) for it. This is the error function defined by \\[\\Phi(x)=\\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{z^2}{2}} dz\\] You can find it in most computer programs. Python: norm.cdf(x) R: prnorm(x) We usually define the limits of the most common outcomes for the normal standard variable The interquartile range \\[P(-0.67 \\leq X \\leq 0.67)=0.50\\] Python: [norm.ppf(0.25), norm.ppf(0.75)] R: c(qnorm(0.25),qnorm(0.75)) \\(95\\%\\) range \\[P(-1.96 \\leq X \\leq 1.96)=0.95\\] Python: [norm.ppf(0.025), norm.ppf(0.975)] R: c(qnorm(0.025),qnorm(0.975)) \\(99\\%\\) range \\[P(-2.58 \\leq X \\leq 2.58)=0.99\\] Python: [norm.ppf(0.005), norm.ppf(0.995)] R: c(qnorm(0.005),qnorm(0.995)) 9.7 Standardization All normal variables can be standardized. This means that if \\(X \\sim N(\\mu, \\sigma^2)\\), then we can transform the variable to a standardized variable \\[Z=\\frac{X-\\mu}{\\sigma}\\] which will have density: \\[f(z)=\\frac{1}{ \\sqrt{2\\pi}}e^{-\\frac{z^2}{2}}\\] Therefore, for any \\(X \\sim N(\\mu, \\sigma^2)\\) \\[Z=\\frac{X-\\mu}{\\sigma} \\sim N(0, 1) \\] We can demonstrate this by replacing \\(x=\\sigma z+\\mu\\) and \\(dx=\\sigma dz\\) in the probability equation \\(P(x\\leq X \\leq x +dx)=P(z\\leq Z \\leq z +dz)\\) \\[=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}dx\\] \\[=\\frac{1}{ \\sqrt{2\\pi}}e^{-\\frac{z^2}{2}} dz\\] One of the advantages of standardization if that we can compute the probability of any normal variable \\(X\\sim N(\\mu, \\sigma^2)\\) using the standard distribution \\(F(a)=P(X&lt;a)=P(\\frac{X-\\mu}{\\sigma}&lt;\\frac{a-\\mu}{\\sigma})\\) \\[=P(Z &lt; \\frac{a-\\mu}{\\sigma})= \\Phi \\big(\\frac{a-\\mu}{\\sigma}\\big)\\] For computing \\(P(a\\leq X \\leq b)\\), we use the property of the probability distributions \\(F(b)-F(a)=P(X\\leq b)-P(X\\leq a)\\) \\[=\\Phi \\big(\\frac{b-\\mu}{\\sigma}\\big)-\\Phi \\big(\\frac{a-\\mu}{\\sigma}\\big)\\] Therefore \\(\\Phi(x)\\) is the only function you need to compute probabilities for any normal random variable. Before computers, this property let statisticians rely on a single set of tables, and it still shows how all the randomness of a normal experiment can be captured by a probability function that is free of model‑specific parameters. 9.8 Questions 1) It is not true that for a normally distributed variable \\(\\qquad\\)a: its mean and median are the same; \\(\\qquad\\)b: the standard probability distribution can be used to compute its probabilities; \\(\\qquad\\)c: its interquartile range is twice its standard deviation; \\(\\qquad\\)d: \\(5\\%\\) of its observations are a distance greater than twice its standard deviation 2) For a normal standard variable \\(\\qquad\\)a: \\(50\\%\\) of its observations are between \\((-0.67,0.67)\\); \\(\\qquad\\)b: \\(2\\%\\) of its observations are lower than \\(-2.58\\); \\(\\qquad\\)c: \\(5\\%\\) of its observations are greater than \\(1.96\\); \\(\\qquad\\)d: \\(25\\%\\) of its observations are between \\((-1.96,-0.67)\\) 3) if we know that \\(\\Phi(-0.8416212)=0.2\\) then what is \\(\\Phi(0.8416212)\\) \\(\\qquad\\)a: \\(0.1\\); \\(\\qquad\\)b: \\(0.2\\); \\(\\qquad\\)c: \\(0.8\\); \\(\\qquad\\)d: \\(0.9\\) 4) the third quartile of a normal variable with mean \\(10\\) and standard deviation \\(2\\) is \\(\\qquad\\)a: norm.ppf(1/3, 10, 2)=9.138545; \\(\\qquad\\)b: norm.ppf(1-0.75, 10, 2)=8.65102 ; \\(\\qquad\\)c: norm.ppf(1-1/3, 10, 2)=10.86145 ; \\(\\qquad\\)d: norm.ppf(0.75, 10, 2)= 11.34898 5) probability that a normal variable with mean \\(10\\) and standard deviation \\(2\\) is in \\((-\\infty,10)\\) is \\(\\qquad\\)a: 0.25; \\(\\qquad\\)b: 0.5; \\(\\qquad\\)c: 0.75; \\(\\qquad\\)d: 1: 9.9 Exercises 9.9.0.1 Exercise 1 Find the area under the standard normal curve in the following cases: Between \\(z=0.81\\) and \\(z=1.94\\) (A:0.182) To the right of \\(z=-1.28\\) (A:0.899) To the right of \\(z=2.05\\) or to the left of \\(z=-1.44\\) (A:0.0951) 9.9.0.2 Exercise 2 What is the probability that a man’s height is at least \\(165\\)cm if the population mean is \\(175\\)cm y the standard deviation is \\(10\\)cm? (A:0.841) What is the probability that a man’s height is between \\(165\\)cm and \\(185\\)cm? (A:0.682) What is the height that defines the \\(5\\%\\) of the smallest men? (A:158.55) References "],["sampling-distributions.html", "Chapter 10 Sampling distributions 10.1 Random sample 10.2 Parameter estimation 10.3 Law of Large Numbers 10.4 Inference 10.5 Sample mean 10.6 Prediction 10.7 Validation 10.8 Sample Sum 10.9 Sample Variance 10.10 Distribution of the Sample Variance 10.11 The \\(\\chi^2\\) Distribution 10.12 Questions 10.13 Exercises", " Chapter 10 Sampling distributions When making predictions, the probabilities of the outcomes of an experiment are considered fundamental properties of the system or process under study. To carry out these predictions, we require a model and its parameters to fully specify the probability mass (or density) function that describes the propensities of the outcomes. These parameters may be derived from the experimental design and the underlying theory supporting the experiment. However, when our goal is not just to make predictions but to test a theory or model, we must be able to estimate the parameters from experimental data and compare them to the theoretical values. It is also common to encounter situations where no established theory predicts the value of a parameter—particularly when it represents an unknown physical property of a system. In such cases, the primary objective is to estimate this parameter and draw conclusions based on its value. As with probabilities, the parameters of a probabilistic model are abstract and unobservable, but they can be partially revealed through repeated experimentation. In this chapter, we study the estimation of the mean and variance of normal distributions using random samples. These two quantities are the natural parameters of the normal distribution. The mean represents the expected value of the experiment and often directly corresponds to the physical property of interest—such as the mass or charge of the electron, or the longevity of a pacemaker. The variance, on the other hand, can reflect the uncertainty introduced by the experimental process itself (as in the case of J.J. Thomson’s measurements), or variability among observational units (such as patients in a clinical trial of pacemakers). We will introduce the sample mean (average) and the sample variance as random variables used to estimate these parameters. Both the sample mean and the sample variance have their own probability density functions, known as sampling distributions. We will use these distributions to quantify how precisely we can estimate the mean and variance of the random experiment from data. 10.1 Random sample Example (Electron’s charge to mass ratio) In 1897, J. J. Thomson was the first to measure the charge-to-mass ratio (\\(q/m\\)) of electrons (Thomson 1897), demonstrating that they were the constituents of cathode rays and significantly smaller than hydrogen atoms. Although Thomson originally reported his results as the mass-to-charge ratio (\\(m/e\\)), here we present them as \\(q/m\\) values, expressed in units of \\(Coulomb/Kg \\times 10^{11}\\), as illustrated below We say that J. J. Thomson took a random sample of size 26, meaning he performed 26 independent repetitions of the random experiment: measuring the \\(q/m\\) ratio under varying experimental conditions. The experiments were conducted using different types of gases, yet the observations appeared to cluster around a single unknown value. Definition: A random sample of size \\(n\\) is the repetition of a random experiment \\(n\\) independent times. A random sample is an \\(n\\)-dimensional random variable: \\[(X_1, X_2, \\dots, X_n)\\] where \\(X_i\\) is the random variable from i-th repetition of the random experiment that measures the \\(q/m\\) ratio, each subject to measurement error and with the same probability density function \\(f(x)\\) for all \\(i\\). An observation of a random sample is the set of \\(n\\) values obtained from performing the experiment: \\[(x_1, x_2, \\dots, x_n)\\] Example (Electron Measurement) The observation of a sample of size \\(26\\) for the \\(q/m\\) ratio is a vector with \\(26\\) components: (1.75, 2.94, 2.32, 3.12, 2.08, 2.50, 2.50, 2.85, 2.00, 2.50, 2.50, 2.56, 1.88, 2.12, 2.12, 1.88, 1.96, 1.85, 1.58, 1.88, 2.17, 1.63, 2.08, 1.11, 1.42, 1.00) Clearly, the quantity \\(q/m\\) is a parameter of the physical system—an intrinsic property, a constant. However, the process of measuring this quantity—namely the random variable representing each specific experimental setup and the formulas used to compute the values—varied in each repetition, producing different outcomes due to measurement error or experimental noise. J.J. Thomson did not directly observe the value of \\(q/m\\) but rather measurements—realizations of the random variables \\(X_i\\)—each subject to experimental variability, with the expected value of these measurements equal to \\(q/m\\). At the time, there was no theoretical model predicting its value. The experiments were exploratory, and knowledge emerged not from theory, but from empirical observation. How did J.J. Thomson estimate the value of \\(q/m\\) from the data? To move from raw measurements to a meaningful scientific conclusion, J.J. Thomson needed a systematic way to combine these varied observations into a single best estimate of the expected charge-to-mass ratio. In the next section, we will explore how statistical methods allow us to estimate unknown parameters from random samples like this one. 10.2 Parameter estimation To find likely values for parameters, we use data. Therefore, we take a random sample—that is, we repeat the experiment \\(n\\) times, collect data, and use it to estimate the parameters. Estimates of the Mean and Variance Recall that for a discrete random variable, the mean (or expected value) is defined using the number of possible outcomes \\(m\\) and the probability mass function \\(f(x)\\): \\[ \\mu = \\sum_{i=1}^m x_i f(x_i) \\] This formula represents the center of mass of the probabilities, where \\(f(x_i)\\) is the probability of the outcome \\(x_i\\). This concept is closely related to the sample average, which we define as: \\[ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i \\] Here, the \\(x_i\\)’s are the observed values in a random sample of size \\(n\\). We can also express this average as a weighted sum over the distinct observed values \\(x_i\\) with their associated relative frequencies \\(f_i\\) (i.e., the proportion of times each outcome appears in the sample): \\[ \\bar{x} = \\sum_{i=1}^m x_i f_i \\] If the relative frequencies \\(f_i\\) converge to the probabilities \\(f(x_i)\\) as \\(n \\to \\infty\\), then the average \\(\\bar{x}\\) will converge to the mean \\(\\mu\\). For finite \\(n\\), we treat the average as an estimate of the mean: \\[ \\hat{\\mu} = \\bar{x} \\] Note that \\(\\hat{\\mu}\\) is not the same as \\(\\mu\\). The value \\(\\hat{\\mu}\\) is a statistic—a quantity derived from the data—while \\(\\mu\\) is a parameter, an inherent (but typically unknown) property of the underlying probability distribution. They are conceptually equal only in the limit as \\(n \\to \\infty\\), i.e., if we could repeat J.J. Thomson’s experiment an infinite number of times under identical conditions. When the average is calculated from a small sample, we know it will contain some estimation error, which we will accept for now without quantifying. Similarly, the variance of a discrete random variable is defined in terms of the probability mass function as: \\[ \\sigma^2 = \\sum_{i=1}^m (x_i - \\mu)^2 f(x_i), \\] which represents the expected squared deviation from the mean. From the observed values of a sample of size \\(n\\), we compute the sample variance as: \\[ s^2 = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\bar{x})^2, \\] The use of \\(n - 1\\) will be explained later. If we express the sample variance in terms of the distinct values \\(x_i\\) and their relative frequencies \\(f_i\\), we obtain: \\[ s^2 = \\frac{n}{n - 1} \\sum_{i=1}^m (x_i - \\bar{x})^2 f_i. \\] As the sample size \\(n\\) increases, the sample variance \\(s^2\\) converges to the variance \\(\\sigma^2\\). Thus, we take \\(s^2\\) as an estimate of \\(\\sigma^2\\) when working with finite samples. Example (Electron) We assume that the value obtained for the \\(q/m\\) experiment follows a normal probability density function. \\[X \\sim N(x; \\mu, \\sigma^2)\\] Where \\(E(X)=\\mu\\) is the parameter \\(\\mu=q/m\\). the average of the sample is \\(\\bar{x}=2.09\\) Python: np.mean(x) R: mean(x) and \\(s^2=0.51^2=0.26\\) Python: np.var(x, ddof=1) R: var(x) that we can take as the estimated values of \\(\\mu\\) and \\(\\sigma^2\\). So the fitted model is \\[X \\sim N(x; \\mu=2.09, \\sigma^2=0.51^2)\\] The fitted distribution represents J.J. Thomson’s experiment with its mean and its variance. The sample average was not expected to exactly equal the value of \\(q/m\\), as it is subject to random error. Had he taken another sample, the average would likely have been different. By contrast, \\(q/m\\) appears to be a physical invariant with no associated dispersion. Today’s more accurate value of \\(q/m\\) is \\(q/m \\sim \\frac{1.602176634\\times 10^{-19}}{9.1093837015\\times 10^{−31}} C/Kg =\\) \\[1.758820024\\times 10^{11} C/Kg\\] The error that J.J. Thomson’s made in his 1897 paper was \\(0.33\\times 10^{11} C/kg\\), about \\(18\\%\\) in relation to the current estimate. Is this an expected error of his experiment? If he was to re-do his experiment how close to the current value of \\(q/m\\) he would typically be? 10.3 Law of Large Numbers When we estimate parameters using data—for example, by using \\[ \\hat{\\mu} = \\bar{x} \\] as an estimate of \\(\\mu\\), or \\[ \\hat{\\sigma}^2 = s^2 \\] as an estimate of \\(\\sigma^2\\)—we know that we are introducing an error. If we were to take a different sample of \\(q/m\\) measurements (say, of size 24), the estimate would likely change, because the sample average \\(\\bar{x}\\) depends on the data and is therefore variable. This raises an important question: When we perform an experiment, can we quantify how good our estimate of the parameter is? The Sample Mean as a Random Variable The first thing to recognize is that the numerical value we calculate, \\(\\bar{x}\\), is just one observation of a random variable, which we denote by \\(\\bar{X}\\). Definition: The sample mean (or average) of a random sample of size \\(n\\) is defined as: \\[ \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i \\] Here, \\((X_1, X_2, \\dots, X_n)\\) are independent and identically distributed (i.i.d.) random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and constitute the \\(n\\)-dimensional random variable of the sample. The average \\(\\bar{X}\\) is itself a random variable. For instance, in our sample of 24 measurements, we observed: \\[ \\bar{x} = 2.09 \\] Had we taken another sample of size 24, the average would likely have been different. Why Use \\(\\bar{X}\\) to Estimate \\(\\mu\\)? The value \\(\\bar{x}\\) can be used to estimate the unknown parameter \\(\\mu = q/m\\) because is is an observation of the random variable \\(\\bar{X}\\) with two very important and general properties: Unbiasedness: \\[ E(\\bar{X}) = \\mu \\] Consistency as \\(n \\to \\infty\\): \\[ lim_{n \\to \\infty} V(\\bar{X}) = 0 \\] Demonstration of the Properties Expected value of the mean: \\[ E(\\bar{X}) = E\\left(\\frac{1}{n} \\sum_{i=1}^n X_i\\right) = \\frac{1}{n} \\sum_{i=1}^n E(X_i) = \\mu \\] This holds because the \\(X_i\\) are i.i.d. random variables with expected value \\(\\mu\\). Variance of the mean: \\[ V(\\bar{X}) = V\\left(\\frac{1}{n} \\sum_{i=1}^n X_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^n V(X_i) = \\frac{\\sigma^2}{n} \\] Since the variance shrinks as \\(1/n\\), the sample mean becomes more stable as the sample size increases. Estimating \\(\\mu\\) These two properties imply that \\(\\bar{x}\\), the observed average, converges closer and closer to the value \\(\\mu\\) as the sample size \\(n\\) increases. This phenomenon is known as the law of large numbers. It holds regardless of the underlying probability distribution of the \\(X_i\\), provided they are independent and identically distributed with finite variance. In other words: The estimation error in using \\(\\bar{x}\\) as an approximation for \\(\\mu\\) decreases as \\(n\\) increases. This is because the variability (variance) of \\(\\bar{X}\\) decreases with \\(n\\). So when we write: \\[ \\bar{x} = \\hat{\\mu} \\] we understand that this estimate becomes increasingly reliable as the sample grows. 10.4 Inference We know that when we take large samples, the estimation error tends to be small. However, for a given sample size \\(n\\), we would like to quantify how far our estimate might be from the parameter value. This gives us an idea of how accurate our estimate is. Specifically, we may ask: What is the probability of making an error of a given size when estimating \\(\\mu\\) with \\(\\bar{x}\\)? When we calculate probabilities related to an estimator, we are performing inference. Inference problems often involve questions such as: What is the probability that the estimate \\(\\bar{x}\\) differs from the parameter \\(\\mu\\) by no more than some margin \\(m\\)? Mathematically, this is expressed as: \\[ P(-m \\leq \\bar{X} - \\mu \\leq m) \\] This represents the probability that the difference between the random variable \\(\\bar{X}\\) (the sample mean) and the parameter \\(\\mu\\) lies within a range of size \\(2m\\). To calculate such probabilities, we need: A probability model for \\(\\bar{X}\\) (i.e., its probability distribution) The parameters of that model (e.g., the mean and variance of the distribution) What, then, is the probability distribution of \\(\\bar{X}\\), and what about the distribution of \\(S^2\\) (the sample variance), so that we can calculate the relevant probabilities? These probability functions are known as sampling distributions because they describe the behavior of statistics like \\(\\bar{X}\\) and \\(S^2\\) when we take repeated samples. Understanding these distributions allows us to calculate their probabilities and make informed statements about how close our estimates are likely to be to the parameters. Example (Electron) We can pose an inference question based on J.J. Thomson’s experiment. the mean \\(\\mu\\) is unknown, but the sample average from 24 repetitions is \\(\\bar{x} = 2.09\\) (in \\(10^{11} C/kg\\) units). Assuming that each experimental measurement follows a normal distribution and that the standard deviation is approximately \\(\\hat{\\sigma} = 0.51\\), what is the probability that the sample mean \\(\\bar{X}\\) lies within \\(0.33\\) of the mean \\(\\mu\\)? In other words, we want to find the probability that the sample mean is within a margin of error of \\(0.33\\) from \\(\\mu\\): \\[ P(-0.33 \\leq \\bar{X} - \\mu \\leq 0.33) \\] To calculate this probability, we need to know the probability distribution of \\(\\bar{X}\\). 10.5 Sample mean Theorem: If the random variable \\(X\\) follows a normal distribution \\[ X \\sim N(\\mu, \\sigma^2), \\] then the sample mean \\(\\bar{X}\\) is also normally distributed as \\[ \\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right). \\] This fits perfectly with the law of large numbers because: The mean of \\(\\bar{X}\\) is exactly the parameter \\(\\mu\\) we want to estimate: \\[ E(\\bar{X}) = \\mu, \\] The variance of \\(\\bar{X}\\) gets smaller as the sample size \\(n\\) grows: \\[ V(\\bar{X}) = \\frac{\\sigma^2}{n}. \\] We say that \\(\\bar{X}\\) is an unbiased estimator of \\(\\mu\\), meaning that on average it hits the value—like a dart player who on average hits the bull’s-eye. It is also a consistent estimator because as \\(n\\) increases, the variance (or “spread”) of \\(\\bar{X}\\) gets smaller—like a dart player who improves their aim and gets closer to the bull’s-eye every time. The quantity \\[ se = \\sqrt{V(\\bar{X})} = \\frac{\\sigma}{\\sqrt{n}} \\] is called the standard error of the sample mean. It tells us how much we expect our sample mean \\(\\bar{x}\\) to typically vary from the mean \\(\\mu\\). Sometimes it is written as \\(\\sigma_{\\bar{x}}\\), to emphasize that it is the standard deviation of \\(\\bar{X}\\). Because \\(\\bar{X}\\) is normal when \\(X\\) is normal, we can use the normal distribution to calculate probabilities related to \\(\\bar{X}\\). For example, suppose we want to find the probability that our estimate \\(\\bar{X}\\) lies within a margin of error \\(m\\) from \\(\\mu\\): \\[ P(-m \\leq \\bar{X} - \\mu \\leq m). \\] Using the fact that \\(\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\), we can standardize this: \\[ P\\left(-\\frac{m}{\\sigma/\\sqrt{n}} \\leq \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\leq \\frac{m}{\\sigma/\\sqrt{n}}\\right) = P(-z \\leq Z \\leq z), \\] where \\[ z = \\frac{m}{\\sigma/\\sqrt{n}}, \\] and \\(Z\\) is a standard normal variable. This allows us to calculate exactly how likely it is that our sample mean is close to the mean within the margin \\(m\\). Example (Electron) Suppose we want to know the probability that the sample mean \\(\\bar{X}\\) is within a margin of error \\(m = 0.33\\) from the mean \\(\\mu\\). We assume the sample variance estimate \\(\\hat{\\sigma} = 0.51\\) is close to \\(\\sigma\\), and we have a sample size \\(n = 24\\). (Later, Gosset corrected for small samples using the t-distribution, but for now we treat \\(n=24\\) as large enough.) The standardized margin is \\[ z = \\frac{0.33}{0.51 / \\sqrt{24}} \\approx 3.17. \\] The probability that the sample mean is within \\(\\pm 0.33\\) of \\(\\mu\\) is then \\[ P\\left(-3.17 \\leq Z \\leq 3.17\\right) = \\Phi(3.17) - \\Phi(-3.17) \\approx 0.99, \\] where \\(\\Phi\\) is the cumulative distribution function (CDF) of the standard normal distribution. In code: Python: norm.cdf(3.17) - norm.cdf(-3.17) R: pnorm(3.17) - pnorm(-3.17) This means about 99% of the averages from repeated samples of size 24 would lie within 0.33 of the mean \\(\\mu\\). If we take the most recent and precise value of \\(q/m \\approx 1.75\\) (in \\(10^{11} C/kg\\) units) as a better estimate to the mean, this tells us that most samples similar to J.J. Thomson’s would produce averages close to that value with a precision of about \\(0.33\\). Note that the value of \\(q/m\\) is not directly observable and likely has infinite, unpredictable decimal places (i.e. is a normal number). What J.J. Thomson discovered was a consistent pattern in his measurements — a regularity that later researchers could replicate with increasing accuracy in other experiments. 10.6 Prediction While some random experiments aim to infer a property of the system under study, others aim to make predictions about particular outcomes. However, if no theoretical knowledge is available about the parameters of the probabilistic model, these parameters must first be estimated—often by performing a large number of repetitions of the experiment. 10.6.1 Example: Pacemaker Prediction The Micra Transcatheter Pacemaker is a miniaturized pacemaker delivered percutaneously with a high success rate. A key measure of its clinical efficacy is the prediction of minimum virtual battery longevity (VBL). In an early study of 630 patients, the estimated battery longevity—based on usage conditions observed at 12 months—was 12.1 years, with 89% of patients projected to have a longevity of more than 10 years (corresponding to a standard deviation of \\(1.71\\)) (Duray et al. 2017). Devices with projected longevity less than 5 years are unlikely to be marketed or adopted, especially for young or high-risk patients. Fortunately, none of the patients in this study had a projected longevity below 5 years. Question: How can we compute the probability that a Micra pacemaker has a projected longevity of less than 5 years? We assume the longevity of pacemakers follows a normal distribution: \\[ X \\sim \\mathcal{N}(\\mu, \\sigma^2) \\] To compute \\(P(X \\leq 5)\\), we need values for the parameters \\(\\mu\\) and \\(\\sigma^2\\). The reported mean is: \\[ \\bar{x} = 12.1 \\] Python: np.mean(x) R: mean(x) and \\(s^2=1.71^2=2.9\\) Python: np.var(x, ddof=1) R: var(x) that we can take as the estimated values of \\(\\mu\\) and \\(\\sigma^2\\). So the fitted model is \\[X \\sim N(x; \\mu=12.1, \\sigma^2=1.71^2)\\] The probability that a patient is implanted with a pacemaker less than 5 years longevity can be computed using the cumulative distribution function of a normal distribution: \\[ P(X \\leq 5) = F(5; \\mu = 12.1, \\sigma^2 = 1.71^2) = 0.00001 \\] Python: norm.cdf(5, 12.1, 1.71) R: pnorm(5, 12.1, 1.71) That is, the estimated probability is about \\(0.001\\%\\). This provides a probabilistic argument supporting the adoption of the pacemaker, even for children and high-risk patients. As of 2025, it is estimated that 200,000 Micra devices have been implanted. Therefore, we expect only about 2 patients with pacemaker longevity under 5 years. If such rare cases occur, they may be considered unfortunate but extremely low-probability events—comparable to or even lower than other everyday risks, such as dying in a car accident (roughly 1 in 100). When inferring the parameters of a random experiment, we require as many repetitions as possible to produce reliable estimates. When parameters are entirely unknown, inferences from large studies are treated as the gold standard. 10.7 Validation When the parameters of a random experiment have been estimated using random variables with reasonable distributional properties—such as clustering around the expected value—the stability of those estimates can be tested by performing a new set of experiments under different conditions. Repeated validation of parameters that represent properties of a system strengthens the notion that the property in question reflects a fundamental aspect of the system. Example (Pacemaker Validation) Random experiments are characterized by some experimental conditions. In the pacemaker study there are some inclusion and exclusion criteria, for example patients who are entirely pacemaker dependent were excluded for safety reasons. Pacemaker configuration can change for those patients. Imagine that we want to run a new study on 10 of those patients and measure the VBL for each of them. We then need to calculate the probability that the estimate we obtain for \\(\\mu\\) in our new study is within a margin of error from the gold standard taken as true values \\(\\mu\\) and \\(\\sigma^2\\). We can compute, for instance, the probability that \\(\\bar{X}\\) is within 1 year from \\(\\mu\\). Probability densities for \\(X\\) and \\(\\bar{X}\\) Remember that we have two probability functions: The probability function of \\(X\\) that is also the probability function of the random experiment. The probability function of \\(\\bar{X}\\) that is probability function of the sample (a collection of independent random experiments). Let us assume that the the longevity of the pacemakers is \\[X \\sim N(\\mu=12.1, \\sigma^2=1.70^2)\\] In our new study, we plan to obtain a sample of \\(10\\) patients (like the blue crosses on the plot). Since \\(X\\) is normally distributed, the sample mean \\(\\bar{X}\\) is also normally distributed. Therefore, we know the probability distribution of the sample mean \\(\\bar{X}\\) \\[\\bar{X} \\sim N(12.1 , \\frac{1.70^2}{10})\\] We can see that the outcomes of \\(\\bar{X}\\) (like the black dot in the plot) will be more tightly concentrated around \\(\\mu\\) than those of the random variable \\(X\\). In fact, when \\(n\\) is large, the sample means become so close to \\(\\mu\\) and the distribution so sharply peaked that a single observed average (black dot) becomes a sufficiently accurate estimate of \\(\\mu\\). We want to calculate the probability that our estimate is within a margin of error of \\(1\\) year. That is, within a distance of \\(1\\) from the mean: \\(P(-1 \\leq \\bar{X} - 12.1 \\leq 1) = P(11.1 \\leq \\bar{X} \\leq 13.1)\\) Remember the standard error is: \\(se=\\frac{\\sigma}{\\sqrt{n}}=\\frac{1.7}{\\sqrt{10}}= 0.53\\). \\[=\\Phi(13.1; 14.1, 0.53)-\\Phi(10.1; 14.1, 0.53)=0.93\\] Python: norm.cdf(14.1, 12.1, 0.53)-norm.cdf(10.1, 12.1, 0.53) R: pnorm(13.1, 12.1, 1.7/sqrt(10))-pnorm(11.1, 12.1, 1.7/sqrt(10)) We can, therefore, expect that \\(93\\%\\) of the sample means from new studies with a sample size of 10 will fall within the interval \\((11.1, 13.1)\\), providing reassurance about the safety of the device for high-risk patients in our new study. 10.8 Sample Sum Some random experiments consist of the collection of several identical trials, where the aim is to measure the combined contributions of each of them. Think, for example, of the collective effect of identical non-interacting molecules in a gas. In such cases, we may be interested in the sum of the outcomes of each experiment. The sample sum is the random variable \\[ Y = n \\bar{X} = \\sum_{i=1}^n X_i \\] which is a function of the random sample \\((X_1, \\ldots, X_n)\\). 10.8.1 Example (Cables) Imagine that a client asks a metallurgical company to supply 8 cables that together can carry up to 96 tons—that is, 12 tons each. The engineer needs to guarantee that none of the cables will break when loaded with this weight. In stock, there are cables, all produced under the same specifications, that might be suitable. So, the engineer selects 8 of these cables at random and loads each until it breaks. Here are the results (in tons): 13.34642, 13.32620, 13.01459, 13.10811, 12.96999, 13.55309, 13.75557, 12.62747 If the company intends to use all 8 cables simultaneously to carry a total of 96 tons, we should consider adding their individual capacities. The observed value for the sample sum is: \\[ y = 13.21268 \\times 8 = 105.70144 \\text{ tons} \\] What is the probability that when we put all the cables together, they can carry a total weight within 2 tons of the total expected weight that the cables can support? Distribution of the sample sum Theorem: if \\(X\\) follows a normal distribution \\[X \\sim N(\\mu, \\sigma^2)\\] then \\(Y\\) is normal \\[Y \\sim N(n\\mu, n\\sigma^2)\\] and \\(Y\\) has mean \\[E(Y)=n\\mu\\] variance \\[V(Y)=n\\sigma^2\\] As we have the distribution of \\(Y\\) we can compute probabilities on it. Example (Cables) If the breaking load of the cables have been previously certified to be a normal variable \\[X \\sim N(\\mu=13, \\sigma^2=0.35^2)\\] then the sample sum of size \\(8\\) is normal \\[Y \\sim N(n\\mu=104, n\\sigma^2=8\\times 0.35^2)\\] with mean and variance \\(E(Y)=n\\mu=104\\) \\(V(Y)=n\\sigma^2=8\\times 0.35^2=0.98\\); \\(\\sqrt{V(Y)}=0.9899495\\) The probability that when we put all the cables together, they can carry a total weight between \\(102=104-2\\) and \\(106=104+2\\) can be computed using the normal distribution function \\(P(102 \\leq Y \\leq 106)=\\Phi(102; 104, 8\\times 0.35^2)-\\Phi(106; 104, 8\\times 0.35^2)=0.956\\) We can calculate it as: Python: norm.cdf(106, 104, 0.9899495)-norm.cdf(102, 104, 0.9899495) R: pnorm(106, 104, 0.9899495)-pnorm(102, 104, 0.9899495) Therefore, we can expect that \\(95.6\\%\\) of the total weight that 8 cables can carry lies between \\(102\\) and \\(106\\) Tons, which represents a margin of error of \\(2\\) Tons around the total mean \\(n\\mu = 104\\) Tons. This probability is equal to the probability that a single cable breaks between \\(13-\\sqrt{2/8}\\) and \\(13-\\sqrt{2/8}=13.25\\) Tons. The margin of error does not scale linearly with the number of cables; instead, it increases with the square root of the sample size, \\(\\sqrt{n}\\). This is because the individual variations tend to cancel each other out when added—some cables break above the average strength, others below. This slower growth of uncertainty ensures that collective measurements (like the total strength) become more accurate and reliable as the number of independent components increases, which is a fundamental principle of statistical inference. 10.9 Sample Variance The variance is another key parameter of a random experiment. In some cases, it may reflect an intrinsic property of the system; in others, it may relate to the precision of the measurement process. When we estimate the variance of a random experiment using the observed value of the sample variance \\[ s^2 = \\hat{\\sigma}^2 \\] we are also introducing an estimation error. So how can we quantify the error we make? Definition Just as we defined the sample mean \\(\\bar{X}\\) as an estimator of the population mean \\(\\mu\\), we now turn to estimating the population variance \\(\\sigma^2\\). The sample variance \\(S^2\\) of a random sample of size \\(n\\) is defined as \\[ S^2 = \\frac{1}{n - 1} \\sum_{i=1}^n (X_i - \\bar{X})^2 \\] It measures the dispersion of the sample values around the sample mean \\(\\bar{X}\\). The denominator \\(n - 1\\), rather than \\(n\\), ensures that \\(S^2\\) is an unbiased estimator of the population variance \\(\\sigma^2\\). This correction, known as Bessel’s correction, accounts for the fact that we have used the sample mean \\(\\bar{X}\\) — an estimate of \\(\\mu\\) — in computing the deviations. 10.9.1 Example (Cables) In a sample of \\(8\\) cables, the sample variance took the value: \\[ s^2 = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\bar{x})^2 = 0.1275608 \\] The sample variance \\(S^2\\) is: Unbiased: Its expected value equals the true variance: \\[ E(S^2) = V(X) = \\sigma^2 \\] Consistent: Its variance decreases as the sample size increases: \\[ V(S^2) \\to 0 \\quad \\text{as} \\quad n \\to \\infty \\] Therefore, \\(S^2\\) is both an unbiased and consistent estimator of \\(\\sigma^2\\). This justifies using the observed sample variance \\(s^2\\) as an estimate of the true variance: \\[ s^2 = \\hat{\\sigma}^2 \\] Just like \\(\\hat{\\mu} = \\bar{x}\\), the error in estimating \\(\\sigma^2\\) with \\(s^2\\) gets smaller as \\(n\\) becomes larger. Why Do We Divide by \\(n - 1\\)? One might propose estimating \\(\\sigma^2\\) using: \\[ S_n^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2 \\] However, this version is: Biased: \\[ E(S_n^2) = \\sigma^2 - \\frac{\\sigma^2}{n} \\neq \\sigma^2 \\] but Consistent: \\[ V(S_n^2) \\to 0 \\quad \\text{as} \\quad n \\to \\infty \\] The bias appears because \\(S_n^2\\) measures the spread around the sample mean \\(\\bar{X}\\), not the true mean \\(\\mu\\). The error introduced when substituting \\(\\bar{X}\\) for \\(\\mu\\) contributes an extra variance term \\(\\sigma^2 / n\\). We correct for this bias by multiplying \\(S_n^2\\) by \\(\\frac{n}{n - 1}\\): \\[ E\\left( \\frac{n}{n - 1} S_n^2 \\right) = \\sigma^2 \\] This gives us the corrected (unbiased) sample variance: \\[ S^2 = \\frac{n}{n - 1} S_n^2 = \\frac{1}{n - 1} \\sum_{i=1}^n (X_i - \\bar{X})^2 \\] which satisfies: \\[ E(S^2) = \\sigma^2 \\] Example (Quality Control) We encounter inference problems when we are interested in the probability of observing a certain value for the sample variance \\(S^2\\). Consider a quality control process in which cables are expected to have breaking loads close to a specified target value \\(\\mu\\). We want to avoid producing cables whose breaking strength varies too much from the mean. Suppose we take a sample of 8 cables. If the sample variance is too high (specifically, \\(S^2 &gt; 0.3\\)), we stop production and declare that the process is out of control. What is the probability that the sample variance from a sample of 8 cables exceeds the threshold of \\(0.3\\)? 10.10 Distribution of the Sample Variance Theorem: If \\(X \\sim N(\\mu, \\sigma^2)\\), then the random variable \\[ W = \\frac{(n - 1) S^2}{\\sigma^2} \\] follows a chi-squared distribution with \\(df = n - 1\\) degrees of freedom: \\[ W \\sim \\chi^2(n - 1) \\] The probability density function (PDF) of the chi-squared distribution with \\(n - 1\\) degrees of freedom is given by: \\[ f(w) = C_n \\, w^{\\frac{n - 3}{2}} e^{-w/2} \\] where: \\(C_n = \\frac{1}{2^{(n-1)/2} \\, \\Gamma\\left(\\frac{n - 1}{2}\\right)}\\) is a normalizing constant to ensure \\(\\int_0^\\infty f(w) \\, dw = 1\\), \\(\\Gamma(x)\\) is the gamma function, a generalization of the factorial for real numbers. If the value of \\(\\sigma^2\\) is known, we can use the distribution of \\(W\\) to compute probabilities related to the sample variance \\(S^2\\). 10.11 The \\(\\chi^2\\) Distribution The chi-squared distribution has a parameter called the degrees of freedom \\(df = n - 1\\), which depends on the sample size. The shape of the distribution changes with different values of \\(df\\). The chi-squared distribution underpins the t-distribution used when inferring the mean with an unknown \\(\\sigma\\) and small \\(n\\), as we see in the following chapter. Below, we explore how the probability density function behaves for different values of \\(df\\). Example (Variations in Cable Strength) If we know that our cables are certified as \\[X \\sim N(\\mu=13, \\sigma^2=0.35^2)\\] so \\[W=\\frac{(n-1)S^2}{\\sigma^2}= \\frac{7S^2}{0.35^2} \\sim \\chi^2(n-1)\\] we can calculate \\[P(S^2 &gt; 0.3)=P(\\frac{(n-1)S^2}{\\sigma^2} &gt; \\frac{(n-1)0.3}{\\sigma^2 } )\\] \\(=P(W &gt; \\frac{7\\times0.3}{0.35^2})=P(W &gt; 17.14286)\\) \\(=1-P(W \\leq 17.14286)\\) \\(= 1- F_{\\chi^2,df=7}(17.14286)=0.016\\) Python: 1-chi2.cdf(17.14286, df=7) R: 1-pchisq(17.14286, 7) There is only a \\(1\\%\\) chance of getting a value greater than \\(0.3\\). So \\(s^2&gt;0.3\\) seems to be a good criteria to stop production and review the process. Consider that sample of \\(8\\) cables that the engineer took was to perform a quality control test of the production line 13.34642, 13.32620, 13.01459, 13.10811, 12.96999, 13.55309, 13.75557, 12.62747 and the observed sample variance was \\(s^2=0.1275608\\). Therefore, the sample is not very dispersed because \\(s^2 &lt; 0.3\\) and we believe that all is well and manufacturing is under control. 10.12 Questions 1) The sample mean is an unbiased estimator of the population mean because \\(\\qquad\\)a: The expected value of the sample mean is the population mean; \\(\\qquad\\)b: The expected value of the population mean is the sample mean; \\(\\qquad\\)c: The standard error approaches zero as \\(n\\) approaches infinity; \\(\\qquad\\)d: The variance of the sample mean approaches zero as \\(n\\) approaches infinity; 2) Why is the statistic \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i -\\bar{X})^2\\) used? instead of \\(S_n^2=\\frac{1}{n}\\sum_{i=1}^{n}(X_i -\\bar{X})^2\\) to estimate the variance of a random variable? \\(\\qquad\\)a: because its variance is \\(0\\); \\(\\qquad\\)b: because it is a consistent estimator of \\(\\sigma^2\\); \\(\\qquad\\)c: because it is an unbiased estimator of \\(\\sigma^2\\); \\(\\qquad\\)d: because it is the mean square distance to the sample mean (\\(\\bar{X}\\)); 3) What is the variance of the sample mean \\(\\bar{X}=\\frac{1}{n}\\sum_{i=1}^n X_i\\)? \\(\\qquad\\)a:\\(\\sigma\\); \\(\\qquad\\)b:\\(\\frac{\\sigma}{\\sqrt{n}}\\); \\(\\qquad\\)c:\\(\\sigma^2\\); \\(\\qquad\\)d:\\(\\frac{\\sigma^2}{n}\\); 4) What is the mean and variance of the sample sum? \\(\\qquad\\)a:\\(\\mu\\), \\(n\\sigma\\); \\(\\qquad\\)b:\\(n\\mu\\),\\(n\\sigma\\); \\(\\qquad\\)c:\\(\\mu\\), \\(n\\sigma^2\\); \\(\\qquad\\)d:\\(n\\mu\\), \\(n\\sigma^2\\); 5) An inference question requires to \\(\\qquad\\)a: calculate the expected value of an estimator; \\(\\qquad\\)b: estimate the value of a parameter; \\(\\qquad\\)c: calculate a probability of an estimator; \\(\\qquad\\)d: fit a probability model; 10.13 Exercises 10.13.0.1 Exercise 1 Imagine that in the cable example the client asks to sell them one cables that can carry \\(12\\) Tons. The engineer takes \\(8\\) sample of cables in stock at random, and load them until they break with the results 13.34642, 13.32620, 13.01459, 13.10811, 12.96999, 13.55309, 13.75557, 12.62747 If he does not know the certified breaking load, but he assumes that it is a normal variable, is it reasonable to sell one cable from the stock? (A: \\(P(X&lt;12)=0.0003\\)). 10.13.1 From Estimation to Inference So far, we have seen how to use a sample to estimate population parameters such as the mean and variance. But these so called point estimates alone are not enough. Scientific conclusions require an understanding of how reliable those estimates are. This leads us to inference questions such as: How precise is our estimate? What range of values is consistent with the data? Could the observed estimate have occurred by chance? In the following chapters, we explore how the sampling distributions of estimators form the basis for answering these questions through confidence intervals and hypothesis tests. 10.13.1.1 Exercise 2 An electronics company manufactures resistors that have an average resistance of 100 ohms and a standard deviation of 10 ohms. The resistance distribution is normal. What is the sample mean of \\(n=25\\) resistors? (R:100) What is the variance of the sample mean of \\(n=25\\) resistors? (R:4) What is the standard error of the sample mean of \\(n=25\\) resistors? (R:2) Find the probability that a random sample of \\(n = 25\\) resistors have an average resistance of less than \\(95\\) ohms (R: 0.0062) 10.13.1.2 Exercise 3 A battery model charges an average of \\(75\\%\\) of its capacity in one hour with a standard deviation of \\(15\\%\\). If the battery charge is a normal variable, what is the probability that the charge difference between the sample mean of \\(25\\) batteries and the mean charge is at most \\(5\\%\\)? (R:0.9044) If we charge \\(100\\) batteries, what is that probability? (R:0.9991) If instead we only charge \\(9\\) batteries, what charge \\(c\\) is exceeded by the sample mean with probability \\(0.015\\)? (A:85.850) 10.13.1.3 Exercise 4 On January 28, 1986, the space shuttle Challenger exploded just seconds after launch, killing all seven crew members. The night before, concerns were raised about the potential damage that the low temperature forecast at launch time—\\(29°F\\) (\\(–1.6°C\\))—could cause to two rubber O-rings that sealed sections of the solid rocket boosters. The failure of engineers to properly analyze available data, combined with pressure to avoid further delays in the mission, contributed to the disaster. Engineers had access to data from previous shuttle launches where damage to the O-rings and they the temperatures on the days of those launches were recorded (Tufte 1997). Crucially ignoring the severity of the the damage and the launches with no damage, some arguments against delaying the mission were based on the temperatures of the launches with some damage only. These are the the temperatures of the launches that registered some damage 53, 57, 58, 63, 70, 75 What is the probability that a launch with some damage is observed at temperature \\(29°F\\)? (A: \\(P(X&lt; 29)=3.10\\times 10^{-5}\\)). References "],["central-limit-theorem.html", "Chapter 11 Central limit theorem 11.1 Margin of error 11.2 Averages of normal variables 11.3 Central Limit Theorem 11.4 Sample sum and CLT 11.5 Unknown \\(\\sigma\\) 11.6 T-statistic 11.7 Questions 11.8 Exercises", " Chapter 11 Central limit theorem In this chapter we will discuss more fully the margin of errors when estimating the mean of the population distribution with the average. In the design of components, usually a predefined tolerance in the acceptability of the component’s properties is conventionally given for certification. When testing the components with a testing sample, this puts limits to how much variability we can tolerate in the sample average. Statistically, we may set the margin of error such that the probability that the average is within the error is at least a fixed number, usually \\(95\\%\\). When observing an average of a natural process and some theoretical or gold-standard is known for the mean and variance we can compute the margin of error to have a prediction boundary on where to find the average when we perform the data collection for the sample. Decisions and inferences when the sample data is obtained will be the subject of confidence intervals and hypothesis testing. Before that, in this chapter and the next chapter, we will first deal with random experiments that are not modeled with the normal distribution. We will discuss how the central limit theorem will allow us to compute the margin of error for any type of distribution if the sample is large. We will also introduce the t-statistic, for computing the margin of error when the sample is small but the population distribution is normal. 11.1 Margin of error When deciding whether the error of estimation of \\(\\mu\\) by the sample mean \\(\\bar{x}\\) is large or not, we usually compare it with a predefined tolerance. The margin of error at \\(5\\%\\) level is the distance from \\(\\mu\\) that captures \\(95\\%\\) of the averages \\(\\bar{X}\\): \\[P(-m \\leq \\bar{X}-\\mu \\leq m)=P(\\mu-m \\leq \\bar{X} \\leq\\mu + m)=0.95\\] This means that \\(95\\%\\) of the outcomes of \\(\\bar{X}\\) from a random sample are a distance \\(m\\) from \\(\\mu\\). 11.2 Averages of normal variables We want to know the number \\(m\\) in the equation \\[P(\\mu-m \\leq \\bar{X} \\leq\\mu + m)=0.95\\] To solve this equation we need two steps. First, we need to know the distribution of \\(\\bar{X}\\). When \\(X\\) is normal (\\(X \\sim N(\\mu, \\sigma^2)\\)) then \\[\\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})\\] We then need to standardize \\(\\bar{X}\\). Remember that to standardize a normal variable, we subtract its mean an divide it by its standard deviation. \\[Z=\\frac{\\bar{X}-E(\\bar{X})}{\\sqrt{V(\\bar{X})}} =\\frac{\\bar{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0,1)\\] \\(Z\\) is then standard normal. Remember the probability density function for a standard normal variable and its quantiles Substituting the mean of \\(\\bar{X}\\) and its standard deviation into the equation for the margin of error, we have: \\(P(\\mu-m \\leq \\bar{X} \\leq\\mu + m)=P(-\\frac{m}{\\sigma/\\sqrt{n}} \\leq \\frac{\\bar{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\leq\\frac{m}{\\sigma/\\sqrt{n}})\\) \\[=P(-\\frac{m}{\\sigma/\\sqrt{n}} \\leq Z \\leq\\frac{m}{\\sigma/\\sqrt{n}})=0.95\\] Compare it with the plot above, where \\[P(-1.96 \\leq Z \\leq 1.96)=0.95\\] Therefore, \\[\\frac{m}{\\sigma/\\sqrt{n}}=1.96\\] is the distance from \\(z=0\\) that captures \\(95\\%\\) of the distribution of the standard normal variable. Threfore, the margin of error at \\(5\\%\\) is \\[m=1.96\\frac{\\sigma}{\\sqrt{n}}\\] The limits of the interval \\((-1.96, 1.96)\\) for \\(Z\\) are computed from \\[\\Phi^{-1}(0.975)=1.96\\] where \\(\\Phi^{-1}\\) is the inverse of the standard normal distribution (norm.ppf(0.975)), and retrieves the value of \\(z\\) that has accumulated \\(97.5\\%\\) of probability. \\(2.5\\%\\) are left out at the higher tail of the distribution, that add to the other \\(2.5\\%\\) left out at the left of \\(\\Phi^{-1}(0.025)=-1.96\\), because the distribution is symmetric. Example (Resistors) Imagine we have implemented a manufacturing process for producing \\(2.5\\,\\Omega\\) resistors. We want to certify that this process is highly capable at the 1% tolerance level. This means that the standard deviation of the process should be no more than one-sixth of \\(2.5 \\times 0.01\\), following the criteria for a so-called Six Sigma process capability (Cp). In other words: The mean resistance should be close to \\(2.5\\,\\Omega\\) The standard deviation should be less than or equal to \\(0.004\\,\\Omega\\) Nearly all resistors should fall within the range \\((2.49, 2.51)\\,\\Omega\\) We could assume (i.e. from historical measurements) that the resistance follows a normal distribution: \\[ X \\sim N(\\mu = 2.5, \\sigma^2 = 0.004^2) \\] Although large samples are required for formal certification, we take a pilot sample of 10 resistors to quickly test the process. Since we will estimate the mean \\(\\mu\\) using the sample mean \\(\\bar{X}\\), and this estimate is subject to sampling variability, we are interested in computing the margin of error at the 5% level for \\(\\bar{X}\\). The sample mean \\(\\bar{X}\\) has: Expected value: \\(E(\\bar{X}) = \\mu = 2.5\\) Standard error: \\[ \\text{SE} = \\sqrt{\\text{Var}(\\bar{X})} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{0.004}{\\sqrt{10}} \\] Then, the 95% margin of error is: \\[ m = 1.96 \\times \\frac{0.004}{\\sqrt{10}} = 0.00248 \\] Therefore, we can expect that in 95% of such pilot samples of size 10, the sample mean resistance \\(\\bar{x}\\) will lie within the interval: \\[ (2.5 - 0.00248,\\ 2.5 + 0.00248) = (2.49752,\\ 2.50248)\\,\\Omega \\] Obtaining a sample average within the specified interval provides preliminary reassurance that the process may meet the required precision, supporting the decision to pursue a larger-scale certification study. 11.3 Central Limit Theorem We could solve the margin of error because we assumed that that variable \\(X\\) was normal. What if \\(X\\) follows any other probability distribution? Theorem: For any random variable \\(X\\) with any type of probability function \\[X \\sim f(x)\\] the standardized statistic \\[Z=\\frac{\\bar{X}-E(\\bar{X})}{\\sqrt{V(\\bar{X})}}\\] approximates to a standard distribution \\[Z \\sim_d N(0,1)\\] when \\(n\\rightarrow \\infty\\) Consequence: We can compute probabilities for \\(\\bar{X}\\) if \\(n\\) is large, using the normal distribution: \\[\\bar{X} \\sim_{aprox} N(\\mu, \\frac{\\sigma^2}{n})\\] This is topically a good approximation when \\(n\\geq 30\\). Example (Number of Individuals per Species) In their foundational work in ecology R.A. Fisher et al. analysed the number of individuals for each moth species that was captured over 4 years at Rothamsted Experimental Station (Rothamsted Research) (R. A. Fisher, Corbet, and Williams 1943). They showed that most species were observed only once and that fewer species were observed with large number of individuals. While the last model they proposed was a Log-Series model, the first model they tried was a Poisson distribution \\[X \\sim Poisson(\\lambda=11.10)\\] Where \\(X\\) is the number of individuals of a species in a survey. The random experiment is to take a survey of a given size, identify a species and count the number of individuals in the species. The Poisson model underestimated rare species but showed that the observed patterns in species abundances can arise from random sampling processes — not necessarily from deterministic biological interactions. The mean and variance are: \\(E(X)=\\lambda=11.10\\) \\(V(X)=\\lambda=11.10\\) Imagine, we go to Rothamsted Research and plan a \\(30\\) day campaign of moth collection and count each day the number of species collected with 1 specimen, 2 specimens, and so on. What would be the expected number of individuals in a randomly chosen species and its margin of error, if we assume that the biodiversity of moths has not changed since Fisher’s time? Fisher’s mean and the standard error of the average species abundance \\(\\bar{X}\\) in \\(30\\) surveys are: \\(E(\\bar{X})=\\lambda=11.101\\) \\(se=\\sqrt{\\frac{V(X)}{n}}=\\sqrt{\\frac{\\lambda}{n}}=0.608\\) As \\(n \\geq 30\\) \\[Z=\\frac{\\bar{X}-\\lambda}{\\sqrt{\\frac{\\lambda}{n}}}\\] is almost standard normal variable and: \\[\\bar{X} \\sim_{aprox} N(\\lambda, \\frac{\\lambda}{n})\\] The margin of error at \\(5\\%\\) level can be computed again with the standard distribution \\[m=\\Phi^{-1}(0.975) \\sqrt{\\frac{V(X)}{n}}=1.96\\sqrt{\\frac{11.10}{30}}=1.20\\] We can expect \\(95\\%\\) of the averages of the abundance of a species over \\(30\\) surveys to fall between \\((11.10-1.20, 11.10+1.20)= (9.9, 12.3)\\) If the observed average falls outside this interval, it may indicate a statistically significant change in moth biodiversity since Fisher’s study, assuming the Poisson model and historical parameters still hold. 11.4 Sample sum and CLT The sample sum is the statistic \\[Y=X_1+X_2+...X_n=\\sum_{i=1}^n X_i=n \\bar{X}\\] with mean \\[E(Y)=n\\mu\\] variance \\[V(Y)=n\\sigma^2\\] The CLT tells us that for any random variable \\(X\\) with unknown (any type of) distribution \\[X \\sim f(x)\\] the standardized statistic \\[Z=\\frac{\\bar{X}-E(\\bar{X})}{\\sqrt{V(\\bar{X})}}\\] approximates to a standard distribution \\[Z \\sim_d N(0,1)\\] when \\(n\\rightarrow \\infty\\). \\(Z\\) can also be written as \\[Z=\\frac{n\\bar{X}-nE(\\bar{X})}{\\sqrt{n^2V(\\bar{X})}}=\\frac{Y-E(Y)}{\\sqrt{V(Y)}}=\\frac{Y-n\\mu}{\\sqrt{n}\\sigma}\\] Consequence: We can compute probabilities for the sample sum \\(Y=n\\bar{X}\\) if \\(n\\) is large, using the normal distribution: \\[Y \\sim_{aprox} N(n\\mu, n\\sigma^2)\\] Example (De Moivre-Laplace’s insight) For a Bernoulli trial \\(X \\sim Bernoulli(p)\\), its mean is \\(\\mu=p\\) and variance \\(\\sigma^2=p(1-p)\\). Now, the sample sum \\(Y=n\\bar{X}\\) is a random variable that counts the number of events with probability \\(p\\) in a repetition of \\(n\\) trials, therefore \\[Y \\sim Binom(n, p)\\] with mean \\(E(Y)=np\\) and variance \\(V(Y)=np(1-p)\\). If we apply the CLT for the sample sum of Brenouilli trials then we have \\[Y \\sim Binom(n, p) \\sim_{aprox} N(np, np(1-p))\\] we can then proximate the binomial probability mass function with the normal probability density when \\(n\\) is big. This approximation is good when both \\(np\\) and \\(n(1-p)\\) are greater than \\(5\\). The significance of De Moivre-Laplace’s discovery cannot be overstated. It provided a practical method for approximating probabilities of binomial variables using the normal distribution, laying the groundwork for modern statistical inference. More profoundly, it offered a deep insight into how regular patterns observed in continuous (analog) systems can emerge from the randomness of discrete (digital) processes when the sample size is large. An early application of this insight was in genetics, where continuous traits such as height or skin color were explained by the cumulative effect of many discrete mutations. A striking modern example is how the discrete activations of many individual neurons can collectively produce continuous patterns of behavior—an insight that has been fundamental to modeling in artificial intelligence. 11.5 Unknown \\(\\sigma\\) Gold-standards for the values of the mean are usually established using \\(\\bar{x}\\) large samples sizes, as the law of large numbers tells us that the average will be close to the mean. If we are repeating the experiment in a new sample, the margin of error can be computed with the standard deviation \\(s\\) of the gold-standard, as an estimate of the variance. For any random variable \\(X\\) with any type or unkown distribution \\[X \\sim f(x)\\] and unknown variance \\(\\sigma\\), we can still estimate the standard error like \\[\\hat{se}=\\frac{s}{\\sqrt{n}}\\] when \\(n\\) is large and write the standardized statistic \\[Z=\\frac{\\bar{X}-\\mu}{\\frac{s}{\\sqrt{n}}}\\sim N(0,1) \\] that allows us to use the margin of error at \\(5\\%\\) \\[m= 1.96 \\frac{s}{\\sqrt{n}}\\] for the average of a new \\(n\\) sample of the gold-standard experiment. 11.5.1 Example: Pacemaker Prediction The Micra Transcatheter Pacemaker is a miniaturized pacemaker. A study of 630 patients, established the expected virtual battery longevity at \\(\\mu=12.1\\) years** with standard deviation of \\(s=1.71\\) (Duray et al. 2017). If we want to run a new study on 10 new patients and measure the VBL for each of them then the estandard error is \\[se=\\frac{s}{\\sqrt{n}}=\\frac{1.7}{\\sqrt{10}}= 0.53\\] and the margin of error \\[m= 1.96\\times se= 1.0388\\] thus \\(95\\%\\) of the samples of size 10 will have averages within a year from the gold-standard mean. 11.6 T-statistic Substitution of the standard deviation \\(\\sigma\\) for the standard deviation of the sample \\(s\\) is only reasonable when size of the gold-standard sample is large. Small sample expetiments of size \\(n\\) are often called pilots where both \\(\\mu\\) and \\(\\sigma^2\\) are unknown. In this cases we cannot use the central limit theorem for establishing gold-standards However, if \\(X\\) is normal \\[X \\sim N(\\mu, \\sigma^2)\\] then the standardized statistic \\[T=\\frac{\\bar{X}-\\mu}{\\frac{S}{\\sqrt{n}}} \\sim t_{n - 1}\\] Follows a \\(t\\)-distribution with \\(n-1\\) degrees of freedom, that allow us to compute probabilities on \\(\\bar{X}\\), and obtain a margin of error for the average if we were to repeat the pilot study. The probability density function (PDF) of the Student’s t-distribution with \\(n - 1\\) degrees of freedom is given by: \\[ f(t) = C_n \\left(1 + \\frac{t^2}{n - 1}\\right)^{-\\frac{n}{2}} \\] where: \\(C_n = \\frac{\\Gamma\\left(\\frac{n}{2}\\right)}{\\sqrt{(n - 1)\\pi} \\, \\Gamma\\left(\\frac{n - 1}{2}\\right)}\\) is a normalizing constant ensuring \\(\\int_{-\\infty}^{\\infty} f(t) \\, dt = 1\\), \\(\\Gamma(x)\\) is the gamma function, which generalizes the factorial to real (and complex) numbers. The t-distribution resembles the standard normal distribution but has heavier tails, reflecting the added uncertainty from estimating the population variance. To compute the margin of error \\(m\\) at \\(5\\%\\) level when \\(n\\) is small, \\(\\sigma\\) unknown but \\(X\\) normal \\(P(\\mu-m \\leq \\bar{X} \\leq\\mu + m)=P(-\\frac{m}{s/\\sqrt{n}} \\leq \\frac{\\bar{X}-\\mu}{\\frac{s}{\\sqrt{n}}} \\leq\\frac{m}{s/\\sqrt{n}})\\) \\[=P(-\\frac{m}{s/\\sqrt{n}} \\leq T \\leq\\frac{m}{s/\\sqrt{n}})=0.95\\] We use the \\(t\\)-distribution \\[m=t_{0.025, n-1} \\frac{s}{\\sqrt{n}}\\] where \\(t_{0.025, n-1}\\) is the value \\(T\\) that leaves \\(2.5\\%\\) at each side of \\(t\\)-distribution with \\(n-1\\) degrees of freedom (\\(0.025\\)-quantile) Example (Cables) A sample of 8 cables was taken to perform a quality control test on the breaking load (in tons) of the cables. 13.34642, 13.32620, 13.01459, 13.10811, 12.96999, 13.55309, 13.75557, 12.62747 The observed mean and standard deviation of the sample were \\(\\bar{x}=13.21\\) and \\(s=0.36\\). Therefore, the margin of error is \\[m=t_{0.025, n-1} \\frac{s}{\\sqrt{n}}=2.36\\times \\hat{se}=2.36\\frac{0.36}{\\sqrt{8}}=0.1\\] where \\(t_{0.025, n-1}=2.36\\). In subsequent samples of \\(8\\) cables we can expect \\(95\\%\\) of the averages to fall between \\((13.21-0.1,13.21+0.1)\\). Python: t.sf(0.025, 8-1) R: qt(0.025, 8-1, lower.tail = FALSE) 11.7 Questions 1) A magnetic resonance imaging of the brain’s hippocampus has \\(100\\) pixels. We expect \\(90\\%\\) of the pixels to be white (brain tissue). According to the central limit theorem, what is the probability that the scanning of a patient has at most \\(85\\%\\) of white pixels? \\(\\qquad\\)a:norm.cdf(0.9, 0.85, sqrt(0.85*0.15)/10); \\(\\qquad\\)b:norm.pdf(0.85, 0.9, sqrt(0.9*0.1)/10); \\(\\qquad\\)c:norm.cdf(0.85, 0.9, sqrt(0.9*0.1)/10); \\(\\qquad\\)d:norm.pdf(0.9, 0.85, sqrt(0.85*0.15)/10) 2) For a standard normal variable, if we the number \\(z_{0.025}\\) in the definition of the margin of error \\(m=z_{0.025} \\frac{\\sigma}{\\sqrt{n}}\\), then it will refer to \\(\\qquad\\)a: The first quartile; \\(\\qquad\\)b: The number at which the distribution has accumulated \\(0.975\\) of probability; \\(\\qquad\\)c: The number at which the distribution has accumulated \\(0.025\\) probability; \\(\\qquad\\)d: The third quartile; 3) The importance of the central limit theorem is that is applyes to the standardization of \\(\\qquad\\)a: A random variable; \\(\\qquad\\)b: The sample mean of a normal variable; \\(\\qquad\\)c: The sample mean of a random variable; \\(\\qquad\\)d: A normal variable; 11.8 Exercises 11.8.0.1 Exercise 1 An electronic component is needed for the correct functioning of a telescope. It needs to be replaced immediately when it wears out. The mean life of the component (\\(\\mu\\)) is \\(100\\) hours and its standard deviation \\(\\sigma\\) is \\(30\\) hours. what is the probability that the average of the mean life of \\(50\\) components is within \\(1\\) hour from the mean life of a single component? (A: 0.1863) How many components do we need such that the telescope is operational \\(2750\\) consecutive hours with at least \\(0.95\\) probability? (A: 31) 11.8.0.2 Exercise 2 The probability that a particular mutation is found in the population is \\(0.4\\). If we test \\(2000\\) people for the mutation: What is the probability that the total number of people with the mutation is between \\(791\\) and \\(809\\)? (A: 0.31) hint: Use the CLT with a sample of \\(2000\\) Bernoulli trials. This is known as the normal approximation of the binomial distribution. 11.8.0.3 Exercise 3 An automated machine fills test tubes with biological samples with mean \\(\\mu=130\\)mg and a standard deviation of \\(\\sigma=5\\)mg. For a random sample of size \\(50\\) What is the probability that the sample mean (average) is between \\(128\\) and \\(132\\)gr? (A: 0.995) what is the margin of error at \\(5\\%\\)? (A: 1.385929) what should be the size of the sample \\(n\\) such that the margin of error at \\(5\\%\\) is \\(1\\)? (A: 97) 11.8.0.4 Exercise 4 In the Caribbean, there appears to be an average of \\(6\\) hurricanes per year. Considering that hurricane formation is a Poisson process, meteorologists plan to estimate the mean time between the formation of two hurricanes. They plan to collect a sample of size \\(36\\) for the times between two hurricanes. What is the probability that their sample average is between \\(50\\) and \\(60\\) days? (A: 0.385975) Which should be the sample size such that they have a probability of \\(0.025\\) that the sample mean is greater than \\(70\\) days? (A: 169) References "],["maximum-likelihood.html", "Chapter 12 Maximum likelihood 12.1 Statistic 12.2 Properties 12.3 Maximum likelihood 12.4 Maximum likelihood 12.5 Questions 12.6 Exercises", " Chapter 12 Maximum likelihood n 1922, R.A. Fisher published a seminal paper that laid the foundations of modern mathematical statistics (R. A. Fisher 1922). While important statistical techniques—such as regression analysis and tests of goodness-of-fit—had already been developed, their mathematical underpinnings had remained largely unformalized. This lack of rigor, Fisher argued, was partly due to the nature of the field, which deals with uncertainty and error, and where precise definitions were often not considered practically necessary. Another major source of confusion was linguistic: the same terms were often used to refer both to the unknown quantities that researchers sought to estimate (parameters) and to the values computed from data (estimates). For example, think of the distinction between \\(\\mu\\), the expected value of a normal random variable, and \\(\\bar{x}\\), the sample mean used to estimate it. Clarifying such distinctions was a crucial step in the formalization of statistical inference, enabling the generalization of estimation methods to a wide variety of models. In this chapter, we will explore what an estimator is and illustrate its use in general applications such as radioactive detection, biodiversity estimation, and the analysis of system complexity. We will introduce a general framework for obtaining estimators of model parameters, focusing in particular on the method of maximum likelihood, first introduced by Fisher. Fisher’s key insight was to treat the likelihood of the observed values of random samples as a function of the unknown parameters, allowing for a principled method to derive estimates. 12.1 Statistic It is time to formalize some of the notions that we have already dealt with. The intention of using a more precise language is to be able to generalize the basic concepts to any type of probability models. In particular, we need to formalize the notion of statistics, estimator and their properties, such as the error we can make when estimating a parameter with the observed value of an estimator. To extract useful information of a random experiment from the sample we use statistics. Definition A statistic is any function of a random sample \\[T(X_1,X_2, ..., X_n)\\] It usually returns a number, but frequency tables and plots like histograms can also be considered statistics. Statistics are random variables and their probability distributions are called sampling distributions. Statistics have different functions: Description of a sample’s data location: \\(\\bar{X}\\) Minimum: \\(\\min\\{X_i\\}\\) Maximum: \\(\\max\\{X_i\\}\\) Estimation of a probability model’s parameters We use \\(\\bar{X}\\) to estimate \\(\\mu\\) We use \\(S^2\\) to estimate \\(\\sigma^2\\) Inference for predicting the outcomes of the estimators for the mean we will use the statistics \\(Z\\), \\(T\\) for the variance we will use \\(\\chi^2\\) This last point will be developed in the following chapters. The key fact is that statistics are random variables whose observed values we take as information about the experiment. However, every time we take a new sample their value change. Definition of estimators An estimator is a statistic whose observed values are used to estimate the parameters of the probability model used to describe the outcome of the random experiments under study. When we use an estimator, the information that we are interested in extracting is the properties of the random experiment encoded in the parameters. If we write the probability model of the random experiment as the probability function \\[X \\rightarrow f(x; \\theta)\\] then \\(\\theta\\) is a parameter and the statistics \\(\\Theta\\) is a random variable whose observations \\(\\hat{\\theta}\\) we take as estimations of \\(\\theta\\). The estimate \\(\\hat{\\theta}\\) is a numerical value obtained by evaluating the estimator \\(\\Theta\\) on the data and is used as an approximation of the unknown parameter \\(\\theta\\). \\[\\hat{\\theta} \\approx \\theta\\] Let us unpack this statement. There are three different quantities that we must consider: \\(\\theta\\) is a parameter of the probability model that describes the experiment \\(f(x; \\theta)\\). \\(\\Theta\\) is an estimator of \\(\\theta\\): A random variable. \\(\\hat{\\theta}\\) is the point estimate of \\(\\theta\\): An outcome or a realized value of \\(\\Theta\\). Example (Sample mean) When the probability density function for the outcomes of a random experiment is modeled by a normal density \\[X \\rightarrow N(\\mu, \\sigma^2)\\] we can identify the three different quantities: The mean: \\(\\mu\\) is a parameter of the population distribution \\(N(\\mu, \\sigma^2)\\). The average: \\(\\bar{X}\\) is an estimator of \\(\\mu\\). The point estimate of the mean: \\(\\bar{x}=\\hat{\\mu}\\) is the estimate of \\(\\mu\\). Example (Sample variance) When we have a normal random variable \\[X \\rightarrow N(\\mu, \\sigma^2)\\] \\(\\sigma^2\\) is a parameter of the population distribution \\(S^2\\) is an estimator of \\(\\sigma^2\\) \\(s^2=\\hat{\\sigma}^2\\) is the estimate of \\(\\sigma^2\\) 12.2 Properties The estimators have important properties that allow us to determine the type of errors we can make when using them to estimate parameters. There are two types of error: the systematic error measured by the bias, and the random error measured by the variability of the sample. When there is low systematic error we say that the estimate is accurate (low bias) and if the observations from the sample do no vary much between then we say that the estimate is precise. An estimator \\(\\Theta\\) is unbiased if its expected value is the parameter \\[E(\\Theta)=\\theta\\] Therefore, the estimator is accurate and does not have systemic error. For example: \\(\\bar{X}\\) is an unbiased estimator of \\(\\mu\\) because \\(E(\\bar{X})=\\mu\\) \\(S^2\\) is an unbiased estimator of \\(\\sigma^2\\) because \\(E(S^2)=\\sigma^2\\) An estimator is consistent when its observed values get closer and closer as the sample size is increased \\[lim_{n\\rightarrow \\infty} V(\\Theta) = 0\\] Therefore the estimator is increases precision when the sample size increases. For example: \\(\\bar{X}\\) is consistent because \\(V(\\bar{X})=\\frac{\\sigma^2}{n}\\rightarrow 0\\) when \\(n \\rightarrow \\infty\\). The mean squared error \\(mse\\) of \\(\\Theta\\) is its expected squared difference from the parameter \\[mse(\\Theta)=E([\\Theta - \\theta]^2)\\] or equivalently is the sum of the errors in consistency and bias \\[mse(\\Theta)=se^2 + bias^2\\] where \\(se=\\sqrt{V(\\Theta)}\\) is the standard error. Efficient estimators are those with the lowest variance among unbiased estimators. 12.3 Maximum likelihood Now that we know what estimators are and what makes them good, we need a method to derive them from data. The method of maximum likelihood does this by choosing an observed value of the estimator as a likely value of a parameter that which makes the observed data most probable. Remember that the parameter is un-observable and, if we do not know its theoretical value, we are trying to learn its most likely value using experimental data. How can then we obtain estimators of the parameters of any probability model? The method of maximum likelihood offers a strategy to propose estimators. Example (Alpha particles) In 1910, Rutherford y Geiger counted the number of \\(\\alpha\\) particles that a specimen of polonium would emit each \\(7.5\\) seconds. The absolute frequencies that they reported for the number of particles each \\(7.5\\) seconds from a total of \\(2608\\) observations were \\[ \\begin{array}{cc} \\mathbf{outcome} &amp; \\mathbf{n_i} \\\\ 0 &amp; 57 \\\\ 1 &amp; 203 \\\\ 2 &amp; 383 \\\\ 3 &amp; 525 \\\\ 4 &amp; 532 \\\\ 5 &amp; 408 \\\\ 6 &amp; 273 \\\\ 7 &amp; 139 \\\\ 8 &amp; 45 \\\\ 9 &amp; 27 \\\\ 10 &amp; 10 \\\\ 11 &amp; 4 \\\\ 12 &amp; 0 \\\\ 13 &amp; 1 \\\\ 14 &amp; 1 \\\\ \\hline \\mathbf{sum} &amp; 2608 \\\\ \\end{array} \\] They observed, for instance, that \\(57\\) of the times they did not observed any particle within \\(7.5\\) seconds, \\(203\\) of the times they counted one particles within \\(7.5\\) seconds, etc. They wanted to show that particles were emitted from independent sources (atoms) and find a probabilistic model for the counts. What is a probability function that can describe the data? Proposing a probability model To answer the question on how to select a probabilistic model when we have some data, we follow the steps: Step 1. we propose probability function on the nature of the random variable (continuous or discrete) that depends on parameters, Step 2. we derive the estimators for the parameters, by maximum likelihood, Step 3. finally, we use the estimator to estimate the parameters with the data. In many applications, we can propose parametric models that is a model that belongs to a family of a probability functions that depends on some parameters. Proposing a probability model is done by following the general properties of the observations, or by what we expect to observe. Modelling requires experience, skill and knowledge of several mathematical functions. However, in most cases well known models are typically applied. Example (Alpha particles) In the appendix of Rutherford and Geiger’s paper, Bateman showed that if there were independent sources alpha particles, the number of particles detected in a time interval would follow a Poisson distribution. That is \\[X \\sim Poisson (\\lambda)\\] Therefore, once selected the theoretical model, they needed to find the value of the parameter \\(\\lambda\\) for their data. Many values of the parameters could explain the data. We are interested in one criterion to choose one particular value. The maximum likelihood method will gives us the estimator for \\(\\lambda\\) \\[\\hat{\\lambda}_{ml}\\] using an optimal probabilistic argument. 12.4 Maximum likelihood The objective is to find the value of the parameter that we believe can best represent the data. The method of maximum likelihood is based on the search for the parameter value that makes the observation of the sample the most probable. Remember, a random sample is a random variable \\[M=(X_1,...X_n)\\] an observed sample is an outcome of \\(M\\), a set of numerical values that we actually obtained when we repeated the random experiment \\[m=(x_1,...x_{n=2608})=c(10, 14, ... 0, 4)\\] Maximum likelihood step 1 First, we calculate the probability of having observed the particular \\(n\\)-sample: \\(x_1,...x_n\\). This is the product of probabilities for each observation because the repetitions of the random experiment are independent of one another. If the probabilistic model of is \\[X \\sim f(x, \\theta)\\] Then the probability of observing the data is \\(P(M=x_1,...x_n)=P(X=x_1)P(X=x_2)...P(X=x_n)\\) \\[=f(x_1;\\theta)f(x_2;\\theta) ...f(x_n;\\theta)\\] We call this function the likelihood function and we consider that: Once the data are observed, they are fixed The unknown is the parameter \\(\\theta\\) \\[L(\\theta)= \\Pi_{i=1..n} f(x_i; \\theta)\\] Example (Alpha particles) For the alpha particle experiment the model in Poisson \\[X \\sim f(x; \\lambda)= \\frac{e^{-\\lambda}\\lambda^x}{x!}\\] and the unknown the parameter \\(\\lambda\\). Therefore, the likelihood is \\(L(\\lambda;x_1,..x_n)= \\frac{e^{-\\lambda}\\lambda^{x_1}}{x_1!}\\frac{e^{-\\lambda}\\lambda^{x_2}}{x_2!}...\\frac{e^{-\\lambda}\\lambda^{x_n}}{x_1!}=\\) \\[\\frac{e^{-n\\lambda}\\lambda^{\\sum {x_i}}}{x_1!x_2!..x_n!}\\] Maximum likelihood step 2 We then ask: what is the value of \\(\\theta\\) that makes the observed sample the most probable event? We thus want to maximize \\(L(\\theta)\\) with respect to \\(\\theta\\). Since we have the multiplication of many factors, it is easier to maximize the logarithm of \\(L(\\theta)\\). This is called the log-likelihood function: \\[\\ln L(\\theta;x_1,..x_n)\\] Example (Alpha particles) In the alpha particles example, we therefore take the logarithm and obtain the Log-likelihood \\[\\ln L(\\alpha;x_1,..x_n)= -n\\lambda - \\sum{x_i}\\ln(\\lambda)-\\ln(x_1!x_2!...x_3!)\\] Maximum likelihood step 3 Finally we maximize the log-likelihood with respect to the parameter. Therefore, we differentiate the log-likelihood with respect to the parameter \\(\\theta\\), equate to zero and solve for the maximum. \\[\\frac{d \\ln L(\\theta)}{d \\theta} \\big|_{\\hat{\\theta}}=0 \\] The value of the parameter at the maximum is called the maximum likelihood estimate for the parameter and it is written with a hat \\(\\hat{\\theta}\\). Example (Alpha particles) We derive the log-likelihood \\[\\frac{d \\ln L(\\lambda)}{d \\lambda}= -n + \\frac{\\sum{x_i}}{\\lambda}\\] The maximum is where the derivative is \\(0\\). This maximum is the value of our estimator \\(\\hat{\\lambda}_{ml}\\). \\[\\hat{\\lambda}_{ml}=\\frac{1}{n}\\sum_{i=1}^{n}\\] The estimator of the parameter is therefore (note the capital letters) \\[\\Lambda=\\frac{1}{n}\\sum_{i=1}^{n}\\] Which is a the sample mean, a random variable function of the random sample \\[(X_1, X_2, ... X_n)\\] We then have the fact that the sample mean is the maximum likelihood estimator of the parameter \\(\\lambda\\) of a Poisson model. Estimating the parameters with the data In our example, we then have the observation of the random sample as a set of 2608 numbers \\((x_1, x_2, ...x_{2608})\\), we therefore substitute the numbers in the estimator and this will give us its observed value. \\[\\hat{\\lambda}_{ml}=-\\frac{1}{n}\\sum_{i=1}^{n} =\\sum_{i=1}^{n} x_if_i =3.871549\\] Using the relative frequency table. Therefore, the maximum likelihood estimate of the parameter is \\(3.871549\\), as reported by Rutherford and Gaiger. If we substitute this value in the probability function, and overlay it with the bar plot, we can see that it gives us a suitable description of the data. Let us look at the log-likelihood function for the \\(2608\\) observations. Remember, data is fixed the data of the experiment and that \\(\\lambda\\) varies. The function has a maximum. However, if we take another sample this function changes and so does its maximum. Example (Number of Individuals per Species) In another influential work, Fisher and colleagues analysed the number of individuals for each moth species that was captured over 4 years at Rothamsted Experimental Station (R. A. Fisher, Corbet, and Williams 1943). This is their data aggregated in absolute frequencies \\[ \\begin{array}{cc} \\mathbf{outcome} &amp; \\mathbf{n_i} \\\\ 1 &amp; 102 \\\\ 2 &amp; 41 \\\\ 3 &amp; 18 \\\\ 4 &amp; 12 \\\\ 5 &amp; 8 \\\\ 6 &amp; 5 \\\\ 7 &amp; 1 \\\\ 8 &amp; 2 \\\\ 9 &amp; 1 \\\\ 10 &amp; 2 \\\\ 12 &amp; 1 \\\\ 13 &amp; 1 \\\\ 16 &amp; 1 \\\\ \\hline \\mathbf{sum} &amp; 195\\\\ \\end{array} \\] that represents the number of individuals observed for a species during the four years campaign. This is \\(102\\) species were observed only once, \\(42\\) species twice and so on. The random variable \\(X\\) is then the number of events in a period of time on a particular species. Here the species is the observation unit or the repetition of the random experiment. The random experiment was repeated 195 times, and Fisher proposed that the random variable \\(X\\) followed a log-series distribution \\[X \\sim \\frac{-\\theta^x}{x\\ln(1-\\theta)}\\] where \\(\\theta \\in (0,1)\\), rather than a Poisson distribution. To find the value of \\(\\theta\\) we can use the method of Maximum Likelihood, where we first propose the likelihood of the data \\[ L(\\theta) = \\prod_{i=1}^{n} \\left( -\\frac{\\theta^{x_i}}{x_i \\log(1 - \\theta)} \\right) \\] Where \\(x\\) is the dis-aggregated data, one observation (number of counts in 4 years) per species \\((1, 1, ...1_{102}, 2, ...2_{41}, ..., 12, 13, 16)\\). We can then optimize the logarithm of \\(L(\\theta)\\). \\[ \\ln L(\\theta) = \\sum_{i=1}^{n} \\ln\\left( -\\frac{\\theta^{x_i}}{x_i \\ln(1 - \\theta)} \\right) \\] While the solution cannot be written in terms of known functions, we can solve it numerically, giving the maximum at \\[\\hat{\\theta}=0.77\\] As reported by Fisher. He also defined the biodiversity index \\[\\alpha=N \\frac{1-\\theta}{\\theta}\\sim 134.1\\] Were \\(N=449\\) is the total number of individuals observed. This value represents a system of high richness-many species are present- and high evenness-individuals are fairly well distributed among species. Example (The Normal Distribution) Gauss used astronomical data from Giuseppe Piazzi to infer the true position of Ceres at a given time, Gauss derived the error function \\[f(x; \\mu, \\sigma^2)= \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2\\sigma^2} (x-\\mu)^2}\\] Where the true position of Ceres was the mean \\(\\mu\\), or the expected value of the outcomes. How did Gauss combined the data for having the best estimate for the position of Ceres? What is the statistic that can describe best its position? In modern times, this question can be formulated as: What is the maximum likelihood estimator of \\(\\mu\\) for a random normal variable? Maximum likelihood of the normal distribution For a random normal variable \\[X \\rightarrow N(\\mu, \\sigma^2)\\]. What are the estimators of \\(\\mu\\) and \\(\\sigma^2\\) that maximize the probability of the observed data? We follow the maximum likelihood method: Step 1. The likelihood function, or the probability of having observed the sample \\((x_1, ....x_n)\\) is \\(L(\\mu, \\sigma^2)=\\Pi_{i=1..n} f(x_i;\\mu,\\sigma)\\) \\[=\\big( \\frac{1}{\\sigma \\sqrt{2 \\pi}}\\big)^n e^{-\\frac{1}{2\\sigma^2} \\sum_i(x_i-\\mu)^2}\\] Step 2. We take the log of \\(L\\), and compute the log-likelihood \\[\\ln L(\\mu, \\sigma^2)=-n \\ln(\\sigma \\sqrt{2 \\pi})-\\frac{1}{2\\sigma^2} \\Sigma_i(x_i-\\mu)^2\\] The estimates of \\(\\mu\\) and \\(\\sigma^2\\) are where the likelihood is maximum. They give the highest probability for the data. Step 3. We differentiate with respect to \\(\\mu\\) and \\(\\sigma^2\\). These two derivatives give us two equations, one for each of the parameters. For deriving respect to \\(\\sigma^2\\), it is easier to make a substitution \\(t=\\sigma^2\\). \\(\\frac{d \\ln L(\\mu, \\sigma^2)}{d\\mu}=\\frac{1}{\\sigma^2} \\sum_i(x_i-\\mu)\\) \\(\\frac{d \\ln L(\\mu, \\sigma^2)}{d\\sigma^2}=-\\frac{n}{2 \\sigma^2}+\\frac{1}{2\\sigma^4} \\sum_i(x_i-\\mu)^2\\) The derivatives are \\(0\\) at the maxima \\(\\frac{1}{\\hat{\\sigma}^2} \\sum_i(x_i-\\hat{\\mu})=0\\) \\(-\\frac{n}{2 \\hat{\\sigma}^2}+\\frac{1}{2\\hat{\\sigma}^4} \\sum_i(x_i-\\hat{\\mu})^2=0\\) solving both equations for the parameters we find for \\(\\mu\\) \\[\\hat{\\mu}_{ml}=\\frac{1}{n}\\sum_i x_i=\\bar{x}\\] and for \\(\\sigma^2\\) \\[\\hat{\\sigma}^2_{ml}=\\frac{1}{n}\\sum_i(x_i-\\bar{x})^2\\] Therefore, the sample mean (average) \\(\\bar{X}\\) is the maximum likelihood estimator of the mean \\(\\mu\\). Gauss showed that the statistics that we should trust most (that with highest likelihood) for the real position of the Ceres was the average. Gauss solving the position of Ceres, not only discovered the normal distribution, but also created the regression analysis and showed the importance of the average. It is due to him that we often use the average as a preferred statistics, assuming that the observations follow a normal distribution. In addition, the maximum likelihood estimator of \\(\\sigma^2\\) is a biased estimator because it can be shown that \\[E(\\hat{\\sigma}^2_{ml})=\\sigma^2-\\frac{\\sigma^2}{n}\\neq \\sigma^2\\] It was Fisher who showed that this estimator was important, as he used it to generalize the central limit theorem. 12.5 Questions 1) An estimator is not \\(\\qquad\\)a: a statistic; \\(\\qquad\\)b: a random variable; \\(\\qquad\\)c: discrete; \\(\\qquad\\)d: an observation of the parameter; 2) An estimator is unbiased if \\(\\qquad\\)a: it is the parameter that it estimates; \\(\\qquad\\)b: depends on \\(1/n\\); \\(\\qquad\\)c: its variance is small; \\(\\qquad\\)d: its expected value is the parameter it estimates; 3) An estimator is consistent if \\(\\qquad\\)a: it is the parameter that it estimates; \\(\\qquad\\)b: depends on \\(1/n\\); \\(\\qquad\\)c: its variance is small; \\(\\qquad\\)d: its expected value is the parameter it estimates; 4) The maximum likelihood method \\(\\qquad\\)a: Produces estimators based on the probability of the observations; \\(\\qquad\\)b: produces unbiased estimators; \\(\\qquad\\)c: produces consistent estimators; \\(\\qquad\\)d: produces estimators equal to those of the method of moments; 12.6 Exercises 12.6.0.1 Exercise 1 Take a random variable with the following probability density function \\[ f(x)= \\begin{cases} (1+\\theta)x^\\theta,&amp; \\text{if } x\\in (0,1)\\\\ 0,&amp; x\\notin (0,1) \\end{cases} \\] What is the maximum likelihood estimator for \\(\\theta\\)? If we take a \\(5\\)-sample with observations \\(x_1 = 0.92; \\qquad x_2 = 0.79; \\qquad x_3 = 0.90; \\qquad x_4 = 0.65; \\qquad x_5 = 0.86\\) What is the estimated value of the parameter \\(\\theta\\)? Compute \\(E(X)=\\mu\\) as a function of \\(\\theta\\). What is the maximum likelihood estimator for \\(\\mu\\)? 12.6.0.2 Exercise 2 For a random variable with a binomial probability function \\[f(x; p)=\\binom n x p^x(1-p)^{n-x}\\] What is the maximum-likelihood estimator of \\(p\\) for a sample of size \\(1\\) of this random variable? In one exam of \\(100\\) students we observed \\(x_1=68\\) students that passed the exam. What is the estimate of the \\(p\\)? 12.6.0.3 Exercise 3 Take a random variable with the following probability density function \\[ f(x)= \\begin{cases} \\lambda e^{-\\lambda x},&amp; \\text{if } 0 \\leq x\\\\ 0,&amp; otherwise \\end{cases} \\] What is the maximum likelihood estimator for \\(\\lambda\\)? If we take a \\(5\\)-sample with observations \\(x_1 = 0.223 \\qquad x_2 = 0.681; \\qquad x_3 = 0.117; \\qquad x_4 = 0.150; \\qquad x_5 = 0.520\\) What is the estimated value of the parameter \\(\\lambda\\)? What is the maximum likelihood estimator of the parameter \\(\\alpha=\\frac{n}{\\lambda}\\) Is \\(\\alpha\\) an unbiased and consistent estimator of the mean of the sample sum \\(E(Y)\\), where \\(Y=\\sum_1^n X_i\\)? References "],["interval-estimation.html", "Chapter 13 Interval estimation 13.1 Revisiting parameter estimation and marging of error 13.2 Interval estimation for the mean 13.3 Confidence Interval Estimation 13.4 Estimation of the variance 13.5 Confidence interval for the variance 13.6 Questions 13.7 Exercises 13.8 Practice", " Chapter 13 Interval estimation We perform repeated measurements of a random experiment to learn about the invariant properties that define it. To separate the signal from random variation, we can propose a probabilistic model whose parameters represent abstract properties of the underlying process. These parameters are assumed to remain fixed, while the randomness in the observations is described by the probability distribution over possible outcomes. The assumption of parameter invariance is often reasonable. For instance, we do not expect the electric charge of an electron to change. However, other quantities—such as the average height of a population—may vary across generations. In more complex or extreme systems, such as city size distributions or gene connectivity networks, parameters like the mean or variance may not even be well defined; these systems may exhibit scale-free behavior. Given the diversity of experimental settings, the nature of the parameters we estimate can vary widely. To begin, we will consider simpler cases where, under controlled conditions, we can assume that the parameters are fixed. This allows us to define estimators that will tend to cluster around the value of the parameter. In this framework, the only source of variation in parameter estimation comes from the randomness in the sample. For example, when using the average to estimate the mean of a random variable, it is the average—not the mean of the variable—that changes across samples. This setup naturally leads to a central question: How confident can we be that the unknown (but fixed) parameter is close to its estimated value? This is a challenging question, since we do not know the parameter’s location. How can we answer it using only the data we have? The key idea is to consider what would happen if we repeated the sampling process many times. If we assume a probability model for the data, we can ask: How far would the new estimates typically fall from the original one? If the estimator is unbiased, the spread of its sampling distribution can give us a sense of how close the estimates are likely to be to the true parameter value—because repeated estimates will cluster around it. The approach is known as frequentist, imaging repetitions of the sample. In this chapter, we introduce the concept of confidence intervals for means, proportions, and variances. We will derive formulas for confidence intervals under various conditions, including when the population variance is known or unknown, and when the sample size is large. 13.1 Revisiting parameter estimation and marging of error The expected value or the mean is a fundamental parameter of the random experiment. The researchers typically ask, if an experiment is repeated, the observations will gather about a number in a consistent manner? Estimation of the mean We have seen that whenever we take a random sample \\((X_1, X_2, ... X_n)\\), the sample mean \\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^n X_i\\] is an estimator of the mean \\(\\mu\\) of the random variable \\(X\\). That is \\[\\bar{x}=\\hat{\\mu}\\] We take the average (the observation of the sample mean), as the value of \\(\\mu\\) we trust the most given our data (maximum likelihood). The estimator is unbiased because its values center about the parameter it is estimating \\(E(\\bar{X})=\\mu\\) and consistent because as \\(n\\) increases then it is closer to the parameter, as its variance gets smaller \\(V(\\bar{X})=\\frac{\\sigma^2}{n}\\) where \\(\\sigma^2\\) is the variance of \\(X\\). We call the quantity \\(\\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{n}}\\) the standard error (\\(se\\)). Since \\(\\bar{X}\\) is a random variable the estimation of the mean changes when we take another sample. Therefore, we know that we are making an error every time we take \\(\\bar{x}\\) for \\(\\mu\\). Margin of error When deciding whether the error in estimation \\[\\bar{X}-\\mu\\] is large or not, we defined the margin of error. The margin of error at \\(5\\%\\) is the distance \\(m\\) from \\(\\mu\\) such that \\(\\bar{X}\\) has a probability of \\(95\\%\\) to be observed within: \\[P(-m \\leq \\bar{X}-\\mu \\leq m)=0.95\\] If we assume that the distribution of \\(X\\) is normal, \\(X \\sim N(\\mu, \\sigma^2)\\), then the standardized error from the mean \\[T=\\frac{\\bar{X}-\\mu}{\\frac{S}{\\sqrt{n}}}\\] follows a \\(t\\)-distribution with \\(n-1\\) degrees of freedom \\[T \\sim t_{n-1}\\] where \\(S^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar{X})^2\\) is the sample variance. The margin of error \\(m\\) at \\(5\\%\\) is given by the \\(P(\\mu-m \\leq \\bar{X} \\leq\\mu + m)\\) \\[=P(-\\frac{m}{S/\\sqrt{n}} \\leq T \\leq\\frac{m}{S/\\sqrt{n}})=0.95\\] Since \\(T\\) is contained in the interval \\((-t_{0.025, n-1},t_{0.025, n-1})\\) with \\(95\\%\\) of the probability, then the margin of error is \\[m=t_{0.025, n-1} \\frac{s}{\\sqrt{n}}\\] where the limit of the interval \\(t_{0.025, n-1}\\) is the value of \\(T\\) that leaves \\(2.5\\%\\) of probability at the right hand side of the \\(t\\)-distribution with \\(n-1\\) degrees of freedom Python: t.sf(0.025, n-1) R: qt(0.025, n-1, lower.tail = FALSE) Note that the margin of error is the expectation of how far the average will be observed from the mean with \\(95\\%\\) of probability. Clearly if the observed average is luckily the mean then the error is zero, the errors will be symmetrical about zero and higher errors will be less probable. 13.2 Interval estimation for the mean The problem is that in many experiments we do not know \\(\\mu\\). If we are going to collect the first sample of the experiment, we do not have an expectation of how far the average is going to be observed from the mean, nor an expectation of its deviation from it. Example (CRISPR) CRISPR-Cas9 is a technology that enables gene editing. CRISPR-Cas system uses a guide RNA to direct the Cas9 enzyme to a specific DNA sequence, where it creates a cut. This allows researchers to disrupt, correct, or insert genetic material at targeted sites in the genome. The first-in-human study using CRISPR-edited cells, assessed the safety and feasibility of editing immune T cells to combat a form of lung cancer that had not responded to standard therapies (Lu et al. 2020). This is the reported efficiency of editing in the study \\[ \\begin{array}{cc} \\mathbf{Patient} &amp; \\mathbf{Editing Efficiency (\\%)} \\\\ C-03 &amp; 16 \\\\ C-02 &amp; 34 \\\\ C-01 &amp; 10 \\\\ B-03 &amp; 16 \\\\ B-02 &amp; 12 \\\\ B-01 &amp; 30 \\\\ A-04 &amp; 22 \\\\ A-03 &amp; 10 \\\\ A-02 &amp; 20 \\\\ A-01 &amp; 18 \\\\ PreA-02 &amp; 8 \\\\ PreA-01 &amp; 16 \\\\ \\end{array} \\] This data is the repetition of a random experiment 12 times, where the efficiency was the random variable and the observation unit a patient. As the same experimental procedure was applied, it is expected that the efficiency will cluster around some value that represents the CRISPR experiment of the study. The observations (crosses) and the average (dot) is all we have. We can assume that if the researchers were to repeat the sample on 12 different patients, the average will not be far from the value that they observed, because the average clusters around the mean. However, we do not know where the mean is. Nonetheless, we can assume that editing efficiency is normally distributed \\[X \\sim N(\\mu, \\sigma^2)\\] at unknown \\(\\mu\\) and \\(\\sigma\\). Data suggest that the expected editing efficiency \\(\\mu\\) is about \\(\\bar{x}=17.66\\). But, how confident are we? after all, we know that when we take the value of \\(\\bar{x}\\) for \\(\\mu\\), we know we are making a mistake but do not know how big it actually is. The key is to note that the dispersion of sample (\\(s\\)) will give us an idea on how the average will disperse (\\(s/\\sqrt{n}\\)). Definition To address the question of where \\(\\mu\\) is, we define the confidence interval for \\(\\mu\\). From the margin of error equation \\[P(-m \\leq \\bar{X} - \\mu \\leq m)=0.95\\] We solve for \\(\\mu\\), which is indeed the real unknown \\[P(\\bar{X} - m \\leq \\mu \\leq \\bar{X} + m)=0.95\\] The left and right limits of the inequality are random variables which motivate the definition for the random confidence interval at \\(95\\%\\): \\[(L,U)=(\\bar{X} - m,\\bar{X} + m)\\] This interval is a new random variable and it has by definition a probability of \\(0.95\\) to contain \\(\\mu\\). The observed interval that we obtain from the experiment is (lower case) \\[(l,u)=(\\bar{x} - m,\\bar{x} + m)\\] This interval either contains or it does not contain the parameter \\(\\mu\\): we will never know! However, we can still say that we have a confidence of \\(95\\%\\) that the interval \\((l,u)\\) has captured the true unknown parameter \\(\\mu\\). Think of a lottery scratch ticket that we are not allow to scratch to see the prize before buying it. The ticket either has or does not have the prize, only that we do not know which case it is. However, we can be confident that the ticked does not have the main prize, because the probabilities of the game (not the instance in my hand) are low. 13.3 Confidence Interval Estimation We can estimate confidence intervals when we are able find an standardization of an estimator with a known distribution. Standardization allows separating the random variation from the parameters, which is not always possible but can be done for some useful and widely applied examples. Notice that the interval depends of the margin error, which is a parameter (property) of the estimator. As such, the observed interval needs to be computed with a required statistical interpretation and, consequently, it is not a pure datum of the experiment. Hence, confidence intervals are estimated rather then directly observed from the experiment. 13.3.1 Estimation of the mean for normal variables When the random variable \\(X\\) is a normal variable, we can readily write the confidence interval at \\(95\\%\\) \\[(l,u)=(\\bar{x} - m, \\bar{x} + m)\\] where \\[m=t_{0.025, n-1} \\frac{s}{\\sqrt{n}}\\] That is: \\[(l,u)=(\\bar{x} - t_{0.025, n-1} \\frac{s}{\\sqrt{n}}, \\bar{x} + t_{0.025, n-1} \\frac{s}{\\sqrt{n}})\\] Example (CRISPR) In the CRISPR example, we assume that \\(X\\) is normally distributed and from the data we can compute \\(\\bar{x}= 17.66\\) and \\(s=7.9\\). Then the margin of error is \\[m=t_{0.025, 11} \\frac{s}{\\sqrt{n}}=2.2\\frac{7.9}{\\sqrt{12}}=5.04\\] and the \\(95\\%\\) confidence interval is \\((l,u)=(\\bar{x} - m, \\bar{x} + m)=\\) \\[(12.61, 22.71)\\] Which means that we are \\(95\\%\\) confident that the efficiency of the T-cell editing can be found between \\(12.6\\%\\) and \\(22.7\\%\\) Python: from scipy.stats import ttest_1samp x = [16, 34, 10, 16, 12, 30, 22, 10, 20, 18, 8, 16] res = ttest_1samp(x, popmean=0) print(res.confidence_interval) R: x &lt;- c(16, 34, 10, 16, 12, 30, 22, 10, 20, 18, 8, 16) t.test(x)$conf.int We can also write the interval as \\[\\hat{\\mu}=\\bar{x} \\pm m = 17.6 \\pm 5.0\\] This means that, when estimating the mean by the average, we are confident that the efficiency is between the tenths and twenties of percentage points, and less confident in the figures on percentage units. We have no confidence on the decimal places. Remember that the confidence interval \\((l,u)\\) (black below) is an observation of the random confidence interval \\((L,U)\\). Therefore, if we change the sample then \\((l,u)\\) changes (grey below). The frequentist statistician encourages us to imagine selecting a different group of \\(12\\) patients, compute the confidence intervals, and repeat this process an infinite amount of times. Then, about \\(95\\%\\) of the confidence intervals will contain the unknown \\(\\mu\\). We just do not know which intervals, including the estimated one, will contain \\(\\mu\\)! We can say that Lu and colleagues achieved a T-cell editing efficiency of about \\(17\\%\\) as they repeated the experiment 12 times in similar conditions. It is clear, however, that other laboratories may obtain different efficiencies, as some of the conditions will change. Therefore, we expect that the parameter \\(\\mu\\) will change between laboratories in addition to sampling error. At \\(17\\%\\) efficiency, they showed that the treatment appear to be safe for 12 cancer patients, but as efficiency is improved and more patients are recruited then patient safety will need to be updated. Confidence level We can change our confidence from \\(95\\%\\) to \\(99\\%\\). When we computed the margin of error at \\(95\\%\\), we left out \\(\\alpha=0.05\\) probability, \\(0.025\\) on each side of the standardized error. Now, we can leave out \\(\\alpha=0.01\\) probability, \\(0.005\\) on each side. Therefore the \\(99\\%\\) confidence interval is \\[(l,u) = (\\bar{x} - t_{0.005, n-1}\\frac{s}{\\sqrt{n}},\\bar{x} + t_{0.005, n-1}\\frac{s}{\\sqrt{n}})\\] where \\(t_{0.005, n-1}=F^{-1}(0.995)\\) is the inverse of the probability density function for the \\(t\\)-distribution. Python: t.ppf(1-0.005, n-1) R: qt(0.005, n-1, lower.tail = FALSE) Example (Impact energy) A metallic material is tested for impact to measure the energy required to cut it at a given temperature. Ten specimens of A238 steel were cut at 60ºC at the following impact energies (J): 64.1, 64.7, 64.5, 64.6, 64.5, 64.3, 64.6, 64.8, 64.2, 64.3 If we assume that the impact energy is normally distributed we can estimate the \\(99\\%\\) confidence interval for the cutting energy, as an intrinsic propioerty of the material. From the data we have \\(\\bar{x}=64.46\\) \\(s=0.227\\) we assume \\(\\alpha=0.01\\) (the confidence limit) \\(t_{0.005,9}=3.24\\) obtained from \\(t_{0.005,9}=\\) t.ppf(1-0.005, 9) The confidence interval is then \\((l,u)=(\\bar{x}- t_{0.005,9}\\frac{s}{\\sqrt{n}},\\bar{x}+t_{0.005,9} \\frac{s}{\\sqrt{n}})\\) \\[=(64.22, 64.69)\\] Python: from scipy import stats x = [64.1,64.7,64.5,64.6,64.5,64.3,64.6,64.8,64.2,64.3] res = stats.ttest_1samp(x, popmean=0) res.confidence_interval(confidence_level=0.99) R: x &lt;- c(64.1,64.7,64.5,64.6,64.5,64.3,64.6,64.8,64.2,64.3) t.test(x, conf.level = 0.99)$conf.int Note that the confidence interval at \\(95\\%\\) is \\((64.29, 64.62)\\), which is smaller than the \\(99\\%\\) interval. The wider intervals with increasing confidence shows the trade-off between confidence and precision. 13.3.2 Estimation of the proportion for dichotomic variables Example (Mendel’s peas) Mendel first produced a large number of hybrid pea plants. These were plants for which one parent descended from a long line of plants that produced round peas, and the other from a long line that produced wrinkled peas. He then took pairs of these hybrid plants and produced 253 pea plants. He collected a total of 7,324 seeds from the hybrid offspring and observed that 5,474 were round and 1,850 were wrinkled (Mendel 1901). What is the estimated proportion of round seeds in the offspring observed by Mendel? The outcome \\(X_i\\) that the \\(i\\)-th seed is a round pea is a Bernoulli trial \\[X_i \\sim Bernoulli(p)\\] with mean \\(\\mu=p\\) and variance \\(\\sigma^2=p(1-p)\\). Mendel’s sample would look something like \\[(x_1,x_2, x_3, ...x_{n=7324})=(0,1,0,.. 1, 0)\\] with \\(5474\\) ones and in a total of \\(7324\\) repetitions of the trial. The sample has an average \\[\\bar{x}=\\frac{1}{7324}\\sum_{i=1}^{7324} x_i=\\frac{5474}{7324}=0.747\\] Which is nothing but the relative frequency of the round peas. Since, in general, the sample mean is an unbiased estimator of \\(\\mu\\), then, for the Bernoulli trial, this the relative frequency is a point estimate of the parameter \\(p\\) \\[\\hat{p}=\\bar{x}=0.747\\] This makes sense because \\(\\bar{x}\\) is the observed relative frequency of ones \\(f_1\\) in the sample. And as such, it is an estimator of the probability of observing a one in a Bernoulli trial \\[f_1 =\\hat{P}(X=1)\\] However, how confident are we about this estimation? That is, how confident are we to take the relative frequency as the value of the probability? We want a confidence interval for \\(p\\). Confidence intervals for proportions When \\(n\\hat{p}&gt;5\\) and \\(n(1-\\hat{p})&gt;5\\), the standardized error of estimating \\(p\\) with \\(\\bar{X}\\) can be approximated to a standard normal variable with the help of the central limit theorem, using De Moivre’ normal approximation of the Binomial. \\[Z=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}= \\frac{\\bar{X}-p}{\\big[\\frac{p(1-p)}{n} \\big]^{1/2}}\\sim N(0,1)\\] Therefore the margin of error \\(m\\) at \\(95\\%\\) is \\[m= z_{0.025}\\frac{\\sqrt{p(1-p)}}{\\sqrt{n}}\\] Since we do not know what the value of \\(p\\) is then we will estimate it with \\(\\bar{x}\\) and then \\[m= z_{0.025}\\frac{\\sqrt{\\bar{x}(1-\\bar{x})}}{\\sqrt{n}}\\] Which is a good approximation when the central limit theorem starts to hold. Therefore, an approximate \\(95\\%\\) CI interval of \\(p\\) is: \\[CI=(l,u)=(\\bar{x}-z_{0.025}\\big[\\frac{\\bar{x}(1-\\bar{x})}{n} \\big]^{1/2}, \\bar{x}+z_{0.025}\\big[\\frac{\\bar{x}(1-\\bar{x})}{n} \\big]^{1/2})\\] Note that we could not solve exactly the margin of error without reference to the parameter \\(p\\). We were able to introduce an estimate because the central limit theorem achieves a suitable standardization when \\(n\\) is large. More accurate confidence intervals for small \\(n\\) or far from \\(p=1\\) and \\(p=0\\) have been developed. Example (Mendel’s peas) In Mendel’s experiments, he counted \\(5474\\) rounded peas in a total of \\(7324\\) total seeds and he therefore obtained \\(\\bar{x}=0.747\\) (relative frequency as a point estimate of the proportion) \\(z_{0.025}=1.96\\) (using norm.sf(0.025)). Therefore the \\(95\\%\\) confidence interval for \\(p\\) is \\((l,u)=(0.747-0.013, 0.747+0.013)\\) \\[=(0.734, 0.760)\\] Python: from statsmodels.stats.proportion import proportion_confint(5474, 7324) R: prop.test(5474, 7324)$conf.int The estimated probability of observing a round pea is \\[\\hat{p} = 0.747 \\pm 0.013\\] which validates Mendel’s first law, as it aligns closely with the theoretical value deduced for \\(p\\). Let \\(A\\) represent the allele for round peas. In Mendel’s experiments, all offspring were produced from hybrid parents carrying alleles \\((A, A&#39;)\\). Since the round trait is dominant, a pea will be round if it inherits at least one \\(A\\) allele. This corresponds to the event \\((A_m \\cup A_f)\\), which occurs in three out of four possible allele combinations. Therefore, the theoretical probability of a round pea is \\(p = 3/4 = 0.75\\), a value that is well captured by the inferred confidence interval of the relative frequency. Mendel’s success lay in identifying a simple biological system where probabilities could be reasoned and predictions made about where relative frequencies would cluster. However, it is important to remember that this reasoning was grounded in prior observation, experimentation, and reflection on empirical data. A map cannot be drawn without first exploring the territory. 13.4 Estimation of the variance The variance is another fundamental parameter of the random experiment. The random experiment will be characterized by a variability that may be intrinsic of the observation unit or a characteristic of the experimental error when measuring an outcome. Researchers in this case may ask, if an experiment is repeated, how far the observations will be from the mean? We have seen that whenever we take a random sample \\((X_1, X_2, ... X_n)\\), the sample variance \\[S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})^2\\] is an estimator of the variance \\(\\sigma^2\\) of the random variable \\(X\\). The estimator is unbiased because its values are centered about the parameter it is estimating \\[E(S^2)=\\sigma^2\\] and it is also consistent. We can then take a the value of \\(s^2\\) from a particular sample as the value of \\(\\sigma^2\\), which is a characteristic of random experiment. That is \\[s^2=\\hat{\\sigma}^2\\] Since \\(S^2\\) is a random variable the estimation of the variance changes when we take another sample. Example (impact energy) A metallic material is tested for impact to measure the energy required to cut it at a given temperature. Ten specimens of A238 steel were cut at 60ºC at the following impact energies (J): 64.1, 64.7, 64.5, 64.6, 64.5, 64.3, 64.6, 64.8, 64.2, 64.3 While the cutting energy appears to cluster about a value, the specimens show variability on their energies. Some are cut easier than others. How stable is the cutting energy of the specimens? what is estimation of the variance of these data? The sample variance is \\(s^2=0.051\\) Python: import numpy as np x = [64.1, 64.7, 64.5, 64.6, 64.5, 64.3, 64.6, 64.8, 64.2, 64.3] np.var(x, ddof=1) R: x &lt;- c(64.1, 64.7, 64.5, 64.6, 64.5, 64.3, 64.6, 64.8, 64.2, 64.3) var(x) How confident can we be on the decimals places of this estimation? What is the confidence interval for the variance? 13.5 Confidence interval for the variance To compute a confidence interval of the variance, we need a statistics that is a function of \\(S^2\\) and allows to measure the standardize error we make when we estimate \\(\\sigma^2\\) by \\(s^2\\). If we assume that the distribution of \\(X\\) is normal, \\(X \\sim N(\\mu, \\sigma^2)\\), then the standardized error from the variance \\[W=\\frac{S^2(n-1)}{\\sigma^2}\\] follows a \\(\\chi^2\\) distribution with \\(n-1\\) degrees of freedom \\[W \\sim \\chi^2_{n-1}\\] This is an error rate that if the observed sample variance \\(s^2\\) is luckily the variance \\(\\sigma^2\\) then the rate is \\((n-1)\\). The errors will distribute asymmetrical about \\((n-1)\\) and higher errors and those close to \\(0\\) will be less probable. Using this error rate, we will look for a \\(95\\%\\) confidence interval \\((L,U)\\) of \\(\\sigma^2\\) such that the random interval \\[P(L \\leq \\sigma^2 \\leq U)=0.95\\] captures \\(\\sigma^2\\) with \\(95\\%\\) of probability. We start by determining the values that capture the \\(95\\%\\) of the \\(\\chi^2\\)-distribution \\[P(\\chi^2_{0.975,n-1} \\leq W \\leq \\chi^2_{0.025,n-1})=0.95\\] Replacing the value of \\(W\\) \\[P(\\chi^2_{0.975,n-1} \\leq \\frac{S^2}{\\sigma^2}(n-1) \\leq \\chi^2_{0.025,n-1})=0.95\\] and solving for \\(\\sigma^2\\) \\[P(\\frac{S^2 (n-1)}{\\chi^2_{0.025,n-1}}\\leq \\sigma^2 \\leq \\frac{S^2(n-1)}{\\chi^2_{0.975,n-1}})=0.95\\] We find a random interval that captures \\(\\sigma^2\\) with \\(95\\%\\) confidence \\[(L,U) = (\\frac{S^2 (n-1)}{\\chi^2_{0.025,n-1}},\\frac{S^2(n-1)}{\\chi^2_{0.975,n-1}})\\] The observed \\(95\\%\\) confidence interval (script size) is \\[(l,u) = (\\frac{s^2 (n-1)}{\\chi^2_{0.025,n-1}},\\frac{s^2(n-1)}{\\chi^2_{0.975,n-1}})\\] where \\(\\chi^2_{0.975,n-1}=\\) chi2.sf(0.975, df=n-1) is the value of \\(W\\) that leaves \\(0.975\\) probability to the right hand side. \\(\\chi^2_{0.025,n-1}=\\)chi2.sf(0.025, df=n-1) is the value of \\(W\\) that leaves \\(0.025\\) probability to the right hand side. Example (impact energy) Ten specimens of A238 steel were cut at 60ºC at finding a sample variance \\(s^2=0.05155556\\). To estimate the confidence interval for the variance we compute \\(\\chi^2_{0.975,n-1}=2.700389\\) \\(\\chi^2_{0.025,n-1}=19.02277\\) Python: chi2.ppf(1 - 0.975, df=9) chi2.ppf(1 - 0.025, df=9) R: qchisq(0.975, 9, lower.tail = FALSE) qchisq(0.025, 9, lower.tail = FALSE) Therefore \\[(l,u)= (\\frac{0.227^2 (10-1)}{19.02277},\\frac{0.227^2(10-1)}{2.700389})=(0.02,0.17)\\] We find that we can be \\(95\\%\\) confident to have captured \\(\\sigma^2\\) within this interval. Note that the interval for the variance is not symmetric and we cannot formulate it as an estimate \\(\\pm\\) margin of error. Given an average, we can then estimate with \\(s\\) the expected distance of a subsequent observation of the random experiment, that is its standard deviation \\(\\sigma\\). The confidence interval for the standard deviation will be the square root of the the interval for the variance, and will give us the best and the worst case at which we can expect the observation to fall (orange) from either side of the mean. 13.6 Questions 1) The margin of error at \\(95\\%\\) confidence of a normal variable is \\(\\qquad\\)a: \\(\\frac{s}{\\sqrt{n}}\\); \\(\\qquad\\)b: \\(1.96\\times se\\); \\(\\qquad\\)c: \\(\\frac{\\sigma}{\\sqrt{n}}\\); \\(\\qquad\\)d: \\(\\sigma\\) 2) when we talk about \\(z_{0.025}\\) we mean: \\(\\qquad\\)a: The value of a normal standard variable that has accumulated up to \\(99.75\\%\\) of probability; \\(\\qquad\\)b: The value of a normal standard variable that has accumulates up to \\(0.25\\%\\) of probability; \\(\\qquad\\)c: The probability of a standard variable up to \\(99.75\\%\\); \\(\\qquad\\)d: The probability of a standard variable up to \\(0.25\\%\\) 3) The random confident interval \\((L,U)\\) for the mean at \\(95\\%\\) \\(\\qquad\\)a: is a two dimensional parameter of the sample distribution; \\(\\qquad\\)b: gives the limits where \\(\\mu\\) has a probability of occurring \\(95\\%\\) of the times; \\(\\qquad\\)c: is an estimate of the average; \\(\\qquad\\)d: captures \\(\\mu\\) \\(95\\%\\) of the times 4) A confidence interval for the mean written as \\(\\hat{\\mu}=56.99 \\pm 0.01\\) \\(\\qquad\\)a: indicates that we are \\(\\%99\\) confident that the mean is \\(56.99\\); \\(\\qquad\\)b: indicates that we cannot trust the last decimal place on the estimation of the mean; \\(\\qquad\\)c: indicates that the mean of the population is at \\(56.99\\) with error \\(0.01\\); \\(\\qquad\\)d: indicates that we can trust the unit figure (\\(6\\)) on the estimation of the mean 5)If we know the value of \\(\\mu\\) and find that the confidence interval did not catch it then \\(\\qquad\\)a: the confidence interval is not well computed; \\(\\qquad\\)b: it is a rare observation of the confidence interval; \\(\\qquad\\)c: the confidence interval does not estimate the mean; \\(\\qquad\\)f: there is little probability of finding the mean in the confidence interval 13.7 Exercises 13.7.0.1 Exercise 1 In a scientific paper, the authors report a \\(95\\%\\) confidence interval of \\((228, 232)\\) for the natural frequency (Hz) of a metallic beam. They used a sample of size \\(25\\) and considered that the measurements were distributed normally. What is the mean and the standard deviation of the measurements? Compute the \\(99\\%\\) confidence interval. hints: in R \\(t_{0.025, 24}=\\) t.ppf(1-0.025, 24)\\(\\sim 2\\) in R \\(t_{0.005, 24}=\\)t.ppf(1-0.005, 24)\\(\\sim 2.8\\) 13.7.0.2 Exercise 2 compute \\(95\\%\\) CI the mean of a normal variable with known variance \\(\\sigma^2=9\\) and \\(\\bar{x}=22\\), using a sample of size \\(36\\). 13.7.0.3 Exercise 3 This year, \\(17\\) of \\(1000\\) of patients with influenza developed complications. Compute the \\(99\\%\\) confidence interval for the proportion of complications. The previous year \\(2\\%\\) showed complications. Can we say with \\(99\\%\\) confidence that this year there is a significant drop in influenza complications? 13.7.0.4 Exercise 4 What is the confidence interval for population variance of a normal variable if we take a random sample of size \\(n=10\\) and observe a sample variance of \\(0.5\\)? 13.8 Practice Load misophonia data https://alejandro-isglobal.github.io/SDA/data/data_0.txt Compute the confidence interval for the mean of the cephalometric measures. (“Angulo_convexidad”, “protusion.mandibular”, “Angulo_cuelloYtercio”, “Subnasal_H”) Compute the confidence interval for the proportion of misophonic (“Misofonia”), and depression (“depresion.dic”). Compute the confidence interval for the variance of the age (“Edad”). What is the confidence interval for the standard deviation of the population? Solutions References "],["hypothesis-testing.html", "Chapter 14 Hypothesis Testing 14.1 Hypothesis formulation 14.2 Hypothesis Testing 14.3 Hypothesis testing for the mean 14.4 Hypothesis testing for the proportion 14.5 Hypothesis Testing for the Variance 14.6 Errors in hypothesis testing 14.7 Exercises 14.8 Practice", " Chapter 14 Hypothesis Testing When we perform an experiment, we often want to determine whether changes we introduce have a real effect. For instance, we might want to know if we can influence the outcome of the experiment, or whether a new condition alters the results in a meaningful way. Often, we already have expectations—based on prior knowledge—of how the data should behave when these new conditions are not present. But since the outcomes of experiments are subject to random variation in any case, how can we distinguish a real effect from random noise? The strategy is to construct a probability model for the experiment and assess how the model’s parameters change under different conditions. We use random samples to estimate the parameters and by comparing the estimations under different conditions, we can decide whether the parameters shift in a way that provides evidence of a significant effect. The key challenge is to determine whether the observed variation in the parameter estimation is genuinely due to the changes in conditions or simply a result of sampling variability. In this chapter, we introduce the framework of hypothesis testing for means, proportions, and variances. We will define the null hypothesis (which assumes no change or effect) and the alternative hypothesis (which assumes a meaningful change). Using data, we will learn how to decide whether the evidence supports rejecting the null hypothesis in favor of the alternative. Finally, we will discuss the types of errors that can occur in hypothesis testing—false positives (Type I errors) and false negatives (Type II errors)—and how they affect our conclusions. 14.1 Hypothesis formulation Example (Lithium-ion (Li-ion) batteries) Li-ion batteries are expected to play a pivotal role in supporting the global transition to clean energy and electrified transportation. Their performance, measured in discharged capacity (in milliampere-hours, mAh), is highly sensitive to storage conditions such as temperature and state of charge (SOC). The CALCE battery group systematically studied how different storage temperatures and SOC levels affect long-term battery degradation (Center for Advanced Life Cycle Engineering (CALCE) 2025). The discharged capacity refers to the total amount of electric charge a battery can deliver during a full discharge cycle after being fully charged. For example, batteries stored fully charged for three weeks at 50°C show a discharged capacity of 1.561 mAh after being recharged. This raises a practical and important question: Does storing the battery in a fully discharged state instead improve its discharged capacity after storage? Or more broadly: Should a battery be stored fully charged or fully discharged to minimize degradation over time? We are interested in testing whether the mean discharged capacity of batteries stored fully discharged for three weeks differs from 1.561 mAh, after they have been fully recharged. To answer this, we can formulate two mutually exclusive statements: The mean discharged capacity of the discharged-storage batteries is equal to 1.561 mAh The mean discharged capacity of the discharged-storage batteries is different from 1.561 mAh Only one of these statements can be true. The goal of the experiment is to collect data and decide which of the two better reflects the behavior of the batteries. It is important to note that due to natural variability, some individual batteries may show capacities greater than 1.561 mAh, and some lower. The key question is about the mean performance of the group, not individual observations. Let \\(\\mu\\) represent the mean discharged capacity of batteries stored fully discharged. Then, the above statements can be reformulated as statistical hypotheses: \\(H_0: \\mu = 1.561\\,\\text{mAh}\\) (null hypothesis) \\(H_1: \\mu \\neq 1.561\\,\\text{mAh}\\) (alternative hypothesis) Here, \\(H_0\\) assumes no effect of storage condition—that is, fully discharged storage results in the same capacity as fully charged storage. In contrast, \\(H_1\\) suggests that the storage condition does influence the battery’s performance. These two statements constitute the basis for hypothesis testing. Definition In statistics, a statement (conjecture) about the probability function of a random variable is called a hypothesis. The hypothesis is usually written in two dichotomous statements The null hypothesis \\(H_0\\): when the conjecture is false. It usually refers to the status quo. The data may be explained by the status quo. The alternative hypothesis \\(H_1\\): when the conjecture is true. It usually refers to research hypothesis. The data may be explained by the alternative to the status quo. Example (Fertilizer) What are the null and the alternative hypothesis for the following situation? Fertilizer developers want to test whether their new product has a real effect on the growth of plants. Being \\(\\mu_0\\) the mean growth of the plants without fertilizer (known) and \\(\\mu\\) the mean growth of the plants with the fertilizer (unknown) \\(H_0:\\mu \\leq \\mu_0\\) (The fertilizer may do nothing: status quo) \\(H_1:\\mu &gt; \\mu_0\\) (The fertilizer may have the desired effect: research interest) Example (Chemotherapy) Pharmaceutical companies need to know if a novel chemotherapy can cure 90% of cancer patients. Being \\(p_0\\) the proportion of patients that are cured without the chemotherapy (known) and \\(p\\) the proportion that are cures with the chemotherapy (unknown) \\(H_0:p \\leq p_0\\) (The chemotherapy may do nothing: status quo) \\(H_1: p &gt; p_0\\) (The chemotherapy may have the desired effect: research interest) Note that our new improved experiment has the parameter \\(p\\) and we want to know how it compares to the experiment without any additional improvement that has the parameter \\(p_0\\). We want to decide between \\(H_0\\) and \\(H_1\\). There are two options: We reject the alternative hypothesis \\(H_1\\); that is, we accept the null hypothesis \\(H_0\\). We accept the alternative hypothesis \\(H_1\\) (our interest); that is, we reject the null hypothesis \\(H_0\\). Example (Li-ion Batteries) Suppose we want to store Li-ion batteries but are unsure whether to store them fully charged or fully discharged to minimize long-term degradation. To address this, we formulate a hypothesis test: \\(H_0: \\mu = 1.561\\,\\text{mAh}\\). The mean discharge capacity is the same whether batteries are stored fully charged or fully discharged. \\(H_1: \\mu \\neq 1.561\\,\\text{mAh}\\). The mean discharge capacity differs when batteries are stored fully discharged compared to fully charged. To decide between \\(H_0\\) and \\(H_1\\), we measure the discharge capacity of 12 batteries that were stored fully discharged for three weeks. From the data, the average discharge capacity of the 12 batteries stored fully discharged is \\(\\bar{x} = 1.569232\\) mAh. At first glance, this might suggest that storing batteries discharged reduces degradation compared to storing them fully charged. However, we must consider that \\(\\bar{x}\\) is the result of a random sample and may vary. Sometimes it will be higher than 1.561mAh, and sometimes lower. Therefore, the observation \\(\\bar{x} \\ne 1.561\\) is not, by itself, sufficient evidence to reject \\(H_0\\). This leads us to ask: If \\(H_0\\) is true, what range of values would we typically expect for \\(\\bar{x}\\), based on a sample of 12 batteries? In other words, is \\(\\bar{x} = 1.569232\\) a typical result, assuming the expected value of the new experiment is indeed 1.561 mAh? To answer this, we need a formal decision process, which we introduce in the following sections. 14.2 Hypothesis Testing Let us summarize the different cases, methods, and types of hypothesis testing. We will then discuss each case using a specific example. Hypotheses can be tested — or decided upon — using confidence intervals. Therefore, we will focus on the same cases we encountered when constructing confidence intervals, namely: Hypothesis test for the mean \\(\\mu\\), when \\(X \\sim N(\\mu, \\sigma^2)\\) Hypothesis test for the proportion \\(p\\), when \\(X \\sim \\text{Bernoulli}(p)\\) and both \\(np &gt; 5\\) and \\(n(1-p) &gt; 5\\) Hypothesis test for the variance \\(\\sigma^2\\), when \\(X \\sim N(\\mu, \\sigma^2)\\) There are three methods for testing hypotheses: Using confidence intervals Using a rejection region Using a p-value All three methods are mathematically equivalent and lead to the same decision. Finally, there are three types of hypothesis tests: Two-tailed test Upper-tailed test Lower-tailed test 14.3 Hypothesis testing for the mean A tow tailed hypothesis contrast is of the form \\(H_0:\\mu = \\mu_0\\) (status quo) \\(H_1:\\mu \\neq \\mu_0\\) (research interest) This is called two tailed because the alternative hypothesis \\(H_1\\) requires that the mean \\(\\mu\\) of the new experiment is either lower or higher than the mean \\(\\mu_0\\) of the old experiment, or gold standard. This hypothesis can be tested in different cases. For these hypothesis we will assume that \\(X\\) is a normal variable. 14.3.1 Hypothesis test with a confidence interval Remember that the confidence interval at \\(95\\%\\) for the mean is \\[(l,u)=(\\bar{x}-t_{0.025, n-1} \\frac{s}{\\sqrt{n}},\\bar{x}+t_{0.025, n-1} \\frac{s}{\\sqrt{n}})\\] when the outcomes of the random experiment distribute normally \\(X\\sim N(\\mu, \\sigma^2)\\). Testing Criteria: We can have only two options: If the confidence interval contains the null hypothesis \\(\\mu_0\\): \\[\\mu_0\\in (l,u)\\] then we accept \\(H_0\\) with \\(95\\%\\) confidence. We do not know where the mean the experiment under the new conditions is but we are confident that it is within the confidence interval. Since \\(\\mu_0\\) is within the interval we cannot discard that \\(\\mu_0\\) is the expected value of our experiment. We then say that the data could have been produced by the old experiment, rendering the new conditions irrelevant. If the confidence interval does not contain the null hypothesis\\[\\mu_0\\notin (l,u)\\] then we reject \\(H_0\\) with \\(95\\%\\) confidence. Since we are confident that the mean of the new experiment is within the interval and \\(\\mu_0\\) is not within it, then we can discard that the mean is \\(\\mu_0\\). We then say that the data could not have been produced by the old experiment, rendering the new conditions meaningful. Example (Li-ion Batteries) Twelve fully discharged Li-ion batteries were stored for three weeks, then they were re-charged and their discharged capacity was measured 1.5681, 1.5578, 1.572, 1.5637, 1.5769, 1.5627, 1.5757, 1.5782, 1.5806, 1.5645, 1.5484, 1.5821 From the histogram of the data we see that we can assume that the discharge capacity distributes normally Therefore, we can compute the confidence interval for the mean \\(\\mu\\) of this sample \\[(l,u)= (1.562, 1.575)\\] The confidence interval tells us that we trust with \\(95\\%\\) confidence that the \\(\\mu\\) is in the interval. We don not know the true value of \\(\\mu\\) but we see that \\(\\mu_0=1.561\\) is not in it. \\[\\mu_0\\notin (1.562, 1.575)\\] our conclusion is to reject that \\(H_0\\) could have produced our observed interval. We also say that the data supports the research question that storing discharged batteries prevents their deterioration. More technically, we say that we do not reject \\(H_0\\). 14.3.2 Hypothesis test with acceptance/rejection zones An equivalent way to test the hypothesis is to see if our set of observations are either common or rare if we assume that the null hypothesis is true. Let us remember the hypothesis contrast \\(H_0:\\mu = \\mu_0\\) (status quo) \\(H_1:\\mu \\neq \\mu_0\\) (research interest) To test the hypothesis with a rejection zone we compute the standardized statistic \\[T=\\frac{\\bar{X}-\\mu_0}{\\frac{S}{\\sqrt{n}}}\\] when the null hypothesis is true. This is the standardized error that we will make if we estimate \\(\\mu_0\\) with the average \\(\\bar{X}\\) of the experiment under the new conditions. Note that we are standardizing with respect to \\(\\mu_0\\) (the null hypothesis). Remember that the interval \\[(-t_{0.025, n-1}, t_{0.025, n-1})\\] defines the most common values of \\(T\\) since they capture \\(95\\%\\) of the probability ouf the outcomes of T \\[P(-t_{0.025, n-1} \\leq T \\leq t_{0.025, n-1})=0.95\\] Testing criteria: If the observed statistics \\(t_{obs}\\) under the null hypothesis is in the acceptance region \\[t_{obs}=\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}} \\in (-t_{0.025, n-1}, t_{0.025, n-1})\\] we accept \\(H_0\\) with \\(95\\%\\) confidence. In this case, the observed error that we make when we estimate \\(\\mu_0\\) by \\(\\bar{x}\\) is a typical sampling error. The interval \\((-t_{0.025, n-1}, t_{0.025, n-1})\\) is called acceptance interval of \\(H_0\\) at \\(95\\%\\) confidence level. If the observed statistics \\(t_{obs}\\) under the null hypothesis is not in the acceptance region \\[t_{obs} \\notin (-t_{0.025, n-1}, t_{0.025, n-1})\\] then we reject \\(H_0\\) with \\(95\\%\\) confidence. In this case, the observed error that we make when we estimate \\(\\mu_0\\) by \\(\\bar{x}\\) is too large for sampling error, and therefore the assumption that the null hypothesis was true does not hold. The region \\((-t_{0.025, n-1}] \\cup[t_{0.025, n-1})\\) is called the rejection zone. Example (Li-ion Batteries) To test whether storing discharged batteries is better than storing them fully charged, we calculate the the standardized error that we make when we estimate \\(\\mu_0\\) with \\(\\bar{X}\\) \\[t_{obs}=\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}}=2.7921 \\notin (-t_{0.025, n-1}=-2.20, t_{0.025, n-1}=2.20)\\] Think of this regions as a region of tolerance for the error. Because our observation is not within, we have enough evidence to distrust the that the changing the storage conditions is irrelevant. We conclude that our observed average is not a typical observation of \\(\\bar{x}\\) when the null hypothesis \\(\\mu_0\\) is true. Therefore, we again reject that the data is consistent \\(H_0\\) and say that we have evidence that support the notion that storing discharged batteries is better. 14.3.3 Hypothesis test with a P-value We can also contrast the two tail hypothesis by calculating the probability that the average of another sample from the null hypothesis will be even rarer than the average we just observed. We define the \\(pvalue\\) as \\[pvalue = P(T \\leq -t_{obs}) + P(t_{obs} \\geq T) = 2 (1-F(|t_{obs}|))\\] That is the probability that if we were to we take another sample of the same size from the null hypothesis, we are able to obtain at least the error that we observed. If our observed error is rare for the null hypothesis then this probability value will be small. Testing criteria: If the observed \\(pvalue\\) is \\[pvalue \\geq \\alpha =1-0.95=0.05\\] then we accept the null hypothesis \\(H_0\\) with \\(95\\%\\) confidence. If the observed \\(pvalue\\) is \\[pvalue &lt; \\alpha =1-0.95=0.05\\] then we reject \\(H_0\\) and accept our research question with \\(5\\%\\) significance level. \\(\\alpha\\) is the significance level. It gives us how much of the distribution we are leaving out, and defines the region that we consider estimation errors to be beyond the tolerance of the null hypothesis. Remember: We always trust our data. If the null hypothesis says that our data is a rare observation we then distrust the null hypothesis, and reject it. Example (Li-ion Batteries) Let us think that the average \\(\\bar{x}=1.569232\\) could be achieved by batteries that were stored fully charged. Perhaps, we were lucky and select a sample made of good batteries with high discharge capacity, no matter the storage conditions. If that is so then how lucky were we? For our data the observed statistic was \\(t_{obs}=2.7921\\) and its p-value is \\[pvalue=2\\times (1-F(2.7921))=0.017\\] Python: 2*(1- t.sf(2.7921, df=11)) R: 2*(1- pt(2.7921, 11)) We conclude that if we test \\(12\\) batteries and store them fully charged, it is unlikely to obtain an average discharge capacity this far from the \\(1.561mAh\\). Only about \\(1\\%\\) of fully charged battery samples will get this far. Since this probability is lower than our tolerance, or significance level \\(alpha=0.05\\), again conclude that storing discharged batteries is a better option. The entire analysis in Python and R can be done with the following code Python: from scipy import stats data = [1.5681, 1.5578, 1.572, 1.5637, 1.5769, 1.5627, 1.5757, 1.5782, 1.5806, 1.5645, 1.5484, 1.5821] stats.ttest_1samp(data, popmean=1.561) R: x &lt;- c(1.5681, 1.5578, 1.572, 1.5637, 1.5769, 1.5627, 1.5757, 1.5782, 1.5806, 1.5645, 1.5484, 1.5821) t.test(observaciones, mu=1.561) 14.3.4 Upper tail hypothesis We may be interested in only testing for the fact that our experiment’s mean has a higher mean than the null hypothesis’ mean. The upper-tailed test is \\(H_0:\\mu \\leq \\mu\\). The new conditions of the experiment have a mean that is at most the null hypothesis mean. \\(H_1:\\mu &gt; \\mu\\). The new conditions of the experiment have a mean that is higher the null hypothesis mean. This is called upper-tailed test because the alternative hypothesis \\(H_1\\) requires that the mean \\(\\mu\\) is higher than \\(\\mu_0\\). The testing criteria are the same as those for the two tail hypothesis. Testing criteria: Confidence interval: If the upper-tailed confidence interval contains the null hypothesis \\[\\mu_0\\in (l,u)=(\\bar{x}-t_{0.05} \\frac{s}{\\sqrt{n}}, \\infty)\\] where \\(t_{0.05, n-1}=F^{-1}(0.95)=\\)t.ppf(1-0.05, df=n-1), then we accept \\(H_0\\) with \\(95\\%\\) confidence. Otherwise we reject it. Note that this test is from the point of view of the data, we are not centering the confidence interval around \\(\\bar{x}\\), instead we are leaving all the \\(5\\%\\) of the rare errors to the left of the average. We are, therefore, asking if \\(\\mu_0\\) is significant lower than the average. Rejection/acceptance region: If the observed statistics \\(t_{obs}\\) under the null hypothesis is in the acceptance region \\[t_{obs}=\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}} \\in (-\\infty, t_{0.05, n-1})\\] then we accept \\(H_0\\) with \\(95\\%\\) confidence. Otherwise, we reject it. Note that this test is from the point of view of the null hypothesis. We are leaving all the \\(5\\%\\) of the rare errors to the right of the null hypothesis and therefore ask if the average is significantly higher than \\(\\mu_0\\). \\(pvalue\\): If the observed upper-tailed \\[pvalue= 1-F(t_{obs})\\] 1-t.cdf(zobs, df=n-1) is greater than \\(\\alpha=1-0.95=0.05\\) \\[pvalue \\geq \\alpha =0.05\\] then we accept \\(H_0\\) with \\(95\\%\\) confidence. Otherwise we reject it. Note that this test is again from the point of view of the null hypothesis. We are asking: if we were to take another average from sampling the experiments of the null hypothesis, what is the probability that it will be higher than the observed one? Example (Li-ion Batteries) In the example of the Li-ion batteries, we may be interested to test whether storage discharged batteries improves discharge capacity. Therefore, the upper-tailed hypothesis is \\(H_0:\\mu \\leq 1.561\\) (storing discharged batteries is at most as good as storing them fully charged) \\(H_1:\\mu &gt; 1.561\\) (storing discharged batteries is better then storing them fully charged: research interest) We will test the higher tail of the distribution. For the data that we discussed before, we then reject \\(H_0\\) at \\(95\\%\\) confidence because of any of the three equivalent contrasts: The upper tailed confidence interval does not contain the null hypothesis \\(\\mu_0=1.561\\) \\[\\mu_0=1.561 \\notin (\\bar{x}-t_{0.05, n-1} \\frac{s}{\\sqrt{n}}, \\infty)=(1.563937, \\infty)\\] where \\(t_{0.05, n-1}=\\)t.ppf(0.95, 11)= 1.795885 We have that the acceptance region for \\(H_0\\) is: \\[(-\\infty, t_{0.05, n-1})=( -\\infty, 1.795885)\\] and that the observed standardized error is not in the region \\[t_{obs} = \\frac{1.569232-1.561}{\\frac{0.01}{\\sqrt{12}}}=2.7921 \\notin ( -\\infty, 1.795885)\\] The upper tail \\(pvalue\\) is lower than \\(\\alpha=0.05\\) \\[pvalue=1-F(2.7921)=0.0087 &lt;0.05\\] where \\(pvalue=\\)t.sf(2.7921, 11). The hypothesis test is performed in Python and R like Python: from scipy import stats data = [1.5681, 1.5578, 1.572, 1.5637, 1.5769, 1.5627, 1.5757, 1.5782, 1.5806, 1.5645, 1.5484, 1.5821] stats.ttest_1samp(data, popmean=1.561, alternative=&#39;greater&#39;) R: x &lt;- c(1.5681, 1.5578, 1.572, 1.5637, 1.5769, 1.5627, 1.5757, 1.5782, 1.5806, 1.5645, 1.5484, 1.5821) t.test(observaciones, mu=1.561, alternative=&quot;greater&quot;) We then conclude that evidence show that storing battering fully discharge improves discharge capacity. 14.3.5 Paired t-test Example (Gosset’s Soporific) In some cases, we are not sure about the numerical value of a parameter under the null hypothesis, but we know that we want to improve the value of a parameter in two different conditions. In the famous 1908 paper of Gosset, titled “The probable error of the mean”, were he proposed the \\(T\\) statistics (Student 1908), he analyzed the soporific effect of the hyoscyamine hydrobromide. It was already known at the time that the change of the structural configuration from dextro (deviating polarized light to the right) to leaevo (deviating polarized light to the left) of the molecule would hchange its biological effect. To test the differences - Ten individuals were given the dextro version of the molecule and wrote down the additional hours slept under this treatment 0.7, -1.6, -0.2, -1.2, -0.1, 3.4, 3.7, 0.8, 0, 2 with average \\(0.75\\) The same 10 individuals were given the laevo version of the molecule and wrote down the additional hours slept under this treatment 1.9, 0.8, 1.1, 0.1, -0.1, 4.4, 5.5, 1.6, 4.6, 3.4 with average \\(2.33\\) The scientific hypothesis was that the laevo version of the molecule was better than the dextro one. For each individual, Gosset computed the difference between the treatments. Taking \\(X\\) as the difference between treatments, this was the sample observed for \\(X\\) 1.2, 2.4, 1.3, 1.3, 0, 1, 1.8, 0.8, 4.6, 1.4 The average hours gained by laevo with respect to dextro was \\(1.58\\), and its standard deviation \\(s=1.229995\\). The scientific question can be stated as upper-tailed paired t-test: \\(H_0:\\mu \\leq 0\\) (no treatment difference: \\(\\mu_2-\\mu_1=0\\)) \\(H_1:\\mu &gt; 0\\) (treatment 2-laeveo have higher gains in sleeping hours than treatment 1-dextro: \\(\\mu_2-\\mu_1&gt;0\\)) Where \\(\\mu\\) is the mean of the differences between treatments, and the null hypothesis states that there is no difference. If we suppose that \\(X\\) is normal, the standardized error that we make when we estimate the null difference \\(\\mu_0=0\\) by the average of the differences is \\[T=\\frac{\\bar{X}}{\\frac{S}{\\sqrt{n}}}\\] and its observation \\[t_{obs}=\\frac{\\bar{x}}{\\frac{s}{\\sqrt{n}}}\\] which is also known as the signal to noise ratio. we can test the hypothesis for the difference \\(X=treatmet_1-treatment_2\\) using a upper tailed test which give us \\[\\bar{x}=1.58\\] and \\[pvalue=0.0014\\] This is obtained from the following code in Python and R Python: from scipy import stats x = [1.2, 2.4, 1.3, 1.3, 0, 1, 1.8, 0.8, 4.6, 1.4] stats.ttest_1samp(x, popmean=0, alternative=&#39;greater&#39;) R: x &lt;- c(1.2, 2.4, 1.3, 1.3, 0, 1, 1.8, 0.8, 4.6, 1.4) t.test(x, mean=0, alternative=&quot;greater&quot;) Gosset therefore observed a significant increase in \\(1.58\\) hours of sleep between laevo and dextro-hyoscyamine hydrobromide. We thus reject that the difference between soporific means is \\(0\\) or that their means are equal. Equivalently, we can test the hypothesis using a paired t-test, where we introduce the observation for each separate condition, and state that the observations are paired. Python: from scipy import stats medicine1 &lt;- [0.7, -1.6, -0.2, -1.2, -0.1, 3.4, 3.7, 0.8, 0, 2] medicine2 &lt;- [1.9, 0.8, 1.1, 0.1, -0.1, 4.4, 5.5, 1.6, 4.6, 3.4] stats.ttest_rel(medicine2, medicine1, alternative=&#39;greater&#39;) R: medicine1 &lt;- c(0.7,-1.6,-0.2,-1.2,-0.1,3.4,3.7,0.8,0,2) medicine2 &lt;- c(1.9,0.8,1.1,0.1,-0.1,4.4,5.5,1.6,4.6,3.4) t.test(medicine2, medicine1, alternative=&quot;greater&quot;, paired = TRUE) The experiment not only confirmed the biological consequences of isomers but it was the first ever cross-over design of a clinical trial. Gosset thus designed and used a statistical tool with key consequences in the analysis and design of pharmacological experiments, among others. In the following plot, we show all the statistical elements for this example. In red, we show the null hypothesis of no difference between treatments. The \\(95\\%\\) confidence interval for the difference is shown in the upper part. The CI does not contain the null. In the second row, we see the data represented in a box plot. At the bottom row, we see the raw data, that is the individual observations for the change in hours for each patient. 14.3.6 Lower tail hypothesis Example (NaCl) When \\(11.6g\\) of NaCl is dissolved in \\(100 g\\) of water it has a molar concentration of \\(1.92 mol/L\\). Imagine we design a process to remove salt from this concentration and obtain the following results 1.716, 1.889, 1.783, 1.849, 1.891 If we are interested only in the case that we are able to remove salt from the concentration then we rather propose a lower tail hypothesis. The lower tail hypothesis are of the form \\(H_0:\\mu \\geq \\mu_0\\). The new conditions of the experiment give an expectation that is at most as the initial one -status quo. \\(H_1:\\mu &lt; \\mu_0\\). The new conditions of the experiment give an expectation that is lower than the initial one -research interest Testing criteria: Note that the lower tail is given by the alternative \\(H_1\\). We want to test that the average concentration after the process is lower than the initial concentration. The contrast criteria are the same as for the other types of hypothesis. For this type of hypothesis, we will accept the null hypothesis if \\(\\mu_0\\) is in the confidence interval: \\[\\mu_0\\in (l,u)=(-\\infty, \\bar{x}+t_{0.05,n-1} \\frac{s}{\\sqrt{n}})\\] or, \\(t_{obs}\\) is in the aceptance region: \\[t_{obs}\\in (t_{0.05,n-1}, \\infty)\\] or, the \\(pvalue\\) on the lower tail of the distribution. \\[pvalue=F_t(t_{obs},n-1)\\] is higher than \\(\\alpha=0.05\\) In any other case, we reject \\(H_0\\) and accept the alternative hypothesis. Example(NaCl) If we are efficient at removing salt then we would like to provide evidence for the alternative lower tail hypothesis. \\(H_0:\\mu \\geq 1.92\\) (null) \\(H_1:\\mu &lt; 1.92\\) (alternative) We can assume that the concentration is normal. Therefore, we only need to change the “alternative” argument to “less” in the t-test functions Python: from scipy import stats data = [1.716, 1.889, 1.783, 1.849, 1.891] stats.ttest_1samp(data, popmean=1.92, alternative=&#39;less&#39;) R: x &lt;- c(1.716, 1.889, 1.783, 1.849, 1.891) t.test(x, mu=1.92, alternative=&quot;less&quot;) We see that the expected value of the null hypothesis \\(\\mu_0=1.92\\) is not within the \\(95\\%\\) lower-tail confidence interval \\((-\\infty, 1.897376)\\), and that the \\(pvalue=0.002\\) is lower than the significance level \\(\\alpha=0.05\\). Therefore, we can reject the null hypothesis and say that our eveidence support the alternative, meaning that the prototipe is able to exctract salt from the mix. If we compare result with that of the two tail test, we will see that the \\(pvalue\\) is reduced in half, and therefore we have more confidence in rejecting the lower tail hypothesis than the two-sided hypothesis. Hypothesis Testing with Large \\(n\\) and Any Distribution On many occasions, \\(X\\) is not normally distributed, but if we can take large samples (\\(n \\ge 30\\)), then we can use the Central Limit Theorem (CLT). The CLT tells us that the sampling distribution of the sample mean \\(\\bar{X}\\) is approximately normal, regardless of the distribution of \\(X\\), provided \\(X\\) has finite variance. The standardized error from the null hypothesis can then be approximated by a standard normal distribution: \\[ Z = \\frac{\\bar{X} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}} \\approx N(0, 1) \\] If the population standard deviation \\(\\sigma\\) is unknown, we replace it with the sample standard deviation \\(S\\), and use the t-statistic: \\[ T = \\frac{\\bar{X} - \\mu_0}{\\frac{S}{\\sqrt{n}}} \\sim t_{n-1} \\] For large \\(n\\), the t-distribution approaches the standard normal distribution, so we may approximate: \\[ T \\approx N(0, 1) \\quad \\text{for large } n \\] Note: The use of the normal approximation for the \\(T\\)-statistic is common when \\(n \\ge 30\\), but strictly speaking, the statistic follows a \\(t\\)-distribution with \\(n-1\\) degrees of freedom. 14.4 Hypothesis testing for the proportion If our random experiment is a Bernoulli trial \\(X \\sim Bernoulli(p)\\), we can formulate hypothesis contrasts for the probability \\(p\\) of an event in the trial, or the proportion. Consider an upper tailed hypothesis \\(H_0: p \\leq p_0\\) (status quo) \\(H_1: p&gt; p_0\\) (research interest) In this case, we test a hypothesis for the proportion if \\(X\\) is a Bernoulli trial, and \\(np\\), \\(n(1-p)\\) are both greater than \\(5\\), so we can apply the central limit theorem. Remember that if we take a the sample of \\(n\\) Bernoulli trials \\((1,0,1,...0)\\), \\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^n X_i\\] is the relative frequency for the number of ‘’ones’’ obtained in the sample. This is an estimator of \\(p\\). If we assume that the null hypothesis is true then \\(X \\sim Bernoulli(p_0)\\) and the standardized error that we make when we estimate \\(p_0\\) with \\(\\bar{X}\\) is \\[Z=\\frac{\\bar{X}-p_0}{\\frac{\\sqrt{p_0(1-p_0)}}{\\sqrt{n}}} \\sim N(0,1)\\] \\(\\sigma=\\sqrt{p_0(1-p_0)}\\) is the standard deviation of \\(X\\) when the null hypothesis is true. Remeber that \\(V(X)=\\sigma^2=p_0(1-p_0)\\). With this \\(Z\\) statistic, we can accept or reject the null hypothesis using any of the three testing criteria. Example (Process Improvement) We may be satisfied with a new process if \\(90\\%\\) of the times we improve the previous process. If we run a sample of \\(200\\) new processes and find that \\(188\\) times we improved the old process. This makes a relative frequency of \\(0.94\\) or we managed to improve the process \\(94%\\) of the times. Can we be satisfied with the new process at \\(95\\%\\) confidence? To answer this questions, we can formulate an upper-tailed hypothesis contrast for the null hypothesis \\(p_0=0.9\\). Therefore, the null and alternative hypotheses are \\(H_0: p \\leq p_0=0.9\\) (The new process is not satisfactory to the requires reliability: status quo) \\(H_1: p&gt; p_0=0.9\\) (The new process improves the older one reliably: research interest) We assume that if the null hypothesis is true then The probability model of a random experiment is \\[X \\sim Bernoulli (p_0)\\] 2. and check that when \\(np_0=180&gt;5\\) and \\(n(1-p_0)=20&gt;5\\) Therefore, we can apply the test for the proportions. We can use any of the three criteria to test the hypothesis. For this example, we believe that have a satisfactory process because, we reject \\(H_0\\) at \\(95\\%\\) following The upper tail confidence interval for the \\(p\\) does not include \\(p_0\\) \\(p_0=0.9 \\notin (\\bar{x}-z_{0.05}\\big[\\frac{p_0(1-p_0)}{n} \\big]^{1/2},1)= (0.905,1)\\) The observed standardized error from the null is not in the acceptance region \\[z_{obs}= \\frac{\\bar{X}-p_0}{\\big[\\frac{p_0(1-p_0)}{n} \\big]^{1/2}} =\\frac{0.94-0.90}{\\sqrt{0.00045}}=1.88563 \\notin (-\\infty, z_{0.05})=(-\\infty, 1.644)\\] The upper tail \\(pvalue\\) is lower than \\(\\alpha=0.05\\): \\[pvalue=1-\\Phi(1.885618)=0.02967323&lt;0.05\\] Python: 1-norm.cdf(1.885618) R: 1-pnorm(1.885618) The test can be performed in Python and R using the code Python: from statsmodels.stats.proportion import proportions_ztest proportions_ztest(188, 200, value=0.9, alternative=&#39;larger&#39;) R: prop.test(188, 200, p=0.9, alternative=&quot;greater&quot;, correct = FALSE) 14.5 Hypothesis Testing for the Variance In many cases, experiments are run to test the dispersion of data. Such as, when complying with strict design standards where measurements must be between certain values. Or when different treatments are applied to different groups, we want to see the dispersion of outcomes between the groups. Also the dispersion of the data may be a property that offer important insights into the system under study. A tow tailed hypothesis contrast for the variance is of the form \\(H_0:\\sigma = \\sigma_0\\) (status quo) \\(H_1:\\sigma \\neq \\sigma_0\\) (research interest) This hypothesis for \\(\\sigma\\), or equivalently for \\(\\sigma^2\\), can be tested when the random variable \\(X\\) is a normally distributed. Remember that if we take a random sample \\[S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})^2\\] is the sample variance. This is an unbiased and consistent estimator of \\(\\sigma^2\\). If we assume that the null hypothesis is true and \\(X \\sim N(\\mu, \\sigma_0)\\) then the error ratio that we make when we estimate \\(\\sigma^2\\) with \\(s^2\\) is \\[W=\\frac{(n-1)S^2}{\\sigma_0^2}\\] Note that when \\(W=n-1\\), we make no error. \\(W\\) follows a \\(\\chi^2\\) (chi-squared) distribution with \\(n-1\\) degrees of freedom. \\[W \\sim \\chi^2(n-1)\\] With the observation of \\(W\\), we can accept or reject the null hypothesis using any of the three testing criteria. Example (Hart Rate Variability) The heart rate, as measured by an electrocardiogram (ECG), shows a distinct periodic pattern, with the peak signal corresponding to the R-wave of the QRS complex. However, the periodicity of the hart rate is not exact. In fact, heart rates that get close to perfect periodicity is bad news. The lack of variability in the intervals between successive R-peaks—known as RR intervals—is indicative of cardiac dysfunction and is a known predictor of mortality. Conversely, high variability in the heart rate is associated with better cardiovascular health. The fractal and nonlinear complexity of this variability is the focus of ongoing research in cardiology, as it reflects the dynamic interplay of autonomic regulation and physiological adaptability. Irurzun and colleagues measured RR intervals by 24 hour Holter monitoring on 147 healthy patients to determine RR change at different ages and between sexes(Irurzun et al. 2021). Patient “00” was, for instance, a 53 year old male with the following distribution of pre-processed RR values during waking hours in milliseconds. The sample variance of the patient is \\(s^2=8029.751\\). The expected RR healthy variability of a 53 year is \\(\\sigma^2_0=8000ms\\), according to the reference Framingham Heart Study. Is patient “00” within the range of the expected value of healthy RR variability and a suitable candidate for the study? We therefore want to contrast the two tailed tail hypotheses \\(H_0:\\sigma^2 \\leq \\sigma_0^2=8000\\) (Patient has a very normal RR variability) \\(H_1:\\sigma^2 \\neq \\sigma_0^2=8000\\) (Patient has a RR variability different than the expected value) Let us test the hypothesis using the acceptance region. The contrast statistics is \\[W=\\frac{(n-1)S^2}{\\sigma_0^2} \\sim \\chi^2(n-1)\\] and the threshold limit \\(\\alpha=0.05\\). Therefore, the acceptance region \\(P(\\chi^2_{0.975,2999} \\leq W\\leq \\chi^2_{0.025,29999})=0.95\\) is \\[(\\chi^2_{0.975,2999}, \\chi^2_{0.025,2999})=(2849.11,3152.678)\\] Python: chi2.ppf(0.975,19) chi2.ppf(0.025,19) R: qchisq(0.025, 2999) qchisq(0.975, 2999) For our data, the observed standardized error ratio is: \\[w_{obs}=\\frac{2999 \\times8029.751}{8000}=3010.153\\] which falls within the acceptance region \\[w_{obs}=3010.153\\in (2849.11,3152.678)\\] and therefore we do not reject the null hypothesis and conclude that RR data of patient “00” supports that he has a very normal RR variability. The \\(95\\%\\) CI interval for his RR variance is \\((l,u) = (\\frac{s^2 (n-1)}{\\chi^2_{0.025,n-1}},\\frac{s^2(n-1)}{\\chi^2_{0.975,n-1}})\\) \\[( 7902.782, 8159.819)\\] which clearly contains the expected value \\(\\sigma^2_0=8000\\) for his age and sex. This patient, has a RR variance that it is very close to the expected variance measured as a gold reference in a large population sample. The healthy range, contrasted with clinical data, is, however, much larger \\((3600, 14400)ms^2\\). Obtaining a RR variance within the specified interval provides reassurance that the patient is healthy, supporting the decision to be included in the study. 14.6 Errors in hypothesis testing The result of an upper tail hypothesis test may be to reject the null hypothesis: \\[H_0: \\mu\\leq\\mu_0\\] when \\(H_0\\) is actually true. In the case of the batteries, we rejected the null hypothesis that the discharge capacity of stored batteries will not change if they were stored fully charged or discharged. Data made us believe that storing discharged batteries was better and can recommend this type of storage. We must bear in mind that the decisions are based on the data. It may well be that the observed statistic has fallen, by chance, far from the null hypothesis, in the rejection zone of \\(H_0\\) even when this hypothesis is true. The statistic is a random variable and one observation can have a large value by chance. In our experiment, we selected, without knowing, batteries that had a previously high discharge capacity in any condition. Larger than the null \\(\\mu_0=1.561\\). If the selection error is systematic we call it selection bias. Another possible source of the error could be that the type of batteries that we are testing do not have a discharge capacity of \\(\\mu_0=1.561\\) when fully charged. They have a higher discharged capacity equal to that when they are stored fully discharged. The null hypothesis is badly represented by \\(\\mu_0\\), which we really do not know in our experiment. We need a control group to assess it. A solution is to test the same battery in two different conditions, randomly prescribed, and perform a paired test. In the case that the first test influences the outcome of second test on the same specimen, two random samples of each condition can be taken and its difference compared. When we perform the hypothesis test, we do not know if \\(H_0\\) is true. Let us imagine that we found by other means that \\(H_0\\) is really true. The probability of rejecting the truth (\\(H_0\\)) is precisely the level of statistical significance \\(\\alpha\\). We call this probability the probability of making a type 1 error. Taking an upper tailed hypothesis test for the mean at significance level \\(5\\%\\), we have that the probability that the standardized error falls in the rejection zone is \\[\\alpha = P(Z&gt; t_{0.05, n-1})=0.05\\] A type 1 error is also called a false positive because our research interest is in \\(H_1\\). When we reject \\(H_0\\), we accept \\(H_1\\) and say that our test is positive. Accepting \\(H_1\\) translates to announcing a discovery, so the type 1 error is announcing a discovery that is not true: we falsely claimed a discovery because the data suggested it, we were fooled by it. There is another type of error. The result of an upper tail hypothesis test can be accepting the null hypothesis: \\[H_0: \\mu\\leq\\mu_0\\] when this is not true. Again for the batteries, imagine now that our data made us accept that the changing conditions of the batteries do not affect their discharged capacity (\\(H_0\\)) when actually it does. We chose by chance batteries that had already a lower discharged capacity no matter the condition. In this case, it may be that the observed random error fell, due to randomness, close to the null hypothesis, in the acceptance zone of \\(H_0\\), when really \\(H_1\\) is true. If we found out somehow that, for example, \\(\\mu\\) really does have a value of \\(\\mu_1\\) (blue dot in the plot) then the alternative hypothesis would be exactly: \\[H_1: \\mu=\\mu_1\\] If \\(H_1\\) is indeed true (blue line, which we do not know when we perform the hypothesis test) then the statistic is really a random variable \\(Y\\) that has mean \\[E(Y)=\\frac{\\mu_1-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\] and most of them will fall close to this value, therefore, in the rejection area of \\(H_0\\), validating the hypothesis test. However, there are cases in which the observed statistic falls within the acceptance zone of \\(H_0\\) due to randomness (in the blue shaded area), despite the fact that the statistics are produced by \\(H_1\\). In these cases we accept \\(H_0\\) when it is not true. This error is called a type 2 error or a false negative. Since our research interest is in \\(H_1\\) we failed to accept it. rejecting \\(H_1\\) translates to discarding a discovery, so the type 2 error is ignoring a discovery that is actually true. For an upper tailed test for the mean and a significance level \\(\\alpha\\) this probability is \\[\\beta= P(Y &lt; t_{0.05, n-1})\\] Where \\(Y \\sim T(n-1, ncp=\\frac{\\mu_1-\\mu_0}{s/\\sqrt{n}})\\) is the true distribution of the observed statistics, and corresponds to a t-distributions shifted to the right by the parameter \\(ncp\\). We also say that the statistical power of the test is \\(1-\\beta\\). Example (Power Calculation of Gossets’ Soporifics) The results of pre-clinical findings, like those of Gosset, would require extensive validation to be approved for clinical use. A clinical trial, aimed to validate an effect, needs to compute the statistical power expected to reject the null to be approved. Imagine we aim to validate Gossets experiments, replicating his findings. The hypotheses that we want to contrast are \\(H_0 : \\mu = 0\\) (no soporific difference between treatments) \\(H_1 : \\mu &gt; 0\\) (higher soporific effect of the laevo isomer than the dextro one) and prove that the mean difference between treatments is positive on \\(100\\) patients, with a statistical significance level of \\(5\\%\\). Assuming the values of the previous Gosset’s study that suggested that the difference between the means was \\(\\mu=\\mu_1=1.58\\) hours and \\(\\sigma=1.23\\) hours, what is the type 2 error we should expect if we repeat Gosset’s experiment using a sample size of \\(100\\) patients? The probability of accepting the null hypothesis is \\[\\alpha = P(T&gt; t_{0.05, 100-1})=0.05\\] where \\(t_{0.05, 99}\\)t.ppf(1-0.05, 99)\\(=1.66\\). The type 2 error is therefore \\[\\beta= P(Y &gt; 1.66)\\] that is, the probability of accepting that the laevo version of the molecule does not produce higher soporific effects that the dexo one, when in fact it does. For the upper tailed test for the mean, the observed statistics actually distributes as \\[Y \\sim T(100-1, ncp=\\frac{1.58}{1.23/\\sqrt{10}}= 4.06)\\] and the type 2 error is \\[\\beta = F_{t(n-1, ncp)}(4.06)=0.008\\] Python: nct.cdf(1.66, 99, ncp=12.84553) R: pt(1.66, 99, ncp=12.84553) Therefore, if were were to repeat Gosset’s experiment using \\(100\\) patients an infinite amount of times, only \\(\\alpha=5\\%\\) of the times we would announce that the laevo molecule is more soporific than the dextro one when it really is not, while \\(\\beta=0.8\\%\\) of the times we would announce that we do no see a difference when there is one. In this case, the statistical power to detect a biological effect of changing the molecule’s chirality is \\(1-\\beta=0.991\\) or \\(99.1\\%\\). When designing an experiment is customary to fix the power at \\(80\\%\\) and then compute the sample size \\(n\\) required to achieve this power. For Gosset’s experiment, the effect is so strong that with only \\(4\\) patients we will achieve a statistical power of \\(80\\%\\). 14.6.1 Sensitivity and Specificity When we carry out a hypothesis test we have two possibilities for each condition: \\(H_1\\) is actually : true (\\(\\mu=\\mu_1\\)) or false (\\(\\mu=\\mu_0\\)). We also have two outcomes of the hypothesis test: The test for \\(H_1\\) is: positive (\\(t_{obs}\\) in the acceptance zone of \\(H_1\\)) or negative (\\(t_{obs}\\) in the acceptance zone of \\(H_0\\)). Example (PCR) We do a PCR to test for an infection. The hypothesis test is \\(H_0\\) no infection \\(H_1\\) there is infection We do the PCR test and it gives us negative: we reject the infection (\\(H_1\\)) positive: we accept the infection (\\(H_1\\)) We can write the contingency table for the probabilities of the results of the hypothesis test as \\(H_1\\) is true \\(H_1\\) is false The test on \\(H_1\\) is positive \\(1-\\beta\\) \\(\\alpha\\) The test on \\(H_1\\) is negative \\(\\beta\\) \\(1-\\alpha\\) sum 1 1 we therefore have The type 2 error rate: probability of a false negative (ignore a finding when it is true) \\[\\beta=P(negative|H_1)\\] The True positive rate: This is the power or sensitivity of a test (claiming a discovery when it is true, the main objective) \\[1-\\beta=P(positive|H_1)\\] The Type 1 error rate: probability of a false positive (state a discovery when it is false) \\[\\alpha=P(positive|H_0)\\] The True Negative rate: This is the specificity of a test (ignore a finding when it is false) \\[1-\\alpha=P(negative|H_0)\\] 14.7 Exercises 14.7.0.1 Exercise 1 Imagine we take a random sample of size \\(n = 41\\) of a normal random variable \\(X\\), and find that the sample average is \\(10\\) and the sample variance is \\(1.5\\). What is then the confidence interval for the mean of \\(X\\) at \\(95\\%\\) confidence level? Consider that \\(t_{0.025,40}=\\) t.ppf(0.975, 40) \\(\\sim 2\\). Test the hypothesis that the mean of \\(X\\) is different than \\(10.5\\), using a \\(5\\%\\) significance threshold. Write the code to calculate the P-value to test the hypothesis that the mean of \\(\\mu\\) is lower than \\(10.5\\), using a \\(5\\%\\) significance threshold. Consider that the code for the T probability distribution with \\(n-1\\) degrees of freedom is t.cdf(tobs, n-1). 14.7.0.2 Exercise 2 \\(10\\) gas condensates showed the following concentrations of mercury (in \\(ng/ml\\)): \\(23.3\\), \\(22.5\\), \\(21.9\\), \\(21.5\\), \\(19.9\\), \\(21.3\\), \\(21.7\\), \\(23.8\\), \\(22.6\\), \\(24.7\\) Assuming that the mercury concentration is distributed normally across gas condensates, test the hypothesis that condensate does not surpass the toxicity limit established at \\(24 ng/ml\\). 14.7.0.3 Exercise 3 The manufacturer of gene expression microarrays guarantees that at least \\(97\\%\\) of the microarrays they produce have high-quality signals. A customer receives a batch of \\(200\\) pieces and finds that \\(8\\) unperformed. Should the customer return the lot due to poor quality? 14.8 Practice Load misophonia data https://alejandro-isglobal.github.io/SDA/data/data_0.txt We have four measures of anxiety: Trait: ansiedad.rasgo (are you an anxious person?) continuous:0-100 State: ansiedad.estado (are you currently feeling anxious?) continuous:0-100 Diagnosed: ansiedad.medicada (have you been diagnosed with an anxiety disorder?) binary (si, no) Excess: ansiedad.dif (difference between State and Trait) We are interested in the variable misofonia.dif, that is the observed excess of anxiety from the trait \\(excess = state - trait\\) Test the following hypotheses Is excess in anxiety different from 0? is it higher? Is excess in anxiety higher than 0 for men and women separately? Is the proportion of anxious patients different from \\(0.03\\)? Solutions References "],["contingency-tables.html", "Chapter 15 Contingency tables 15.1 Difference between proportions 15.2 Difference between proportions 15.3 Contingency table of conditional probabilities 15.4 Test for the difference between proportions 15.5 \\(\\chi^2\\) test 15.6 Fisher’s exact test 15.7 Hypergeometric distribution 15.8 Difference between several proportions 15.9 Goodness of fit 15.10 Questions 15.11 Practice", " Chapter 15 Contingency tables Our aim is often to compare the estimate of a parameter—describing a property of a random experiment—to a reference value. This reference value defines the probability distribution of the experiment under previously known or established conditions. If we change the conditions of the experiment to improve that property, our goal is to demonstrate that the new estimate is significantly different from the reference. On the other hand, if we aim to validate the reference by repeating the experiment under the same conditions, our goal is to show that the estimate remains close to the reference value. In either case, the reference must be known without uncertainty prior to the experiment. Reference values can sometimes be derived unambiguously from theoretical considerations. For example, Mendel’s 0.75 probability that the offspring of hybrid pea plants produce round seeds is supported by a theoretical genetic model. When no such model exists—for instance, in the case of the mass-to-charge ratio of the electron—then a reference may be based on a gold standard established by highly accurate previous measurements. But what can we do when no such reference value exists for the null hypothesis? Suppose we only know the two different conditions we want to compare. Gosset’s experiment on testing the soporific effects of two structural configurations of the same molecule provides a solution. He proposed using the same individual to test both variants, and measuring the difference in effect. The reference value then becomes the null difference in soporific effect between the two molecular structures across individuals. A further complication arises in experiments where each specimen (or experimental unit) can only be tested under a single condition. In such cases, two independent samples must be taken—one per condition. Equivalently, each trial of the random experiment produces two measurements: one of the outcome, and one of the condition. Here, the null hypothesis is formulated as the statistical independence between the outcome and the conditions. In this chapter, we will examine how to test for statistical independence between a random variable with discrete outcomes and two or more conditions of the experiment. We will use the contingency table of conditional probabilities to define the null hypothesis, and test it using the chi-squared \\(\\chi^2\\) statistic. We will also introduce Fisher’s exact test, especially useful when sample sizes are small. We will illustrate both methods using historical data: the link between smoking and lung cancer, and the effect of natural selection on humans through changes in the frequency of the lactase persistence mutation. Finally, when the goal is to test whether observed relative frequencies match those expected from a theoretical model, the \\(\\chi^2\\) test can also be applied. We will illustrate this important use in the linear dependence of the lactase mutation with the latitude of European human populations. 15.1 Difference between proportions Example (Smoking and Lung Cancer) In 1950 Doll and Hill established smoking as a potential cause of lung cancer (Doll and Hill 1950). Before them, smoking was not considered as a potential cause of the increasing incidence in lung cancer. The study became a template in epidemiological studies and help to change the public view on the use of tobacco. The control measures that came decades later, based of scientific evidence encourage by this work, helped to save millions of lives. Doll and Hill took a sample of lung cancer patients and controls and recorded whether they were smokers or not. We can imagine that the random sample of the smoking status (yes:1, no:0) for the lung cancer patients 1, 0, 0, 1, 0, … 0 where the first patient was a smoker and and the last one was not. Smoking status is a Bernoulli trial \\(Y\\) with outcomes \\((0, 1)\\) and has a probability mass function \\[Y_A \\sim Bernoulli (p_A)\\] The parameter \\(p_A\\) is the probability of smoking under the lung cancer condition \\(A\\). We also write down the smoking status of a patient who is a control (non-smoker), and observe 0, 0, 1, 0, 0, … 1 Smoking status is a Bernoulli trial with probability \\(p_B\\) \\[Y_B \\sim Bernoulli (p_B)\\] The parameter \\(p_B\\) is the probability of smoking under the control condition \\(B\\). 15.2 Difference between proportions In this study one random experiment has a two-value outcome: (smoking status, diagnosis). That is the observation of a patient is determined by two variables both of which are categorical These are the possible outcomes of each variable: Smoking \\(\\in\\{yes, no\\}\\) Cancer diagnosis \\(\\in \\{A:cancer,B:control\\}\\) Repeating the experiment \\(n\\) times, the data matrix for the patients would look like \\[ \\begin{array}{ccc} \\mathbf{Individual} &amp; \\mathbf{Condition} &amp; \\mathbf{Outcome} \\\\ c_1 &amp; \\text{A} &amp; \\text{1} \\\\ c_2 &amp; \\text{A} &amp; \\text{0} \\\\ c_3 &amp; \\text{A} &amp; \\text{1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ c_{n-2} &amp; \\text{B} &amp; \\text{1} \\\\ c_{n-1} &amp; \\text{B} &amp; \\text{1} \\\\ c_{n}&amp; \\text{B} &amp; \\text{1} \\\\ \\end{array} \\] The first patient was diagnosed for lung cancer and was a smoker. Question: Our research question is to find whether the outcome and the condition are statistically dependent variables. We want to know if the probability of smoking is higher in lung cancer patients than in controls. 15.3 Contingency table of conditional probabilities Our experiment is such that we make diagnosis conditioned to the exposure. We then have the conditional probability table \\[ \\begin{array}{cc|c} &amp; \\mathbf{A:cancer} &amp; \\mathbf{B:control} \\\\ \\mathbf{smoking: yes} &amp; P(yes \\mid A) &amp; P(yes \\mid B) \\\\ \\mathbf{smoking: no} &amp; P(no \\mid A) &amp; P(no \\mid B) \\\\ \\mathbf{sum} &amp; 1 &amp; 1 \\end{array} \\] We want to know if the probability of smoking in condition \\(A\\) is different from the probability of smoking in condition \\(B\\), and therefore the hypothesis contrast is \\(H_0: p_A=p_B\\), no differences of smoking between cases and controls. \\(H_1: p_A\\neq p_B\\), the probabilities of smoking are different between cases and controls. 15.4 Test for the difference between proportions If we consider that the null hypothesis is true, then the cancer tag (“A” or “B”) is statistically independent from whether an individual in the study was a smoker or not. Remember the definition of statistical independence: if smoking is independent from diagnosis status \\[p=P(yes)=P(yes|A)=P(yes|B)\\] Therefore the hypothesis contrast can be written as: \\(H_0: p=p_A=p_B\\) \\(H_1: p\\neq p_A \\neq p_B\\) We do not know the values of \\(p\\), \\(p_A\\) and \\(p_B\\). Therefore we need to estimate them from the data. Example (Smoking and Lung Cancer) Doll and Hill reported the following observations for females in their study: They included \\(n_A=60\\) cancer patients and observed \\(41\\) smokers. They included \\(n_B=60\\) controls and observed \\(28\\) smokers. For long cancer patients (\\(A\\)), we have that the estimated probability of smoking is the relative frequency of smokers, that is \\(\\bar{y}_A=\\hat{p}_A=41/60=0.68\\) Likewise, for controls (\\(B\\)), we have that the estimated probability of smoking is \\(\\bar{y}_B=\\hat{p}_B=28/60=0.47\\) If the null hypothesis is true then the overall probability of smoking \\(p\\) can be estimated from either condition, or from both of them taken together, as if they all had the same diagnosis: \\[\\hat{p}=\\frac{n_A\\hat{p}_A+n_B\\hat{p}_B}{n_A+n_B}=\\frac{41+28}{60+60}=0.575\\] And \\(\\hat{p}\\) is called the pooled proportion, an estimate of the marginal probability of smoking \\(p\\). 15.5 \\(\\chi^2\\) test The standardized error that we make when we estimate the pooled probability \\(\\hat{p}\\) with the relative frequency of smokers using the lung cancer patients only is \\[Z_A= \\frac{\\bar{Y}_A-\\hat{p}}{\\sqrt\\frac{\\hat{p}(1-\\hat{p})}{n_A}}\\] Remember that one observation of \\(\\bar{Y}_A\\) is what we call \\(\\hat{p}_A\\), an estimate of the conditional probability of smoking if receiving a cancer diagnosis. Likewise for condition \\(B\\), the error made in using controls only (\\(B\\)) to estimate the pooled proportion is \\[Z_B= \\frac{\\bar{Y}_B-\\hat{p}}{\\sqrt\\frac{\\hat{p}(1-\\hat{p})}{n_B}}\\] In the plot, each cross is a smoker (either \\(1\\) or \\(0\\)) the dotted line is the estimated probability \\(\\hat{p}\\) of all subjects taken together (the pooled proportion) and the dots are the estimated probabilities of smoking in the lung cancer patients (\\(A\\)) and controls (\\(B\\)). The conditional probability table by cancer and control condition is \\[ \\begin{array}{cc|c} &amp; \\mathbf{A:cancer} &amp; \\mathbf{B:control} \\\\ \\mathbf{smoking: yes} &amp; 0.68 &amp; 0.47 \\\\ \\mathbf{smoking: no} &amp; 0.32 &amp; 0.53 \\\\ \\mathbf{sum} &amp; 1 &amp; 1 \\end{array} \\] Which we can illustrate in a barplot The probabilities of smoking within each diagnosis are different but look similar. Is the difference significant? Can we reject the null hypothesis taking into account the variation on the frequencies about the pooled proportion? Using the CLT, the total standardized squared error of estimating the proportion of smoking using each condition separately is the sum of the errors \\[W= Z_A^2+Z_B^2\\sim \\chi^2(1)\\] which is a random variable that follows a \\(\\chi^2\\) distribution with 1 degree of freedom. If the null hypothesis is true, we can use any condition to estimate the pool proportion. Using the marginals We formulated the null hypothesis from the statistical independence given by conditional probabilities. Another alternative is to formulate it from the product of the marginals. Therefore, the hypothesis test can also be written as \\(H_0:P(yes, A)=P(yes)P(A)\\). Cancer diagnosis and smoking are statistically independent as the joint probability is the product of the marginals. \\(H_1:P(yes, A)\\neq P(yes)P(A)\\). Cancer diagnosis and smoking are statistically dependent. If diagnosis and smoking status are truly independent from each other then one can show that the variable \\(W\\) above can also be computed as the sum of the squared errors that we make when we estimate the joint probabilities using the marginals (i.e. \\(\\hat{P}(yes,A)=\\hat{p}\\hat{P}(A)\\)), which we can if the null hypothesis is true. Therefore, the total standardized squared error of before can be re-written as: \\[W= \\sum_{j\\in\\{yes,no\\}} \\sum_{i\\in\\{A,B\\}} \\frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}\\] where, for instance, \\(O_{yes, A}=\\hat{P}(yes,A)\\) is the observed joint frequency of smoking and lung cancer, and \\(E_{yes, A}=\\hat{P}(yes)\\hat{P}(A)\\) is the expected frequency derived from the product of the marginals. From the joint probability table this statistics is the sum of the squared differences between the cells in the table and the products of the marginals, and easy to compute. If the observed value for \\(W\\) is a large, its probability under the null hypothesis is low, and we then may reject this hypothesis. The value of \\(W\\) is exactly the same however we compute it. Example (Smoking and Lung Cancer) When we compute the observed value of \\(W\\), using the first form, we find \\(w_{obs}= \\frac{(\\hat{p}_A-\\hat{p})^2}{\\frac{\\hat{p}(1-\\hat{p})}{n_A}} +\\frac{(\\hat{p}_B-\\hat{p})^2}{\\frac{\\hat{p}(1-\\hat{p})}{n_B}}\\) \\[=\\frac{(0.6833-0.575)^2}{{\\frac{0.575(1-0.575)}{60}}}+ \\frac{(0.4666-0.575)^2}{{\\frac{0.575(1-0.575)}{60}}}= 5.76\\] In terms of the expected value of \\(W\\) (\\(E(W)=1\\) -degrees of freedom) when we make no mistake) the null hypothesis is rewritten: \\(H_0:w=1\\). Lung cancer and smoking are statistically independent and the error in the estimation of the pooled proportion is low using each condition separately. \\(H_1:w&gt;1\\). Lung cancer and smoking are statistically dependent and the error in the estimation of the pooled proportion is high using each condition separately. We can test whether this value is large under the null hypothesis. If the null is true this statistics is close to zero, otherwise is large. We can then compute the \\(pvalue\\) given by \\[pvalue=P(W \\geq w_{obs}) =0.016\\] Pyhton: chi2.sf(5.76, 1) R: pchisq(5.76, 1, lower.tail = FALSE) This \\(pvalue\\) is not lower than the significance level \\(\\alpha=0.05\\) and therefore Doll and Hill reject \\(H_0\\), and concluded that the frequencies of smoking are different between cancer patients and controls, or, equivalently, that the frequency of smoking statistically depends from patient diagnosis, or, equivalently, that the frequency of smoking is significantly associated lung cancer diagnosis. The analysis can be done with the following code, with the contingency table for the absolute frequencies. import numpy as np from scipy.stats import chi2_contingency observed = np.array([[19, 32], [41, 28]]) chi2, p, dof, expected = chi2_contingency(observed, correction=False) print(f&quot;Chi-squared statistic: {chi2:.4f}&quot;) print(f&quot;P-value: {p:.4f}&quot;) R: conttable &lt;- cbind(c(19, 32),c(41,28)) chisq.test(conttable, correct = FALSE) The analysis presented here was only one of the evidences of the effect of smoking and cancer. Doll and Hill had to argue against the possibility that lung cancer patients started to smoke after diagnosis (reverse causation) and show that the size of the effect depended on the amount of cigarettes smokes (dose response). Hill famously listed 9 criteria to help support a causal relationship underlying a statistical association (Hill 1965). 15.6 Fisher’s exact test Another approach is Fisher’s exact test. When the conditions of the CLT break, often when one of the cells in the contingency table is very small, or the relative frequencies are close to 1 or 0, then the \\(\\chi^2\\) test cannot be applied. Example(Smoking and Lung Cancer in Males) Doll and Hill also reported the following observations for males in their study: They included \\(n_A=649\\) cancer patients and observed \\(647\\) smokers. Only \\(2\\) where non-smokers. They included \\(n_B=649\\) controls and observed \\(622\\) smokers. \\(27\\) were non-smokers. The hypothesis contrast is \\(H_0: p_A \\leq 1269/1298=0.977\\) For cancer patients (condition \\(A\\)) the probability of smoking is at most the same as in the patients and controls together. \\(H_1: p_A &lt; 1269/1298=0.977\\) The alternative hypothesis assumes that the probability of smoking in cancer patients \\(A\\) is higher than for all subjects together, indicating that there is an increase in the number of smokers in the cancer patients. The observed frequency of smokers in lung cancer is \\(\\hat{p}_A=647/649=0.997\\), but is it statistically significant greater than \\(0.977\\)? Smoking was widely spread among males in the 50’s. To test the statistical association under this conditions where the CLT does not hold then we use Fisher’s exact test. 15.7 Hypergeometric distribution Take a ballot for each of the \\(N=1298\\) patients and controls into an urn: There are a total of \\(N\\) ballots: There are \\(K=1269\\) ballots marked as smokers \\(N-K=29\\) ballots marked as not smokers. Then, if we take a \\(n=649\\) randomly selected ballots (similar in number to lung cancer patients), we may ask: What is the probability of observing more than \\(647\\) patients who are smokers? The probability of obtaining \\(Y\\) smokers in a sample of \\(n\\) drawn from a population of \\(N\\) where \\(K\\) are smokers is \\(P(Y=y)=P(one\\,sample) \\times (Number\\, of\\, ways\\, of\\, obtaining\\, x)\\) \\[=\\frac{1}{\\binom N n}\\binom K y \\binom {N-K} {n-y}\\] Therefore, \\(Y\\) follows a hypergeometric distribution \\[Y \\sim Hypergeometric(N,K,n)\\] Remember that \\(647\\) smokers were actually observed. Therefore, we are asking how rare that observation is in the case where diagnosis conditioning plays no role, or that cases and controls come from the same urn. For this probabilistic model, we compute the probability \\(P(Y&gt;647)\\), or put simply the \\(pvalue\\). If the observed value for \\(y_{obs}=647\\) is a rare high observation for the null hypothesis, we then reject the probabilistic urn model for the null. The upper tail \\(pvalue\\) for an observation \\(647\\) is \\[pvalue=P(Y &gt; 647)= 1- F_{hyper(N,K,n)}(647) = 0.00000064\\] In Python and R from scipy.stats import hypergeom 1-hypergeom.cdf(18, 600, 64, 200) R: 1-phyper(646, 1269, 29, 649) Which is lower than the significance level \\(\\alpha=0.05\\) and therefore we also reject \\(H_0\\) for males and conclude: that the frequency of smoking is significantly associated with cancer diagnosis also in males. The odds ratio gives us the strength of the of the change in the in the smoker proportion between controls and lung cancer: \\[OR=\\frac{p_A/(1-p_A)}{p_B/(1-p_B)}=14.04\\] It is a magnitude of the observed statistical association between smoking and lung cancer. We use the \\(OR\\) to distinguish between the different cases When \\(p_A\\) is equal to \\(p_B\\) then \\(OR=1\\) (independence of smoking and lung cancer) When \\(p_A\\) is lower than \\(p_B\\) then \\(OR&lt;1\\) When \\(p_A\\) is greater than \\(p_B\\) then \\(OR&gt;1\\) We say that for an \\(OR=14\\) in the case of the smoking, there was an increase of \\(14\\)-fold on the risk of cancer due to smoking. Python: import numpy as np from scipy.stats import fisher_exact observed = np.array([[27, 622], [2, 647]]) odds_ratio, p_value = fisher_exact(observed, alternative=&quot;greater&quot;) print(f&quot;Odds ratio: {odds_ratio:.4f}&quot;) print(f&quot;P-value: {p_value:.4f}&quot;) R: matrix &lt;- cbind(c(27, 622),c(2, 647)) fisher.test(matrix, alternative = &quot;greater&quot;) 15.8 Difference between several proportions We would also like to to know if the outcome of a random experiment may vary across multiple condition. We then formulate the hypothesis contrast: \\(H_0: p=p_A=p_B,...=p_M\\) The null hypothesis assumes that conditions (\\(i=\\{A,B,... ,M\\}\\)) and the outcome (\\(j=\\{1,0\\}\\)) are all independent and then they have the same probability of the outcome \\(j=1\\). \\(H_1:\\) at least for one \\(i\\), \\(p\\neq p_i\\) The alternative hypothesis assumes that the outcome is statistical independent from at least one condition. For deciding between hypothesis, we use the the so called \\(\\chi^2\\)-test for the homogeneity between conditions. Example (Selection of the Lactase Mutant) En el 2004 Bersaglieri et al (Bersaglieri et al. 2004) showed that the the human populations have been under the selection of the lactase alelle –13910*T in which a citocine was replaced by a tiamine in the DNA sequence of the lactase gene LCT. The change of a nucleotide at the given position gave the carriers tactate tolerance, promoting herding 5000–10000 years ago. Here are the number of chromosomes observed for each population and the allele frequencies (in a transposed contingency table) that they reported on eight European populations only \\[ \\small \\begin{array}{c|c|ccc} \\textbf{Population} &amp; \\textbf{Chromosomes (n)} &amp; \\textbf{lactase (T)} &amp; \\textbf{lactase (C)} &amp; \\textbf{sum} \\\\ \\hline \\textbf{Sardinian} &amp; 56 &amp; 0.071 &amp; 0.929 &amp; 1 \\\\ \\textbf{French Basque} &amp; 48 &amp; 0.667 &amp; 0.333 &amp; 1 \\\\ \\textbf{Tuscan} &amp; 16 &amp; 0.063 &amp; 0.937 &amp; 1 \\\\ \\textbf{Adygei} &amp; 34 &amp; 0.118 &amp; 0.882 &amp; 1 \\\\ \\textbf{French} &amp; 58 &amp; 0.431 &amp; 0.569 &amp; 1 \\\\ \\textbf{North Italian} &amp; 28 &amp; 0.357 &amp; 0.643 &amp; 1 \\\\ \\textbf{Orcadian} &amp; 32 &amp; 0.688 &amp; 0.312 &amp; 1 \\\\ \\textbf{Swedish Finnish} &amp; 360 &amp; 0.815 &amp; 0.185 &amp; 1 \\\\ \\end{array} \\] We have that the standardized squared error from the null hypothesis can be written as: \\[W= Z_A^2+Z_B^2+Z_C^2+Z_D^2+Z_E^2+Z_F^2+Z_G^2+Z_H^2\\sim \\chi^2(8-1)\\] which is a random variable that follows a \\(\\chi^2\\) distribution with \\(7=8-1\\) degrees of freedom (number of populations \\(-1\\)). Each term in \\(W\\) is the squared standardized error observed when we estimate the pooled proportion of the mutant allele \\(\\hat{p}\\) using each population separately. The estimated probability of the mutant allele in all populations together is the pooled proportion \\(\\hat{p}=\\frac{n_A\\hat{p}_A+n_B\\hat{p}_B+n_C\\hat{p}_C+n_D\\hat{p}_D+n_E\\hat{p}_E+n_F\\hat{p}_F+n_G\\hat{p}_G+n_H\\hat{p}_H}{n_A+n_B+n_C+n_D+n_E+n_F+n_G+n_H}\\) \\[=0.62\\] and the computation of the observed \\(W\\) is \\[w_{obs}=204.28\\] We can now test how rare this observation is under the null hypothesis that all the probabilities of the mutant allele across the populations are equal (i.e. homogeneous). We compute the \\(pvalue\\) \\[pvalue=P(W \\geq w_{obs}) =1-F_{\\chi^2(7)}(204.28)= 1.0\\times 10^{-40}\\] The observed \\(\\chi^2\\) statistic and the corresponding \\(pvalue\\) can be obtained in Python and R with the following code: import numpy as np from scipy.stats import chi2_contingency observed = np.array([[4, 32, 1, 4, 25, 10, 22, 293 ], [52, 16, 15, 30, 33, 18, 10, 67]]) chi2_stat, p_value, dof, expected = chi2_contingency(observed) print(f&quot;Chi-squared statistic: {chi2_stat:.4f}&quot;) print(f&quot;P-value: {p_value:.4f}&quot;) print(f&quot;Degrees of freedom: {dof}&quot;) R: observed &lt;- rbind(c(4, 32, 1, 4, 25, 10, 22, 293), c(52, 16, 15, 30, 33, 18, 10, 67)) chisq.test(observed) where the \\(\\chi^2\\)-test is computed with the number of chromosomes observed for each allele and population. That is the product of the total number of chromosomes in each population and their relative frequencies. We see that the \\(pvalue\\) is much lower than the significance level \\(\\alpha=0.05\\) and therefore we reject \\(H_0\\) and conclude that: The frequency of the T allele of the LCT gene is significantly associated the differences in the European populations, providing evidence for an underlying selection of lactate tolerance. We can illustrate the result a conditional frequency table by European population or with a bar plot. The \\(\\chi^2\\)-test tests the homogeneity of all the proportions of the mutant allele across the populations in relation to the overall mutant frequency \\(\\hat{p}=0.62\\). Note that the pooled proportion takes into account the number of individuals in each population. As the Swedish are the population that contribute the most to the pooled proportion, it strongly leans toward this population. In the figure showing the frequency of the lactase mutant across European populations, we have ordered the populations according their South-North latitude. We see that, except for the French Basque, the frequency increases with latitude. This remark supports the hypothesis that lactase persistence has been favored by positive selection in high-latitude regions of Europe, where sunlight is scarce and the synthesis of vitamin D is limited. In these environments, milk consumption provided a valuable source of calcium and vitamin D, both important for maintaining bone health. How can provide further evidence of this observation? 15.9 Goodness of fit A key insight of the \\(\\chi^2\\) test is to realize that the overall proportion offers a model to the entire data. If we split the data into categories (conditions), each of them can be used to estimate the overall proportion \\(p\\). If the model is a good fit to the data, the estimate from each category will fall close to the proportion, distributing around its value as a normal variable. A good fit means low accumulated error from all the estimates and, therefore, a low value of the observed \\(\\chi^2\\)-statistic, \\(w_{obs}\\). Under this perspective, the \\(pvalue\\) offers a measure of how good the data fits the null hypothesis. If it is large, we say that the data fits the null probability model, if it is small then the null probability model is not a good model for the data. Example (Selection of the Lactase Mutant) In the figure showing the frequency of the lactase mutant across European populations, we have ordered the populations according their South-North latitude. We see that, except for the French Basque, the frequency increases with latitude. The constant overall proportion \\(\\hat{p}=0.62\\) does not appear to be a good fit across the populations. The \\(\\chi^2\\)-test gave a very low \\(pvalue\\) and therefore, we discard that the null hypothesis is a suitable probabilistic model for the entire data. How can we improve the model? An option is to plot the bars at the estimated latitudes for each population (\\(i\\)) and make the proportion \\(p=p(x_i)\\) depend of the latitude \\(x_i\\) as a linear function \\[p(x_i)=\\alpha+\\beta\\times x_i\\] instead of being equal for all the populations. We may leave the French basques out because of their high frequency that could be due to a combination of genetic isolation, cultural continuity, and historical reliance on pastoralism. A popular method to estimate \\(\\alpha\\) and \\(\\beta\\) is to find the values that minimize the squared differences between the observed frequency \\(\\hat{p}_i\\) and its expected value by the latitude model \\(\\hat{p}(x_i)\\) \\[Q(\\alpha, \\beta)= \\sum_{i=1}^{M} (\\hat{p}_i -\\hat{p}(x_i))^2\\] where \\(M\\) are the number of conditions (population). The values \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) minimize the sum of squares \\(Q\\). As such the error of the estimates to the new line are \\[Z_i= \\frac{\\bar{Y}_i-\\hat{p}(x_i)}{\\sqrt\\frac{\\hat{p}(x_i)(1-\\hat{p}(x_i))}{n_i}}\\] each evaluated at their own latitude \\(\\hat{p}(x_i)\\). If we sum all the errors we have \\[W= \\sum_{i=1}^{M} Z^2 \\sim\\chi^2(M-2)\\] This statistic distributes as \\(\\chi^2\\) with \\(M-2\\) degrees of freedom, and we can use it to measure the goodness of fit of the model to the data. The model has two parameters that reduce the degrees of freedom of the statistic. Each term, assumes that the error from the condition \\(i\\) is a standard normal variable. \\[ \\small \\begin{array}{c|c|cc} \\textbf{Population} &amp; \\textbf{Chromosomes (n)} &amp; \\textbf{Observed: } \\hat{p}_i &amp; \\textbf{Expected: } \\hat{p}(x_i) \\\\ \\hline \\textbf{Sardinian} &amp; 56 &amp; 0.071 &amp; 0.053\\\\ \\textbf{Tuscan} &amp; 16 &amp; 0.063 &amp; 0.163\\\\ \\textbf{Adygei} &amp; 34 &amp; 0.118 &amp; 0.200\\\\ \\textbf{French} &amp; 58 &amp; 0.431 &amp; 0.311\\\\ \\textbf{North Italian} &amp; 28 &amp; 0.357 &amp; 0.274 \\\\ \\textbf{Orcadian} &amp; 32 &amp; 0.688 &amp; 0.752\\\\ \\textbf{Swedish Finnish} &amp; 360 &amp; 0.789 &amp; 0.185\\\\ \\end{array} \\] We see for our lactase example that the observed statistic, computed from the table, is \\(w_{obs}=\\sum_{i=1}^M Z_i^2= \\sum_{i=1}^M \\frac{(\\hat{p}_i-\\hat{p}(x_i))^2}{\\frac{\\hat{p}(x_i)(1-\\hat{p}(x_i))}{n_i}}\\) \\[=10.01\\] with \\(7-2\\) degrees of freedom (removing the Basques). The p-value is \\(pvalue=0.07\\), indicating that the linear is consistent with the data. We typically accept suitable fits to the data that have \\(pvalue&gt;0.05\\). We thus offer additional evidence to show that the lack of homogeneity of the frequencies may be explained by the changes the environment that go along with different latitudes. The \\(\\chi^2\\)-test is a general goodness of fit test for curve fitting. The expected value of an outcome at a point \\(x\\) can be derived from fully theoretical models and compared against the observed data that is assumed to distribute normally about its theoretical value. 15.10 Questions 1) The \\(\\chi^2\\) test for contingency table is used to \\(\\qquad\\)a: test the associations of a categorial variable; \\(\\qquad\\)b: test statistical independence between two categorical variables; \\(\\qquad\\)c: test statistical independence between a continuous and a categorical variable; \\(\\qquad\\)d: test significance of a categorial variable 2) In the Fisher test we compute the \\(pvalue\\) using \\(\\qquad\\)a: the central limit theorem; \\(\\qquad\\)b: a conditional probability; \\(\\qquad\\)c: the relative frequencies; \\(\\qquad\\)d: a parametric model 3) The probability to heal from an injury without treatment is 0.6. The probability to heal from an injury with treatment is 0.8. What is the odd ratio of healing with treatment with respect of healing without it. \\(\\qquad\\)a: 0.376; \\(\\qquad\\)b: 1.5; \\(\\qquad\\)c: 2.666; \\(\\qquad\\)d: 0.6666 4) The null hypothesis in testing for differences in proportions across different conditions assume \\(\\qquad\\)a: different conditions for different proportions; \\(\\qquad\\)b: the same condition across different proportion; \\(\\qquad\\)c: different proportions across the conditions; \\(\\qquad\\)d: the same proportion across the conditions 5) if the \\(pvalue\\) is lower than \\(0.05\\) in a test of several proportions then \\(\\qquad\\)a: we accept that at least one proportion is different from the rest; \\(\\qquad\\)b: We accept that all proportions are different between each other; \\(\\qquad\\)c: We accept the all proportions are equal; \\(\\qquad\\)d: We accept that there is one single proportion 15.11 Practice Load hospital data (https://alejandro-isglobal.github.io/SDA/data/hospital.txt) Test the hypothesis that the infection proportion at hospitals C and D are different. Use a \\(\\chi^2\\) test and a exact Fisher test. Interpret the OR. Make a bar plot Test the hypothesis that the infection proportion at hospitals A, B and E are different. Make a bar plot Solutions References "],["mean-differences-between-two-samples.html", "Chapter 16 Mean differences between two samples 16.1 Difference in means between two groups 16.2 Data 16.3 Difference between means 16.4 Hypothesis test 16.5 Estiamtor of the mean difference 16.6 Standardized error 16.7 Standardized error for the null 16.8 Mean differences when \\(n\\) is small 16.9 Data 16.10 Difference between means 16.11 Hypothesis test 16.12 Estimator of the mean difference 16.13 Standardized error for the null 16.14 Mean differences with unequall variances 16.15 Data 16.16 Questions 16.17 Practice", " Chapter 16 Mean differences between two samples Testing statistical dependence lies at the core of statistical inference. In many scientific studies, we want to know whether changes in one factor are associated with changes in an outcome. To answer this, experiments are planned with great care so that we can identify and measure the sources of variation in the data. Variation in an outcome can come from several sources. Some of it is due to measurement error, which improved techniques can often reduce. Some comes from intrinsic differences between the observational units (people, animals, samples, etc.). If these differences are irrelevant to the main question, we can control for them — for example, by grouping similar units together and repeating the experiment under the same conditions. Sometimes, however, controlling all conditions is impractical or undesirable. In those cases, we can use randomization so that uncontrolled influences become part of the random error, which can then be handled statistically. In the kind of experiments we consider here, we deliberately change one condition of interest, repeat the random experiment, and record the outcome. Each experimental unit is observed under only one condition — we do not (or cannot) measure the same unit under multiple conditions. When the outcome is a continuous variable, we may ask how its expected value changes as the condition changes. The theoretical question is: Does changing the condition alter the probability distribution of the outcome, and therefore its mean? Experimentally, if the averages differ between conditions, we want to know: To what extent can we conclude that the outcome is statistically dependent on the condition? In this chapter, we will focus on testing statistical independence between one continuous variable and two discrete conditions (or groups). We will start with the case where \\(n\\) is large and the central limit theorem applies, and illustrate its use using the transformative finding in preventive health from the Framingham cohort that established hypertension as a risk factor for stroke. Next, we will introduce the \\(t\\)-test, which applies when \\(n\\) is small and the outcomes are normally distributed with equal variances. This will be illustrated with James Clerk Maxwell’s counterintuitive discovery that gas viscosity is independent of pressure — a key finding in the foundation of statistical mechanics. Finally, we will consider the case where the conditions also change the variances of normally distributed outcomes. Here, we will present the two-sample \\(t\\)-test for unequal variances and demonstrate its application to comparing the weights of leptin knocked out mice, demonstrating that hormone encoded by the gene plays a role in obesity. 16.1 Difference in means between two groups We will consider an outcome of a random experiment that follows a normal probability model: \\[ Y \\sim N(\\mu, \\sigma^2) \\] We aim to measure this outcome several times under two different conditions, or in two groups, called \\(A\\) and \\(B\\). Our goal is to determine whether the expected value (mean) of the random variable changes between these conditions. When testing a hypothesis about the mean, but without knowing a fixed reference value \\(\\mu_0\\) for comparison, we often estimate it by repeating the experiment under a reference condition \\(A\\), often referred to as the control condition. We then compare this estimate to the mean obtained under a new condition \\(B\\), where we may have applied a new treatment or selected units from a specific case. Examples include testing clinical differences between cases and controls, comparing a drug to a placebo, or changing the conditions of a physical system. Example (Framingham cohort) In 1948, 5209 men and women from Fremingham, Massachusetts, were recruited to be followed during their life course to find the causes of hart disease. The cohort, was transform epidemiological research with unprecedented insights into cardiovascular disease. From this investigations we now know, for example, that hypertension is a risk factor for stroke (Kannel et al. 1961), which has changed clinical practice, ever since. Let us explore this finding with available data from the cohort. While access to three generations of full data can be obtained to perform comprehensive analysis, here we will illustrate the fining with the teaching data set from the Framingham Heart Study. In this data matrix, we have can take for instance two conditions: \\(A:Stroke\\) and \\(B:No \\,stroke\\) and assume that the level of systolic blood preasure (SYBP) in a randomly chosen man between 40 and 59 years of age who reported a stroke has a probability density \\[Y_A \\sim N(\\mu_A, \\sigma_A^2)\\] the level in a man with the same characteristics but who has not suffer a stroke has a probability density for SYBP \\[Y_B \\sim N(\\mu_B, \\sigma_B^2)\\] 16.2 Data One random experiment has two variables: \\((c, y)\\): \\(C \\in \\{A,B\\}\\) is a discrete variable with two possible outcomes, one for each experimental condition. \\(Y\\) a continuous variable, whose observation produces the outcome of interest The repetition of the random experiment \\(n\\) times is therefore a table such as \\[ \\begin{array}{ccc} \\mathbf{Individual} &amp; \\mathbf{Condition} &amp; \\mathbf{Outcome} \\\\ c_1 &amp; \\text{A} &amp; \\text{121.0} \\\\ c_2 &amp; \\text{A} &amp; \\text{127.5} \\\\ c_3 &amp; \\text{A} &amp; \\text{141.0} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ c_{n-2} &amp; \\text{B} &amp; \\text{153.5} \\\\ c_{n-1} &amp; \\text{B} &amp; \\text{160.0} \\\\ c_{n}&amp; \\text{B} &amp; \\text{173.0} \\\\ \\end{array} \\] For example, subject \\(1\\) was under condition \\(A\\) and had outcome \\(121.0\\) for \\(Y\\). Example (Framingham cohort) In the Framingham cohort the SYBP is a continuous outcome in 40-59 year old man \\(SYBP \\in (83.5, 217)\\) And the condition is whether the same individual reported having a stroke or not: \\(stroke \\in \\{yes:A,no:B\\}\\) \\(201\\) men reported having a stroke and \\(2760\\) reported not having one. We want to know whether the probability of having high systolic blood pressure changes between stroke patients or controls. We may also ask whether the expected value of SYBP is significantly higher in stroke patients. Our research interest is to show that SYSBP is statistically dependent from stroke status. 16.3 Difference between means We take the SYSBP levels conditioned to each stroke, and observed: \\(n_A=201\\) stroke patients had a mean of \\(\\bar{y}_A=140.91\\) and \\(s=21.79\\) $n_B=2760 $ non-stroke controls had a mean of \\(\\bar{y}_B=133.99\\) and \\(s=18.42\\) We can show a plot for the mean in each condition overlying the histogram of the data. Or we can overlay the histograms horizontally The plots suggest that stroke patients will have more probability of high SYBP than controls, however some stroke patients will have very low SYBP, and some controls high. The dispersion of the outcome is large in both groups but, on average stroke patients have more SYPD. Is the difference between the averages \\(\\bar{y}_A\\) and \\(\\bar{y}_B\\) (the dots at the bottom of the histogram) statistically significant? How confident are we on the magnitude of this difference? That is, could have we got this lower higher levels of SYBP in stroke patients by chance alone (null hypothesis) or there is biological difference that cannot be fully explained by chance (alternative hypothesis)? 16.4 Hypothesis test Let us formulate the hypothesis contrast \\(H_0: \\mu_A=\\mu_B\\) The null hypothesis is that both groups have the same mean. \\(H_1: \\mu_A \\neq \\mu_B\\) The alternative hypothesis (research interest) is that the means are different between groups. Only one can be true. How would the data decide? To make things easier let us redefine the contrast. If we consider \\(\\delta\\), the difference between means \\(\\delta=\\mu_A-\\mu_B\\), then the hypotheses can be written as \\(H_0: \\delta=0\\) \\(H_1: \\delta\\neq 0\\) Either the difference between the means is zero or not. \\(\\delta\\) is the parameter of interest in our study, and to test hypotheses on it, we need to find an estimator for it. 16.5 Estiamtor of the mean difference The statistic \\(D=\\bar{Y}_A-\\bar{Y}_B\\), that is the difference in averages, is an estimator of \\(\\delta\\). In particular, we can show that the the estimator is unbiased because its expected value is the parameter \\[E(D)=E(\\bar{Y}_A-\\bar{Y}_B)=\\mu_A-\\mu_B=\\delta\\] and consistent because its variance gets smaller when \\(n=n_A+n_B\\) gets bigger \\[\\sigma_D^2=V(\\bar{Y}_A-\\bar{Y}_B)=\\frac{\\sigma^2_A}{n_A}+\\frac{\\sigma^2_B}{n_B}\\] This means that we can take one observed value of \\(D\\), that is \\(d_{obs}\\), for the estimation of the parameter \\(\\delta\\). It is important to have a clear idea of the different components in the estimation and the hypothesis test, as they are shown below. Those are the distributions of the data, the distributions of the averages and the means for each condition, and their difference. Remember that, for instance, the average \\(\\bar{Y}_A\\) is also called the sample mean, and it has a distribution that is centered on \\(\\mu_A\\) and has variance \\(\\frac{\\sigma_A^2}{n_A}\\). 16.6 Standardized error If \\(Y_A\\) and \\(Y_B\\) are both normal independent variables, their averages are normal and so it is their difference \\(D\\). Therefore, the standardization of \\(D\\) is approximately standard normal \\[Z=\\frac{D-\\delta}{\\sqrt{V(D)}}=\\frac{\\bar{Y}_A-\\bar{Y}_B -\\delta}{\\sqrt{\\frac{s^2_A}{n_A}+\\frac{s^2_B}{n_B}}} \\sim_{approx} N(0,1)\\] when both \\(n_A\\) and \\(n_B\\) are large. It is approximate because as we do not know the variances \\(\\sigma^2_A\\) and \\(\\sigma^2_B\\), we estimated them with \\(s_A^2\\) and \\(s_B^2\\). We can also use this approximation when we do not know the distributions of \\(Y_A\\) and \\(Y_B\\), as a result of the central limit theorem, when both groups have large sample sizes. 16.7 Standardized error for the null If the null hypothesis is true (\\(\\delta=0\\)), then when we estimate \\(\\delta\\) with \\(D\\) we make an error. When \\(n\\) is large, the standardized error from the null hypothesis is the statistic \\[Z=\\frac{\\bar{Y}_A-\\bar{Y}_B}{\\sqrt{\\frac{s_A^2}{n_A}+\\frac{s^2_B}{n_B}}}\\] that follows a standard normal distribution because of the central limit theorem. To test the hypothesis we then ask if the observed \\(z_{obs}\\) is within the acceptance region of the null hypothesis. In particular, we ask: is the probability of observing a more extreme value than \\(z_{obs}\\) lower than \\(\\alpha=0.05\\) if the null hypothesis is true? The value of \\(z_{obs}\\) is the standardized value of the observed difference between the averages: \\[z_{obs}=\\frac{d_{obs}}{\\sqrt{\\frac{s_A^2}{n_A}+\\frac{s^2_B}{n_B}}}\\] For our data, we have that the observed standardized mean difference is \\[z_{obs}=\\frac{\\bar{y}_A-\\bar{y}_B }{\\sqrt{\\frac{s^2_A}{n_A}+\\frac{s^2_B}{n_B}}}=\\frac{140.91-131.99}{\\sqrt{\\frac{21.79}{201}+\\frac{18.42}{2760}}}=5.95\\] Since our hypothesis is two tailed then the the two-tailed \\(pvalue\\) is \\[pvalue=2(1-\\Phi(5.95))=2.68 \\times 10^{-9}\\] which is much lower than \\(\\alpha\\). Therefore, we reject the null hypothesis that \\(H_0:\\delta=0\\) that is that the SYBP in stroke patients and controls are equal. In other words, we have strong evidence that the mean SYBP between conditions are different. In Python and R, we use the fact that the standardized statistic \\(Z\\) when \\(n\\) is large is close to a \\(t\\) distribution with large degrees of freedom, even when the outcome distributions are not normal. We can use the \\(t\\)-test as an approximation for the normal standard in this cases Python import pandas as pd from scipy import stats # Assuming dat is already a pandas DataFrame # Example: dat = pd.DataFrame({&quot;SYSBP&quot;: [...], &quot;STROKE&quot;: [...]}) # Split into two groups group_no_stroke = dat.loc[dat[&quot;STROKE&quot;] == &quot;No Stroke&quot;, &quot;SYSBP&quot;] group_stroke = dat.loc[dat[&quot;STROKE&quot;] == &quot;Stroke&quot;, &quot;SYSBP&quot;] # Perform independent two-sample t-test (equal_var=False is Welch&#39;s t-test) t_stat, p_value = stats.ttest_ind(group_no_stroke, group_stroke, equal_var=False) ##Framingham Data library(riskCommunicator) data(framingham) sel &lt;- framingham$SEX == 1 &amp; framingham$AGE &gt; 40 &amp; framingham$AGE &lt; 59 dat &lt;- data.frame( SYSBP = framingham$SYSBP[sel], STROKE = factor(framingham$STROKE[sel], labels = c(&quot;No Stroke&quot;, &quot;Stroke&quot;)) ) t.test(SYSBP~STROKE, data=dat) We may conclude that systolic blood pressure is different between stroke patients and control, suggesting high SYBP as a potential factor for the disease. However, this simple preliminary approach needs to be taken with care. The data analyzed did not record whether the SYBP was measured after or before the stroke. In addition, we do not know which of those control patients had a stroke after they reported their status. To determine whether SYBP is an independent cause of stroke, the Framingham cohort followed the patients for several years recording SYBP and cardiovascular health. Their more refined analysis took the continuous values of SYBP as conditions for the occurrence of stroke during follow up. We can illustrate the results of the comparison between two group means in three different ways: Using a bar plot with confidence intervals for the estimate in the mean of each group. Note that in a report we usually add the confidence intervals for the means. Non-overlapping confidence intervals also indicate that the difference between means is statistically significant, as this is an equivalent criteria to reject the null hypothesis. Using a boxplot. In this plot, we do not show the means but a summary of the distribution properties of the data at each condition. Remember that the properties are the median (the middle line), the quartiles (the box edges) and the \\(5\\%\\) and \\(95\\%\\) quantiles (the whiskers). Using a violin plot. These are smoothed mirrored histograms overlaid. Here, you do not see the averages of each group but the modes (peaks of the histograms). However, you get the idea that the probability of high SYBP is higher in stroke patients that in controls. 16.8 Mean differences when \\(n\\) is small Example (Gas viscosity) James C. Maxwell grounded the field of statistical mechanics with a remarkable prediction. He developed the theory, designed the experiment and measure in the laboratory the fact that the viscosity of gases do not depend on their pressure (Maxwell 1866). This counter-intuitive phenomena was derived from the molecular properties of gasses and help convince the scientific community that statistical properties of molecules underlie some properties of macroscopic objects. Theoretically, we showed that gas viscosity depends on two phenomena that exactly cancel out. He deigned a rotating pendulum in a container that could be subject to different gas pressures. When the pendulum rotates then its period to complete a full swing gets longer as the number of molecules in the gas get larger because it hits more molecules slowing it down. However, in gases, as the pendulum hits the molecules, if the pressure is increased, then those molecules would not travel far and get dragged along with the pendulum, creating a gas pocked around it diminishing the drag force. These to phenomena cancel out making the viscosity independent of the pressure. We how the results of Maxwell’s experiment, dividing the pressures reported into two conditions low (\\(&lt;6\\))mmHg and high (\\(&gt;19\\))mmHg and the time the pendulum took for a full swing (s). While the pressures were low to examine rarefied gases, the change of in pressure in the experiment was threefold. Maxwell pre-processed the time periods to compute the rate of decay of arc lengths to compare it with theoretical values. Here, we illustrate the raw measurements of gas viscosity throughout pendulum periods. \\[ \\begin{array}{ccc} \\mathbf{Experiment} &amp; \\mathbf{Pressure} &amp; \\mathbf{Period} \\\\ 1 &amp; \\text{Low}&amp; 362.66\\\\ 2 &amp; \\text{Low}&amp; 362.80\\\\ 3 &amp; \\text{Low}&amp; 364.04\\\\ 4 &amp; \\text{Low}&amp; 362.72\\\\ 5 &amp; \\text{Low}&amp; 362.94\\\\ 6 &amp; \\text{Low}&amp; 363.80\\\\ 7 &amp; \\text{High}&amp; 362.64\\\\ 8 &amp; \\text{High}&amp; 362.50\\\\ 9 &amp; \\text{High}&amp; 362.86\\\\ 10&amp; \\text{High}&amp; 363.80\\\\ 11&amp; \\text{High}&amp; 362.89\\\\ 12&amp; \\text{High}&amp; 363.90\\\\ \\end{array} \\] The experiment was then repeated a small number of times \\(12\\). We can make a boxplot for each condition. We assume that the period of the pendulum under high pressure is normal random variable \\[Y_A \\sim N(\\mu_A, \\sigma^2)\\] the period of the pendulum under low pressure is also normally distributed \\[Y_B \\sim N(\\mu_B, \\sigma^2)\\] both distributions have the same variance \\(\\sigma^2\\). 16.9 Data One random experiment has two outcomes: \\((preassure, period)\\). The pressure is a categorical variable and determines the conditions of the experiment: \\(Pressure \\in \\{High:A, Low:B\\}\\) The period is a continuous variable and it is the outcome of interest. \\(T \\in (360, 365)\\) (seconds) 16.10 Difference between means Maxwell measured the periods conditioned to two different pressures, and observed: \\(n_A=6\\) experiments under high pressure had period average \\(\\bar{y}_A=363.0983\\) an standard deviation \\(s_A=0.600547\\) \\(n_B=6\\) experiments under low pressure had period average \\(\\bar{y}_B=363.16\\) an standard deviation \\(s_B=0.6009326\\) We can draw violin plots per group We see that the distributions are very similar similar and their means differ in 0.06 seconds. Can we have a statistical test for this difference? 16.11 Hypothesis test We can again formulate the hypothesis on the difference between means \\(\\delta=\\mu_{high} -\\mu_{low}\\) \\(H_0: \\delta=0\\). The null hypothesis (research interest) assumes that the pendulum will take the same time to swing in high and low pressures. \\(H_1: \\delta \\neq 0\\). Therefore, the alternative hypothesis is that the mean periods under different pressures are different. 16.12 Estimator of the mean difference The statistic \\(D=\\bar{Y}_A-\\bar{Y}_B\\) is again an unbiased estimator of \\(\\delta\\) \\[E(D)=\\delta\\] Since the periods for \\(A\\) and \\(B\\) are normal variables with the same variance \\(\\sigma^2\\) each sample variance in each group is an estimator of \\(\\sigma^2\\). That is \\[\\hat{\\sigma}^2=s^2_A=s^2_B\\] then (Theorem) the standardized error \\[T=\\frac{\\bar{Y}_A-\\bar{Y}_B -\\delta}{\\sqrt{s_p^2(\\frac{1}{n_A}+\\frac{1}{n_B})}} \\sim T(n_A+n_B-2)\\] follows exactly a T-distribution with \\(n_A+n_B-2\\) degrees of freedom. The pooled variance \\(s_p^2\\), is an estimator of \\(\\sigma^2\\) \\[\\hat{\\sigma}^2=s_p^2= \\frac{(n_A-1) s^2_A+(n_B-1) s^2_B}{n_A+n_B-2}\\] 16.13 Standardized error for the null If the null hypothesis is true (\\(\\delta=0\\)), then when we estimate \\(\\delta\\) with \\(D\\) we make an error. The standardized error from the null hypothesis is the statistic \\[T=\\frac{\\bar{Y}_A-\\bar{Y}_B }{\\sqrt{s_p^2(\\frac{1}{n_A}+\\frac{1}{n_B})}} \\sim T(n_A+n_B-2)\\] that follows a T-distribution. To test the hypothesis, we then ask if the observed \\(t_{obs}\\) falls within the acceptance region of the null hypothesis. In particular, we ask: is the probability of observing a more extreme value than \\(t_{obs}\\) lower than \\(\\alpha=0.05\\), if the null hypothesis is true? The value of \\(t_{obs}\\) is the standardized value of the observed difference between the averages: \\(t_{obs}=\\frac{d_{obs}}{\\sqrt{s_p^2(\\frac{1}{n_A}+\\frac{1}{n_B})}}\\) \\[=\\frac{\\bar{y}_A-\\bar{y}_B }{\\sqrt{\\frac{s^2_p}{n_A}+\\frac{s^2_p}{n_B}}}=\\frac{363.0983-363.16}{\\sqrt{\\frac{0.3608883}{6}+\\frac{0.3608883}{6}}}=-0.178\\] The two-tailed \\(pvalue\\) of \\(t_{obs}\\) is \\[pvalue=2(1-F_{t,10}(0.138))=0.89\\] which is higher than \\(\\alpha=0.05\\). Therefore, the data shows that there are no significant differences between the periods under low or high pressure, which is consistent with Maxwell’s theoretical prediction, and gave experimental support to a counter-intuitive gas property that could be satisfactory explained from the statistical properties of its molecular components. We can compute this in Python and R Pyhton: import pandas as pd from scipy import stats # Create the dataframe dat = pd.DataFrame({ &quot;Pressure&quot;: [&quot;Low&quot;]*6 + [&quot;High&quot;]*6, &quot;Period&quot;: [362.66, 362.8, 364.04, 362.72, 362.94, 363.8, 362.64, 362.5, 362.86, 363.8, 362.89, 363.9]}) # Split into groups low = dat.loc[dat[&quot;Pressure&quot;] == &quot;Low&quot;, &quot;Period&quot;] high = dat.loc[dat[&quot;Pressure&quot;] == &quot;High&quot;, &quot;Period&quot;] # Two-sample t-test with equal variances t_stat, p_val = stats.ttest_ind(low, high, equal_var=True) print(&quot;t-statistic:&quot;, t_stat) print(&quot;p-value:&quot;, p_val) R: dat &lt;- data.frame(Pressure=c(&quot;Low&quot;, &quot;Low&quot;, &quot;Low&quot;, &quot;Low&quot;, &quot;Low&quot;, &quot;Low&quot;, &quot;High&quot;, &quot;High&quot;, &quot;High&quot;, &quot;High&quot;, &quot;High&quot;, &quot;High&quot;), Period=c(362.66, 362.8, 364.04, 362.72, 362.94, 363.8, 362.64, 362.5, 362.86, 363.8, 362.89, 363.9)) t.test(Period ~Pressure, data=dat, var.equal=TRUE) 16.14 Mean differences with unequall variances Exemple (Leptin knockouts) Leptin is an adipose tissue hormone that creates the sensation of satiety after eating. It is believe to play an important role into the development of obesity at early ages. Ramos-Lobos and colleagues tested the additional effect of leptin during neurodevelopment in mice (Ramos-Lobo et al. 2019). They produced 7 male mice for which their leptin gene was knocked out, and could not produce the hormone. While 16 mice were left with normal leptin function. These are called wild type. We can use this data to show that the levels leptin can produce weight gains in the animals. We therefore ask whether the mean weight of the animals is different between wild types and knock-outs. As the event of knocking out a gene is clearly before the development of obesity, this type of genetic experiments are considered to suggest a causal link between a gene and a trait. We assume that the weight of the control animals (wild type) has a probability density \\[Y_A \\sim N(\\mu_A, \\sigma^2)\\] the weight of the animals with no leptin gene has a probability density \\[Y_B \\sim N(\\mu_B, \\sigma^2)\\] both distributions have the same variance \\(\\sigma^2\\). 16.15 Data One random experiment in this study has two outcome types: \\((leptin, weight)\\). Leptin is categorical variable that determines the condition of the experiment whether the animal has a functioning gene (leptin+) or not (leptin-) \\(leptin \\in \\{leptin+:A,leptin-:B\\}\\) The outcome of interest is the weight of the animal, and it is a continuous variable \\(weigth \\in (20, 60)\\) The data looks like \\[ \\begin{array}{ccc} \\mathbf{Mouse} &amp; \\mathbf{Leptin} &amp; \\mathbf{Weigth} \\\\ 1 &amp;\\text{Leptin} &amp;27.67\\\\ 2 &amp;\\text{Leptin+} &amp;27.40\\\\ 3 &amp;\\text{Leptin+} &amp;25.77\\\\ 4 &amp;\\text{Leptin+} &amp;25.60\\\\ 5 &amp;\\text{Leptin+} &amp;25.03\\\\ 6 &amp;\\text{Leptin+} &amp;25.90\\\\ 7 &amp;\\text{Leptin+} &amp;26.67\\\\ 8 &amp;\\text{Leptin+} &amp;25.60\\\\ 9 &amp;\\text{Leptin+} &amp;28.93\\\\ 10 &amp;\\text{Leptin+} &amp;31.83\\\\ 11 &amp;\\text{Leptin+} &amp;25.90\\\\ 12 &amp;\\text{Leptin+} &amp;26.30\\\\ 13 &amp;\\text{Leptin+} &amp;27.90\\\\ 14 &amp;\\text{Leptin+} &amp;26.77\\\\ 15 &amp;\\text{Leptin+} &amp;25.83\\\\ 16 &amp;\\text{Leptin+} &amp;20.87\\\\ 17 &amp;\\text{Leptin-} &amp;46.57\\\\ 18 &amp;\\text{Leptin-} &amp;40.43\\\\ 19 &amp;\\text{Leptin-} &amp;41.97\\\\ 20 &amp;\\text{Leptin-} &amp;41.17\\\\ 21 &amp;\\text{Leptin-} &amp;41.57\\\\ 22 &amp;\\text{Leptin+} &amp;46.17\\\\ 23 &amp;\\text{Leptin+} &amp;53.83 \\end{array} \\] The box plot suggests that the variances for each group are different because the box for the leptin- group is thinner (less dispersed) than the box for the leptin+ group. It appears that knocking out the gene not only reduces the mean weight but also the weight variance. Therefore it is better to assume that the weight of the wild type animal (leptin+) has a probability density \\[Y_A \\sim N(\\mu_A, \\sigma_A^2)\\] the weight of the animal with no leptin has a probability density \\[Y_B \\sim N(\\mu_B, \\sigma_B^2)\\] When we assume unequal variances in each group, the standardized error for the null hypothesis (\\(\\delta=0\\)) \\[T=\\frac{\\bar{Y}_A-\\bar{Y}_B }{\\sqrt{\\frac{s_A^2}{n_A}+\\frac{s_B^2}{n_B}}} \\sim_{aprox} T(\\nu)\\] approximately follows a t-distribution with \\[\\nu=\\frac{(\\frac{s_A^2}{n_A}+\\frac{s_B^2}{n_B})^2}{\\frac{(s_A^2/n_B)^2}{n_A-1}+\\frac{(s_B^2/n_B)^2}{n_B-1}}\\] degrees of freedom. The brilliant idea of Welsh was to fix the form of the variance of \\(D\\) (denominator of \\(T\\)), and then ask which was the closest \\(t\\)-distribution that would describe \\(T\\). For that he needed to adjust the degrees of freedom. This is called a Welsh test. In Python and R, the Welsh test is obtained by setting the parameter of equal variances to false in the t-test functions Python: stats.ttest_ind(groupA, groupB, equal_var=False) R: t.test(groupA, groupB, var.equal=FALSE) For the mouse data, we observe a very significant increase in \\(18.03\\)gr (\\(pvalue=2.444 \\times 10^{-5}\\)) in weight between the wild-type mice and leptin knockouts, demonstrating the effect of leptin hormone in mouse weight and a possible role in human obesity. Additional studies also required to supplement the knockout mice with leptin hormone and observe the reduction of weight. Such interventions demonstrated the causal role of the hormone on weight. For human studies, similar evidence could be obtained from randomized drug-placebo studies of patients with leptin deficiency who are given leptin supplementation or placebo. Note that if we had use the equal variances \\(t\\)-test, we would have obtained a more significant result \\(pvalue=3.376854 \\times 10^{-11}\\) but this model is less appropriate based on how the data distributes. 16.16 Questions 1) We test for the difference between the means of two random variables when we have measured \\(\\qquad\\)a: two continuous random variables; \\(\\qquad\\)b: two categorical random variables; \\(\\qquad\\)c: one dichotomic variable and one continuous random variable; \\(\\qquad\\)d: any categorical variable and one continuous random variable; 2) The statistic \\(D=\\bar{Y}_A-\\bar{Y}_B\\) estimates \\(\\qquad\\)a: The difference between the averages of \\(Y_A\\) and \\(Y_B\\); \\(\\qquad\\)b: The difference between the means of \\(Y_A\\) and \\(Y_B\\); \\(\\qquad\\)c: The variation of \\(\\bar{Y}_A\\) with respect to \\(\\bar{Y}_B\\); \\(\\qquad\\)d: \\(d_{obs}\\) 3) We can use a \\(Z\\)-test only when \\(\\qquad\\)a: we have a large sample of the continuous variable in one of the conditions; \\(\\qquad\\)b: we have large samples of the continuous variable in each condition; \\(\\qquad\\)c: the variances of the continuous variable are equal in each conditions; \\(\\qquad\\)d: when the distributions of the continuous variable in each condition are normal 4) We can use a \\(t\\)-test only when \\(\\qquad\\)a: we have small samples of the continuous variable in each condition; \\(\\qquad\\)b: the distributions of the continuous variable in each condition are normal; \\(\\qquad\\)c: \\(D\\) is unbiased; \\(\\qquad\\)d: we cannot use a \\(Z\\)-test 5) For the data \\[ \\begin{array}{ccc} \\mathbf{Subject} &amp; \\mathbf{Y} &amp; \\mathbf{C} \\\\ 1&amp;1.1&amp;A\\\\ 2&amp;0.9&amp;A\\\\ 3&amp;0.8&amp;B\\\\ 4&amp;0.6&amp;B\\\\ \\end{array} \\] assuming that \\(Y\\) is normally distributed, which is the best test? \\(\\qquad\\)a: \\(t\\)-test with equal variances \\(\\qquad\\)b: \\(t\\)-test with unequal variances \\(\\qquad\\)c: \\(z\\)-test with equal variances \\(\\qquad\\)d: \\(z\\)-test with unequal variances 16.17 Practice Load leptin data https://alejandro-isglobal.github.io/SDA/data/dataleptin.txt Test the hypothesis that in the control animals, the weight of females is different than the weight of males Test the hypothesis that the leptin KO animals have higher weight than the KO animals with supplemented leptin Make a bar plot and a boxplot for the latter case. Compute the confidence intervals for the weight, in each group. Solutions References "],["mean-differences-across-several-groups.html", "Chapter 17 Mean differences across several groups 17.1 Different means among several conditions 17.2 Data 17.3 Difference between means 17.4 Hypothesis test 17.5 Variance components estimators 17.6 Analysis of variance (ANOVA) 17.7 ANOVA for Two Groups 17.8 Linear model 17.9 2-way ANOVA 17.10 Data 17.11 Modeling residuals 17.12 2-way ANOVA linear model 17.13 Hypothesis tests 17.14 Variance components 17.15 2-way ANOVA with interaction 17.16 Linear model 17.17 Hypothesis tests 17.18 Variance components 17.19 Questions 17.20 Practice", " Chapter 17 Mean differences across several groups Experiments may be subjected to more than two conditions. We will first consider experiments with categorical and mutually exclusive conditions: if an experimental unit belongs to one condition, it cannot belong to another. For instance, in a study of all-cause mortality, patients may die from coronary disease, cancer, or respiratory conditions; or in a population genetics study, we may examine allele frequencies across different global populations. In both cases, we would like to know whether the set of conditions under study influences the outcome. If at least one condition changes the outcome, that is enough to conclude that the categorical random variable encoding the conditions (as different levels) influences the properties of the experiment, and therefore its outcomes. If the outcome is a continuous variable, we are often interested in how its mean changes across the categories of the condition variable. The categories of the condition variable are usually referred to as groups. In this chapter, we will generalize the test for differences between two means to the case of many groups, that is, many levels of a categorical variable. We will do this using analysis of variance (ANOVA), which compares the variance within groups against the variance between groups. We will illustrate this analysis with data showing that leptin deficiency can cause obesity. In ANOVA, we explain the observed variation by decomposing it into additive sources. Some sources of variation are of primary interest and define the hypotheses we want to test, while others correspond to systematic error that we try to control for. Using Michelson and Morley’s classic data, we will see that accounting for multiple sources of variation in the measurement of the speed of light was key to showing that light travels at the same speed in any direction. Conditions may also arise from different types of categorical variables, and in such cases they are not mutually exclusive. For example, when studying mortality, disease type (cause of death) may be one condition type, and sex another; in genetics, we may study allele frequencies across different regions (first type of condition) and across different species (second type). To handle these situations, we will introduce the two-way ANOVA, which allows us to measure the independent contributions of two different condition types. Finally, we will use two-way ANOVA with interaction, which tests whether at least one combination of the two categorical variables produces effects that cannot be explained by the independent contributions of each variable alone. We will illustrate this by analyzing the differential effect of leptin deficiency in the weight of females. 17.1 Different means among several conditions Example (Leptin knockouts) Leptin is a hormone that regulates satiety. Ramos-Lobo and colleagues tested the additional effect of leptin during neurodevelopment in mice (Ramos-Lobo et al. 2019). In their study, they reported the weight of 16 male mice with normal leptin function, 7 male mice in which the leptin gene was knocked out, and 10 knockout mice that received leptin hormone injections. The inclusion of this third group demonstrates that lean weight can be partially restored by leptin supplementation, showing that leptin deficiency itself causes weight gain. For this data, we assume three experimental conditions: The weight of the control animals (wild type, Leptin+) follows \\[ Y_A \\sim N(\\mu_A, \\sigma^2) \\] The weight of the knockout animals injected with leptin follows (Leptin-(sup)) \\[ Y_B \\sim N(\\mu_B, \\sigma^2) \\] The weight of the knockout animals without leptin follows (Leptin-) \\[ Y_C \\sim N(\\mu_C, \\sigma^2) \\] 17.2 Data One random experiment in this study can be considered to have two outcome components: \\((\\text{leptin}, \\text{weight})\\). Leptin status is a categorical variable determining the experimental condition: \\[ leptin \\in \\{ \\text{Leptin+ : A},\\ \\text{Leptin-(sup) : B},\\ \\text{Leptin- : C} \\} \\] Weight is the continuous outcome of interest: \\[ weight \\in (20, 60) \\] The data look like: \\[ \\begin{array}{ccc} \\mathbf{Mouse} &amp; \\mathbf{Leptin} &amp; \\mathbf{Weight} \\\\ 1 &amp; \\text{Leptin+} &amp; 27.67 \\\\ 2 &amp; \\text{Leptin+} &amp; 27.40 \\\\ 3 &amp; \\text{Leptin+} &amp; 25.77 \\\\ 4 &amp; \\text{Leptin+} &amp; 25.60 \\\\ 5 &amp; \\text{Leptin+} &amp; 25.03 \\\\ 6 &amp; \\text{Leptin+} &amp; 25.90 \\\\ 7 &amp; \\text{Leptin+} &amp; 26.67 \\\\ 8 &amp; \\text{Leptin+} &amp; 25.60 \\\\ 9 &amp; \\text{Leptin+} &amp; 28.93 \\\\ 10 &amp; \\text{Leptin+} &amp; 31.83 \\\\ 11 &amp; \\text{Leptin+} &amp; 25.90 \\\\ 12 &amp; \\text{Leptin+} &amp; 26.30 \\\\ 13 &amp; \\text{Leptin+} &amp; 27.90 \\\\ 14 &amp; \\text{Leptin+} &amp; 26.77 \\\\ 15 &amp; \\text{Leptin+} &amp; 25.83 \\\\ 16 &amp; \\text{Leptin+} &amp; 20.87 \\\\ 17 &amp; \\text{Leptin-(sup)} &amp; 24.33 \\\\ 18 &amp; \\text{Leptin-(sup)} &amp; 22.37 \\\\ 19 &amp; \\text{Leptin-(sup)} &amp; 26.10 \\\\ 20 &amp; \\text{Leptin-(sup)} &amp; 17.50 \\\\ 21 &amp; \\text{Leptin-(sup)} &amp; 35.17 \\\\ 22 &amp; \\text{Leptin-(sup)} &amp; 25.97 \\\\ 23 &amp; \\text{Leptin-(sup)} &amp; 27.67 \\\\ 24 &amp; \\text{Leptin-(sup)} &amp; 23.37 \\\\ 25 &amp; \\text{Leptin-(sup)} &amp; 31.83 \\\\ 26 &amp; \\text{Leptin-(sup)} &amp; 22.37 \\\\ 27 &amp; \\text{Leptin-} &amp; 46.57 \\\\ 28 &amp; \\text{Leptin-} &amp; 40.43 \\\\ 29 &amp; \\text{Leptin-} &amp; 41.97 \\\\ 30 &amp; \\text{Leptin-} &amp; 41.17 \\\\ 31 &amp; \\text{Leptin-} &amp; 41.57 \\\\ 32 &amp; \\text{Leptin-} &amp; 46.17 \\\\ 33 &amp; \\text{Leptin-} &amp; 53.83 \\\\ \\end{array} \\] 17.3 Difference between means We compute the mean and standard deviation of weights within each group: \\(n_A = 16\\) control mice (Leptin+) had a mean \\(\\bar{y}_A = 26.50\\) and \\(s_A = 2.25\\) \\(n_B = 10\\) knockout mice with leptin replacement (Leptin-(sup)) had a mean \\(\\bar{y}_B = 25.67\\) and \\(s_B = 5.03\\) \\(n_C = 7\\) knockout mice without leptin (Leptin-) had a mean \\(\\bar{y}_C = 44.53\\) and \\(s_C = 4.77\\) We can visualize the distributions with bar plots and violin plots: The wild type mice (Leptin+) and the knockout mice that received leptin supplementation (Leptin-(sup)) show similar mean weights. In contrast, knockout mice without leptin (Leptin-) have a much higher average weight. Since the confidence interval for this group does not overlap with those of the other two groups, we conclude that the mean weight of leptin-deficient mice is significantly higher. The violin plots further illustrate differences in the distribution of weights across groups. When testing differences in means between two groups, the null hypothesis assumes that the means are equal. When comparing more than two groups, the null hypothesis is that all groups share the same mean. The alternative is that at least one group has a different mean. This motivates the use of analysis of variance (ANOVA). 17.4 Hypothesis test Let us now formulate the hypothesis contrast: Null hypothesis \\[ H_0: \\mu=\\mu_A = \\mu_B = \\mu_C \\] The null hypothesis assumes that there are no differences among the group means. In this case, all bars in the bar plot would align at the same height, within the margin of their confidence intervals. Alternative hypothesis \\[ H_1: \\text{at least one } \\mu \\neq \\mu_i \\quad \\text{for some $j$} \\] The alternative hypothesis assumes that at least one group mean is different from the others. At least one bar in the bar plot is clearly distinct from the rest. How can we test these hypotheses? We seek a statistic whose value allows us to decide whether to reject or retain \\(H_0\\). For simplicity, assume that all groups have the same number of observations, \\[ n_A = n_B = n_C = n \\] This means each condition contains the same number of mice in the leptin example, for a total of \\(3n.\\) mice. Such a situation is called a balanced design. The results we will derive still hold approximately if we replace \\(n\\) by the average number of observations per group, \\[ n = \\frac{1}{k} \\sum_{i=1}^k n_i. \\] 17.4.1 Distribution of group means under \\(H_0\\) If the null hypothesis is true, there are no differences between group means and they all coincide with a single common mean \\(\\mu\\): \\[ \\mu_A = \\mu_B = \\mu_C = \\mu. \\] Therefore, each group average follows the same distribution: \\[ \\bar{Y}_A, \\bar{Y}_B, \\bar{Y}_C \\sim N\\!\\left(\\mu, \\frac{\\sigma^2}{n.}\\right). \\] The observed vector of means, \\[ m = (\\bar{y}_A, \\bar{y}_B, \\bar{y}_C), \\] can be interpreted as the result of an averaging experiment: we repeatedly sample \\(n\\) weights in each of the three conditions and take their averages. If \\(H_0\\) holds, these three averages are independent estimates of the same underlying mean \\(\\mu\\). 17.4.2 Sources of variation We can distinguish two sources of dispersion in the obervatons of the experiment: Within-group variance (\\(\\sigma^2\\)) – the natural dispersion of individual outcomes in each condition, which reflects the variability of the original random experiment. Between-group variance (\\(\\sigma^2_{\\bar{Y}}\\)) – the dispersion of the group averages, which reflects how different the sample means are from one another. Under the null hypothesis, changing conditions does not affect the group averages, because the outcomes are independent of the conditions. By the law of large numbers, as the number of experiments \\(n\\) in each group increases, the averages converge to \\(\\mu\\). Hence, the variance of the averages decreases as \\[ H_0: \\quad \\sigma^2_{\\bar{Y}} = \\frac{\\sigma^2}{n}. \\] If, however, one group has a true mean different from \\(\\mu\\), its average will not get close to \\(\\mu\\) as \\(n.\\) grows. In that case, the between-group variance will be larger than expected under the null: \\[ H_1: \\quad \\sigma^2_{\\bar{Y}} &gt; \\frac{\\sigma^2}{n}. \\] 17.5 Variance components estimators To test the null hypothesis, we need estimators for the two variance components: the within-group variance \\(\\sigma^2\\), the between-group variance \\(\\sigma^2_{\\bar{Y}}\\). Estimator of the within-group variance Within each group, the data vary around their group mean. For example, in group \\(A\\), the sample variance is \\[ S_A^2=\\frac{1}{n-1} \\sum_{i=1}^{n} (Y_{Ai}-\\bar{Y}_{A})^2. \\] where \\(i\\) denotes the \\(i\\)-th repetition of the experiment under condition \\(A\\). If we only considered the control mice (group \\(A\\)), this sample variance \\(S_A^2\\) would be an unbiased estimator of the population variance \\(\\sigma^2\\). Since we assume all groups share the same variance \\(\\sigma^2\\), we can combine information from all groups by averaging their sample variances: \\[ S_e^2 = \\frac{1}{k}\\sum_{j=1}^k S_j^2. \\] where \\(k\\) is the number of groups, or conditions of the experiment. This pooled estimator \\(S_e^2\\) is called the mean square error (MSE). It represents the variability of the random experiment within conditions. Estimator of the between-group variance Now let us look at the variability of the group means. This is the right-hand side of the plot, where we see how the group means disperse around the overall mean (black point in in the plot). For each group mean \\(\\bar{Y}_j\\): \\[ V(\\bar{Y}_j) = \\frac{\\sigma^2}{n}, \\quad \\text{if } H_0 \\text{ is true}. \\] Thus, under the null hypothesis, the group means behave like repeated estimates of the same mean \\(\\mu\\). The sample variance of the group means is \\[ S^2_{tr}=\\frac{1}{k-1} \\sum_{j=1}^k (\\bar{Y}_{j}-\\bar{Y})^2, \\] where \\(\\bar{Y}\\) is the overall mean across all observations: \\[ \\bar{Y}=\\frac{1}{k}\\sum_{j=1}^k \\bar{Y}_j = \\frac{1}{kn}\\sum_{j=1}^k \\sum_{i=1}^n Y_{ji}. \\] This statistic \\(S^2_{tr}\\) estimates the variance of the group means, \\(\\sigma^2_{\\bar{Y}}\\). The F-ratio If the null hypothesis is true and the number of observations per group grows large, then all the group means converge to the same overall mean \\(\\mu\\). Consequently, the variance of the group means should be close to its expected value \\(\\sigma^2/n\\). Remember that \\(V(\\bar{Y})=\\sigma^2/n\\) which is estimated by \\(S_{tr}^2\\). On the other hand, \\(S_e^2\\) estimates \\(\\sigma^2\\) and therefore \\(S_e^2/n\\) is another estimation of the group variance \\(\\sigma^2/n\\). This leads to the F statistic: \\[ F=\\frac{S_{tr}^2}{S_e^2/n}. \\] Under \\(H_0\\), \\(F\\) is close to \\(1\\). As \\(n\\) increases the outcomes get close to their means, which get close to the overall mean. If the null hypothesis is not true and we take more and more observations in each group, then at least one group mean will not converge to the common mean. For example, if \\(\\mu_B=\\mu_C=\\mu \\neq \\mu_A\\), the averages from groups \\(B\\) and \\(C\\) will get closer to \\(\\mu\\), but the average from group \\(A\\) will concentrate around \\(\\mu_A \\neq \\mu\\). As a result, the expected value of \\(S^2_{tr}\\) will be larger than \\(\\frac{\\sigma^2}{n}\\), because there is an irreducible component of variability due to the difference of the group \\(A\\) with the rest. In this situation, we say that the data contain treatment variance (variance explained by group differences) in addition to the random within-group variance. Consequently, the \\(F\\) statistic will tend to be greater than 1, providing evidence to reject the null hypothesis. Under \\(H_a\\), at least one group mean differs, so \\(S_{tr}^2\\) becomes larger than \\(\\sigma^2/n\\), making \\(F &gt; 1\\). Intuitively, \\(F\\) measures how much the group means are separated relative to the noise inside groups. Sums of squares formulation In practice, the \\(F\\)-statistic is reported as a ratio of mean sums of squares: \\[ F = \\frac{MST}{MSE}, \\] where Mean Square for Treatments (MST): \\[ MST = \\frac{1}{k-1}\\sum_{j=1}^k n(\\bar{Y}_j-\\bar{Y})^2 = \\frac{SS_{Group}}{k-1}, \\] Mean Square Error (MSE): \\[ MSE = \\frac{1}{k(n-1)}\\sum_{j=1}^k\\sum_{i=1}^n (Y_{ji}-\\bar{Y}_j)^2 = \\frac{SS_{Residual}}{k(n-1)}. \\] Since sums of squares are quadratic forms of normally distributed variables, each scaled variance follows a chi-square distribution with its respective degrees of freedom. Their ratio follows an F distribution: \\[ F \\sim F(k-1,\\,k(n-1)). \\] Here, \\(k\\) is the number of groups, and \\(n\\) the number of observations per group (balanced design). In unbalanced designs, \\(n\\) is replaced by the actual group sizes, and the degrees of freedom are adjusted accordingly. 17.6 Analysis of variance (ANOVA) The hypothesis test for the difference in means across several groups is then \\(H_0: \\mu_1=\\mu_2= ...=\\mu_k\\). There are no difference between group means. Then, the observed value of \\(F\\), e.g \\(f_{obs}\\) will be near \\(1\\). \\(H_1:\\) at least one \\(\\mu_i\\) is different from the rest. Then, the observed value of \\(F\\), e.g \\(f_{obs}\\) will be greater than \\(1\\). Example (Mice knockouts) The value of the \\(f_{obs}\\) computed from the group (treatments) mean and residual (error) mean squares can be calculated in any statistical software that usually outputs the ANOVA table Python: import pandas as pd import statsmodels.api as sm from statsmodels.formula.api import ols dat = pd.DataFrame({&quot;group&quot;: [&quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;,&quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, Leptin-&quot;, &quot;Leptin-&quot;, &quot;Leptin-&quot;, &quot;Leptin-&quot;, &quot;Leptin-&quot;, &quot;Leptin-&quot;, &quot;Leptin-&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;], &quot;weight&quot;: [27.67, 27.4, 25.77, 25.6, 25.03, 25.9, 26.67, 25.6, 28.93, 31.83, 25.9, 26.3, 27.9, 26.77, 25.83, 20.87, 46.57, 40.43, 41.97, 41.17, 41.57, 46.17, 53.83, 24.33, 22.37, 26.1, 17.5, 35.17, 25.97, 27.67, 23.37, 31.83, 22.37]}) model = ols(&#39;weight ~ group&#39;, data=dat).fit() anova_table = sm.stats.anova_lm(model, typ=2) print(anova_table) R: dat &lt;- data.frame( group=c(&quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;, &quot;Leptin+&quot;,&quot;Leptin+&quot;, &quot;Leptin-&quot;, &quot;Leptin-&quot;, &quot;Leptin-&quot;, &quot;Leptin-&quot;, &quot;Leptin-&quot;, &quot;Leptin-&quot;, &quot;Leptin-&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;,&quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;, &quot;Leptin-(sup)&quot;), weight=c(27.67, 27.4, 25.77, 25.6, 25.03, 25.9, 26.67, 25.6, 28.93, 31.83, 25.9, 26.3, 27.9, 26.77, 25.83, 20.87, 46.57, 40.43, 41.97, 41.17, 41.57, 46.17, 53.83, 24.33, 22.37, 26.1, 17.5, 35.17, 25.97, 27.67, 23.37, 31.83, 22.37)) summary(aov(lm(weight~group, data=dat))) \\[ \\begin{array}{cccccc} &amp;\\mathbf{Df} &amp; \\mathbf{Sum Sq} &amp; \\mathbf{Mean Sq} &amp; \\mathbf{F value} &amp; \\mathbf{Pr(&gt;F)} \\\\ \\mathbf{group} &amp; 2 &amp;1861.5&amp; 930.8 &amp; 63.37 &amp;1.69\\times10^{-11}\\\\ \\mathbf{Residuals}&amp; 30 &amp;440.6&amp; 14.7 &amp; &amp; \\\\ \\end{array} \\] In this table \\(MST\\) is the mean squares for the group (treatment), computed as the corresponding sum of squares divided by the degrees of freedom (\\(k-1=2\\)). \\[MST=\\frac{1}{k-1}SSq_{group}=930.77\\] \\(MSE\\) is the mean squares for the residuals (error), computed as the corresponding sum of squares divided by the degrees on freedom (\\(k(n-1)=30\\)), where \\(n=(16 + 7 + 10)/3=11\\) is the average number of observations in each group \\[MSE=\\frac{1}{k(n-1)}SSq_{Residual}=14.69\\] The observed value of the statistic is \\[f_{obs}=\\frac{MST}{MSE}=63.373\\] To test the the null hypothesis, we then compute the probability that in a future experiment, if the null hypothesis is true, we observe a higher value than \\(63.373\\): \\(P(F&gt;63.373)\\). This probability is the upper tailed p-value \\[pvalue=1-F_{Fisher(2,30)}(63.373)=1.694 \\times 10^{-11}\\] which is much lower than \\(\\alpha=0.05\\), suggesting significant differences in at least one group mean. Therefore, we reject the null hypothesis and accept that at least one of the groups has a different mean from the rest. In this experiment, we observed a significant difference between groups (ANOVA test \\(F(2,30)=63.373, pvalue= 1.69 \\times 10^{-11}\\)), indicating that at least one group has a different mean from the rest. We call it an omnibus test. ANOVA test is simultaneously testing all the groups and by itself it does not tell us which groups or group is different. Clearly, from the plots, the knockout mice has higher weights than the other two groups. 17.7 ANOVA for Two Groups ANOVA is often complemented by pairwise comparisons. When there are only two groups, ANOVA is equivalent to the two-sample \\(t\\)-test. In fact, the \\(F\\)-test in ANOVA can be seen as a direct generalization of the \\(t\\)-test to more than two groups. Let us explicitly compute the observed value of \\(F\\) when we have only two groups of equal size \\(n\\): \\[ f_{obs}=\\frac{\\tfrac{1}{2}(\\bar{y}_{A}-\\bar{y}_{B})^2}{\\tfrac{s_p^2}{n}} \\] Here, the numerator comes from simplifying \\(S_{tr}^2\\) for two groups, and \\(s_p^2\\) is the pooled variance: \\[ s_p^2 = \\frac{s_A^2 + s_B^2}{2} \\] which is obtained from \\(S_e^2\\) when both groups have the same number of observations. Taking the square root of \\(f_{obs}\\) and rearranging, we recover the familiar \\(t\\)-statistic: \\[ t_{obs} = \\sqrt{f_{obs}} = \\frac{\\bar{y}_{A} - \\bar{y}_{B}}{\\sqrt{\\tfrac{2s_p^2}{n}}} \\] Thus, the two-sample \\(t\\)-test for the difference between two balanced groups with equal variances is just a special case of the ANOVA \\(F\\)-test, with \\(f_{obs}=t_{obs}^2\\). In other words, ANOVA is a direct generalization of the \\(t\\)-test to multiple groups. Example (Wild type and knockout mice) We are now interested in comparing the weights between knockout mice with supplemented leptin (group B) and wild type mice (group C). The goal is to confirm that lean weight has been restored by exogenous supplementation of the hormone. We perform a \\(t\\)-test. The observed statistic is \\[t_{obs}=\\frac{\\bar{y}_A-\\bar{y}_B }{\\sqrt{\\frac{s^2_p}{n_A}+\\frac{s^2_p}{n_B}}}=\\frac{26.49813-25.668}{\\sqrt{\\frac{12.66079}{10}+\\frac{12.66079}{7}}}=0.5787438\\] using the pooled variance for unbalanced groups. Therefore, the observed \\(F\\) is \\[f_{obs}=t_{obs}^2=(0.5787)^2=0.3349\\] The upper-tailed p-value for \\(f_{obs}\\) using the Fisher distribution is \\[pvalue=1-F_{Fisher,1,18}(0.3349)= 0.56\\] with \\(k=2\\) and \\(n=18\\) as the average number of observations per group. We can confirm that the \\(t\\)-test and the ANOVA for two groups give the same result. R: t.test(weight~group, data=dat, subset = which(dat$group!=&quot;Leptin-&quot;), equal.variance=FALSE) summary(aov(lm(weight~group, data=dat, subset = which(dat$group!=&quot;Leptin-&quot;)))) Note that knockout mice with leptin supplementation recovered wild-type weight. When we perform a two-sample \\(t\\)-test between these two groups, we find that we cannot reject the hypothesis that their expected weights are the same (\\(t_{obs}=0.5787\\), \\(pvalue=0.56\\)). Demonstrating that a phenotype caused by knocking out a gene can be rescued by exogenous supplementation provides strong evidence that leptin deficiency can lead to significant weight gain. 17.8 Linear model The following formulation is useful for integrating different types of analyses. Let us classify the observations of the random variable \\(Y\\) using the group \\(j\\) and the particular observation \\(i\\). Let us consider first a group \\(j\\) and that the expected value of \\(Y\\) in group \\(j\\) is \\[E(Y_j)=E(Y_{ji}\\mid j)=\\mu_j.\\] We are making explicit that each condition has its own mean, which we estimate with the sample average \\(\\bar{Y}_j\\). Now we can write \\[\\mu_j=\\mu + \\alpha_j,\\] where \\(\\mu\\) is the overall expected value of all outcomes across groups (\\(E(Y)=\\mu\\)), and \\(\\alpha_j\\) is the amount by which the mean in group \\(j\\) deviates from \\(\\mu\\). For example, \\(\\mu_{\\text{leptin+}}\\) is the expected weight of a wild-type mouse, \\(\\mu\\) is the overall average weight across all groups, and \\(\\alpha_{\\text{leptin+}}\\) is the deviation of the wild-type mean from the overall mean. Under the null hypothesis, \\(\\alpha_j=0\\) for all \\(j\\). Let us now take a random sample of size \\(n_j\\) from condition \\(j\\): \\[M=(Y_{j1}, Y_{j2}, \\ldots, Y_{jn_j}).\\] For instance, \\(Y_{\\text{leptin+},3}\\) is the random variable measuring the weight of the 3rd mouse under the wild-type condition, while \\(y_{\\text{leptin+},3}\\) denotes its observed value. We now assume that we can separate the systematic and random components of the variation and write the linear model: \\[Y_{ji} = \\mu_j + \\varepsilon_{ji},\\] or equivalently, \\[Y_{ji} = \\mu + \\alpha_j + \\varepsilon_{ji},\\] where \\(\\varepsilon_{ji}\\) is a random variable called the error, which has expected value \\(E(\\varepsilon_{ji})=0\\) and variance \\(V(\\varepsilon_{ji})=\\sigma^2\\). It represents the deviation of the individual observation from its group mean. For instance, \\(\\varepsilon_{\\text{leptin+},3}\\) is the difference between the weight of the 3rd wild-type mouse and the group mean \\(\\mu_{\\text{leptin+}}\\). In ANOVA, we assume that the variance of \\(Y\\) is the same across groups (homoscedasticity): \\[V(Y_j)=\\sigma^2 \\quad \\text{for all } j.\\] This formulation makes explicit all the components that contribute to the variation of each observation in the experiment. Sources of variation When repeating a random experiment under condition \\(j\\) for the \\(i\\)-th time, the outcome \\(Y_{ji}\\) can be explained as the sum of: the overall mean \\(\\mu\\) the effect of condition \\(j\\) on the overall mean \\(\\alpha_j\\) the random error \\(\\varepsilon_{ji}\\) around the group mean \\(\\mu_j\\), with variance \\(\\sigma^2\\) Hypothesis contrast In this formulation the hypothesis contrast can be written as: \\(H_0:\\ \\alpha_j=0 \\ \\text{for all } j\\). That is, there are no differences between the group means and the overall mean \\(\\mu\\). \\(H_1: \\alpha_j \\neq 0\\) for at least one \\(j\\). At least one group mean differs from the overall mean \\(\\mu\\). We again use the \\(F\\) statistic to decide between the two hypotheses. Example (Speed of light) In 1887, Michelson and Morley attempted to explain why stars appear to slightly change their positions in the sky depending on the Earth’s motion around the Sun (Michelson and Morley 1887). If the Earth were moving away from a star, its light would arrive later, creating a small apparent shift in its position when compared to the stars toward which the Earth is moving. If light behaves as a wave, then the Earth would be moving relative to the medium that supports the wave — the so-called ether. However, the angular deviations predicted by this theory were much larger than those actually observed. To address this discrepancy, Michelson and Morley devised their famous experiment. They mounted a device on the edge of a rotating platform and sent a light beam into it. The beam was split at the center of the platform into two perpendicular rays. Each ray was reflected by mirrors placed at the edges of the apparatus and then recombined at the center, producing an interference pattern directed back toward the starting point. The interference generated a bright fringe whose position could be precisely measured. Crucially, the position of this fringe depended on the relative distances traveled by the two rays, which in turn should have been affected by the direction and speed of the Earth’s motion through the ether. They reasoned that when the apparatus was rotated to an angle \\(r\\) such that one ray aligned with the Earth’s motion through the ether, the other ray would experience the maximum relative path difference. In that configuration, the displacement of the interference fringe would also reach its maximum at \\(r\\). The apparatus, mounted on a mercury-supported rotating table, was turned through 16 different orientations (with the final position coinciding with the first) and the measurements repeated over three consecutive days. The outcomes, expressed in units of screw divisions (each corresponding to 50 times the wavelength of light), recorded the observed fringe deviations. \\[ \\begin{array}{cccc} \\mathbf{Angle} &amp; \\mathbf{July 8} &amp; \\mathbf{July 9} &amp; \\mathbf{July 10} \\\\ 1&amp; 44.7&amp; 57.4&amp; 27.3\\\\ 2&amp; 44.0&amp; 57.3&amp; 23.5\\\\ 3&amp; 43.5&amp; 58.2&amp; 22.0\\\\ 4&amp; 39.7&amp; 59.2&amp; 19.3\\\\ 5&amp; 35.2&amp; 58.7&amp; 19.2\\\\ 6&amp; 34.7&amp; 60.2&amp; 19.3\\\\ 7&amp; 34.3&amp; 60.8&amp; 18.7\\\\ 8&amp; 32.5&amp; 62.2&amp; 18.8\\\\ 9&amp; 28.2&amp; 61.5&amp; 16.2\\\\ 10&amp; 26.2&amp; 63.3&amp; 14.3\\\\ 11&amp; 23.8&amp; 65.8&amp; 13.3\\\\ 12&amp; 23.2&amp; 67.3&amp; 12.8\\\\ 13&amp; 20.3&amp; 69.7&amp; 13.3\\\\ 14&amp; 18.7&amp; 70.7&amp; 12.3\\\\ 15&amp; 17.5&amp; 73.0&amp; 10.2\\\\ 16&amp; 16.8&amp; 70.2&amp; 7.3\\\\ 17&amp; 13.7&amp; 72.2&amp; 6.5\\\\ \\end{array} \\] If we treat the angles as different experimental conditions, each measured three times (once per day), then we can write \\[Y_{angle, day} = \\mu + \\alpha_{angle} +\\varepsilon_{angle,day}\\] where \\(E(Y_{\\text{angle}, \\text{day}})=\\mu_{\\text{angle}}=\\mu + \\alpha_{\\text{angle}}\\), and \\(\\text{day}\\in\\{\\text{July 8}\\), \\(\\text{July 9}\\), \\(\\text{July 10}\\}\\). Our goal is to determine whether at least one \\(\\alpha_{\\text{angle}}\\) differs from the others, thereby rejecting the null hypothesis using ANOVA. When the board is set at a particular angle \\(r\\) such that the path difference between the two rays is maximal, the deviation of the fringe should also be maximal, and thus so should \\(\\alpha_r\\). However, as the figure shows, the experiment displayed large systematic errors that do not conform to an ANOVA model. First, within a single day, the measurements exhibited linear trends: the shift increased steadily with the angle. Second, across days, the results were systematically different: the measurements were consistently highest on July 9 and lowest on July 10. To address the first error, the technique of the time was to compute the difference between the 17th and the 1st angle (which coincide because the board completes a full rotation) and divide it by 16. Each of these accumulated errors was then successively subtracted from the original measurements, producing a normalized outcome \\(Y^*_{\\text{angle}, i}\\). The modern equivalent of this correction is to fit a regression line and use the residuals, as we will see in the next section. For the second error, we average the measurements across days and explicitly include the day effect in the model of the normalized outcomes: \\[ Y^*_{\\text{angle}, \\,\\text{day}} = \\mu^* + \\alpha^*_{\\text{angle}} + \\beta^*_{\\text{day}} + \\varepsilon^*_{\\text{angle},\\text{day}} . \\] A further corrected outcome, obtained by subtracting the daily mean \\(\\beta^*_{\\text{day}}\\), satisfies the ANOVA model we want to test: \\[ Y^{**}_{\\text{angle}, \\,\\text{day}} \\;=\\; Y^{*}_{\\text{angle}, \\,\\text{day}} - \\beta^*_{\\text{day}} \\;=\\; \\mu^* + \\alpha^*_{\\text{angle}} + \\varepsilon^*_{\\text{angle},\\text{day}} . \\] The bar plot shows that the corrected normalized outcomes at a given angle can now be considered as replications of the same experiment across three days. Michelson and Morley focused their analysis on half a rotation of the board (up to angle 8), resembling the pattern shown for the noon measurements, although they did not report in detail how they treated systematic errors (Handschy 1982). In the plot, we can identify all the components of the model. The estimate of \\(\\mu^*\\) is represented by the dotted line around zero, as the daily variation \\(\\beta^*_{\\text{day}}\\) has been removed. The effect of the angle \\(\\alpha^*_{\\text{angle}}\\) is shown by the bars, and the within-angle variability is reflected in the scatter of the individual points. The observed values of the model (denoted with lowercase) are \\[ y^{**}_{\\text{angle}, \\,\\text{day}} \\;=\\; \\hat{\\mu}^* + \\hat{\\alpha}^*_{\\text{angle}} + r^*_{\\text{angle}, \\,\\text{day}} , \\] where \\(r^*_{\\text{angle}, \\,\\text{day}}\\) are the residuals (the observed errors), \\(\\hat{\\mu}^*\\) is the overall mean of the corrected normalized observations, and \\(\\hat{\\alpha}^*_{\\text{angle}}\\) are the estimated deviations at a fixed angle. Since the corrections for systematic errors were applied uniformly across all angles, they do not affect the hypothesis test, whose aim is to detect a privileged direction of Earth’s movement relative to the ether, and thus a maximal fringe shift at a specific angle. The ANOVA table for the normalized data is \\[ \\begin{array}{cccccc} &amp;\\mathbf{Df} &amp; \\mathbf{Sum Sq} &amp; \\mathbf{Mean Sq} &amp; \\mathbf{F value} &amp; \\mathbf{Pr(&gt;F)} \\\\ \\mathbf{Angle} &amp; 1 &amp; 0.06 &amp; 0.0633 &amp; 0.04 &amp; 0.842 \\\\ \\mathbf{Residuals} &amp; 22 &amp; 34.44 &amp; 1.5653 &amp; &amp;\\\\ \\end{array} \\] This ANOVA table shows that the data are consistent with the null hypothesis (\\(pvalue = 0.842\\)), meaning that all angles have the same average fringe shift after correcting for systematic errors. Thus, the experiment supports a model in which there is no privileged direction along which the speed of light could be reduced. Based solely on the maximum observed fringe shift, Michelson and Morley’s original analysis concluded that the Earth’s velocity relative to the ether was undetectable with their apparatus, with an estimated measurement error of about one-sixth of the Earth’s orbital speed. Later, Einstein argued that their results are consistent with the principle that the speed of light is invariant and constitutes the maximum possible speed, and he derived the theoretical consequences in the theory of special relativity. 17.9 2-way ANOVA The ANOVA approach allows the analysis of the joint effects of two or more different types of conditions, each with potentially different numbers of groups. In the Michelson and Morley experiment, they had six unreported measurements for each angle on each day, and they reported only the averages. In the original (unavailable) data, not only the angle but also the day could be considered as two different types of conditions, each with its own levels or groups. Example (Leptin knockout) Let us consider again the experiment of Ramos-Lobo on leptin knockout mice (Ramos-Lobo et al. 2019). In this experiment, female mice were also included, introducing an additional condition: sex. Since weight depends on the sex of the animal, we may ask whether weight is still statistically dependent on leptin deficiency even when we further condition on sex. Here we are asking two simultaneous questions: Is there an effect of sex on the weight of the mice? Is there an effect of leptin deficiency on the weight of the mice? 17.10 Data We introduce a conditional outcome. One random experiment — that is, one mouse — has three measurements: \\((leptin, sex, weight)\\). Categorical variable for the first condition (leptin status) with two groups: \\(leptin \\in \\{\\text{KOplus: A}, \\text{knockout: B}\\}\\) Categorical variable for the second condition (sex) with two groups: \\(sex \\in \\{\\text{male: a}, \\text{female: b}\\}\\) Outcome of interest (continuous variable): \\(weight \\in (20, 60)\\) The data looks like \\[ \\begin{array}{cccc} \\mathbf{Mouse} &amp; \\mathbf{Leptin} &amp; \\mathbf{Sex} &amp; \\mathbf{Weigth} \\\\ 1&amp;\\text{Leptin-}&amp;\\text{M}&amp;46.57 \\\\ 2&amp;\\text{Leptin-}&amp;\\text{M}&amp;40.43 \\\\ 3&amp;\\text{Leptin-}&amp;\\text{M}&amp;41.97 \\\\ 4&amp;\\text{Leptin-}&amp;\\text{M}&amp;41.17 \\\\ 5&amp;\\text{Leptin-}&amp;\\text{M}&amp;41.57 \\\\ 6&amp;\\text{Leptin-}&amp;\\text{M}&amp;46.17 \\\\ 7&amp;\\text{Leptin-}&amp;\\text{M}&amp;53.83 \\\\ 8 &amp; \\text{Leptin-(sup)}&amp;\\text{M}&amp;24.33 \\\\ 9 &amp; \\text{Leptin-(sup)}&amp;\\text{M}&amp;22.37 \\\\ 10&amp; \\text{Leptin-(sup)}&amp;\\text{M}&amp;26.10 \\\\ 11&amp; \\text{Leptin-(sup)}&amp;\\text{M}&amp;17.50 \\\\ 12&amp; \\text{Leptin-(sup)}&amp;\\text{M}&amp;35.17 \\\\ 13&amp; \\text{Leptin-(sup)}&amp;\\text{M}&amp;25.97 \\\\ 14&amp; \\text{Leptin-(sup)}&amp;\\text{M}&amp;27.67 \\\\ 15&amp; \\text{Leptin-(sup)}&amp;\\text{M}&amp;23.37 \\\\ 16&amp; \\text{Leptin-(sup)}&amp;\\text{M}&amp;31.83 \\\\ 17&amp; \\text{Leptin-(sup)}&amp;\\text{M}&amp;22.37 \\\\ 18&amp;\\text{Leptin-}&amp;\\text{F}&amp; 65.80 \\\\ 19&amp;\\text{Leptin-}&amp;\\text{F}&amp; 51.40 \\\\ 20&amp;\\text{Leptin-}&amp;\\text{F}&amp; 54.60 \\\\ 21&amp;\\text{Leptin-}&amp;\\text{F}&amp; 48.30 \\\\ 22&amp;\\text{Leptin-}&amp;\\text{F}&amp; 50.60 \\\\ 23&amp;\\text{Leptin-}&amp;\\text{F}&amp; 48.90 \\\\ 24&amp;\\text{Leptin-}&amp;\\text{F}&amp; 51.20 \\\\ 25&amp;\\text{Leptin-}&amp;\\text{F}&amp; 46.80 \\\\ 26&amp;\\text{Leptin-}&amp;\\text{F}&amp; 50.90 \\\\ 27&amp;\\text{Leptin-}&amp;\\text{F}&amp; 42.70 \\\\ 28&amp; \\text{Leptin-(sup)}&amp;\\text{F}&amp; 28.70 \\\\ 29&amp; \\text{Leptin-(sup)}&amp;\\text{F}&amp; 25.60 \\\\ 30&amp; \\text{Leptin-(sup)}&amp;\\text{F}&amp; 26.40 \\\\ 31&amp; \\text{Leptin-(sup)}&amp;\\text{F}&amp; 22.90 \\\\ 32&amp; \\text{Leptin-(sup)}&amp;\\text{F}&amp; 30.00 \\\\ 33&amp; \\text{Leptin-(sup)}&amp;\\text{F}&amp; 29.70 \\\\ 34&amp; \\text{Leptin-(sup)}&amp;\\text{F}&amp; 26.10 \\\\ 35&amp; \\text{Leptin-(sup)}&amp;\\text{F}&amp; 21.40 \\\\ 36&amp; \\text{Leptin-(sup)}&amp;\\text{F}&amp; 29.50 \\\\ 37 &amp;\\text{Leptin-(sup)}&amp;\\text{F}&amp; 21.90 \\\\ 38 &amp;\\text{Leptin-(sup)}&amp;\\text{F}&amp; 23.70 \\\\ 39 &amp;\\text{Leptin-(sup)}&amp;\\text{F}&amp; 21.00 \\\\ \\end{array} \\] Note that we will examine the association with sex only for the knockout mice that differ in leptin supplementation. 17.11 Modeling residuals Using the following model, we want to test for significant differences in the expected weight between male and female mice, ignoring the leptin condition: \\[Y_{sex,i}= \\mu + \\alpha_{sex} + \\varepsilon_{sex,i}\\] From the ANOVA table \\[ \\begin{array}{cccccc} &amp;\\mathbf{Df} &amp; \\mathbf{Sum Sq} &amp; \\mathbf{Mean Sq} &amp; \\mathbf{F value} &amp; \\mathbf{Pr(&gt;F)} \\\\ \\mathbf{sex} &amp; 1.0 &amp; 134.9 &amp; 134.9&amp; 0.85 &amp; 0.36 \\\\ \\mathbf{Residual} &amp; 37.0 &amp; 5844.8 &amp; 157.9 &amp; &amp; \\end{array} \\] we see that there is no significant effect of sex on the weight of the mice when the leptin groups are not considered. From the bar plot, we also observe that the confidence intervals for each sex overlap, indicating that we cannot reject the null hypothesis that the mean weights are equal between sexes. We see, however, that the points are clustered by leptin group, suggesting that we should adjust for differences in leptin status before testing for differences by sex. It is possible that, after removing the effect of leptin, the mean weight of males may be lower than that of females. The way to remove the effect of leptin is to perform an ANOVA of mouse weight on the leptin group and then obtain the residuals for each observation. These residuals represent the observed values of the error term \\(\\varepsilon_{leptin,i}\\) and are computed as: \\[ r_{leptin,i} = y_{leptin,i} - \\bar{y}_{leptin} \\] That is, we subtract the mean of the leptin group from each observation \\(i\\) within that group. These residuals can then be used to test the effect of sex independent of leptin status. Python: model1 = sm.OLS.from_formula(&#39;weight ~ leptin&#39;, data=filtered_data).fit() model1.resid R: dat$residual.weight &lt;- summary(lm(weight ~ leptin, data=dat))$residuals Then we can perform another ANOVA using the leptin-corrected residuals to test for differences by sex. Let the leptin-corrected outcome be: \\[ Y^*_{leptin,sex,i} = Y_{leptin,sex,i} - \\bar{Y}_{leptin} = \\mu^* + \\alpha^*_{sex} + \\varepsilon^*_{sex,i} \\] where \\(\\bar{Y}_{leptin}\\) is the mean weight of the leptin group for each observation, \\(\\alpha^*_{sex}\\) is the effect of sex after correcting for leptin, and \\(\\varepsilon^*_{sex,i}\\) is the random error variable within sex. The ANOVA table for weight corrected for leptin differences is: \\[ \\begin{array}{cccccc} &amp;\\mathbf{Df} &amp; \\mathbf{Sum Sq} &amp; \\mathbf{Mean Sq} &amp; \\mathbf{F value} &amp; \\mathbf{Pr(&gt;F)} \\\\ \\mathbf{sex} &amp; 1 &amp; 73.94 &amp; 73.94 &amp; 2.956 &amp; 0.093 \\\\ \\mathbf{Residual} &amp; 37 &amp; 925.49 &amp; 25.013 &amp; &amp; \\\\ \\end{array} \\] The lower weight in males is almost significant for the leptin-corrected residuals. However, from this model we cannot determine the contribution of the leptin group to the overall variability of the data. Additionally, if the sample size \\(n\\) is large, it is possible that even if the null hypothesis is true, the means of each sex may still not be close to the grand mean due to a significant effect of leptin. The bar plot of the residuals shows that we have effectively adjusted for the differences in leptin, as the dots for males and females are now intermixed, indicating that the leptin effect has been removed. We can generalize ANOVA to explain the simultaneous contributions of sex and leptin groups. 17.12 2-way ANOVA linear model The ANOVA approach allows for the simultaneous analysis of two factors, each with multiple groups or levels. For simplicity, we discuss a balanced design, where we have \\(n\\) repetitions of the random experiment in each condition (or the average number of repetitions in the general case). Consider the linear model: \\[ Y_{jri} = \\mu + \\alpha_j + \\beta_r + \\varepsilon_{jri} \\] for the \\(i\\)-th repetition of the random experiment (\\(i = 1, \\dots, n\\)) in condition \\(j\\) of factor 1 and condition \\(r\\) of factor 2, with: Grand mean: \\[ E(Y_{jri}) = \\mu \\] which is the expected value of all observations, estimated by the overall average \\(\\bar{Y}\\). Random error: \\[ \\varepsilon_{jri} \\] a random variable with mean \\(E(\\varepsilon_{jri}) = 0\\) and variance \\(V(\\varepsilon_{jri}) = \\sigma^2\\). Deviations of factor 1 (leptin) from the grand mean: \\[ \\alpha_j = \\mu_{j\\cdot} - \\mu \\] for each group \\(j \\in \\{1, \\dots, k\\}\\), with \\(\\sum_j \\alpha_j = 0\\). Deviations of factor 2 (sex) from the grand mean: \\[ \\beta_r = \\mu_{\\cdot r} - \\mu \\] for each group \\(r \\in \\{1, \\dots, m\\}\\), with \\(\\sum_r \\beta_r = 0\\). Here, the dot indicates that we ignore the other factor when computing the mean. The model can also be expressed as: \\[ Y_{jri} = \\mu_{jr} + \\varepsilon_{jri}, \\quad \\text{where } \\mu_{jr} = \\mu_{j\\cdot} + \\mu_{\\cdot r} - \\mu \\] is the mean in the condition defined by group \\(j\\) of factor 1 and group \\(r\\) of factor 2. For example, the condition “male and leptin-” is one of these groups. In our leptin example, there are four such groups corresponding to all combinations of sex and leptin status. 17.13 Hypothesis tests ANOVA allows testing the effects of the two factors simultaneously: Factor 1 (leptin): \\(H_0: \\alpha_1 = \\alpha_2 = \\dots = \\alpha_k = 0\\) — no differences between group means for factor 1. \\(H_1\\): at least one \\(\\alpha_j \\neq 0\\). Factor 2 (sex): \\(H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_m = 0\\) — no differences between group means for factor 2. \\(H_1\\): at least one \\(\\beta_r \\neq 0\\). 17.14 Variance components To evaluate each hypothesis contrast, we need appropriate test statistics. We can estimate the dispersion of the outcomes from their means in each condition, determined by both factors. For instance, the mean for knockout (KO) males (M) is \\[ \\mu_{KO,M} = \\mu_{\\cdot M} + \\mu_{KO\\cdot} - \\mu \\] and can be estimated by the sample averages: \\[ \\hat{\\mu}_{KO,M} = \\bar{Y}_{\\cdot M} + \\bar{Y}_{KO\\cdot} - \\bar{Y} \\] These estimated means correspond to the dots in the following plot. The dispersion of the knockout male observations around the estimated mean \\(\\hat{\\mu}_{KO,M}\\) is given by the sample variance: \\[ S^2_{KO,M} = \\frac{1}{n-1} \\sum_{i=1}^n \\left(Y_{KO,M,i} - \\hat{\\mu}_{KO,M}\\right)^2 \\] If we assume that the variance of weight is the same across all sex–leptin conditions, \\(\\sigma^2\\), we can estimate it with the pooled sample variance: \\[ S_e^2 = \\frac{1}{km} \\sum_{j=1}^k \\sum_{r=1}^m S^2_{jr} \\] which sums the within-condition variances across all sex–leptin groups. \\(S_e^2\\) is an estimator of \\(\\sigma^2\\). We can also estimate the variance of the averages in each group of factor 1 (e.g., leptin) relative to the overall mean: \\[ S^2_{tr1} = \\frac{1}{k-1} \\sum_{j=1}^k \\left(\\bar{Y}_{j\\cdot} - \\bar{Y}\\right)^2 \\] This is an estimator of \\(\\sigma^2 / (nm)\\) under the null hypothesis. The corresponding \\(F\\) statistic is \\[ F_1 = \\frac{nm S^2_{tr1}}{S_e^2} \\sim F(k-1, (kmn-1)-(k-1)-(m-1)) \\] If the null hypothesis is true (no differences between factor 1 group means), \\(F_1\\) will be close to 1. If the alternative is true, at least one group mean differs from the grand mean, and \\(F_1\\) will be greater than 1. Similarly, for factor 2 (e.g., sex), we define \\[ S^2_{tr2} = \\frac{1}{m-1} \\sum_{r=1}^m \\left(\\bar{Y}_{\\cdot r} - \\bar{Y}\\right)^2 \\] and \\[ F_2 = \\frac{nk S^2_{tr2}}{S_e^2} \\sim F(m-1, (kmn-1)-(k-1)-(m-1)) \\] Observed values of \\(F_2\\) far from 1 indicate significant differences between the means of factor 2, leading to rejection of the null hypothesis. All of this is summarized in a two-way ANOVA table: import pandas as pd import statsmodels.api as sm from statsmodels.formula.api import ols dat = pd.DataFrame({ &quot;Mouse&quot;: range(1, 40), &quot;Leptin&quot;: [ &quot;Leptin-&quot;, &quot;Leptin-&quot;, ... ], &quot;Sex&quot;: [ &quot;M&quot;,&quot;M&quot;,... ], &quot;Weight&quot;: [ 46.57, 40.43, 41.97, ... ] }) # Two-way ANOVA model = ols(&#39;Weight ~ Sex + Leptin&#39;, data=dat).fit() anova_table = sm.stats.anova_lm(model, typ=2) print(anova_table) R: dat &lt;- data.frame( Mouse = 1:39, Leptin = c( rep(&quot;Leptin-&quot;, 7), rep(&quot;Leptin-(sup)&quot;, 10), rep(&quot;Leptin-&quot;, 10), rep(&quot;Leptin-(sup)&quot;, 12) ), Sex = c( rep(&quot;M&quot;, 17), rep(&quot;F&quot;, 22) ), Weight = c( 46.57, 40.43, 41.97, 41.17, 41.57, 46.17, 53.83, 24.33, 22.37, 26.10, 17.50, 35.17, 25.97, 27.67, 23.37, 31.83, 22.37, 65.80, 51.40, 54.60, 48.30, 50.60, 48.90, 51.20, 46.80, 50.90, 42.70, 28.70, 25.60, 26.40, 22.90, 30.00, 29.70, 26.10, 21.40, 29.50, 21.90, 23.70, 21.00 ) ) # Two-way ANOVA summary(aov(Weight ~ Sex + Leptin, data = dat)) \\[ \\begin{array}{lccccc} &amp;\\mathbf{Df} &amp; \\mathbf{Sum\\ Sq} &amp; \\mathbf{Mean\\ Sq} &amp; \\mathbf{F\\ value} &amp; \\mathbf{Pr(&gt;F)} \\\\ \\mathbf{sex} &amp; 1 &amp; 134.98 &amp; 134.98 &amp; 5.251 &amp; 0.0279 \\\\ \\mathbf{group} &amp; 1 &amp; 4919.51 &amp; 4919.51 &amp; 191.389 &amp; 5.59 \\times 10^{-16} \\\\ \\mathbf{Residual} &amp; 36 &amp; 925.35 &amp; 25.704 &amp; &amp; \\\\ \\end{array} \\] For example, the observed value of \\(F_1\\) is \\[ f_{1,obs} = \\frac{MST_1}{MSE} = \\frac{\\frac{1}{k-1} SSq_{Sex}}{\\frac{1}{(kmn-1)-(k-1)-(m-1)} SSq_{Residual}} = 5.251 \\] This value is sufficiently large, indicating rejection of the null hypothesis (\\(pvalue = 0.028 &lt; 0.05\\)). Therefore, there is a significant effect of sex when correcting for leptin differences. From the dispersion plot above, males appear to have lower weights than females, both in the leptin-supplemented (sup) and non-supplemented (KO) groups. 17.15 2-way ANOVA with interaction In the previous ANOVA model, we computed the means at each sex-leptin condition \\((j,r)\\) as the sum of the contributions of group \\(j\\) from factor 1 (sex) and group \\(r\\) from factor 2 (leptin) to the overall mean: \\[ \\mu_{jr} = \\alpha_{j} + \\beta_{r} + \\mu \\] However, the distribution of the data around these means is not symmetrical, because these are not the means computed within each condition. When we take weights conditioned on each leptin-by-sex group, we observe: \\(n_{sup,F} = 12\\) supplemented leptin female mice had a weight average of \\(\\bar{y}_{F,plus} = 25.575\\). \\(n_{sup,M} = 10\\) supplemented leptin male mice had a weight average of \\(\\bar{y}_{M,plus} = 25.668\\). \\(n_{KO,F} = 10\\) control female mice had a weight average of \\(\\bar{y}_{F,KO} = 51.120\\). \\(n_{KO,M} = 7\\) leptin KO male mice had a weight average of \\(\\bar{y}_{M,KO} = 44.530\\). These condition-specific means can be visualized with a barplot including confidence intervals. From this plot, we can see that most of the difference between male and female weights comes from the knock-out condition. In the leptin-supplemented groups, the sexes seem to have similar weights. However, the differences between sexes appear larger in the knock-out mice than in the supplemented-with-leptin mice. How can we test the hypothesis that a particular combination of conditions has an effect on the weight of mice? For instance, how can we test whether male knock-outs are less sensitive to weight gain than female knock-outs? The apparently lower-than-expected weight gain of leptin- (KO) males suggests a specific interaction between leptin KO and being male. 17.16 Linear model We can then formulate the linear model with an interaction term that accounts for the specific contribution of each condition given by the groups \\((j,r)\\) of each factor: \\[ Y_{jri} = \\mu + \\alpha_j + \\beta_r + (\\alpha\\beta)_{jr} + \\varepsilon_{jri} \\] Here, each observation in a condition \\((j,r)\\) has a specific expected value: \\[ E(Y \\mid j,r) = \\mu + \\alpha_j + \\beta_r + (\\alpha\\beta)_{jr} \\] This is the mean conditioned on the groups \\((j,r)\\). The condition mean can be computed as the sum of the independent contributions of each group (\\(\\mu_{jr}\\)) plus the specific contribution of the condition \\((\\alpha\\beta)_{jr}\\): \\[ E(Y \\mid j,r) = \\mu_{jr} + (\\alpha\\beta)_{jr} \\] 17.17 Hypothesis tests This ANOVA model allows testing three null hypotheses: First: \\[ H_0: \\alpha_1 = \\alpha_2 = \\dots = \\alpha_k = 0 \\] There is no difference between group means in the first factor. Second: \\[ H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_m = 0 \\] There is no difference between group means in the second factor. Third: \\[ H_0: (\\alpha\\beta)_{jr} = 0 \\] There is no difference between specific condition means. The alternatives are that at least one of the terms is different from 0. 17.18 Variance components The first two hypothesis contrasts can be tested as shown in the previous section. To test the first contrast, we define the \\(F_1\\) statistic that measures the dispersion of the group means of factor 1 relative to the dispersion of the residuals: \\[ F_1 = \\frac{MS_1}{MSE} \\sim F(k-1, (n-1) - (k-1) - (m-1)) \\] To test the second contrast, we define the corresponding \\(F_2\\) statistic for factor 2: \\[ F_2 = \\frac{MS_2}{MSE} \\sim F(m-1, (n-1) - (k-1) - (m-1)) \\] Finally, to test the hypothesis for the interaction term, we compute the \\(F_I\\) statistic, given by the dispersion of the group averages relative to the overall average with respect to the dispersion of the residuals: \\[ F_I = \\frac{MS_I}{MSE} \\sim F\\Big((k-1)(m-1), (nkm-1) - (k-1) - (m-1) - (k-1)(m-1)\\Big) \\] These statistics are summarized in the ANOVA table with interaction, which can be obtained with the following code: Python: model = ols(&#39;weight ~ sex*group&#39;, data=filtered_data).fit() anova_table = sm.stats.anova_lm(model, typ=1) print(anova_table) R: summary(aov(Weight ~ Sex*Leptin, data = dat)) \\[ \\begin{array}{lccccc} &amp;\\mathbf{Df} &amp; \\mathbf{Sum\\ Sq} &amp; \\mathbf{Mean\\ Sq} &amp; \\mathbf{F\\ value} &amp; \\mathbf{Pr(&gt;F)} \\\\ \\mathbf{sex} &amp; 1 &amp; 134.98 &amp; 134.98 &amp; 5.757 &amp; 0.02188 \\\\ \\mathbf{group} &amp; 1 &amp; 4919.51 &amp; 4919.51 &amp; 209.836 &amp; 2.37 \\times 10^{-16} \\\\ \\mathbf{sex:group} &amp; 1 &amp; 104.79 &amp; 104.79 &amp; 4.470 &amp; 0.04169 \\\\ \\mathbf{Residual} &amp; 35 &amp; 820.56 &amp; 23.445 &amp; &amp; \\\\ \\end{array} \\] As we observed from the bar plots, the statistical inference confirms that there are significant differences in weight between leptin groups, significant differences between sexes, and significant interactions between sex and leptin groups. In particular, being male reduces weight in knock-out mice, whereas it increases weight in mice with leptin supplementation. In other words, the effect of sex changes depending on the leptin context. Interactions are better represented in plots displaying the means conditioned on both factors. Significant interactions are indicated by non-parallel lines in such plots. 17.19 Questions 1) We test ANOVA when we have \\(\\qquad\\)a: several continuous random variables; \\(\\qquad\\)b: several categorical variables; \\(\\qquad\\)c: several continuous random variables and several categorical variables; \\(\\qquad\\)d: a continuous random variable and several categorical outcomes; 2) The statistic \\(F=\\frac{MST}{MSE}\\) is \\(\\qquad\\)a: the estimation of \\(\\sigma^2\\) by the residuals; \\(\\qquad\\)b: the estimation of \\(\\sigma^2\\) by the group means; \\(\\qquad\\)c: the estimation of \\(\\sigma^2\\) by the group means minus by the estimation of \\(\\sigma^2\\) by the residuals; \\(\\qquad\\)d: the estimation of \\(\\sigma^2\\) by the group means divided by the estimation of \\(\\sigma^2\\) by the residuals 3) We accept the null hypothesis that the groups have equal means when the value of \\(F=\\frac{MST}{MSE}\\) is \\(\\qquad\\)a: close to 1; \\(\\qquad\\)b: different from 1; \\(\\qquad\\)c: small; \\(\\qquad\\)d: large; 4) ANOVA assumes that the observations in each condition have \\(\\qquad\\)a: the same mean \\(\\mu\\); \\(\\qquad\\)b: the same variance \\(\\sigma^2\\); \\(\\qquad\\)c: different variances; \\(\\qquad\\)d: different means; 5) The interaction term in a an ANOVA tests that \\(\\qquad\\)a: the group means of factor 1 are different from the group means of factor 2 ; \\(\\qquad\\)b: the mean of a group of factor 1 is different from the mean of the other groups of factor 1; \\(\\qquad\\)c: the means of the groups across both factors are not added contributions of each factor; \\(\\qquad\\)d: the means of the groups across both factors are different; 17.20 Practice Load leptin data ( https://alejandro-isglobal.github.io/SDA/data/dataleptin.txt) Use a t-test to test the hypothesis that the weight between sexes is different. Use an ANOVA to test that the weight between sexes is different. Extract residuals from the ANOVA on leptin group and do a second anova on sex. Use an ANOVA on sex and group to test the whether sex and leptin groups are significantly associated with weight Include an interaction term in the previous ANOVA Make a bar plot of weight across all conditions given by both factors Solutions References "],["regression-and-correlation.html", "Chapter 18 Regression and Correlation 18.1 Correlations 18.2 Data 18.3 Normal bivariate 18.4 Estimators 18.5 Correlation coefficient 18.6 Hypothesis contrast 18.7 Regression analysis 18.8 Linear model 18.9 Hypothesis contrast 18.10 Estimators 18.11 Hypothesis testing 18.12 Stratified analysis 18.13 Multiple Regression 18.14 Multiple Regression interaction 18.15 Model diagnostics 18.16 Questions 18.17 Practice", " Chapter 18 Regression and Correlation When analyzing a continuous variable, the conditions under which observations are made may also be continuous. Unlike categorical groups, clearly defined repeating conditions are not available, as a group is defined by a continuous variable whose values cannot be repeated exactly. For example, consider measuring the velocity of a galaxy and its distance from Earth. We might want to know whether the velocity depends on distance—that is, whether higher distances correspond to higher velocities, as evidence of the expansion of the Universe. In this chapter, we will explore how to test the statistical dependence between two continuous random variables. We will introduce correlation as a parameter of a bivariate normal distribution and perform statistical tests on correlation, illustrating Hubble’s law. We will also discuss how to assess statistical dependence when one variable is considered the outcome (dependent) and the other the predictor (independent). We will introduce regression analysis to test linear relationships, using, for example, leptin levels in blood as explained by fat mass. Finally, we will discuss multiple regression, which allows adjusting associations for additional variables that may confound the relationship. For instance, we will examine how monthly variations in atmospheric CO2 can obscure the yearly trend first reported by Keeling in his groundbreaking 1960s work. Sex differences are often important modulatory factors in biomedical research. We will show how the effect of fat mass on leptin levels differs between sexes. In models relating a continuous outcome to a continuous predictor, interactions with additional variables can explain important variation in the outcome. We will conclude by introducing regression analysis with interactions, highlighting how these models capture more complex relationships between variables. 18.1 Correlations In 1929, Edwin Hubble reported the distances and velocities of 22 nebulae, providing the first evidence for the expansion of the Universe (Hubble 1929). He measured distances using different techniques depending on the scale. For relatively nearby galaxies, he relied on supergiant pulsating stars, whose pulsation periods are related to their intrinsic luminosity. By comparing the observed luminosity to the predicted luminosity, Hubble could estimate the distance to the galaxy containing the star. Velocities were determined from the redshift of spectral lines: the faster a galaxy moves away, the more its light is shifted toward the red. From a statistical perspective, Hubble’s data can be seen as a simultaneous measurement of two continuous variables—distance and velocity—for each galaxy. This dataset naturally leads to questions about the relationship between these variables, providing an early example of the kind of correlation and regression analysis that we discuss in this chapter. Hubble’s random experiment is the simultaneous measurement of two continuous outcomes on a galaxy: The velocity of a galaxy, which has a probability density \\[Y \\sim N(\\mu_y, \\sigma_y^2)\\] The distance of the galaxy, with density \\[X \\sim N(\\mu_x, \\sigma_x^2)\\] 18.2 Data One random experiment in Hubble’s study has two outcomes: \\((velocity, distance)\\). Continuous variable (outcome of interest) \\(velocity \\in (-25.0, 1800.0)\\) km/s Continuous variable (explanatory variable) \\(fatmass \\in (0.62, 3.450)\\) Mpc Repeating the experiment \\(n=21\\) times, the data for look like \\[ \\begin{array}{ccc} \\mathbf{Object} &amp; \\mathbf{Velocity} &amp; \\mathbf{Distance} \\\\ 278 &amp; 650 &amp; 1.52 \\\\ 584 &amp; 1800 &amp; 3.45 \\\\ 936 &amp; 1300 &amp; 2.37 \\\\ 1023 &amp; 300 &amp; 0.62 \\\\ 1700 &amp; 800 &amp; 1.16 \\\\ 2681 &amp; 700 &amp; 1.42 \\\\ 2683 &amp; 400 &amp; 0.67 \\\\ 2841 &amp; 600 &amp; 1.24 \\\\ 3034 &amp; 290 &amp; 0.79 \\\\ 3115 &amp; 600 &amp; 1.00 \\\\ 3368 &amp; 940 &amp; 1.74 \\\\ 3379 &amp; 810 &amp; 1.49 \\\\ 3489 &amp; 600 &amp; 1.10 \\\\ 3521 &amp; 730 &amp; 1.27 \\\\ 3623 &amp; 800 &amp; 1.53 \\\\ 4111 &amp; 800 &amp; 1.79 \\\\ 4526 &amp; 580 &amp; 1.20 \\\\ 4565 &amp; 1100 &amp; 2.35 \\\\ 4594 &amp; 1140 &amp; 2.23 \\\\ 5005 &amp; 900 &amp; 2.06 \\\\ 5866 &amp; 650 &amp; 1.73 \\\\ \\end{array} \\] and the plot of velocity against distance is We see a clear linear relationship between the outcomes. How can we test for it? We need to formulate a hypothesis test on a parameter of a join probability distribution that represents this linear relationship, which we will call the correlation between the variables. 18.3 Normal bivariate Let us consider that one random experiment of the study consists on drawing a pair of measurements of both the distance and and the velocity of a a single galaxy. We, therefore, have a random variable that is a pair of measurements that follows a probability distribution. We will assume that the distribution is the two-dimensional version of a normal probability density \\[(Y, X) \\sim N(\\mu_y, \\sigma^2_y, \\mu_x, \\sigma^2_x, \\rho)\\] This is function has five parameters. More explicitly the function is of the form \\[f(y,x)=\\frac{1}{2\\pi \\sigma_y\\sigma_x \\sqrt{1-\\rho^2}}e^{\\frac{(y-\\mu_y)^2}{\\sigma_y^2}-\\frac{2\\rho(y-\\mu_y)(x-\\mu_x)}{\\sigma_y\\sigma_x}+\\frac{(x-\\mu_x)^2}{\\sigma_x^2}}\\] and the parameters are \\(\\mu_y, \\mu_x, \\sigma^2_y, \\sigma_x^2, \\rho\\). These are The marginal mean and variance of \\(Y\\) \\(\\mu_y=E(Y)\\), \\(\\sigma^2_y=V(Y)\\) The marginal mean and variance of \\(X\\) \\(\\mu_x=E(X)\\), \\(\\sigma^2_x=V(X)\\) The correlation between \\(X\\) and \\(Y\\) \\(\\rho=\\frac{E[(Y-\\mu_y)(X-\\mu_x)]}{\\sigma_y\\sigma_x}\\) \\(\\rho\\) is called the correlation coefficient. When we draw the probability density in two dimensions, we see that the marginal densities are the projections of the densities on each axis. Let us define the densities for each variable \\[X \\sim N(\\mu_x, \\sigma_x^2)\\] and \\[Y \\sim N(\\mu_y, \\sigma_y^2)\\] with its parameters. We can plot the 2D density as contour lines and the marginal densities on each axis We can also plot the density in a 3D plot, where the \\(Z\\) axis is the value of the probability density. looking at the contour plots of the 2D distribution, we see that the fifth parameter \\(\\rho\\) defines the direction on which the ellipses are elongated (mayor axis). The density is entirely determined by 5 parameters. 18.4 Estimators We can derive the estimator of all 5 parameters, if we formulate the likelihood function \\[L=\\Pi_{i=1}^n f(y_i,x_i; \\mu_x, \\mu_y, \\sigma^2_x, \\sigma_y^2, \\rho)\\] and maximize it for each parameter. As a result, we obtain the following estimators of the parameters. The estimators for the means are the usual averages \\(\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^n y_i\\) estimates \\(\\mu_y\\) \\(\\bar{X}=\\frac{1}{n}\\sum_{i=1}^n x_i\\) estimates \\(\\mu_x\\) The maximum likelihood of the estimators for the variances are the uncorrected sample variances (dividing by \\(n\\)) \\(S^2_y=\\frac{1}{n}\\sum_{i=1}^n (y_i-\\bar{y})^2\\) estimates \\(\\sigma^2_y\\) \\(S^2_x=\\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar{x})^2\\) estimates \\(\\sigma^2_x\\) The estimator for the correlation is \\(R=\\frac{\\sum_{i=0}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{\\sqrt{\\sum_{i=1}^n(X_i-\\bar{X})^2}\\sqrt{\\sum_{i=1}^n(Y_i-\\bar{Y})^2}}\\) estimates \\(\\rho\\). 18.5 Correlation coefficient \\(R\\) is then a statistics that can be computed from the data and we can take one value as an estimate of \\(\\rho\\). The sampling distribution of \\(R\\) can be obtained if we transform it into a new variable (Fisher’s z transformation) \\[Z=\\frac{1}{2}\\ln (\\frac{1+R}{1-R})\\] The new variable \\(Z\\) is a normal variable \\[Z \\sim_{aprox} N(\\frac{1}{2}\\ln (\\frac{1+\\rho}{1-\\rho}), \\frac{1}{n-3})\\] with mean \\(\\frac{1}{2}\\ln (\\frac{1+\\rho}{1-\\rho})\\) and variance \\(\\frac{1}{n-3}\\). As \\(R\\) estimates \\(\\rho\\) we have that If \\(R\\) is near \\(0\\) then there is no linear relationship between \\(y\\) and \\(x\\) (the mayor and minor axes of the 2D plot are the x and y axes). If \\(R\\) is near \\(1\\) there is strong evidence of a linear relationship between \\(y\\) and \\(x\\) (the mayor axis does not align with neither the x or the y axis). \\(R\\) is then a measure of the statistical dependence between \\(Y\\) and \\(X\\). We can use it to test that hypothesis. 18.6 Hypothesis contrast The hypothesis contrast for the independence on the relationship between \\(Y\\) and \\(X\\) that follow a bivariate normal distribution can be formulated as \\(H_0: \\rho=0\\) (null hypothesis). Therefore, \\(Y\\) and \\(X\\) are consider statistically independent and consequently \\(f(y,x)=f(x)f(y)\\) (the joint probability is the product of the marginals) \\(H_1: \\rho \\neq 0\\) (alternative hypothesis). Therefore, \\(Y\\) and \\(X\\) are statistically dependent. We then use the statistic \\(R\\) to test whether there is a dependency between \\(Y\\) and \\(X\\). For testing the hypothesis contrast, we take the observed value of \\(R\\) \\[r_{obs}=\\hat{\\rho}=\\frac{\\sum_{i=0}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}}\\] and compute its \\(pvalue\\). That is the probability that we obtain a larger value if we repeat the sample when the null hypothesis is true. Under the null hypothesis \\(\\rho=0\\), we then compute the two-tailed \\(pvalue\\) \\[pvalue=2(1- F(|r_{obs}|)= 2(1- F_{N(0, 1/(n-3 )})(|z_{obs}|) \\] where \\[Z \\sim_{aprox} N(0, \\frac{1}{n-3})\\] Example (Hubble’s law) For Hubble’s data, we have that the observed correlation coefficient is \\(r_{obs}=0.956\\) and its transformed value \\[z_{obs}=ln((1+0.956)/(1-0.956))/2=1.89\\] The \\(pvalue\\) is therefore \\[pvalue=2(1- F_{N(0,1/18)}(1.89))=1.4\\times 10^{-11}\\] Therefore, since it is lower than \\(\\alpha=0.05\\), we reject the null hypothesis that galaxy velocity and distance are independent. The correlation is strongly positive, showing that as more distance galaxies are observed, we expect that they are moving away from the Earth and between each other at faster speeds. This result clearly implies that at some time in the past all the mass of the Universe was close together, perhaps at a point before it exploded in a Big Bang. In Python and R: from scipy import stats r, p_value = stats.pearsonr(dat[&quot;velocity&quot;], dat[&quot;distance&quot;]) R: cor.test(dat$velocity, dat$distance) Example (leptin and fat mass) Leptin is a hormone produced by adipose tissue. We want to study the serum leptin levels in the adult population. Zhang et al. studied the gene expression of patients with metabolic syndrome, for which one of its characteristics is high weight (Zhang et al. 2013). They recorded relevant data of 188 individuals that included leptin levels in blood, lean fatmass(kg), age and sex. Data is available at the public repository GEO and can be accessed from R We can use this publicly available data to inquiry on the relationship between fatmas and leptin and contrast it with expected results from previous studies, although this was not the main objective of the study. From this data, we assume that the levels of fatmass (\\(X\\)) and (log) leptin in blood \\(Y\\) are random variables that have a join bivarite normal distribution \\[(X, Y) \\sim N(\\mu_x, \\mu_y, \\sigma_x^2, \\sigma_y^2, \\rho)\\] We are interested in finding whether letpin levels depend on the levels of fatmass. Leptin is a hormone released from adipose tissue. Therefore, as more fatmas we expect more leptin in ciculating blood that regulate regulates satiety. The breaking of satiety regulation by leptin may be a cause for obesity. One random experiment in our study has two outcomes: \\((leptin, fatmass)\\). Continuous variable (outcome of interest) \\(lepting \\in (0, 5)\\) Continuous variable (explanatory variable) \\(fatmass \\in (20,80)\\) Repeating the experiment \\(n=188\\) times, the data for the first five repetitions look like \\[ \\begin{array}{ccc} \\mathbf{Subject} &amp; \\mathbf{Leptin} &amp; \\mathbf{Fatmass} \\\\ 1 &amp; 3.355677 &amp; 45.721 \\\\ 2 &amp; 2.272126 &amp; 43.895 \\\\ 3 &amp; 1.071584 &amp; 47.871 \\\\ 4 &amp; 3.921082 &amp; 65.801 \\\\ 5 &amp; 1.536867 &amp; 56.644 \\\\ ... &amp; ... &amp; ... \\\\ \\end{array} \\] For this data, we have that the observed correlation coefficient is \\(r_{obs}=-0.276\\) and its transformed value \\[z_{obs}=\\frac{1}{2}\\ln(\\frac{1-0.276}{1+0.276})=-0.284\\] The \\(pvalue\\) is therefore \\[pvalue=2(1- F_{N(0,1/185)}(0.284))=0.0001\\] Therefore, since it is lower than \\(\\alpha=0.05\\), we reject the null hypothesis that leptin and fat mass are independent. Note that leptin and fat mass are weakly correlated and negatively \\(r=-0.28\\), although the correlation is highly significant because \\(pvalue=0.0001\\). We should take this result with caution: Looking at the data, we see the fitted 2D probability function with negative correlation, but the marginal histograms are not quite normally distributed. From literature, we know that as we increase fat mass, we should obtain more leptin, not the contrary, as it is released from adipose tissue. How can we improve our analysis, so that our results are as expected? 18.7 Regression analysis To determine if \\(leptin\\) and \\(fatmass\\) are statistically independent, we can rather ask: What is the probability density of leptin at a given value of fat mass. Does the density changes when we change the value of fat mass? We will then consider the that the continuous fat mass value is a condition for leptin, or that changing fat mass will produce a change in leptin levels, as it is the tissue that releases it. We call the condition variable the explanatory or independent variable, and the outcome of interest the explained or dependent variable. Let us explore first the independence of two bivariate normal variables in terms of conditinal probabilities. Remember, two variables \\(Y\\) and \\(X\\) are independent if \\[P(Y|X)=P(Y)\\] Therefore, we can calculate the conditional probability density of \\(Y\\) (leptin) given \\(X\\) (fat mass) from the definition \\[f(y|x)=\\frac{f(y,x)}{f(y)} =N(\\mu_{y|x}, \\sigma^2_{y|x})\\] This turns up to be a normal distribution. The conditional probability density is the profile, or a slice, of the 2D distribution at a given value of \\(x\\). The conditional density at a given point is the dotted line in red. Its mean is the red dot and, in terms of the parameter of the binomial normal density, is given by \\[\\mu_{y|x}=\\mu_y+\\frac{\\sigma_y\\rho}{\\sigma_x}(x-\\mu_x)\\] If we manipulate this equation, we see that the conditional mean is a linear function of \\(x\\) \\[\\mu_{y|x}=(\\mu_y-\\mu_x \\frac{\\sigma_y\\rho}{\\sigma_x})+\\frac{\\sigma_y \\rho}{\\sigma_x} x=\\alpha + \\beta x\\] where \\(\\alpha=\\mu_y-\\mu_x \\beta\\) and \\(\\beta=\\frac{\\sigma_y \\rho}{\\sigma_x}\\). This line is called a regression line (red dotted line in the plot). That is, when we move along the \\(X\\) axis, the conditional mean of \\(Y\\) moves linearly. The variance of the conditional density is \\[\\sigma^2_{y|x}= \\sigma_y^2(1-\\rho^2)\\] Solving for \\(\\rho^2\\), we have \\[\\rho^2=\\frac{\\sigma_y^2-\\sigma^2_{y|x}}{\\sigma_y^2} \\] \\(\\rho^2\\) is then the proportion of the total variance that is explained by the regression line. For instance, \\(\\rho^2=1\\) when the conditional variability \\(\\sigma^2_{y|x}\\) is zero ans the conditional distribution shinks to its mean; that is, all the observations fall on the regression line. 18.8 Linear model Consider the linear model for the values of \\(Y\\) conditioned to the value of \\(x_i\\) \\[Y_i = \\alpha + \\beta x_i +\\varepsilon_{i}\\] \\(i\\) is the index of the observation from \\((1,...n)\\), typically one for every \\(x_i\\) as \\(x_i\\) is continuous, and \\(\\varepsilon_{i}\\) is the random error, a random variable with expected value \\(E(\\varepsilon_{i})=0\\) and variance \\(V(\\varepsilon_{i})=\\sigma_{y|x}^2\\). The expected value of \\(Y_{x_i}\\) is the regression line \\[\\mu_{y|x_i}=\\alpha + \\beta x_i\\] with \\[\\alpha=\\mu_y-\\beta\\mu_x\\] and \\[\\beta=\\rho\\frac{\\sigma_y}{\\sigma_x}\\] 18.9 Hypothesis contrast Null hypothesis: \\(H_0: \\beta=0\\). \\(Y\\) and \\(X\\) are statistically independent, and therefore \\(f(y|x)=f(y)\\). If this is the case then the regression line is flat, as the conditional density is the same no matter the point of the slice. The null hypothesis model is shown in the plot Note that the marginal probability (projection of the probability on the \\(y\\) axis) would be identical to all the conditional probabilities, which is the condition for independence. Alternative hypothesis: \\(H_1: \\beta\\neq 0\\). \\(Y\\) and \\(X\\) are statistically dependent. If this is the case then we have the following model Note that the marginal probability would be the combination of the conditional probabilities. 18.10 Estimators \\(\\beta\\) is therefore our parameter of interest and we need a probability distribution to test its hypothesis contrast \\(\\beta=\\rho\\frac{\\sigma_y}{\\sigma_x}\\) suggest the estimator for \\(\\beta\\) \\[B=\\frac{\\sum_{i=1}^m(x_i-\\bar{x})(Y_i-\\bar{Y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\] \\(\\alpha=\\mu_y-\\beta\\mu_x\\) suggests the estimator for \\(\\alpha\\) \\[A=\\bar{Y}- \\hat{\\beta}\\bar{x}\\] The estimators \\(A\\) and \\(B\\) for \\(\\alpha\\) and \\(\\beta\\) can formally be derived from minimizing the sum of squares for the error given \\(x_i\\) \\[SSE=\\sum_{i=1}^n(Y_i-\\bar{Y_i})^2=\\sum_{i=1}^n(Y_i-\\alpha + \\beta x_i)^2\\] with respect to \\(\\alpha\\) and \\(\\beta\\). If \\(\\bar{Y_i}\\) is normal then \\[B \\sim N(\\beta, \\frac{n\\sigma^2_y}{{(n-2)s^2_x}})\\] with mean \\(E(B)=\\beta\\) and, therefore, it is an unbiased estimator. 18.11 Hypothesis testing The standardized error, we make when we estimate \\(\\beta\\) with \\(B\\) \\[\\frac{B -\\beta}{\\sqrt{\\frac{ns^2_y}{{(n-2)s^2_x}}}} \\sim t(n-2)\\] that follows a t-distribution with \\(n-2\\) degrees of freedom. If the null hypothesis is true then \\(\\beta=0\\) and the observed error is \\[t_{obs}= \\frac{\\hat{\\beta}}{\\sqrt{\\frac{ns^2_y}{{(n-2)s^2_x}}}} \\sim t(n-2)\\] Where \\(\\hat{\\beta}\\) is the observed value of the random variable \\(B\\). Example (leptin and fatmass) We can perform the statistical test in Pyhton and R: Python: import pandas as pd from scipy.stats import pearsonr ##loading dat model = ols(&#39;leptin ~ fatmass&#39;, data=data).fit() print(model.summary()) R: ##loading dat summary(lm(leptin ~ fatmass, data=dat)) The outut is a table like \\[ \\begin{array}{lcccc} &amp; \\mathbf{Estimate} &amp; \\mathbf{Std.\\ Error} &amp; \\mathbf{t\\ value} &amp; \\mathbf{Pr(&gt;|t|)} \\\\ \\mathbf{(Intercept)} &amp; 3.720 &amp; 0.295 &amp; 12.604 &amp; &lt;2 \\times 10^{-16} \\ ^{***} \\\\ \\mathbf{fatmass} &amp; -0.0226 &amp; 0.00576 &amp; -3.926 &amp; 0.000121 \\ ^{***} \\\\ \\end{array} \\] The observed value of \\(B\\) is \\[\\hat{\\beta}= \\frac{\\sum_{i=1}^m(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}= -0.02262\\] and the observed value of \\(A\\) is \\[\\hat{\\alpha}=\\bar{y}-\\hat{\\beta}\\bar{x}= 3.72012\\] Only \\(7\\%\\) of the variability is explained by the regression line, since (R-squared: 0.07653) \\[r_{obs}^2=0.07653\\] We can draw the regression line We see that the regression line is negative and there is dispersion about it. Remember that the correct way to interpret the line is to think that these are the mean values of \\(Y\\) conditioned on \\(X\\). The bands, are the contious confidence intervals at a fix value of \\(X\\). 18.12 Stratified analysis We can include other conditions in the regression analysis. In general, it is important to adjust for other factors that we believe are correlated with the outcome \\(Y\\) and the condition \\(x\\), as they may explain part of the association. These are called confounders. Example (Fat mass and leptin) In the datudy of Zhang and colleageues (Zhang et al. 2013) they measure othre determinats of leptin levels, such as sex and age. The leptin level may have multiple covariates or explanatory variables that can affect the association between fat mass and letpin. The first observations of the complete data sex looks like \\[ \\begin{array}{ccccc} \\mathbf{Subject} &amp; \\mathbf{Leptin} &amp; \\mathbf{Fatmass} &amp; \\mathbf{Sex} &amp; \\mathbf{Age} \\\\ 1 &amp; 3.356 &amp; 45.721 &amp; F &amp; 45 \\\\ 2 &amp; 2.272 &amp; 43.895 &amp; F &amp; 77 \\\\ 3 &amp; 1.072 &amp; 47.871 &amp; M &amp; 79 \\\\ 4 &amp; 3.921 &amp; 65.801 &amp; F &amp; 58 \\\\ 5 &amp; 1.537 &amp; 56.644 &amp; M &amp; 42 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\end{array} \\] Consider the previous regression and color the points according to their sex We clearly see that the negative association between leptin and fat mass is given by the effect of sex. As males have lower leptin levels and higher fat mass then the negative correlation between leptin and fat mass is due to sex. 18.13 Multiple Regression The linear regression can be extended to include several number of factors. Consider the linear model with one additional factor \\[Y_{jr} = \\alpha + \\beta x_j +\\gamma z_{r}+\\varepsilon_{jr}\\] Here, our effect of interest is the coefficient \\(\\beta\\) of \\(x_i\\) (leptin) and we want to adjust for the effect of \\(z_j\\) (sex). The model is similar to the linear model we used for ANOVA, only that now the treatment effects are continuous and depend on the variable \\(X\\). Running the model in Python or R we obtain Python: import pandas as pd from scipy.stats import pearsonr from statsmodels.formula.api import ols model = ols(&#39;leptin ~ fatmass + sex&#39;, data=dat).fit() print(model.summary()) R: summary(lm(leptin ~ fatmass + sex, data=dat)) \\[ \\begin{array}{lcccc} &amp; \\mathbf{Estimate} &amp; \\mathbf{Std.\\ Error} &amp; \\mathbf{t\\ value} &amp; \\mathbf{Pr(&gt;|t|)} \\\\ \\mathbf{(Intercept)} &amp; 1.716 &amp; 0.277 &amp; 6.187 &amp; 3.84 \\times 10^{-9} \\ ^{***} \\\\ \\mathbf{fatmass} &amp; 0.0279 &amp; 0.00604 &amp; 4.630 &amp; 6.88 \\times 10^{-6} \\ ^{***} \\\\ \\mathbf{sexM} &amp; -1.636 &amp; 0.136 &amp; -12.023 &amp; &lt;2 \\times 10^{-16} \\ ^{***} \\\\ \\end{array} \\] Therefore, we observe that there is a positive increase in leptin when we adjust for sex \\(\\hat{\\beta}=4.63\\), \\(pvalue=6.88 \\times 10^{-6}\\). In addition males have a negative association with leptin, compared with women \\(\\gamma=-12.02\\), \\(pvalue&lt;2 \\times 10^{-16}\\). But within each sex the association is positive as expected. The regression lines for each sex will be parallel but displaced by the average effect of changing from female (reference) to male. Example(C02 concentration in atmosphere) Perhaps one of the most consequential observations of the second half of the 20 century was reported by Keeling in 1960 (Keeling 1960). Using continuous recording within less than three years (1958-1960), and over different geographic regions and methods, he showed that the levels of CO2 were significantly increasing in the atmosphere. He started CO2 survey in Hawaii in Mauna Loa, which have continue for 66 years. The data from on this station from his 1960 paper is \\[ \\begin{array}{cccc} \\mathbf{Observation} &amp; \\mathbf{CO2 (ppm)} &amp; \\mathbf{Month} &amp; \\mathbf{Year} \\\\ 1 &amp; 313.4 &amp; 3 &amp; 1958 \\\\ 2 &amp; 314.4 &amp; 4 &amp; 1958 \\\\ 3 &amp; 315.1 &amp; 5 &amp; 1958 \\\\ 5 &amp; 312.9 &amp; 7 &amp; 1958 \\\\ 6 &amp; 312.3 &amp; 8 &amp; 1958 \\\\ 7 &amp; 311.6 &amp; 9 &amp; 1958 \\\\ 9 &amp; 310.6 &amp; 11 &amp; 1958 \\\\ 10 &amp; 311.6 &amp; 12 &amp; 1958 \\\\ 11 &amp; 312.5 &amp; 1 &amp; 1959 \\\\ 12 &amp; 313.5 &amp; 2 &amp; 1959 \\\\ 13 &amp; 314.0 &amp; 3 &amp; 1959 \\\\ 14 &amp; 314.7 &amp; 4 &amp; 1959 \\\\ 15 &amp; 315.3 &amp; 5 &amp; 1959 \\\\ 16 &amp; 315.2 &amp; 6 &amp; 1959 \\\\ 17 &amp; 313.5 &amp; 7 &amp; 1959 \\\\ 18 &amp; 311.9 &amp; 8 &amp; 1959 \\\\ 19 &amp; 311.1 &amp; 9 &amp; 1959 \\\\ 20 &amp; 310.5 &amp; 10 &amp; 1959 \\\\ 21 &amp; 311.8 &amp; 11 &amp; 1959 \\\\ 22 &amp; 312.5 &amp; 12 &amp; 1959 \\\\ 23 &amp; 313.4 &amp; 1 &amp; 1960 \\\\ 24 &amp; 313.7 &amp; 2 &amp; 1960 \\\\ 25 &amp; 314.4 &amp; 3 &amp; 1960 \\\\ \\end{array} \\] The plot of the observations per month though out years (now known as Kelling plot is) Can we say from this data that C02 concentrations are getting increasing in Mauna Loa thoughout the years? Finding a year trend from the inspection of the data is difficult because the variations per month are large, in comparison with the year variation. Maximum CO2 coincides with summer in the northern hemisphere when vegetation is the greenest, and vegetation respiration maximum. Therefore, the monthly variation is a confounding factor for the regressing CO2 concentration on years. The model we are interested in fitting is \\[Y_{year, month} = \\alpha + \\beta x_{year} + z_{month}+\\varepsilon_{year, month}\\] where \\(Y\\) is the CO2 concentrations (outcome), \\(\\beta\\) is the effect of years (explanatory variable), and \\(z_{month}\\) is the effect of the month (covariate). The covariate month is better encoded as categorical as we expect each month to have its own specific effect (with respect to January). Encoded as numeric the model will fit pair-wise comarisons between January and each month, as 11 covariates for the effect of year. The code in if Python and R Pyhton: import pandas as pd import statsmodels.api as sm import statsmodels.formula.api as smf df = pd.DataFrame({ &#39;y&#39;: [313.4, 314.4, 315.1, 312.9, 312.3, 311.6, 310.6, 311.6, 312.5, 313.5, 314.0, 314.7, 315.3, 315.2, 313.5, 311.9, 311.1, 310.5, 311.8, 312.5, 313.4, 313.7, 314.4], &#39;month&#39;: [3,4,5,7,8,9,11,12,1,2,3,4,5,6,7,8,9,10,11,12,1,2,3], &#39;year&#39;: [1958,1958,1958,1958,1958,1958,1958,1958, 1959,1959,1959,1959,1959,1959,1959,1959, 1959,1959,1959,1959,1960,1960,1960] }) model = smf.ols(&#39;y ~ year + C(month)&#39;, data=df).fit() print(model.summary()) R: df &lt;- data.frame( y = c(313.4, 314.4, 315.1, 312.9, 312.3, 311.6, 310.6, 311.6, 312.5, 313.5, 314.0, 314.7, 315.3, 315.2, 313.5, 311.9, 311.1, 310.5, 311.8, 312.5, 313.4, 313.7, 314.4), month = c(3,4,5,7,8,9,11,12,1,2,3,4,5,6,7,8,9,10,11,12,1,2,3), year = c(1958,1958,1958,1958,1958,1958,1958,1958, 1959,1959,1959,1959,1959,1959,1959,1959, 1959,1959,1959,1959,1960,1960,1960) ) summary(lm(y~year+factor(month), data=df)) with results \\[ \\begin{array}{lcccc} &amp; \\mathbf{Estimate} &amp; \\mathbf{Std.\\ Error} &amp; \\mathbf{t\\ value} &amp; \\mathbf{Pr(&gt;|t|)} \\\\ \\mathbf{(Intercept)} &amp; -500.9962 &amp; 286.0446 &amp; -1.751 &amp; 0.110420 \\\\ \\mathbf{year} &amp; 0.4154 &amp; 0.1460 &amp; 2.846 &amp; 0.017383 \\ ^{*} \\\\ \\mathbf{factor(month)2} &amp; 0.6500 &amp; 0.3722 &amp; 1.746 &amp; 0.111309 \\\\ \\mathbf{factor(month)3} &amp; 1.1910 &amp; 0.3475 &amp; 3.427 &amp; 0.006466 \\ ^{**} \\\\ \\mathbf{factor(month)4} &amp; 2.0154 &amp; 0.3998 &amp; 5.041 &amp; 0.000506 \\ ^{***} \\\\ \\mathbf{factor(month)5} &amp; 2.6654 &amp; 0.3998 &amp; 6.667 &amp; 5.59 \\times 10^{-5} \\ ^{***} \\\\ \\mathbf{factor(month)6} &amp; 2.4577 &amp; 0.4616 &amp; 5.324 &amp; 0.000336 \\ ^{***} \\\\ \\mathbf{factor(month)7} &amp; 0.6654 &amp; 0.3998 &amp; 1.664 &amp; 0.127009 \\\\ \\mathbf{factor(month)8} &amp; -0.4346 &amp; 0.3998 &amp; -1.087 &amp; 0.302483 \\\\ \\mathbf{factor(month)9} &amp; -1.1846 &amp; 0.3998 &amp; -2.963 &amp; 0.014211 \\ ^{*} \\\\ \\mathbf{factor(month)10}&amp; -2.2423 &amp; 0.4616 &amp; -4.857 &amp; 0.000664 \\ ^{***} \\\\ \\mathbf{factor(month)11}&amp; -1.3346 &amp; 0.3998 &amp; -3.338 &amp; 0.007511 \\ ^{**} \\\\ \\mathbf{factor(month)12}&amp; -0.4846 &amp; 0.3998 &amp; -1.212 &amp; 0.253294 \\\\ \\end{array} \\] We see an significant increase in \\(\\hat{\\beta}=0.4154\\) ppm per year \\(pvalue=0.01\\), adjusting for month variation. While this was preliminary data on global CO2 emissions, a year plot of the residuals from fitting the CO2 concentrations on moths clearly shows the trend. Additionally, continuous monitoring at Mauna Loa for over 66 year reveals how the underlying yearly trend has largely superseded the monthly variations. 18.14 Multiple Regression interaction Sometimes the explanatory variable has an effect that is modulated by a covariate. That is, the strngth of the effect depends on another condition. Example (Fat mass, letptin and sex) For the association between fat mass and leptin we can fit the regression model separating, or stratifying, by sex. We find that the strength of the association between leptin and fat mass depends on sex. That is, the slope of the regression line changes between sexes. We can furthermore include interactions between conditions in the regression. Consider the linear model \\[Y_{jr} = \\alpha + \\beta x_{j} +\\gamma z_{r} + \\delta (xz)_{jr} +\\varepsilon_{jr}\\] The parameter \\(\\delta\\) will add a contribution to the slope \\(\\beta\\) that is specific to the condition \\(r\\). For instance, if \\(z_i \\in (0,1)\\) then when \\(z=0\\) the coefficient of \\(x_i\\) is \\(\\beta\\). But when \\(z=1\\) the coefficient of \\(x_i\\) is \\(\\beta+\\gamma\\). Example (Fat mass and leptin) In the fat mass and leptin relationship, if \\(z\\) is sex, then \\(\\gamma\\) will test the differences in \\(\\beta\\)s between males and females. We can further adjust by age. \\[Y_{jrs} = \\alpha + \\beta\\times fatmass_{j} +\\gamma\\times sex_{r} + \\delta\\times (fatmass\\times sex)_{jr} + \\epsilon_s age+\\varepsilon_{jrs}\\] We can re-fit the model with the interaction term Python: import pandas as pd from scipy.stats import pearsonr model = ols(&#39;leptin ~ fatmass*sex + age&#39;, data=dat).fit() print(model.summary()) R: summary(lm(leptin ~ fatmass*sex + age, data=dat)) with th efollowing results \\[ \\begin{array}{lcccc} &amp; \\mathbf{Estimate} &amp; \\mathbf{Std.\\ Error} &amp; \\mathbf{t\\ value} &amp; \\mathbf{Pr(&gt;|t|)} \\\\ \\mathbf{(Intercept)} &amp; 1.800 &amp; 0.337 &amp; 5.337 &amp; 2.77 \\times 10^{-7} \\ ^{***} \\\\ \\mathbf{fatmass} &amp; 0.0200 &amp; 0.00695 &amp; 2.881 &amp; 0.00443 \\ ^{**} \\\\ \\mathbf{sexM} &amp; -3.218 &amp; 0.775 &amp; -4.151 &amp; 5.07 \\times 10^{-5} \\ ^{***} \\\\ \\mathbf{age} &amp; 0.00585 &amp; 0.00294 &amp; 1.989 &amp; 0.0482 \\ ^{*} \\\\ \\mathbf{fatmass{:}sexM} &amp; 0.0284 &amp; 0.0135 &amp; 2.104 &amp; 0.0368 \\ ^{*} \\\\ \\end{array} \\] The data suggest a steeper increase of leptin with body fat in males than in females (interaction: \\(0.028427\\), \\(pvalue=0.03\\)), as we saw in the figures of the stratified analysis. A positive and significant interaction means that the slope for males is higher than the slope for females. The effect of fatmass on letpin levels is stronger in males. We also observe a signification effect of age on leptin levels. As the coefficient is a slope, we can say that for a year increase in the age of the patient, we see a significant increase in \\(0.005\\) units of leptin levels (\\(pvalue=0.048\\)). Therefore age is also a contributing factor of leptin levels. The model now explains \\(50\\%\\) of the variance as \\(R^2=0.5\\). 18.15 Model diagnostics All linear models have been made on the supposition that Errors are distributed normally Errors have the same variance There are a number of plots to check that at least the data is consistent with these suppositions. In particular, we are interested to see that the residuals distribute symmetrically against the fitted values, revealing equal variance (homoscedasticity). We also want to check that the points in a quantile-quantile (QQ) plot fall in the line, revealing how close the residuals distribute normally. R: mod &lt;- lm(leptin ~ fatmass * sex + age, data = dat) plot(mod, which = 1, main = &quot;Residuals vs Fitted&quot;) plot(mod, which = 2, main = &quot;Normal Q-Q&quot;) The residuals versus fitted values plot shows that the residuals are fairly randomly scattered around zero, with no strong pattern, suggesting that the linear model fits the data reasonably well and that the assumption of linearity is broadly satisfied. While there may be a slight concentration of residuals at certain fitted values, it is not pronounced. The Normal Q-Q plot indicates that the residuals are approximately normally distributed, as most points follow the reference line given by the nornal quantiles, with only minor deviations at the extremes, which are typical in real datasets. Overall, the model age appears to provide a reasonable fit, with assumptions of linearity and normality largely met. 18.16 Questions 1) We perform a regression analysis when we have \\(\\qquad\\)a: one categorical variable; \\(\\qquad\\)b: two categorical variables; \\(\\qquad\\)c: one continuous random variable; \\(\\qquad\\)d: two continuous random variables; 2) The correlation coefficient \\(\\rho\\) is \\(\\qquad\\)a: a parameter of a 2D probability density; \\(\\qquad\\)b: a statistic of the relationship between two continuous variables; \\(\\qquad\\)c: the linear coefficient btween of two variables; \\(\\qquad\\)d: the variance explained by one continuous variable on other; 3) \\(\\beta\\) in the linear model \\[Y_{x_i} = \\alpha + \\beta x_i +E_{i}\\] \\(\\qquad\\)a: is a statistic that measure the linear relationship between \\(x\\) and \\(y\\); \\(\\qquad\\)b: is zero for the null hypothesis; \\(\\qquad\\)c: is a parameter with expected value of zero; \\(\\qquad\\)d: is the correlation between \\(x\\) and \\(y\\) 4) Why do we adjust for a variable in a regression coefficient? \\(\\qquad\\)a: to test the interaction between the variable and \\(x\\); \\(\\qquad\\)b: to stratify the relationship between \\(x\\) and \\(y\\); \\(\\qquad\\)c: to remove confounding in the relationship between \\(x\\) and \\(y\\); \\(\\qquad\\)d: to improve the significance of the relationship between \\(x\\) and \\(y\\); 5) The regression analysis assumes that \\(\\qquad\\)a: the errors have mean 0 and equal variances; \\(\\qquad\\)b: the errors distribute normally with mean 0 and equal variances; \\(\\qquad\\)c: the errors distribute normally with different mean and equal variances; \\(\\qquad\\)d: the errors distribute normally with different mean and different variances; 18.17 Practice 18.17.0.1 Practice Load misophonia data https://alejandro-isglobal.github.io/SDA/data/data_0.txt We have four measures of anxiety: Trait: ansiedad.rasgo (are you an anxious person?) continuous:0-100 State: ansiedad.estado (are you currently feeling anxious?) continuous:0-100 Diagnosed: ansiedad.medicada (have you been diagnosed with an anxiety disorder?) binary (si, no) Excess: ansiedad.dif (difference between State and Trait) We formulate the following hypothesis: Participants who enrolled in the study had an increased level of anxiety from their baseline (trait) that is related to their: age sex anxiety state. We are interested in the variable anxiety.dif, that is the observed excess of anxiety from the trait \\(excess = state - trait\\) Answer the following questions: Are the state and trait of anxiety correlated? Is excess in anxiety higher in older people? Is excess in anxiety higher in older people after adjusting by sex? Is the interaction between age and sex significant on excess anxiety? Solutions References "],["apendix.html", "Chapter 19 Apendix 19.1 Solutions to Questions 19.2 Summary tables, Python and R code 19.3 Summary of common probability models 19.4 Summary of hypothesis", " Chapter 19 Apendix 19.1 Solutions to Questions \\[ \\begin{array}{c|ccccc} \\textbf{Chapter} &amp; Q1 &amp; Q2 &amp; Q3 &amp; Q4 &amp; Q5 \\\\ \\hline 2 &amp; c &amp; a &amp; d &amp; d &amp; b \\\\ 3 &amp; a &amp; b &amp; b &amp; b &amp; a \\\\ 4 &amp; c &amp; b &amp; d &amp; b &amp; b \\\\ 5 &amp; d &amp; c &amp; b &amp; c &amp; b \\\\ 7 &amp; d &amp; a &amp; d &amp; a &amp; d \\\\ 8 &amp; d &amp; b &amp; a &amp; &amp; \\\\ 9 &amp; c &amp; a &amp; c &amp; d &amp; b \\\\ 10 &amp; a &amp; c &amp; d &amp; d &amp; c \\\\ 11 &amp; c &amp; b &amp; c &amp; &amp; \\\\ 12 &amp; d &amp; d &amp; b &amp; a &amp; \\\\ 13 &amp; b &amp; a &amp; d &amp; b &amp; b \\\\ 15 &amp; b &amp; d &amp; c &amp; d &amp; a \\\\ 16 &amp; c &amp; b &amp; b &amp; b &amp; a \\\\ 17 &amp; d &amp; d &amp; a &amp; b &amp; c \\\\ 18 &amp; d &amp; a &amp; b &amp; c &amp; b \\\\ \\end{array} \\] 19.2 Summary tables, Python and R code 19.2.1 Creating Data Frames and Loading Text Files Task Python (pandas) R Define a simple data frame import pandas as pd df &lt;- data.frame() df = pd.DataFrame({'col1': [1,2,3], 'col2':[4,5,6]}) df &lt;- data.frame(col1 = c(1,2,3), col2 = c(4,5,6)) Load a CSV file df = pd.read_csv('file.csv') df &lt;- read.csv('file.csv') Load a tab-delimited text file df = pd.read_csv('file.txt', sep='\\t') df &lt;- read.delim('file.txt') Preview first rows df.head() head(df) Preview last rows df.tail() tail(df) Note: In Python, you need to import the following libraries: import numpy as np import pandas as pd import matplotlib.pyplot as plt 19.2.2 Python and R Functions for Data Description Function / Task Python R Mean (average) np.mean(data) mean(data) Standard deviation (sd) np.std(data, ddof=1) sd(data) Median np.median(data) median(data) Quantiles / IQR np.quantile(data, [0.25, 0.5, 0.75]) quantile(data, probs = c(0.25, 0.5, 0.75)) Relative frequencies pd.value_counts(data, normalize=True) table(data) / length(data) Cumulative frequencies np.cumsum(np.bincount(data)) / len(data) cumsum(table(data)) / length(data) Histogram plt.hist(data, bins=10, density=True) hist(data, breaks=10, freq=FALSE) Boxplot plt.boxplot(data) boxplot(data) ECDF (empirical CDF) np.sort(data); y = np.arange(1,len(data)+1)/len(data) plot(sort(data), (1:length(data))/length(data), type='s') Bar plot pd.value_counts(data).plot.bar() barplot(table(data)) Pie chart pd.value_counts(data).plot.pie() pie(table(data)) 19.3 Summary of common probability models Model Experiment range of X f(x) E(X) V(X) Uniform Measuring an integer or real number \\([a, b]\\) \\(\\frac{1}{n}\\) \\(\\frac{b+a}{2}\\) \\(\\frac{(b-a+1)^2-1}{12}\\) Bernoulli Observing \\(A\\) \\((0,1)\\) \\((1-p)^{1-x}p^x\\) \\(p\\) \\(p(1-p)\\) Binomial Counting # of \\(A\\) events in \\(n\\) repetitions of Bernoulli trials \\((0,1,.., n)\\) \\(\\binom n x (1-p)^{n-x}p^x\\) \\(np\\) \\(np(1-p)\\) Negative Binomial for events Counting # of \\(A&#39;\\) events in repetitions of Bernoulli trials until \\(r\\) events \\(A\\) are observed \\((0,1,..)\\) \\(\\binom {x+r-1} x (1-p)^xp^r\\) \\(\\frac{r(1-p)}{p}\\) \\(\\frac{r(1-p)}{p^2}\\) Hypergeom Counting # of \\(A\\) events in a sample \\(n\\) from population \\(N\\) with \\(K\\) # of \\(A\\) events \\(\\max(0, n+K-N)\\), … \\(\\min(K, n)\\) \\(\\frac{1}{\\binom N n}\\binom K x \\binom {N-K} {n-x}\\) \\(n*\\frac{N}{K}\\) \\(n \\frac{N}{K} (1-\\frac{N}{K})\\frac{N-n}{N-1}\\) Poisson Counting # \\(A\\) events in an interval 0,1, .. \\(\\frac{e^{-\\lambda}\\lambda^x}{x!}\\) \\(\\lambda\\) \\(\\lambda\\) Exponential Measuring an interval between two events \\(A\\) \\([0,\\infty)\\) \\(\\lambda e^{-\\lambda x}\\) \\(\\frac{1}{\\lambda}\\) \\(\\frac{1}{\\lambda^2}\\) Normal Measuring values with symmetric errors whose most likely value is the average \\((-\\infty, \\infty)\\) \\(\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\) \\(\\mu\\) \\(\\sigma^2\\) 19.3.1 Python and R Functions for Probability Models Model Python (PDF/PMF) R f(x) R F(x) Uniform (continuous) uniform.pdf(x, a, b) dunif(x, a, b) punif(x, a, b) Binomial binom.pmf(x, n, p) dbinom(x, n, p) pbinom(x, n, p) Negative Binomial (events) nbinom.pmf(x, r, p) dnbinom(x, r, p) pnbinom(x, r, p) Hypergeometric hypergeom.pmf(x, N, K, n) dhyper(x, K, N-K, n) phyper(x, K, N-K, n) Poisson poisson.pmf(x, lambda) dpois(x, lambda) ppois(x, lambda) Exponential expon.pdf(x, 0, 1/lambda) dexp(x, lambda) pexp(x, lambda) Normal norm.pdf(x, mu, sigma) dnorm(x, mu, sigma) pnorm(x, mu, sigma) Notes: All Python functions assume you have imported the relevant scipy.stats distribution. For the exponential in Python, note the shape is defined via scale \\(1/\\lambda\\) in scipy.stats.expon. 19.4 Summary of hypothesis Outcome (Y) Condition1 (X) Condition2 (Z) Null Hypothesis Test Normal - - \\(\\mu=\\mu_0\\) T-test Normal - - \\(\\sigma^2=\\sigma_0^2\\) \\(\\chi^2\\)-test Normal - - \\(\\mu_1 - \\mu_2=0\\) Paired T-test Binary - - \\(p=p_0\\) Z-test Binary (exact) - - \\(p=p_0\\) Binomial-test Categorical Categorical (A,B,…) - \\(p_A = p_B = ...\\) \\(\\chi^2\\)-test Categorical Categorical (A,B) - \\(p_A = p_B\\) Fisher-exact test Normal Binary - \\(\\mu_A = \\mu_B\\) T-test Normal Categorical (A,B,…) - \\(\\mu_A = \\mu_B = ...\\) ANOVA (F-test) Normal Categorical (A,B,…) Categorical (L,M,…) \\(\\alpha_A = \\alpha_B = ...; \\beta_L = \\beta_M = ...\\) 2-way ANOVA (F-test) Normal Categorical (A,B,…) Categorical (L,M,…) \\(\\alpha_A \\times \\beta_L = 0\\) 2-way interaction (F-test) Normal Normal - \\(\\rho = \\rho_0\\) Correlation-test Normal any type - \\(\\beta = 0\\) Regression Normal any type any type \\(\\beta = 0\\) Adjusted regression Normal any type any type \\(\\delta_{x,z} = 0\\) Regression with interaction 19.4.1 Python and R Functions for Hypothesis tesing Test Python code R code T-test stats.ttest_1samp(data, popmean) t.test(x, mu=mu0) \\(\\chi^2\\)-test p_value = 1 - stats.chi2.cdf(chi_square, df) var.test(x, sigma.squared = sigma0^2) Paired T-test stats.ttest_rel(x1, x2) t.test(x1, x2, paired=TRUE) Z-test proportions_ztest(count, nobs, value) prop.test(x, n, p=p0) Binomial-test binomtest(successes, trials, p) binom.test(x, n, p=p0) \\(\\chi^2\\)-test (categorical) chi2_contingency(observed) chisq.test(table(X)) Fisher-exact test fisher_exact(table(X,Y)) fisher.test(table(X,Y)) T-test (binary groups) stats.ttest_ind(groupA, groupB, equal_var) t.test(A, B) ANOVA model = ols('Y ~ X', data).fit(); sm.stats.anova_lm(model) aov(Y ~ X, data=df) 2-way ANOVA model = ols('Y ~ X+Z', data).fit(); sm.stats.anova_lm(model) aov(Y ~ X + Z, data=df) 2-way interaction model = ols('Y ~ X*Z', data).fit(); sm.stats.anova_lm(model) aov(Y ~ X*Z, data=df) Correlation pearsonr(X, Y) cor.test(X, Y, method=\"pearson\") Regression ols('Y ~ X', data).fit() lm(Y ~ X, data=df) Adjusted regression ols('Y ~ X+Z', data).fit() lm(Y ~ X + Z, data=df) Regression with interaction ols('Y ~ X*Z', data).fit() lm(Y ~ X*Z, data=df) 19.4.2 Python Libraries Required Purpose Library Functions Used Basic statistics (t-test, chi-square, paired t-test, etc.) scipy.stats ttest_1samp, ttest_rel, ttest_ind, chi2.cdf, pearsonr, fisher_exact Proportions &amp; binomial tests statsmodels.stats.proportion proportions_ztest, binomtest ANOVA &amp; regression statsmodels.api ols, anova_lm Data manipulation pandas DataFrame, data selection and grouping Contingency tables scipy.stats or statsmodels chi2_contingency, fisher_exact "]]
