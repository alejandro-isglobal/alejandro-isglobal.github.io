[["index.html", "Estadística Chapter 1 Objetivo 1.1 Lectura recomendada", " Estadística Alejandro Caceres 2022-10-16 Chapter 1 Objetivo Este es el curso de introducción a la estadística de la EEBE (UPC). Las fechas de exámenes y material de estudio adicional se pueden encontrar en ATENEA 1.1 Lectura recomendada Douglas C. Montgomery and George C. Runger. Applied Statistics and Probability for Engineers 4th Edition. Wiley 2007. "],["descripción-de-datos.html", "Chapter 2 Descripción de datos 2.1 Objetivo 2.2 Estadísticas 2.3 Metodo científico 2.4 Resultado 2.5 Tipos de resultado 2.6 Experimentos aleatorios 2.7 Frecuencias absolutas 2.8 Ejemplo 2.9 Frecuencias relativas 2.10 Ejemplo 2.11 Diagrama de barras 2.12 Gráfico de sectores 2.13 Variables categóricas y ordenadas 2.14 Ejemplo 2.15 Frecuencias acumuladas absolutas y relativas 2.16 Tabla de frecuencia 2.17 Gráfica de frecuencia acumulada 2.18 Variables continuas 2.19 Contenedores 2.20 Crear una variable categórica a partir de una continua 2.21 Tabla de frecuencias para una variable continua 2.22 Histograma 2.23 Tabla de frecuencias para una variable continua 2.24 Histograma 2.25 Gráfica de frecuencia acumulada: Variables continuas 2.26 Resumen estadístico 2.27 Promedio 2.28 Promedio (ordenado categóricamente) 2.29 Promedio (ordenado categóricamente) 2.30 Promedio 2.31 Promedio 2.32 mediana 2.33 Mediana Vs Promedio 2.34 Dispersión 2.35 Dispersión 2.36 Variación de la muestra 2.37 Variación de la muestra 2.38 Desviación Estándar 2.39 RIC 2.40 RIC 2.41 Diagrama de caja", " Chapter 2 Descripción de datos 2.1 Objetivo Datos: discretos, continuos Resumir datos en tablas y figuras. 2.2 Estadísticas Resolver problemas de manera sistemática (ciencia, tecnología e ingeniería) ¡Los humanos modernos usamos un método general históricamente desarrollado durante miles de años!  y aún en desarrollo. Tiene tres componentes principales: observación, lógica y generación de nuevo conocimiento. 2.3 Metodo científico 2.4 Resultado Observación o Realización Una observación es la adquisición de un número o una característica de un experimento  1 0 0 1 0 1 0 1 1  (el número en negrita es una observación en una repetición del experimento) Resultado Un resultado es una de las posibles observaciones de un experimento. 1 es un resultado, 0 es el otro resultado 2.5 Tipos de resultado Categórico: Si el resultado de un experimento solo puede tomar valores discretos (número de piezas de automóvil producidas por hora, número de leucocitos en sangre) Continuo: Si el resultado de un experimento solo puede tomar valores continuos (estado de carga de la batería, temperatura del motor). 2.6 Experimentos aleatorios Definición: Un experimento aleatorio es un experimento que da diferentes resultados cuando se repite de la misma manera. Ejemplos: en el mismo objeto (persona): temperatura, niveles de azúcar. sobre objetos diferentes pero de la misma medida: el peso de un animal. sobre eventos: número de correos electrónicos recibidos en una hora. 2.7 Frecuencias absolutas Cuando repetimos un experimento aleatorio, registramos una lista de resultados. Resumimos las observaciones categóricas contando cuántas veces vimos un resultado en particular. Frecuencia absoluta: \\[n_i\\] es el número de veces que observamos el resultado \\(i\\) 2.8 Ejemplo Experimento aleatorio: extraiga un leucocito de un donante y anote su tipo. Repita el experimento \\(N=119\\) veces. (célula T, célula T, neutrófilo, ..., célula B) ## outcome ni ## 1 T Cell 34 ## 2 B cell 50 ## 3 basophil 20 ## 4 Monocyte 5 ## 5 Neutrophil 10 Por ejemplo: \\(n_1=34\\) es el número total de células T \\(N=\\sum_i n_i=119\\) 2.9 Frecuencias relativas También podemos resumir las observaciones calculando la proporción de cuántas veces vimos un resultado en particular. \\[f_i=n_i/N\\] donde \\(N\\) es el número total de observaciones En nuestro ejemplo se registran \\(n_1=34\\) células T, por lo que la frecuencia relativa nos da la proporción de células T de un total de \\(119\\). 2.10 Ejemplo ## outcome ni fi ## 1 T Cell 34 0.28571429 ## 2 B cell 50 0.42016807 ## 3 basophil 20 0.16806723 ## 4 Monocyte 5 0.04201681 ## 5 Neutrophil 10 0.08403361 Tenemos \\(\\sum_{i=1..M} n_i = N\\) \\(\\sum_{i=1..M} f_i = 1\\) donde \\(M\\) es el número de resultados. 2.11 Diagrama de barras Podemos graficar \\(n_i\\) Vs los resultados, dándonos un gráfico de barras 2.12 Gráfico de sectores Podemos visualizar las frecuencias relativas con un gráfico de sectores Donde el área del círculo representa el 100% de las observaciones (proporción = 1) y las secciones las frecuencias relativas de todos los resultados. 2.13 Variables categóricas y ordenadas Los tipos de células no están ordenados de manera lógica en relación con los resultados. Sin embargo, a veces las variables categóricas se pueden ordenar. Estudio de misofonía: 123 pacientes fueron examinados por misofonía: ansiedad/ira producida por ciertos sonidos Se clasificaron en 4 grupos diferentes según la gravedad. 2.14 Ejemplo Los resultados del estudio son: ## [1] 4 2 0 3 0 0 2 3 0 3 0 2 2 0 2 0 0 3 3 0 3 3 2 0 0 0 4 2 2 0 2 0 0 0 3 0 2 ## [38] 3 2 2 0 2 3 0 0 2 2 3 3 0 0 4 3 3 2 0 2 0 0 0 2 2 0 0 2 3 0 1 3 2 4 3 2 3 ## [75] 0 2 3 2 4 1 2 0 2 0 2 0 2 2 4 3 0 3 0 0 0 2 2 1 3 0 0 3 2 1 3 0 4 4 2 3 3 ## [112] 3 0 3 2 1 2 3 3 4 2 3 2 y su tabla de frecuencias ## outcome ni fi ## 1 0 41 0.33333333 ## 2 1 5 0.04065041 ## 3 2 37 0.30081301 ## 4 3 31 0.25203252 ## 5 4 9 0.07317073 2.15 Frecuencias acumuladas absolutas y relativas La gravedad de la misofonía es categórica y ordenada. Cuando los resultados se pueden ordenar, entonces es útil preguntarse por el número de observaciones que se obtuvieron hasta un resultado dado. Llamamos a este número la frecuencia acumulada absoluta hasta el resultado \\(i\\): \\[N_i=\\sum_{k=1..i} n_k\\] Tambíen es útil calcular la proporción de las observaciones que se obtuvo hasta un resultado dado \\[F_i=\\sum_{k=1..i} f_k\\] 2.16 Tabla de frecuencia ## outcome ni fi Ni Fi ## 0 0 41 0.33333333 41 0.3333333 ## 1 1 5 0.04065041 46 0.3739837 ## 2 2 37 0.30081301 83 0.6747967 ## 3 3 31 0.25203252 114 0.9268293 ## 4 4 9 0.07317073 123 1.0000000 67% de los pacientes tenían misofonía hasta la gravedad 2 37% de los pacientes tienen una gravedad menor o igual a 1 2.17 Gráfica de frecuencia acumulada También podemos graficar la frecuencia acumulada Vs los resultados ___________ ___________ 2.18 Variables continuas El resultado de un experimento aleatorio también puede dar resultados continuos. En el estudio de misofonía, los investigadores se preguntaron si la convexidad de la mandíbula afectaría la gravedad de la misofonía (la hipótesis científica es que el ángulo de convexidad de la mandíbula puede influir en el oído y su sensibilidad). Estos son los resultados para la convexidad de la mandíbula (grados) ## [1] 7.97 18.23 12.27 7.81 9.81 13.50 19.30 7.70 12.30 7.90 12.60 19.00 ## [13] 7.27 14.00 5.40 8.00 11.20 7.75 7.94 16.69 7.62 7.02 7.00 19.20 ## [25] 7.96 14.70 7.24 7.80 7.90 4.70 4.40 14.00 14.40 16.00 1.40 9.76 ## [37] 7.90 7.90 7.40 6.30 7.76 7.30 7.00 11.23 16.00 7.90 7.29 6.91 ## [49] 7.10 13.40 11.60 -1.00 6.00 7.82 4.80 11.00 9.00 11.50 16.00 15.00 ## [61] 1.40 16.80 7.70 16.14 7.12 -1.00 17.00 9.26 18.70 3.40 21.30 7.50 ## [73] 6.03 7.50 19.00 19.01 8.10 7.80 6.10 15.26 7.95 18.00 4.60 15.00 ## [85] 7.50 8.00 16.80 8.54 7.00 18.30 7.80 16.00 14.00 12.30 11.40 8.50 ## [97] 7.00 7.96 17.60 10.00 3.50 6.70 17.00 20.26 6.64 1.80 7.02 2.46 ## [109] 19.00 17.86 6.10 6.64 12.00 6.60 8.70 14.05 7.20 19.70 7.70 6.02 ## [121] 2.50 19.00 6.80 2.19 Contenedores ¡Los resultados continuos no se pueden contar! Las transformamos en variables categóricas ordenadas Cubrimos el rango de las observaciones en intervalos regulares del mismo tamaño (bins) ## [1] &quot;[-1.02,3.46]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; &quot;(16.8,21.3]&quot; 2.20 Crear una variable categórica a partir de una continua Asignamos cada observación a su intervalo: creando una variable categórica ordenada; en este caso con 5 resultados posibles ## [1] &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; ## [6] &quot;(12.4,16.8]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; ## [11] &quot;(12.4,16.8]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; ## [16] &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; ## [21] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; ## [26] &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [31] &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; &quot;[-1.02,3.46]&quot; ## [36] &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [41] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; ## [46] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; ## [51] &quot;(7.92,12.4]&quot; &quot;[-1.02,3.46]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [56] &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; ## [61] &quot;[-1.02,3.46]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; ## [66] &quot;[-1.02,3.46]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;[-1.02,3.46]&quot; ## [71] &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; ## [76] &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; ## [81] &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; ## [86] &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; ## [91] &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; ## [96] &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; ## [101] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; ## [106] &quot;[-1.02,3.46]&quot; &quot;(3.46,7.92]&quot; &quot;[-1.02,3.46]&quot; &quot;(16.8,21.3]&quot; &quot;(16.8,21.3]&quot; ## [111] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; ## [116] &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [121] &quot;[-1.02,3.46]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; 2.21 Tabla de frecuencias para una variable continua ## outcome ni fi Ni Fi ## 1 [-1.02,3.46] 8 0.06504065 8 0.06504065 ## 2 (3.46,7.92] 51 0.41463415 59 0.47967480 ## 3 (7.92,12.4] 26 0.21138211 85 0.69105691 ## 4 (12.4,16.8] 20 0.16260163 105 0.85365854 ## 5 (16.8,21.3] 18 0.14634146 123 1.00000000 2.22 Histograma El histograma es la gráfica de \\(n_i\\) o \\(f_i\\) Vs los resultados (bins). El histograma depende del tamaño de los contenedores. 2.23 Tabla de frecuencias para una variable continua 2.24 Histograma El histograma es la gráfica de \\(n_i\\) o \\(f_i\\) Vs los resultados (bins). El histograma depende del tamaño de los contenedores. 2.25 Gráfica de frecuencia acumulada: Variables continuas También podemos graficar la frecuencia acumulada Vs los resultados 2.26 Resumen estadístico Las estadísticas de resumen son números calculados a partir de los datos que nos dicen características importantes de las variables numéricas (categóricas o continuas). Valores límite: mínimo: el resultado mínimo observado máximo: el resultado máximo observado Valor central para los resultados El promedio se define como \\[\\bar{x}=\\frac{1}{N} \\sum_{j=1..N} x_j\\] donde \\(x_j\\) es la observación \\(j\\) (convexidad) de un total de \\(N\\). 2.27 Promedio La convexidad promedio se puede calcular directamente a partir de las observaciones \\(\\bar{x}= \\frac{1}{N}\\sum_j x_j\\) \\(= \\frac{1}{N}(7.97 + 18.23 + 12.27... + 6.80) = 10.19894\\) 2.28 Promedio (ordenado categóricamente) Para las variables ordenadas categóricamente, podemos usar la tabla de frecuencias para calcular el promedio ## outcome ni fi ## 1 0 41 0.33333333 ## 2 1 5 0.04065041 ## 3 2 37 0.30081301 ## 4 3 31 0.25203252 ## 5 4 9 0.07317073 La severidad promedio de la misofonía en el estudio también puede calcularse a partir de las frecuencias relativas de los resultados \\(\\bar{x}=\\frac{1}{N}\\sum_{i=1...N} x_j=\\frac{1}{N}\\sum_{i=1...M} x_i*n_ {i}=\\sum_{i=1...M} x_i*f_{i}\\) \\(=0*f_{0}+1*f_{1}+2*f_{2}+3*f_{3}+4*f_{4}=1,691057\\) (note el cambio de \\(N\\) a \\(M\\) en la segunda suma) 2.29 Promedio (ordenado categóricamente) En términos de los resultados de las variables ordenadas categóricas, el promedio se puede escribir como \\[\\bar{x}= \\sum_{i = 1...M} x_i f_i\\] de un total de \\(M\\) posibles resultados (número de niveles de gravedad). \\(\\bar{x}\\) es el valor central o centro de gravedad de los resultados. Como si cada resultado tuviera una densidad de masa dada por \\(f_i\\). 2.30 Promedio El promedio no es el resultado de una observación (experimento aleatorio). Es el resultado de una serie de observaciones (muestra). Describe el número donde se equilibran los valores observados. Por eso escuchamos, por ejemplo, que un paciente con una infección puede contagiar a una media de 2,5 personas. 2.31 Promedio 2.32 mediana Otra medida de centralidad es la mediana. La mediana \\(q_{0.5}\\) es el valor \\(x_p\\) \\[mediana(x)=q_{0.5}=x_p\\] debajo del cual encontramos la mitad de las observaciones \\[\\sum_{x\\leq x_p} 1 = \\frac{N}{2}\\] o en términos de frecuencias, es el valor \\(x_p\\) que hace que la frecuencia acumulada \\(F_p\\) sea igual a \\(0.5\\) \\[q_{0.5}=\\sum_{x\\leq x_p} f_x =F_p=0.5\\] 2.33 Mediana Vs Promedio Promedio: Centro de masa (compensa valores distantes) Mediana: La mitad de la masa 2.34 Dispersión Una medida importante de los resultados es su dispersión. Muchos experimentos pueden compartir su media, pero difieren en la dispersión de los valores. 2.35 Dispersión 2.36 Variación de la muestra La dispersión con respecto a la media se mide con el La varianza muestral: \\[s^2=\\frac{1}{N-1} \\sum_{j=1..N} (x_j-\\bar{x})^2\\] Mide la distancia cuadrada promedio de las observaciones al promedio. La razón de \\(N-1\\) se explicará cuando hablemos de inferencia. 2.37 Variación de la muestra En términos de frecuencias de variables categóricas y ordenadas \\[s^2=\\frac{N}{N-1} \\sum_{x} (x-\\bar{x})^2 f_x\\] \\(s^2\\) se puede considerar como el momento de inercia de las observaciones. 2.38 Desviación Estándar La raíz cuadrada de la varianza de la muestra se denomina desviación estándar \\(s\\). La desviación estándar del ángulo de convexidad es \\(s= [\\frac{1}{123-1}((7,97-10,19894)^2+ (18,23-10,19894)^2\\) \\(+ (12,27-10,19894)^2 + ...)]^{1/2} = 5,086707\\) La convexidad de la mandíbula se desvía de su media en \\(5,086707\\). 2.39 RIC La dispersión de datos también se puede medir con respecto a la mediana por el rango intercuartílico Definimos el primer cuartil como el valor \\(x_p\\) que hace que la frecuencia acumulada \\(F_p\\) sea igual a \\(0,25\\) \\[q_{0.25}=\\sum_{x\\leq x_p} f_x =F_p=0.25\\] También definimos el tercer cuartil como el valor \\(x_p\\) que hace que la frecuencia acumulada \\(F_p\\) sea igual a \\(0,75\\) \\[q_{0.75}=\\sum_{x\\leq x_p} f_x =F_p=0.75\\] 2.40 RIC La distancia entre el tercer cuartil y el primer cuartil se denomina rango intercuartílico (RIC) y captura el 50 % central de las observaciones 2.41 Diagrama de caja El rango intercuartílico, la mediana y el 5 % y el 95 % de los datos se pueden visualizar en un diagrama de caja, aquí los valores de los resultados están en el eje y. El IQR es la caja, la mediana es la línea del medio y los bigotes marcan el 5% y el 95% de los datos. "],["probabilidad.html", "Chapter 3 Probabilidad 3.1 Objetivo 3.2 Experimentos aleatorios 3.3 Probabilidad 3.4 Ejemplo 3.5 Ejemplo 3.6 Frecuencia relativa 3.7 En el infinito 3.8 Probabilidad frecuentista 3.9 Probabilidad clásica 3.10 Probabilidades clásicas y frecuentistas 3.11 Probabilidad 3.12 Espacio muestral 3.13 Ejemplos de espacios muestrales 3.14 Espacios muestrales discretos y continuos 3.15 Evento 3.16 Operaciones de eventos 3.17 Ejemplo de operaciones de eventos 3.18 Resultados 3.19 Definición de probabilidad 3.20 Propiedades de probabilidad 3.21 Regla de adición 3.22 Ejemplo de regla de adición 3.23 Diagrama de Venn 3.24 Tabla de probabilidades 3.25 Ejemplo de tabla de probabilidades 3.26 Tabla de contingencia 3.27 Ejemplo de tabla de contingencia 3.28 Estudio de misofonía 3.29 Tabla de contingencia para frecuencias 3.30 Mapa de calor 3.31 Variables continuas 3.32 Variables continuas 3.33 Gráfico de dispersión", " Chapter 3 Probabilidad 3.1 Objetivo Definición de probabilidad Álgebra de probabilidad Probabilidad conjunta 3.2 Experimentos aleatorios Observación y observación es la adquisición de un número o una característica de un experimento Salir Un resultado es una posible observación que es el resultado de un experimento. Experimento aleatorio Un experimento que da resultados diferentes cuando se repite de la misma manera. 3.3 Probabilidad La probabilidad de un resultado es una medida de cuán seguros estamos de observar ese resultado al realizar un experimento aleatorio. 0: Estamos seguros de que la observación no ocurrirá. 1: Estamos seguros de que la observación sucederá. 3.4 Ejemplo Considere las siguientes observaciones de un experimento aleatorio: 1 5 1 2 2 1 2 2 ¿Qué tan seguro estamos de obtener \\(2\\) en la siguiente observación? 3.5 Ejemplo La tabla de frecuencias es ## outcome ni fi ## 1 1 3 0.375 ## 2 2 4 0.500 ## 3 5 1 0.125 La frecuencia relativa \\(f_i\\) es un número entre \\(0\\) y \\(1\\). mide la proporción del total de observaciones que observamos un resultado particular. parece una medida de probabilidad razonable. Como \\(f_2=0.5\\) entonces estaríamos \\(50º%\\) seguros de obtener \\(2\\) en la siguiente repetición del experimento. 3.6 Frecuencia relativa ¿\\(f_i\\) es una buena medida de certeza? Digamos que repetimos el experimento 12 veces más: 1 5 1 2 2 1 2 2 3 1 1 3 3 1 6 3 5 6 4 4 La tabla de frecuencias es ahora ## outcome ni fi ## 1 1 6 0.3 ## 2 2 4 0.2 ## 3 3 4 0.2 ## 4 4 2 0.1 ## 5 5 2 0.1 ## 6 6 2 0.1 Aparecieron nuevos resultados y \\(f_2\\) ahora es \\(0.2\\), ahora estamos un \\(20\\%\\) seguros de obtener \\(2\\) en el próximo experimento la probabilidad no debería depender de \\(N\\) 3.7 En el infinito Digamos que repetimos el experimento 1000 veces: ## outcome ni fi ## 1 1 154 0.154 ## 2 2 165 0.165 ## 3 3 167 0.167 ## 4 4 189 0.189 ## 5 5 173 0.173 ## 6 6 152 0.152 Encontramos que \\(f_i\\) está convergiendo a un valor constante \\[lim_{N\\rightarrow \\infty} f_i = P_i\\] 3.8 Probabilidad frecuentista Llamamos Probabilidad \\(P_i\\) al límite cuando \\(N \\rightarrow \\infty\\) de la frecuencia relativa de observar el resultado \\(i\\) en un experimento aleatorio. Defendida por Venn (1876) La interpretación frecuentista de probabilidades se deriva de datos/experiencia (empírica). No observamos \\(P_i\\), observamos \\(f_i\\) Cuando estimamos \\(P_i\\) con \\(f_i\\) (normalmente cuando \\(N\\) es grande), escribimos: \\[\\hat{P_i}=f_i\\] 3.9 Probabilidad clásica Cada vez que un experimento aleatorio tiene \\(M\\) resultados posibles que son todos igualmente probables, la probabilidad de cada resultado es \\(\\frac{1}{M}\\). Defendida por Laplace (1814). Dado que cada resultado es igualmente probable, declaramos una completa ignorancia y lo mejor que podemos hacer es distribuir equitativamente la misma probabilidad para cada resultado. ¿Y si te dijera que nuestro experimento fue tirar un dado? entonces \\(P_2=1/6=0.166666\\). \\[P_i=lim_{N\\rightarrow \\infty} \\frac{n_i}{N}=\\frac{1}{M}\\] 3.10 Probabilidades clásicas y frecuentistas 3.11 Probabilidad La probabilidad es un número entre \\(0\\) y \\(1\\) que se asigna a cada miembro \\(E\\) de una colección de eventos de un espacio muestral (\\(S\\)) de un experimento aleatorio. \\[P(E) \\in (0,1)\\] donde \\(E \\in S\\) 3.12 Espacio muestral Empezamos razonando cuáles son todos los valores posibles (resultados) que podría dar un experimento aleatorio. Tenga en cuenta que no tenemos que observarlos en un experimento en particular: estamos usando razón/lógica y no observación. Definición: El conjunto de todos los resultados posibles de un experimento aleatorio se denomina espacio muestral del experimento El espacio muestral se denota como \\(S\\). 3.13 Ejemplos de espacios muestrales temperatura 35 y 42 grados centígrados niveles de azúcar: 70-80mg/dL el tamaño de un tornillo de una línea de producción: 70 mm-72 mm número de correos electrónicos recibidos en una hora: 0-100 un lanzamiento de dados: 1, 2, 3, 4, 5, 6 3.14 Espacios muestrales discretos y continuos Un espacio muestral es discreto si consiste en un conjunto de resultados finito o infinito numerable. Un espacio muestral es continuo si contiene un intervalo (ya sea de longitud finita o infinita) de numeros reales. 3.15 Evento Definición: Un evento es un subconjunto del espacio muestral de un experimento aleatorio. Es una colección de resultados. Ejemplos de eventos: El evento de una temperatura saludable: temperatura 37-38 grados centígrados El evento de producir un tornillo con un tamaño: de 71,5 mm El evento de recibir más de 4 correos electrónicos en una hora. El evento de obtener un número menor de 3 en el lanzamiento de un dado Un evento se refiere a un posible conjunto de resultados. 3.16 Operaciones de eventos Para dos eventos \\(A\\) y \\(B\\), podemos construir los siguientes eventos derivados: Complemento \\(A&#39;\\): el evento de no \\(A\\) Unión \\(A \\cup B\\): el evento de \\(A\\) o \\(B\\) Intersección \\(A \\cap B\\): el evento de \\(A\\) y \\(B\\) 3.17 Ejemplo de operaciones de eventos Tomar Evento \\(A:\\{1,2,3\\}\\) un número menor o igual a tres en el lanzamiento de un dado Evento \\(B:\\{2,4,6\\}\\) un número par en el lanzamiento de un dado Nuevos eventos: No menos de tres: \\(A&#39;:\\{4,5,6\\}\\) Menor o igual a tres o par: \\(A \\cup B: \\{1,2,3,4,6\\}\\) Menor o igual a tres y par \\(A \\cap B: \\{2\\}\\) 3.18 Resultados Los resultados son eventos que son mutuamente excluyentes Definición: Dos eventos denotados como \\(E_1\\) y \\(E_2\\), tales que \\[E_1\\cap E_2=\\emptyset\\] No pueden ocurrir al mismo tiempo. Ejemplo: El resultado de obtener \\(1\\) y el resultado de obtener \\(5\\) en el lanzamiento de un dado son mutuamente excluyentes: El evento de obtener \\(1\\) y \\(5\\) está vacío:\\[\\{1\\}\\cap \\{5\\}=\\emptyset\\] 3.19 Definición de probabilidad Una probabilidad es un número que se asigna a cada evento posible (\\(E\\)) de un espacio muestral (\\(S\\)) de un experimento aleatorio que cumple las siguientes propiedades: \\(P(S)=1\\) \\(0 \\leq P(E) \\leq 1\\) cuando \\(E_1\\cap E_2=\\emptyset\\) \\[P(E_1\\cup E_2) = P(E_1) + P(E_2)\\] Propuesto por Kolmogorov (1933) 3.20 Propiedades de probabilidad Kolmogorov dice que podemos construir una tabla de probabilidad (al igual que la tabla de frecuencia relativa) resultado Probabilidad \\(1\\) 1/6 \\(2\\) 1/6 \\(3\\) 1/6 \\(4\\) 1/6 \\(5\\) 1/6 \\(6\\) 1/6 \\(P(1\\cup 2\\cup ...\\cup 6)\\) 1 Como \\(\\{1,2,3,4,5,6\\}\\) son mutuamente excluyentes, entonces \\[P(S)=P(1\\cup 2\\cup ...\\cup 6) = P(1)+P(2)+ ...+P(n)=1\\] 3.21 Regla de adición Cuando \\(A\\) y \\(B\\) no son mutuamente excluyentes, entonces: \\[P(A\\cup B)=P(A) + P(B) - P(A\\cap B)\\] Donde \\(P(A)\\) y \\(P(B)\\) se denominan probabilidades marginales 3.22 Ejemplo de regla de adición Tomar Evento \\(A:\\{1,2,3\\}\\) un número menor o igual a tres en el lanzamiento de un dado Evento \\(B:\\{2,4,6\\}\\) un número par en el lanzamiento de un dado después: \\(P(A): P(1) + P(2) + P(3)=3/6\\) \\(P(G): P(2) + P(4) + P(6)=3/6\\) \\(P(A \\cap B): P(2) = 1/6\\) \\(P(A\\cup B)=P(A) + P(B) - P(A\\cap B)=3/6+3/6-1/6=5/6\\) Nota: \\(P(2)\\) aparece en \\(P(A)\\) y \\(P(B)\\) por eso lo restamos con la intersección 3.23 Diagrama de Venn Tenga en cuenta que siempre se puede descomponer el espacio muestral en conjuntos mutuamente excluyentes que involucran las intersecciones: \\(S=\\{A\\cap B, A \\cap B&#39;, A&#39;\\cap B, A&#39;\\cap B&#39;\\}\\) Marginales: \\(P(A)=P(A\\cap B&#39;) + P(A \\cap B)=2/6+1/6=3/6\\) \\(P(B)=P(A&#39;\\cap B) +P(A \\cap B)=2/6+1/6=3/6\\) 3.24 Tabla de probabilidades Veamos la tabla de probabilidades. resultado Probabilidad \\(A\\cap B\\) \\(P(A\\cap B)\\) \\(A\\cap B&#39;\\) \\(P(A\\cap B&#39;)\\) \\(A&#39;\\cap B\\) \\(P(A&#39;\\cap B)\\) \\(A&#39;\\cap B&#39;\\) \\(P(A&#39;\\cap B&#39;)\\) suma \\(1\\) 3.25 Ejemplo de tabla de probabilidades También escribimos \\(A \\cap B\\) como \\((A,B)\\) y lo llamamos la probabilidad conjunta de \\(A\\) y \\(B\\) En nuestro ejemplo: resultado Probabilidad \\((A,B)\\) \\(P(A, B)=1/6\\) \\((A,B&#39;)\\) \\(P(A,B&#39;)=2/6\\) \\((A&#39;, B)\\) \\(P(A&#39;, B)=2/6\\) \\((A&#39;, B&#39;)\\) \\(P(A&#39;, B&#39;)=1/6\\) suma \\(1\\) Nota: cada resultado tiene \\(dos\\) valores (uno para la característica del tipo \\(A\\) y otro para el tipo \\(B\\)) 3.26 Tabla de contingencia Podemos organizar la probabilidad de resultados conjuntos en una tabla de contingencia \\(B\\) \\(B&#39;\\) suma \\(A\\) \\(P(A, B )\\) \\(P(A, B&#39; )\\) \\(P(A)\\) \\(A&#39;\\) \\(P(A&#39;, B )\\) \\(P(A&#39;, B&#39; )\\) \\(P(A&#39;)\\) suma \\(P(B)\\) \\(P(B&#39;)\\) 1 marginales: \\(P(A)=P(A, B&#39;) + P(A, B)\\) \\(P(B)=P(A&#39;, B) +P(A, B)\\) 3.27 Ejemplo de tabla de contingencia Evento \\(A:\\{1,2,3\\}\\) un número menor o igual a tres en el lanzamiento de un dado Evento \\(B:\\{2,4,6\\}\\) un número par en el lanzamiento de un dado \\(B\\) \\(B&#39;\\) suma \\(A\\) \\(1/6\\) \\(2/6\\) \\(3/6\\) \\(A&#39;\\) \\(2/6\\) \\(1/6\\) \\(3/6\\) suma \\(3/6\\) \\(3/6\\) 1 Tres formas de la regla de la suma: \\(P(A\\cup B)\\)\\[=P(A) + P(B) - P(A\\cap B)\\] \\[=P(A \\cap B)+P(A\\cap B&#39;)+P(A&#39;\\cap B)\\] \\[=1-P(A&#39;\\cap B&#39;)\\] 3.28 Estudio de misofonía En el estudio de misofonía, se evaluó a los pacientes según la gravedad de su misofonía y si estaban deprimidos. El resultado de un experimento aleatorio es medir la gravedad de la misofonía y el estado de depresión de un paciente. La repetición del experimento aleatorio consistía en realizar las mismas dos mediciones en otro paciente. ## Misofonia.dic depresion.dic ## 1 4 1 ## 2 2 0 ## 3 0 0 ## 4 3 0 ## 5 0 0 ## 6 0 0 ## 7 2 0 ## 8 3 0 ## 9 0 1 ## 10 3 0 ## 11 0 0 ## 12 2 0 ## 13 2 1 ## 14 0 0 ## 15 2 0 ## 16 0 0 ## 17 0 0 ## 18 3 0 ## 19 3 0 ## 20 0 0 ## 21 3 0 ## 22 3 0 ## 23 2 0 ## 24 0 0 ## 25 0 0 ## 26 0 0 ## 27 4 1 ## 28 2 0 ## 29 2 0 ## 30 0 0 ## 31 2 0 ## 32 0 0 ## 33 0 0 ## 34 0 0 ## 35 3 0 ## 36 0 0 ## 37 2 0 ## 38 3 1 ## 39 2 0 ## 40 2 0 ## 41 0 0 ## 42 2 0 ## 43 3 0 ## 44 0 0 ## 45 0 0 ## 46 2 0 ## 47 2 0 ## 48 3 0 ## 49 3 0 ## 50 0 0 ## 51 0 0 ## 52 4 1 ## 53 3 0 ## 54 3 1 ## 55 2 1 ## 56 0 1 ## 57 2 0 ## 58 0 0 ## 59 0 0 ## 60 0 0 ## 61 2 0 ## 62 2 0 ## 63 0 0 ## 64 0 0 ## 65 2 0 ## 66 3 1 ## 67 0 0 ## 68 1 0 ## 69 3 0 ## 70 2 0 ## 71 4 1 ## 72 3 0 ## 73 2 1 ## 74 3 0 ## 75 0 1 ## 76 2 0 ## 77 3 0 ## 78 2 0 ## 79 4 1 ## 80 1 0 ## 81 2 0 ## 82 0 0 ## 83 2 0 ## 84 0 0 ## 85 2 0 ## 86 0 1 ## 87 2 0 ## 88 2 0 ## 89 4 1 ## 90 3 0 ## 91 0 1 ## 92 3 0 ## 93 0 0 ## 94 0 0 ## 95 0 0 ## 96 2 0 ## 97 2 0 ## 98 1 0 ## 99 3 0 ## 100 0 0 ## 101 0 0 ## 102 3 1 ## 103 2 0 ## 104 1 0 ## 105 3 0 ## 106 0 0 ## 107 4 1 ## 108 4 1 ## 109 2 0 ## 110 3 0 ## 111 3 0 ## 112 3 1 ## 113 0 0 ## 114 3 0 ## 115 2 0 ## 116 1 0 ## 117 2 0 ## 118 3 1 ## 119 3 0 ## 120 4 1 ## 121 2 0 ## 122 3 0 ## 123 2 0 3.29 Tabla de contingencia para frecuencias Para el número de observaciones \\(n_{i,j}\\) de cada resultado \\((x_i, y_i)\\), misofonía: \\(x\\in \\{0,1,2,3,4\\}\\) y depresión \\(y\\ en \\{0,1\\}\\) (no:\\(0\\), sí:\\(1\\)) ## ## Depression:0 Depression:1 ## Misophonia:4 0 9 ## Misophonia:3 25 6 ## Misophonia:2 34 3 ## Misophonia:1 5 0 ## Misophonia:0 36 5 para las frecuencias relativas \\(f_{i,j}\\) ## ## Depression:0 Depression:1 ## Misophonia:4 0.00000000 0.07317073 ## Misophonia:3 0.20325203 0.04878049 ## Misophonia:2 0.27642276 0.02439024 ## Misophonia:1 0.04065041 0.00000000 ## Misophonia:0 0.29268293 0.04065041 3.30 Mapa de calor La tabla de contingencia se puede trazar como un mapa de calor 3.31 Variables continuas En el estudio de misofonía también se midió la protrusión mandibular como posible factor cefalométrico de la enfermedad. ## Angulo_convexidad protusion.mandibular ## 1 7.97 13.00 ## 2 18.23 -5.00 ## 3 12.27 11.50 ## 4 7.81 16.80 ## 5 9.81 33.00 ## 6 13.50 2.00 ## 7 19.30 -3.90 ## 8 7.70 16.80 ## 9 12.30 8.00 ## 10 7.90 28.80 ## 11 12.60 3.00 ## 12 19.00 -7.90 ## 13 7.27 28.30 ## 14 14.00 4.00 ## 15 5.40 22.20 ## 16 8.00 0.00 ## 17 11.20 15.00 ## 18 7.75 17.00 ## 19 7.94 49.00 ## 20 16.69 5.00 ## 21 7.62 42.00 ## 22 7.02 28.00 ## 23 7.00 9.40 ## 24 19.20 -13.20 ## 25 7.96 23.00 ## 26 14.70 2.30 ## 27 7.24 25.00 ## 28 7.80 4.90 ## 29 7.90 92.00 ## 30 4.70 6.00 ## 31 4.40 17.00 ## 32 14.00 3.30 ## 33 14.40 10.30 ## 34 16.00 6.30 ## 35 1.40 19.50 ## 36 9.76 22.00 ## 37 7.90 5.00 ## 38 7.90 78.00 ## 39 7.40 9.30 ## 40 6.30 50.60 ## 41 7.76 18.00 ## 42 7.30 18.00 ## 43 7.00 10.00 ## 44 11.23 4.00 ## 45 16.00 13.30 ## 46 7.90 48.00 ## 47 7.29 23.50 ## 48 6.91 37.60 ## 49 7.10 15.00 ## 50 13.40 5.10 ## 51 11.60 -2.20 ## 52 -1.00 32.00 ## 53 6.00 25.00 ## 54 7.82 24.00 ## 55 4.80 33.60 ## 56 11.00 3.30 ## 57 9.00 31.50 ## 58 11.50 12.80 ## 59 16.00 3.00 ## 60 15.00 6.00 ## 61 1.40 21.40 ## 62 16.80 -10.00 ## 63 7.70 19.00 ## 64 16.14 32.00 ## 65 7.12 15.00 ## 66 -1.00 10.00 ## 67 17.00 -16.90 ## 68 9.26 2.00 ## 69 18.70 -10.10 ## 70 3.40 12.20 ## 71 21.30 -11.00 ## 72 7.50 5.20 ## 73 6.03 16.00 ## 74 7.50 5.80 ## 75 19.00 5.20 ## 76 19.01 13.00 ## 77 8.10 13.60 ## 78 7.80 16.10 ## 79 6.10 33.20 ## 80 15.26 4.00 ## 81 7.95 12.00 ## 82 18.00 -1.50 ## 83 4.60 18.30 ## 84 15.00 3.00 ## 85 7.50 15.80 ## 86 8.00 27.10 ## 87 16.80 -10.00 ## 88 8.54 25.00 ## 89 7.00 27.10 ## 90 18.30 -8.00 ## 91 7.80 12.00 ## 92 16.00 -8.00 ## 93 14.00 23.00 ## 94 12.30 5.00 ## 95 11.40 1.00 ## 96 8.50 18.90 ## 97 7.00 15.00 ## 98 7.96 22.00 ## 99 17.60 -3.50 ## 100 10.00 20.00 ## 101 3.50 12.20 ## 102 6.70 14.70 ## 103 17.00 -5.00 ## 104 20.26 -4.15 ## 105 6.64 11.00 ## 106 1.80 -4.00 ## 107 7.02 25.00 ## 108 2.46 35.00 ## 109 19.00 -5.00 ## 110 17.86 -30.00 ## 111 6.10 12.20 ## 112 6.64 19.00 ## 113 12.00 1.60 ## 114 6.60 20.00 ## 115 8.70 17.10 ## 116 14.05 24.00 ## 117 7.20 7.10 ## 118 19.70 -11.00 ## 119 7.70 21.30 ## 120 6.02 5.00 ## 121 2.50 12.90 ## 122 19.00 5.90 ## 123 6.80 5.80 3.32 Variables continuas En el estudio de misofonía también se midió la protrusión mandibular como posible factor cefalométrico de la enfermedad. 3.33 Gráfico de dispersión El histograma depende del tamaño del contenedor (píxel). Si el píxel es lo suficientemente pequeño como para contener una sola observación, el mapa de calor da como resultado un diagrama de dispersión El diagrama de dispersión es la ilustración de una tabla de contingencia para variables continuas cuando el contenedor (píxel) es lo suficientemente pequeño como para contener una sola observación (que consta de un par de valores). "],["probabilidad-condicional.html", "Chapter 4 Probabilidad condicional 4.1 Objetivo 4.2 Probabilidad conjunta 4.3 Diagnósticos 4.4 Prueba de diagnóstico 4.5 Observaciones 4.6 Tablas de contingencia 4.7 La probabilidad condicional 4.8 La probabilidad condicional 4.9 Tabla de contingencia condicional 4.10 Ejemplo de tabla de contingencia condicional 4.11 Regla de multiplicación 4.12 Rendimiento de diagnóstico 4.13 Regla de multiplicación 4.14 Tabla de contingencia en términos de probabilidades condicionales 4.15 Árbol condicional 4.16 Tabla de contingencia en términos de probabilidades condicionales 4.17 Regla de probabilidad total 4.18 Árbol condicional 4.19 Encontrar probabilidades inversas 4.20 Recuperar probabilidades conjuntas 4.21 Condicionales inversas 4.22 Teorema de Bayes 4.23 Ejemplo: teorema de Bayes 4.24 Ejemplo: teorema de Bayes 4.25 Independencia estadística 4.26 Independencia estadística 4.27 Independencia estadística 4.28 Independencia estadística 4.29 Productos de productos marginales 4.30 Ejemplo", " Chapter 4 Probabilidad condicional 4.1 Objetivo Probabilidad condicional Independencia Teorema de Bayes 4.2 Probabilidad conjunta La probabilidad conjunta de dos eventos \\(A\\) y \\(B\\) es \\[P(A,B)=P(A \\cap B)\\] Imaginemos un experimento aleatorio que mide dos tipos diferentes de resultados. altura y peso de un individuo: \\((h, w)\\) hora y lugar de una carga eléctrica: \\((p, t)\\) una tirada de dos dados: (\\(n_1\\),\\(n_2\\)) cruzar dos semáforos en verde: (\\(\\bar{R_1}\\), \\(\\bar{R_2}\\)) En muchos casos, nos interesa saber si los valores de un resultado condicionan los valores del otro. 4.3 Diagnósticos Consideremos una herramienta de diagnóstico Queremos encontrar el estado de un sistema (s): inadecuado (sí) adecuado (no) con una prueba (t): positivo negativo Probamos una batería para saber cuánto tiempo puede vivir. Tensamos un cable para saber si resiste llevar cierta carga. Realizamos una PCR para ver si alguien está infectado. 4.4 Prueba de diagnóstico Consideremos diagnosticar una infección con una nueva prueba. Estado de infección: si (infectado) no (no infectado) Prueba: positivo negativo 4.5 Observaciones Cada individuo es un experimento aleatorio con dos medidas: (Infección, Prueba) Asunto Infección Prueba \\(s_1\\) si positivo \\(s_2\\) no negativo \\(s_3\\) si positivo    \\(s_i\\) no positivo*       \\(s_n\\) si negativo* 4.6 Tablas de contingencia Por el número de observaciones de cada resultado Infección: sí Infección: no suma Test: positivo 18 12 30 Test: negativo 30 300 330 suma 48 312 360 Para las frecuencias relativas, si \\(N&gt;&gt;0\\) tomaremos \\(f_{i,j}=\\hat{P}(x_i, y_j)\\) Infección: sí Infección: no suma Test: positivo 0.05 0.0333 0.0833 Test: negativo 0.0833 0.833 0.9166 suma 0.133 0.866 1 4.7 La probabilidad condicional Pensemos primero en términos de aquellos que están infectados Dentro de los que están infectados (sí), ¿cuál es la probabilidad de dar positivo? Sensibilidad (tasa de verdaderos positivos) \\[\\hat{P}(positivo|sí)=\\frac{n_{positivo,sí}}{n_{sí}}\\] \\[=\\frac{\\frac{n_{positivo,sí}}{N}}{\\frac{n_{sí}}{N}}=\\frac{f_{positivo,sí}}{f_{sí}} \\] Por lo tanto, en el límite, esperamos tener una probabilidad del tipo \\[P(positivo|sí)=\\frac{P(positivo, sí)}{P(sí)}=\\frac{P(positivo \\cap sí)}{P(sí)}\\] 4.8 La probabilidad condicional Definición: La probabilidad condicional de un evento B dado un evento A, denotado como \\(P(A|B)\\), es \\[P(A|B) = \\frac{P(A\\cap B)}{P(B)}\\] se puede probar que la probabilidad condicional satisface los axiomas de probabilidad. la probabilidad condicional es la probabilidad bajo el espacio muestral dado por \\(B\\): \\(S_B\\). 4.9 Tabla de contingencia condicional Infección: Sí Infección: No Test: positivo P(positivo | sí) P(positivo | no) Test: negativo P(negativo | sí) P(negativo | no) suma 1 1 Tasa de verdaderos positivos (Sensibilidad): La probabilidad de dar positivo si se tiene la enfermedad \\(P(positivo|sí)\\) Tasa de verdaderos negativos (Especificidad): La probabilidad de dar negativo si no se tiene la enfermedad \\(P(negativo|no)\\) Tasa de falsos positivos: La probabilidad de dar positivo si no se tiene la enfermedad \\(P(positivo|no)\\) Tasa de falsos negativos: la probabilidad de dar negativo si se tiene la enfermedad \\(P(negativo|sí)\\) 4.10 Ejemplo de tabla de contingencia condicional Tomando las frecuencias como estimaciones de las probabilidades, entonces Infección: Sí Infección: No Test: positivo 18/48 = 0.375 12/312 = 0.038 Test: negativo 30/48 = 0.625 300/312 =0.962 suma 1 1 Nuestra herramienta de diagnóstico tiene baja sensibilidad (0.375) pero alta especificidad (0.962). 4.11 Regla de multiplicación Ahora imaginemos la situación real, donde queremos obtener la probabilidad conjunta de la probabilidad condicional Se (realizaron) PCR para coronavirus [https://www.nejm.org/doi/full/10.1056/NEJMp2015897] en personas en el hospital que estamos seguros de estar infectadas. Este test tiene una sensibilidad del 70%. También se ha probado en el laboratorio en condiciones sin infección con una especificidad del 96 %. Un estudio de prevalencia en España mostró que \\(P(sí)=0.05\\), \\(P(no)=0.95\\) antes del verano. Con estos datos, ¿cuál era la probabilidad de que una persona seleccionada al azar de la población diera positivo y estuviera infectada: \\(P(sí \\cap positivo)=P(sí, positivo)\\)? 4.12 Rendimiento de diagnóstico Para estudiar el rendimiento de una nueva prueba diagnóstica: selecciona muestras que son inadecuadas (enfermedad: sí) y aplica la prueba, tratando de encontrar su sensibilidad: \\(P(positivo|sí)\\) (\\(0.70\\) para PCR) selecciona muestras que son adecuadas (enfermedad: no) y aplica la prueba, tratando de encontrar su especificidad: \\(P(negativo|no)\\) (\\(0.96\\) para PCR) Infección: Sí Infección: No Test: positivo P(positivo|sí)=0.7 P(positivo|no)=0.06 Test: negativo P(negativo|sí)=0.3 P(negativo|no)=0.94 suma 1 1 De esta matriz, ¿podemos obtener \\(P(sí, positivo)\\)? 4.13 Regla de multiplicación ¿Cómo se recupera la probabilidad conjunta de la probabilidad condicional? Para dos eventos \\(A\\) y \\(B\\) tenemos la regla de la multiplicación \\[P(A, B) = P(A|B) P(B)\\] que se sigue de la definición de probabilidad condicional. 4.14 Tabla de contingencia en términos de probabilidades condicionales Infección: Sí Infección: No suma Test: positivo P(positivo | sí)P(sí) P(positivo | no)P(no) P(positivo) Test: negativo P(negativo | sí)P(sí) P(negativo | no) P(no) P(negativo) suma P(sí) P(no) 1 Por ejemplo, la probabilidad de dar \\(positivo\\) y estar infectado \\(sí\\): \\(P(positivo, sí)=P(positivo \\cap sí) = P(positivo|sí) P(sí)\\) 4.15 Árbol condicional 4.16 Tabla de contingencia en términos de probabilidades condicionales Infección: sí Infección: no suma Test: positivo 0.035 0.057 0.092 Test: negativo 0.015 0.893 0.908 suma 0.05 0.95 1 \\(P(positivo,si)= 0.035\\) Pero también encontramos la probabilidad marginal de ser positivo: \\(P(positivo)=0.092\\) 4.17 Regla de probabilidad total Infección: Sí Infección: No suma Test: positivo P(positivo | sí)P(sí) P(positivo | no)P(no) P(positivo) Test: negativo P(negativo | sí)P(sí) P(negativo | no) P(no) P(negativo) suma P(sí) P(no) 1 Cuando escribimos las marginales desconocidas en términos de sus probabilidades condicionales, lo llamamos regla de probabilidad total \\(P(positivo)=P(positivo|sí)P(sí)+P(positivo|no)P(no)\\) \\(P(negativo)=P(negativo|sí)P(sí)+P(negativo|no)P(no)\\) 4.18 Árbol condicional Regla de probabilidad total para la marginal de \\(B\\): ¿De cuántas maneras puedo obtener el resultado \\(B\\)? \\(P(B)=P(B|A)P(A)+P(B|A&#39;)P(A&#39;)\\) 4.19 Encontrar probabilidades inversas De la tabla de contingencia condicional Infección: Sí Infección: No Test: positivo P(positivo | sí) P(positivo | no) Test: negativo P(negativo | sí) P(negativo | no) suma 1 1 ¿Cómo podemos calcular la probabilidad de estar infectado si la prueba da positivo: \\(P(sí|positivo)\\)? 4.20 Recuperar probabilidades conjuntas Recuperamos la tabla de contingencia para probabilidades conjuntas Infección: Sí Infección: No suma Test: positivo P(positivo | sí)P(sí) P(positivo | no)P(no) P(positivo) Test: negativo P(negativo | sí)P(sí) P(negativo | no) P(no) P(negativo) suma P(sí) P(no) 1 4.21 Condicionales inversas Calculamos las probabilidades condicionales para la prueba: \\[P(infección|prueba)=\\frac{P(prueba|infección)P(infección)}{P(prueba)}\\] Infección: Sí Infección: No suma Test: positivo P(sí|positivo) P(sin|positivo) 1 Test: negativo P(sí|negativo) P(sin|negativo) 1 Por ejemplo: \\[P(sí|positivo)=\\frac{P(positivo|sí)P(sí)}{P(positivo)}\\] como normalmente no tenemos \\(P(positivo)\\), usamos la regla de probabilidad total en el denominador \\[P(sí|positivo)=\\frac{P(positivo|sí)P(sí)}{P(positivo|sí)P(sí)+P(positivo|no)P(no)}\\] 4.22 Teorema de Bayes La expresion: \\[P(sí|positivo)=\\frac{P(positivo|sí)P(sí)}{P(positivo|sí)P(sí)+P(positivo|no)P(no)}\\] se llama teorema de Bayes Teorema Si \\(E1, E2, ..., Ek\\) son \\(k\\) eventos mutuamente excluyentes y exhaustivos y \\(B\\) es cualquier evento, \\[P(Ei|B)=\\frac{P(B|Ei)P(Ei)}{P(B|E1)P(E1) +...+ P(B|Ek)P(Ek)} \\] Permite invertir los condicionales: \\[P(B|A) \\rightarrow P(A|B)\\] O diseñe una prueba \\(B\\) en condición controlada \\(A\\) y luego utilícela para inferir la probabilidad de la condición cuando la prueba es positiva. 4.23 Ejemplo: teorema de Bayes Teorema de Bayes: \\[P(sí|positivo) = \\frac{P(positivo|sí) P(sí)}{P(positivo|sí)P(sí)+P(positivo|no)P(no)}\\] sabemos: \\(P(positivo|sí)=0.70\\) \\(P(positivo|no)=1- P(negativo|no)=0.06\\) la probabilidad de infección y no infección en la población: \\(P(sí)=0.05\\) y \\(P(no)=1-P(sí)=0.95\\). Por lo tanto: \\[P(sí|positivo)=0.47\\] Las pruebas no son tan buenas para confirmar infecciones. 4.24 Ejemplo: teorema de Bayes Apliquémoslo ahora a la probabilidad de no estar infectado si la prueba es negativa. \\[P(no|negativo) = \\frac{P(negativo|no) P(no)}{P(negativo|no) P(no)+P(negativo|sí)P(sí)}\\] La sustitución de todos los valores da \\[P(no|negativo)=0.98\\] Las pruebas son buenas para descartar infecciones. 4.25 Independencia estadística En muchas aplicaciones, queremos saber si el conocimiento de un evento condiciona el resultado de otro evento. hay casos en los que queremos saber si los eventos no están condicionados 4.26 Independencia estadística Considere los conductores para los cuales medimos sus fallas superficiales y si su capacidad de conducción es defectuosa. Las probabilidades conjuntas estimadas son fallas (F) sin fallas (F) suma defectuoso (D) \\(0.005\\) \\(0.045\\) \\(0.05\\) sin defectos (D) \\(0.095\\) \\(0.855\\) \\(0.95\\) suma \\(0.1\\) \\(0.9\\) 1 donde, por ejemplo, la probabilidad conjunta de \\(F\\) y \\(D\\) es \\(P(D,F)=0.005\\) Las probabilidades marginales son \\(P(D)=P(D, F) + P(D, F&#39;)=0.05\\) \\(P(F)=P(D, F) + P(D&#39;, F)= 0.1\\). 4.27 Independencia estadística ¿Cuál es la probabilidad condicional de observar un conductor defectuoso si tiene un defecto? F F D P(D|F) = 0.05 P(D|F)=0.05 D P(D|F)=0.95 P(D|F)=0.95 suma 1 1 ¡Las probabilidades marginales y condicionales son las mismas! \\(P(D|F)=P(D|F&#39;)=P(D)\\) \\(P(D&#39;|F)=P(D&#39;|F&#39;)=P(D&#39;)\\) La probabilidad de observar un conductor defectuoso no depende de haber observado o no un defecto. \\[P(D) = P(D|F)\\] 4.28 Independencia estadística Dos eventos \\(A\\) y \\(B\\) son estadísticamente independientes si \\(P(A|B)=P(A)\\); \\(A\\) es independiente de \\(B\\) \\(P(B|A)=P(B)\\); \\(B\\) es independiente de \\(A\\) y por la regla de la multiplicación, su probabilidad conjunta es \\(P(A\\cap B)=P(A|B)P(B)=P(A)P(B)\\) la multiplicación de sus probabilidades marginales. 4.29 Productos de productos marginales F F suma D \\(0.005\\) \\(0.045\\) \\(0.05\\) D \\(0.095\\) \\(0.855\\) \\(0.95\\) suma \\(0.1\\) \\(0.9\\) 1 Confirme que todas las entradas de la matriz son el producto de los marginales. Por ejemplo: \\(P(F)P(D)= P(D \\cap F)\\) \\(P(D&#39;)P(F&#39;)=P(D&#39; \\cap F&#39;)\\) 4.30 Ejemplo Resultados de lanzar dos monedas: \\(S={(H,H), (H,T), (T,H), (T,T)}\\) H T suma H \\(1/4\\) \\(1/4\\) \\(1/2\\) T \\(1/4\\) \\(1/4\\) \\(1/2\\) suma \\(1/2\\) \\(1/2\\) 1 Obtener cara en la primera moneda no condiciona obtener cruz en el resultado de la segunda moneda \\(P(T|H)=P(T)=1/2\\) la probabilidad de obtener cara y después cruz es el producto de cada resultado independiente \\(P(H, T)=P(H)*P(T)=1/4\\) "],["variables-aleatorias-discretas.html", "Chapter 5 Variables aleatorias discretas 5.1 Objetivo 5.2 ¿Cómo asignamos valores de probabilidad a los resultados? 5.3 Variable aleatoria 5.4 Variable aleatoria 5.5 Eventos de observar una variable aleatoria 5.6 Probabilidad de variables aleatorias 5.7 Funciones de probabilidad 5.8 Funciones de probabilidad 5.9 Funciones de probabilidad 5.10 Funciones de probabilidad 5.11 Ejemplo: función de masa de probabilidad 5.12 Tabla de probabilidad para resultados igualmente probables 5.13 Tabla de probabilidad para \\(X\\) 5.14 Ejemplo 5.15 Ejemplo 5.16 Probabilidades y frecuencias 5.17 Probabilidades y frecuencias relativas 5.18 Media y Varianza 5.19 Media y Varianza 5.20 Media 5.21 Ejemplo: Media 5.22 Media y Promedio 5.23 Variación 5.24 Ejemplo: Varianza 5.25 Funciones de \\(X\\) 5.26 Ejemplo: Varianza sobre el origen 5.27 Distribución de probabilidad 5.28 Ejemplo: distribución de probabilidad 5.29 Probability distribution 5.30 Función de probabilidad y Distribución de probabilidad 5.31 Función de probabilidad y Distribución de probabilidad 5.32 Cuantiles 5.33 Resumen", " Chapter 5 Variables aleatorias discretas 5.1 Objetivo Variables aleatorias Función de probabilidad Media y varianza Distribución de probabilidad 5.2 ¿Cómo asignamos valores de probabilidad a los resultados? 5.3 Variable aleatoria Definición: Una variable aleatoria es una función que asigna un número real a cada resultado en el espacio muestral de un experimento aleatorio. Por lo general, una variable aleatoria es el valor de la medida de interés que se realiza en un experimento aleatorio. Una variable aleatoria puede ser: Discreta (nominal, ordinal) Continua (intervalo, relación) 5.4 Variable aleatoria Un valor (o resultado) de una variable aleatoria es uno de los números posibles que la variable puede tomar en un experimento aleatorio. Escribimos la variable aleatoria en mayúsculas. Ejemplo: Si \\(X \\in \\{0,1\\}\\), entonces decimos que \\(X\\) es una variable aleatoria que puede tomar los valores \\(0\\) o \\(1\\). Observación de una variable aleatoria Una observación es la adquisición del valor de una variable aleatoria en un experimento aleatorio Ejemplo: 1 0 0 1 0 1 0 1 1 El número en negrita es una observación de \\(X\\) 5.5 Eventos de observar una variable aleatoria \\(X=1\\) es el evento de observar la variable aleatoria \\(X\\) con valor \\(1\\) \\(X=2\\) es el evento de observar la variable aleatoria \\(X\\) con valor \\(2\\)  En general: \\(X=x\\) es el evento de observar la variable aleatoria \\(X\\) con valor \\(x\\) (pequeño \\(x\\)) Dos valores cualesquiera de una variable aleatoria definen dos eventos mutuamente excluyentes. 5.6 Probabilidad de variables aleatorias Nos interesa asignar probabilidades a los valores de una variable aleatoria. Ya hemos hecho esto para los dados: \\(X \\in \\{1,2,3,4,5,6\\}\\) (interpretación clásica de probabilidad) \\(X\\) Probabilidad \\(1\\) \\(P(X=1)=1/6\\) \\(2\\) \\(P(X=2)=1/6\\) \\(3\\) \\(P(X=3)=1/6\\) \\(4\\) \\(P(X=4)=1/6\\) \\(5\\) \\(P(X=5)=1/6\\) \\(6\\) \\(P(X=6)=1/6\\) 5.7 Funciones de probabilidad Podemos escribir la tabla de probabilidad graficarla o escribirla como la función \\[f(x)=P(X=x)=1/6\\] 5.8 Funciones de probabilidad Podemos crear cualquier tipo de función de probabilidad si respetamos las reglas de probabilidad: 5.9 Funciones de probabilidad Para una variable aleatoria discreta \\(X \\in \\{x_1 , x_2 , .. , x_M\\}\\) , una función de masa de probabilidad siempre es positiva \\(f(x_i)\\geq 0\\) se utiliza para calcular probabilidades \\(f(x_i)=P(X=x_i)\\) y su suma sobre todos los valores de la variable es \\(1\\): \\(\\sum_{i=1}^M f(x_i)=1\\) 5.10 Funciones de probabilidad Tenga en cuenta que la definición de \\(X\\) y su función de masa de probabilidad es general sin referencia a ningún experimento. Las funciones viven en el espacio modelo (abstracto). \\(X\\) y \\(f(x)\\) son objetos abstractos que pueden o no asignarse a un experimento Tenemos la libertad de construirlos como queramos siempre que respetemos su definición. Tienen algunas propiedades que se derivan exclusivamente de su definición. 5.11 Ejemplo: función de masa de probabilidad Considere la siguiente variable aleatoria \\(X\\) sobre los resultados resultado \\(X\\) \\(a\\) 0 \\(b\\) 0 \\(c\\) 1.5 \\(d\\) 1.5 \\(e\\) 2 \\(f\\) 3 Si cada resultado es igualmente probable, ¿cuál es la función de masa de probabilidad de \\(x\\)? 5.12 Tabla de probabilidad para resultados igualmente probables resultado Probabilidad (resultado) \\(a\\) \\(1/6\\) \\(b\\) \\(1/6\\) \\(c\\) \\(1/6\\) \\(d\\) \\(1/6\\) \\(e\\) \\(1/6\\) \\(f\\) \\(1/6\\) 5.13 Tabla de probabilidad para \\(X\\) \\(X\\) \\(f(x)=P(X=x)\\) \\(0\\) \\(P(X=0)=2/6\\) \\(1.5\\) \\(P(X=1.5)=2/6\\) \\(2\\) \\(P(X=2)=1/6\\) \\(3\\) \\(P(X=3)=1/6\\) Podemos calcular, por ejemplo, las siguientes probabilidades de eventos en los valores de \\(X\\) \\(P(X&gt;3)\\) \\(P(X=0\\, \\cup\\, X=2 )\\) \\(P(X \\leq 2)\\) 5.14 Ejemplo Modelo de probabilidad: Considere el siguiente experimento: En una urna ponga \\(8\\) bolas y: marque \\(1\\) bola con el número \\(-2\\) marque \\(2\\) bolas con el número \\(-1\\) marque \\(2\\) bolas con el número \\(0\\) marque \\(2\\) bolas con el número \\(1\\) marque \\(1\\) bola con el número \\(2\\) experimento: Tome una bola y lea el número. \\(X\\) \\(P(X=x)\\) \\(-2\\) \\(1/8=0.125\\) \\(-1\\) \\(2/8=0.25\\) \\(0\\) \\(2/8=0.25\\) \\(1\\) \\(2/8=0.25\\) \\(2\\) \\(1/8=0.125\\) 5.15 Ejemplo Considere otro experimento en el que no sabemos qué hay en la urna anterior. Sacamos una bola \\(30\\) veces, escribimos su númeror y la devolvemos a la urna. no sabemos cuáles son los eventos primarios con iguales probabilidades. y estimamos la función de masa de probabilidad a partir de las frecuencias relativas observadas para cada variable aleatoria \\(X\\) \\(f_i\\) \\(-2\\) \\(0.132\\) \\(-1\\) \\(0.262\\) \\(0\\) \\(0.240\\) \\(1\\) \\(0.248\\) \\(2\\) \\(0.118\\) 5.16 Probabilidades y frecuencias Para calcular las frecuencias relativas \\(f_i\\) trenemos que repetir el experimento \\(N\\) veces (tenemos que volver a poner la bola en la urna cada vez) y al final calcular \\[f_i=n_i/N\\] Estamos suponiendo que: \\[lim_{N \\rightarrow \\infty} f_i = f(x_i)=P(X=x_i)\\] 5.17 Probabilidades y frecuencias relativas En este ejemplo, sabemos el modelo de probabilidad \\(f(x)=P(X=x)\\) por diseño. Nunca observamos \\(f(x)\\) Podemos usar frecuencias relativas para estimar las probabilidades \\[f_i = \\hat{f}(x_i)=\\hat{P}(X=x_i)\\] (\\(f_i\\) depende de \\(N\\)) 5.18 Media y Varianza Las funciones de masa de probabilidad \\(f(x)\\) tienen dos propiedades principales su centro su dispersión Podemos preguntar, ¿Alrededor de qué valores de \\(X\\) se concentró la probabilidad? ¿Qué tan dispersos son los valores de \\(X\\) en relación a sus probabilidades? 5.19 Media y Varianza 5.20 Media Recuerde que el promedio en términos de las frecuencias relativas de los valores de \\(x_i\\) (resultados ordenados categóricos) se puede escribir como \\[\\bar{x}= \\sum_{i=1}^M x_i \\frac{n_i}{N}=\\sum_{i=1}^M x_i f_i\\] Definición La media (\\(\\mu\\)) o valor esperado de una variable aleatoria discreta \\(X\\), \\(E(X)\\), con función de masa \\(f(x)\\) está dada por \\[\\mu = E(X)= \\sum_{i=1}^M x_i f(x_i)\\] Es el centro de gravedad de las probabilidades: El punto donde se equilibran las cargas de probabilidad 5.21 Ejemplo: Media ¿Cuál es la media de \\(X\\) si su función de masa de probabilidad \\(f(x)\\) está dada por \\(P(X=0)=1/16\\) \\(P(X=1)=4/16\\) \\(P(X=2)=6/16\\) \\(P(X=3)=4/16\\) \\(P(X=4)=1/16\\) \\[\\mu =E(X)=\\sum_{i=1}^m x_i f(x_i)\\] \\(E(X)=\\)0 * 1/16 + 1 * 4/16 + 2 * 6/16 + 3 * 4/16 + 4 * 1/16 =2 5.22 Media y Promedio La media \\(\\mu\\) es el centro de gravedad de función de masa de probabilidad y no cambia Por ejemplo de \\(X\\) \\(P(X=x)\\) \\(-2\\) \\(1/8=0.125\\) \\(-1\\) \\(2/8=0.25\\) \\(0\\) \\(2/8=0.25\\) \\(1\\) \\(2/8=0.25\\) \\(2\\) \\(1/8=0.125\\) El promedio \\(\\bar{x}\\) es el centro de gravedad de las observaciones (frequencias relativas) y cambia de acuerdo a los datos. Por ejemplo de \\(X\\) \\(f_i\\) \\(-2\\) \\(0.132\\) \\(-1\\) \\(0.262\\) \\(0\\) \\(0.240\\) \\(1\\) \\(0.248\\) \\(2\\) \\(0.118\\) 5.23 Variación En términos similares definimos la distancia media al cuadrado de la media: Definición La varianza, escrita como \\(\\sigma^2\\) o \\(V(X)\\), de una variable aleatoria discreta \\(X\\) con función de masa \\(f(x)\\) está dada por \\[\\sigma^2 = V(X)= \\sum_{i=1}^M (x_i-\\mu)^2 f(x_i)\\] \\(\\sigma=\\sqrt{V(X)}\\) se llama la desviación estándar de la variable aleatoria Piense en ello como el momento de inercia de las probabilidades sobre la media. 5.24 Ejemplo: Varianza ¿Cuál es la varianza de \\(X\\) si su función de masa de probabilidad \\(f(x)\\) está dada por \\(P(X=0)=1/16\\) \\(P(X=1)=4/16\\) \\(P(X=2)=6/16\\) \\(P(X=3)=4/16\\) \\(P(X=4)=1/16\\) \\[\\sigma^2 =V(X)=\\sum_{i=1}^m (x_i-\\mu)^2 f(x_i)\\] \\(V(X)=\\)(0-2)\\(^2\\)* 1/16 + (1-2)\\(^2\\)* 4/16 + (2- 2)\\(^2\\)* 6/16 + (3-2)\\(^2\\)* 4/16 + (4-2)\\(^2\\)* 1/ 16 = 1 \\[V(X)=\\sigma^2=1\\] \\[\\sigma=1\\] 5.25 Funciones de \\(X\\) Definición Para cualquier función \\(h\\) de una variable aleatoria \\(X\\), con función de masa \\(f(x)\\), su valor esperado viene dado por \\[ E[h(X)]= \\sum_{i=1}^M h(x_i) f(x_i) \\] Esta es una definición importante que nos permite probar tres propiedades importantes de la mediana y la varianza: La media de una función lineal es la función lineal de la media: \\[E(a\\times X +b)= a\\times E(X) +b\\] para \\(a\\) y \\(b\\) escalares (números) . La varianza de una función lineal de \\(X\\) es:\\[V(a\\times X +b)= a^2\\times V(X)\\] La varianza sobre el origen es la varianza sobre la media más la media al cuadrado: \\[E(X^2)=V(X)+E(X)^2\\] 5.26 Ejemplo: Varianza sobre el origen ¿Cuál es la varianza \\(X\\) sobre el origen, \\(E(X^2)\\), si su función de masa de probabilidad \\(f(x)\\) está dada por \\(P(X=0)=1/16\\) \\(P(X=1)=4/16\\) \\(P(X=2)=6/16\\) \\(P(X=3)=4/16\\) \\(P(X=4)=1/16\\) \\[E(X^2) =\\sum_{i=1}^m x_i^2 f(x_i)\\] \\(E(X^2)=\\)(0)\\(^2\\)* 1/16 + (1)\\(^2\\)* 4/16 + (2) \\(^2\\)* 6/16 + (3)\\(^2\\)* 4/16 + (4)\\(^2\\)* 1/16 =5 También podemos verificar: \\[E(X^2)=V(X)+E(X)^2\\] \\(5=1+2^2\\) 5.27 Distribución de probabilidad Definición: La función de distribución de probabilidad se define como \\[F(x)=P(X\\leq x)=\\sum_{x_i\\leq x} f(x_i) \\] Esa es la probabilidad acumulada hasta un valor dado \\(x\\) \\(F(x)\\) satisface: \\(0\\leq F(x) \\leq 1\\) Si \\(x \\leq y\\), entonces \\(F(x) \\leq F(y)\\) 5.28 Ejemplo: distribución de probabilidad Para la función de masa de probabilidad: \\(f(0)=P(X=0)=1/16\\) \\(f(1)=P(X=1)=4/16\\) \\(f(2)=P(X=2)=6/16\\) \\(f(3)=P(X=3)=4/16\\) \\(f(4)=P(X=4)=1/16\\) La distribución de probabilidad es: \\[ F(x)= \\begin{cases} 1/16,&amp; \\text{if } 0 \\leq x &lt; 1\\\\ 5/16,&amp; 1\\leq x &lt; 2\\\\ 11/16,&amp; 2\\leq x &lt; 3\\\\ 15/16,&amp; 4\\leq x &lt; 5\\\\ 16/16,&amp; x \\leq 5\\\\ \\end{cases} \\] Para \\(X \\in \\mathbb{Z}\\) 5.29 Probability distribution 5.30 Función de probabilidad y Distribución de probabilidad Calcule la función de probabilidad de masa de la siguiente distribución de probabilidad: \\(F(0)=1/16\\), \\(F(1)=5/16\\), \\(F(2)=11/16\\), \\(F(3)=15/16\\), \\(F(4)= 16/16\\), Trabajemos al revés. \\(f(0)=F(0)=1/16\\) \\(f(1)=F(1)-f(0)=5/32-1/32=4/16\\) \\(f(2)=F(2)-f(1)-f(0)=F(2)-F(1)=6/16\\) \\(f(3)=F(3)-f(2)-f(1)-f(0)=F(3)-F(2)=4/16\\) \\(f(4)=F(4)-F(3)=1/16\\) 5.31 Función de probabilidad y Distribución de probabilidad La distribución de probabilidad es otra forma de especificar la probabilidad de una variable aleatoria. \\[f(x_i)=F(x_i)-F(x_{i-1})\\] con \\[f(x_1)=F(x_1)\\] para \\(X\\) tomando valores en \\(x_1 \\leq x_2 \\leq ... \\leq x_n\\) 5.32 Cuantiles Definimos el q-cuantil como el valor \\(x_{p}\\) bajo el cual hemos acumulado q*100% de la probabilidad \\[q=\\sum_{i=1}^p f(x_i) = F (x_p)\\] La mediana es valor \\(x_m\\) tal que \\(q=0.5\\) \\[F(x_{m})=0.5\\] El cuantil \\(0.05\\) es el valor \\(x_{r}\\) tal que \\(q=0.05\\) \\[F(x_{r})=0.05\\] El cuantil \\(0,25\\) es el primer cuartil o sea el valor \\(x_{s}\\) tal que \\(q=0,25\\) \\[F(x_{s})=0,25\\] 5.33 Resumen nombres de cantidades modelo (no observado) datos (observados) función de masa de probabilidad // frecuencia relativa \\(f(x_i)=P(X=x_i)\\) \\(f_i=\\frac{n_i}{N}\\) distribución de probabilidad // frecuencia relativa acumulada \\(F(x_i)=P(X \\leq x_i)\\) \\(F_i=\\sum_{k\\leq i} f_k\\) media // promedio \\(\\mu=E(X)=\\sum_{i=1}^M x_i f(x_i)\\) \\(\\bar{x}=\\sum_{j=1}^N x_j/N\\) varianza // varianza de la muestra \\(\\sigma^2=V(X)=\\sum_{i=1}^M (x_i-\\mu)^2 f(x_i)\\) \\(s^2=\\sum_{j=1}^N (x_j-\\bar{x})^2/(N-1)\\) desviación estándar // muestra sd \\(\\sigma=\\sqrt{V(X)}\\) \\(s\\) varianza sobre el origen // 2º momento muestral \\(E(X^2)=\\sum_{i=1}^M x_i^2 f(x_i)\\) \\(m_2= \\sum_{j=1}^N x_j^2/n\\) Tenga en cuenta que: \\(i=1...M\\) es un resultado de la variable aleatoria \\(X\\). \\(j=1...N\\) es una observación de la variable aleatoria \\(X\\). Propiedades: \\(\\sum_{i=1...N} f(x_i)=1\\) \\(f(x_i)=F(x_i)-F(x_{i-1})\\) \\(E(a\\times X +b)= a\\times E(X) +b\\); para los escalares \\(a\\) y \\(b\\). \\(V(a\\times X +b)= a^2\\times V(X)\\) \\(E(X^2)=V(X)+E(X)^2\\) "],["variables-aleatorias-continuas.html", "Chapter 6 Variables aleatorias continuas 6.1 Objetivo 6.2 Variable aleatoria continua 6.3 Variable aleatoria continua 6.4 Variable aleatoria continua 6.5 Variable aleatoria continua 6.6 Variable aleatoria continua 6.7 Área total bajo la curva 6.8 Área bajo la curva 6.9 Área bajo la curva 6.10 Distribución de probabilidad 6.11 Distribución de probabilidad 6.12 Distribución de probabilidad 6.13 Distribución de probabilidad 6.14 Gráficos de probabilidad 6.15 Gráficos de probabilidad 6.16 Media 6.17 Media 6.18 Varianza 6.19 Funciones de \\(X\\) 6.20 Ejemplo", " Chapter 6 Variables aleatorias continuas 6.1 Objetivo Función de densidad de probabilidad Media y varianza Distribución de probabilidad 6.2 Variable aleatoria continua ¿Qué sucede con las variables aleatorias continuas? Reconsideremos el ángulo de convexidad de los pacientes con misofonía (Sección 2.21). Para esta variabler redefinimos los resultados como pequeños intervalos regulares (bins) y calculamos la frecuencia relativa para cada uno de ellos como hicimos en el caso discreto. ## outcome ni fi ## 1 [-1.02,3.46] 8 0.06504065 ## 2 (3.46,7.92] 51 0.41463415 ## 3 (7.92,12.4] 26 0.21138211 ## 4 (12.4,16.8] 20 0.16260163 ## 5 (16.8,21.3] 18 0.14634146 6.3 Variable aleatoria continua Consideremos nuevamente que sus frecuencias relativas son las probabilidades cuando \\(N \\rightarrow \\infty\\) \\[f_i=\\frac{n_i}{N} \\rightarrow f(x_i)=P(X=x_i)\\] La probabilidad depende ahora de la longitud de los bins \\(\\Delta x\\). Si hacemos los contenedores cada vez más pequeños, las frecuencias se hacen más pequeñas y, por lo tanto, \\(P(X=x_i) \\rightarrow 0\\) cuando \\(\\Delta x \\rightarrow 0\\), porque \\(n_i \\rightarrow 0\\) ## outcome ni fi ## 1 [-1.02,0.115] 2 0.01626016 ## 2 (0.115,1.23] 0 0.00000000 ## 3 (1.23,2.34] 3 0.02439024 ## 4 (2.34,3.46] 3 0.02439024 ## 5 (3.46,4.58] 2 0.01626016 ## 6 (4.58,5.69] 4 0.03252033 ## 7 (5.69,6.8] 11 0.08943089 ## 8 (6.8,7.92] 34 0.27642276 ## 9 (7.92,9.04] 12 0.09756098 ## 10 (9.04,10.2] 4 0.03252033 ## 11 (10.2,11.3] 3 0.02439024 ## 12 (11.3,12.4] 7 0.05691057 ## 13 (12.4,13.5] 2 0.01626016 ## 14 (13.5,14.6] 6 0.04878049 ## 15 (14.6,15.7] 4 0.03252033 ## 16 (15.7,16.8] 8 0.06504065 ## 17 (16.8,18] 4 0.03252033 ## 18 (18,19.1] 9 0.07317073 ## 19 (19.1,20.2] 3 0.02439024 ## 20 (20.2,21.3] 2 0.01626016 6.4 Variable aleatoria continua Definimos una cantidad en un punto \\(x\\) que es la cantidad de probabilidad por unidad de distancia que encontraríamos en un contenedor infinitesimal \\(dx\\) en \\(x\\) \\[f(x)= \\frac{P(x\\leq X \\leq x+dx)}{dx}\\] \\(f(x)\\) se llama la función de densidad de probabilidad. Por tanto, la probabilidad de observar \\(x\\) entre \\(x\\) y \\(x+dx\\) está dada por \\[P(x\\leq X \\leq x+dx)= f(x) dx\\] 6.5 Variable aleatoria continua Definición Para una variable aleatoria continua \\(X\\), una función de densidad de probabilidad es tal que La función es positiva: \\(f(x) \\geq 0\\) La probabilidad de observar un valor dentro de un intervalo es el área bajo la curva: \\(P(a\\leq X \\leq b)=\\int_{a}^{b} f(x) dx\\) La probabilidad de observar cualquier valor es 1: \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\) 6.6 Variable aleatoria continua La función de densidad de probabilidad es un paso adelante en la abstracción de probabilidades: sumamos el límite continuo (\\(dx \\rightarrow 0\\)). Todas las propiedades de las probabilidades se traducen en términos de densidades (\\(\\sum \\rightarrow \\int\\)). La asignación de probabilidades a una variable aleatoria se puede realizar con argumentos de equiprobabilidad (clásicos). Las densidades son cantidades matemáticas que algunas asignarán a experimentos y otras no. ¿Qué densidad corresponderá mejor a mi experimento? 6.7 Área total bajo la curva Ejemplo: toma la densidad de probabilidad que podría describir la variable aleatoria que mide dónde cae una gota de lluvia en una canaleta de lluvia de \\(100cm\\) de longitud. \\[ f(x)= \\begin{cases} \\frac{1}{100},&amp; \\text{si } x\\in (0,100)\\\\ 0,&amp; si \\, no \\end{cases} \\] Entonces la probabilidad de cualquier observación es el área total bajo la curva \\(P(-\\infty\\leq X \\leq \\infty)= \\int_{-\\infty}^{\\infty} f(x) dx = 100*0.01= 1\\) 6.8 Área bajo la curva La probabilidad de observar \\(x\\) en un intervalo es el área bajo la curva dentro del intervalo \\(P(20 \\leq X \\leq 60) = \\int_{20}^{60} f(x) dx = (60-20)*0.01=0.4\\) 6.9 Área bajo la curva En general, \\(f(x)\\) debe satisfacer: \\(0 \\leq P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) dx \\leq 1\\) 6.10 Distribución de probabilidad La probabilidad acumulada hasta \\(b\\) está definida por la distribución de probabilidad \\(F\\) \\(F(b) = P(X \\leq b)=\\int_{-\\infty}^bf(x)dx\\) La probabilidad acumulada hasta \\(a\\) es \\(F(a) = P(X \\leq a)\\) 6.11 Distribución de probabilidad La probabilidad entre \\(a\\) y \\(b\\) está definida por la distribución de probabilidad \\(F\\) \\(P(a\\leq X \\leq b) = \\int_a^b f(x)dx=F(b)-F(a)\\) 6.12 Distribución de probabilidad La distribución de probabilidad de una variable aleatoria continua se define como \\(F(a)=P(X\\leq a) =\\int_{-\\infty} ^a f(x)dx\\) con las propiedades que: Está entre \\(0\\) y \\(1\\): \\(F(-\\infty)= 0\\) y \\(F(\\infty)=1\\) Siempre aumenta: si \\(a\\leq b\\) entonces \\(F(a)\\leq F(b)\\) Se puede utilizar para calcular probabilidades: \\(P(a \\leq X \\leq b)=F(b)-F(a)\\) Recupera la densidad de probabilidad: \\(f(x)=\\frac{dF(x)}{dx}\\) Usamos distribuciones de probabilidad para calcular probabilidades de una variable aleatoria en intervalos 6.13 Distribución de probabilidad Para la funcion de densidad uniforme: \\[ f(x)= \\begin{cases} \\frac{1}{100},&amp; \\text{si } x\\in (0,100)\\\\ 0,&amp; si \\,no \\end{cases} \\] La distribución de probabilidad es \\[ F(a)= \\begin{cases} 0,&amp; a \\leq 0 \\\\ \\frac{a}{100},&amp; \\text{si } a\\in [0,100)\\\\ 1, &amp; 100 &lt; a \\\\ \\\\ \\end{cases} \\] 6.14 Gráficos de probabilidad La probabilidad \\(P(20&lt;X&lt;60)\\) es el área bajo la curva de densidad 6.15 Gráficos de probabilidad La probabilidad \\(P(20&lt;X&lt;60)\\) es la diferencia en valores de distribución 6.16 Media Como en el caso discreto, la media mide el centro de la distribución Definición Supongamos que \\(X\\) es una variable aleatoria continua con función de probabilidad densidad \\(f(x)\\). El valor medio o esperado de \\(X\\), denotado como \\(\\mu\\) o \\(E(X)\\), es \\[\\mu=E(X)=\\int_{-\\infty}^\\infty x f(x) dx\\] Es la versión continua del centro de masa. 6.17 Media \\[ f(x)= \\begin{cases} \\frac{1}{100},&amp; \\text{si } x\\in (0,100)\\\\ 0,&amp; si \\, no \\end{cases} \\] \\(E(X)=50\\) 6.18 Varianza Como en el caso discreto, la varianza mide la dispersión con respecto a la media Definición Supongamos que \\(X\\) es una variable aleatoria continua con función de densidad de probabilidad \\(f(x)\\). La varianza de \\(X\\), denotada como \\(\\sigma^2\\) o \\(V(X)\\), es \\[\\sigma^2=V(X)=\\int_{-\\infty}^\\infty (x-\\mu)^2 f(x) dx\\] 6.19 Funciones de \\(X\\) Definición Para cualquier función \\(h\\) de una variable aleatoria \\(X\\), con función de masa \\(f(x)\\), su valor esperado viene dado por \\[E[h(X)]= \\int_{-\\infty}^{\\infty} h(x) f(x)dx\\] Y tenemos las mismas propiedades que en el caso discreto La media de una función lineal es la función lineal de la media: \\[E(a\\times X +b)= a\\times E(X) +b\\] para \\(a\\) y \\(b\\) escalares. La varianza de una función lineal de \\(X\\) es:\\[V(a\\times X +b)= a^2\\times V(X)\\] La varianza sobre el origen es la varianza sobre la media más la media al cuadrado: \\[E(X^2)=V(X)+E(X)^2\\] 6.20 Ejemplo para la densidad de probabilidad \\[ f(x)= \\begin{cases} \\frac{1}{100},&amp; \\text{si } x\\in (0,100)\\\\ 0,&amp; si \\, no \\end{cases} \\] calcule la media calcule la varianza usando \\(E(X^2)=V(X)+E(X)^2\\) calcule \\(P(\\mu-\\sigma\\leq X \\leq \\mu+\\sigma)\\) ¿Cuáles son el primer y tercer cuartiles? "],["modelos-de-probabilidad-para-variables-aleatorias-discretas.html", "Chapter 7 Modelos de probabilidad para variables aleatorias discretas 7.1 Objetivo 7.2 Función de probabilidad 7.3 Modelo de probabilidad 7.4 Modelos paramétricos 7.5 Distribución uniforme (un parámetro) 7.6 Distribución uniforme 7.7 Distribución uniforme (dos parámetros) 7.8 Distribución uniforme (dos parámetros) 7.9 Distribución uniforme 7.10 Distribución uniforme (dos parámetros) 7.11 Parámetros y Modelos 7.12 Parámetros y Modelos 7.13 Ensayo de Bernoulli 7.14 Ensayo de Bernoulli 7.15 Ensayo de Bernoulli 7.16 Ensayo de Bernoulli 7.17 Distribución binomial 7.18 Ejemplos: distribución binomial 7.19 Distribución binomial 7.20 Distribución binomial 7.21 Distribución binomial: Definición 7.22 Distribución binomial: Media y Varianza 7.23 Ejemplo 1 7.24 Ejemplo 1 7.25 Ejemplo 2 7.26 Distribución binomial 7.27 Distribución binomial negativa 7.28 Distribución binomial negativa 7.29 Distribución binomial negativa 7.30 Media y Varianza 7.31 Distribución geométrica 7.32 Ejemplo 7.33 Ejemplo 7.34 Ejemplo 7.35 Ejemplos 7.36 Distribución binomial negativa 7.37 Resumen de modelos de probabilidad", " Chapter 7 Modelos de probabilidad para variables aleatorias discretas 7.1 Objetivo Modelos probabilidad: Funciones de probabilidad uniforme y de Bernoulli Funciones de probabilidad binomial y binomial negativa 7.2 Función de probabilidad Una función de masa de probabilidad de una variable aleatoria discreta \\(X\\) con valores posibles \\(x_1 , x_2 , .. , x_M\\) es cualquier función tal que es positiva: \\(f(x_i)\\geq 0\\) Nos permite calcular probabilidades: \\(f(x_i)=P(X=x_i)\\) La probabilidad de observar algún resultado es \\(1\\) \\(\\sum_{i=1}^M f(x_i)=1\\) Propiedades: Tendencia central: \\(E(X)= \\sum_{i=1}^M x_i f(x_i)\\) Dispersión: \\(V(X)= \\sum_{i=1}^M (x_i-\\mu)^2 f(x_i)\\) Son objetos abstractos con propiedades generales que pueden o no describir un proceso natural o de ingeniería. 7.3 Modelo de probabilidad Un modelo de probabilidad es una función de masa de probabilidad que puede representar las probabilidades de un experimento aleatorio. Ejemplos: \\(f(x)=P(X=x)=1/6\\) representa la probabilidad de los resultados de una tirada de dados. La función de masa de probabilidad \\(X\\) \\(f(x)\\) \\(-2\\) \\(1/8\\) \\(-1\\) \\(2/8\\) \\(0\\) \\(2/8\\) \\(1\\) \\(2/8\\) \\(2\\) \\(1/8\\) Representa la probabilidad de sacar una bola de una urna donde hay dos bolas por etiqueta: \\(-1, 0, 1\\) y una bola por etiqueta: \\(-2, 2\\). 7.4 Modelos paramétricos Cuando realizamos un experimento aleatorio y no sabemos las probabilidades de los resultados: Siempre podemos formular el modelo dado por las frecuencias relativas: \\(\\hat{P}(X=x_i)=f_i\\) (donde \\(i=1...M\\)). Necesitamos encontrar \\(M\\) números cada uno dependiendo de \\(N\\). En muchos casos: Podemos formular funciones de probabilidad \\(f(x)\\) que dependen solamente de muy pocos números. Ejemplo: Un experimento aleatorio con \\(M\\) resultados igualmente probables tiene una función de masa de probabilidad: \\[f(x)=P(X=x)=1/M\\] Solo necesitamos saber \\(M\\). Los números que necesitamos saber para determinar completamente una función de probabilidad se llaman parámetros. 7.5 Distribución uniforme (un parámetro) Definición Una variable aleatoria \\(X\\) con resultados \\(\\{1,...M\\}\\) tiene una distribución uniforme discreta si todos sus resultados \\(M\\) tienen la misma probabilidad \\[f(x)=\\frac{1}{M}\\] Con media y varianza: \\(E(X)= \\frac{M+1}{2}\\) \\(V(X)= \\frac{M^2-1}{12}\\) Nota: \\(E(X)\\) y \\(V(X)\\) también son parámetros. Si conocemos alguno de ellos, entonces podemos determinar completamente la distribución. \\[f(x)=\\frac{1}{2E(X)-1}\\] 7.6 Distribución uniforme 7.7 Distribución uniforme (dos parámetros) Presentemos un nuevo modelo de probabilidad uniforme con dos parámetros: los resultados mínimo y máximo. Si la variable aleatoria toma valores en \\(\\{a, a+1, ...b\\}\\), donde \\(a\\) y \\(b\\) son números enteros y todos los resultados son igualmente probables, entonces \\[f(x)=\\frac{1}{b-a+1}\\] como \\(M=b-a+1\\). Entonces decimos que \\(X\\) se distribuye uniformemente entre \\(a\\) y \\(b\\) y escribimos \\[X \\rightarrow Unif(a,b)\\] 7.8 Distribución uniforme (dos parámetros) Ejemplo: ¿Cuál es la probabilidad de observar a un niño de una edad particular en una escuela primaria (si todas las clases tienen la misma cantidad de niños)? Del experimento sabemos: \\(a=6\\) y \\(b=11\\) entonces \\[X \\rightarrow Unif(a=7, b=11)\\] eso es \\[f(x)=\\frac{1}{6}\\] para \\(x\\in \\{6,7,8,9,10,11\\}\\), y \\(0\\) en caso contrario 7.9 Distribución uniforme El modelo de probabilidad de una variable aleatoria \\(X\\) \\[f(x)=\\frac{1}{b-a+1}\\] para \\(x \\in \\{a, a+1, ...b\\}\\) tiene media y varianza: \\(E(X)= \\frac{b+a}{2}\\) \\(V(X)= \\frac{(b-a+1)^2-1}{12}\\) (Cambiar variables \\(X=Y+a-1\\), \\(y \\in \\{1,...M\\}\\)) Podemos especificar \\(a\\) y \\(b\\) o \\(E(X)\\) y \\(V(X)\\). En nuestro ejemplo: \\(E(X)=(11+6)/2=8.5\\) \\(V(X)=(6^2-1)/12=2.916667\\) 7.10 Distribución uniforme (dos parámetros) 7.11 Parámetros y Modelos Un modelo es una función particular \\(f(x)\\) que describe nuestro experimento Si el modelo es una función conocida que depende de algunos parámetros, al cambiar el valor de los parámetros producimos una familia de modelos El conocimiento de \\(f(x)\\) se reduce al conocimiento del valor de los parámetros Idealmente, el modelo y los parámetros son interpretables Ejemplo: Modelo: Los datos de nuestro experimento se producen mediante un proceso aleatorio en el que cada edad tiene la misma probabilidad de ser observada. Parámetros: \\(a\\) es la edad mínima, \\(E(X)\\) es la edad esperada son propiedades físicas del experimento. 7.12 Parámetros y Modelos Ejemplo: Una familia de modelos obtenidos a partir de distribuciones uniformes de dos parámetros cambiando las varianzas y manteniendo una media constante (\\(E(X)=8.5\\)). Da como resultado cambiar los resultados mínimo y máximo. Nota: solo un modelo tiene sentido para nuestro experimento (solo un modelo puede representar las edades de los niños en una escuela). Podemos pensar en familias que cambian solo en la media, solo el mínimo o solo el máximo 7.13 Ensayo de Bernoulli Intentemos avanzar desde el caso de probabilidad igual y supongamos un modelo con dos resultados (\\(A\\) y \\(B\\)) que tienen probabilidades desiguales Ejemplos: Anotar el sexo de un paciente que acude a urgencias de un hospital (\\(A:masculino\\) y \\(B:femenino\\)). Registrar si una máquina fabricada está defectuosa o no (\\(A:defectuosa\\) y \\(B:buena\\)). Dar en el blanco (\\(A:éxito\\) y \\(B:fracaso\\)). Transmitiendo un píxel correctamente (\\(A:sí\\) y \\(B:no\\)). En estos ejemplos, la probabilidad del resultado \\(A\\) suele ser desconocida. 7.14 Ensayo de Bernoulli Introduciremos la probabilidad de un resultado (\\(A\\)) como el parámetro del modelo: resultado A (éxito): tiene probabilidad \\(p\\) (parámetro) resultado B (fracaso): tiene una probabilidad \\(1-p\\) O podemos escribir la función de masa de probabilidad de \\(K\\) tomando valores \\(\\{0, 1\\}\\) para \\(A\\) y \\(B\\) \\[ f(k)= \\begin{cases} 1-p,&amp; k=0\\, (evento\\, B)\\\\ p,&amp; k=1\\, (evento\\, A) \\end{cases} \\] o más en breve \\[f(k; p)=(1-p)^{1-k} p^k\\] para \\(k=(0,1)\\) Solo necesitamos saber \\(p\\). 7.15 Ensayo de Bernoulli Una variable de Bernoulli \\(K\\) con resultados \\(\\{0, 1\\}\\) tiene una función de masa de probabilidad \\[f(k; p)=(1-p)^{1-k} p^k\\] Con media y varianza: \\(E(K)=p\\) \\(V(K)=(1-p)p\\) Nota: La probabilidad del resultado \\(A\\) es el parámetro \\(p\\) que es lo mismo que \\(f(0)=P(X=0)\\). Como \\(p\\) suele ser desconocido, normalmente lo estimamos por la frecuencia relativa (más sobre esto en las secciones de inferencia): \\(\\hat{p}=f_A=\\frac{n_A}{N}\\) 7.16 Ensayo de Bernoulli 7.17 Distribución binomial Cuando estamos interesados en aprender sobre un ensayo de Bernoulli en particular Repetimos el ensayo de Bernoulli \\(N\\) veces y contamos cuantas veces obtuvimos \\(A\\) (\\(n_A\\)). Definimos una variable aleatoria \\(X=n_A\\) tomando valores \\(x \\in {0,1,...N}\\) Ahora preguntamos por la probabilidad de observar \\(x\\) eventos de tipo \\(A\\) en la repetición de \\(n\\) ensayos independientes de Bernoulli, cuando la probabilidad de observar \\(A\\) es \\(p\\). \\[P(X=x)=f(x)=?\\] 7.18 Ejemplos: distribución binomial Anotar el sexo de \\(n=10\\) pacientes que acuden a urgencias de un hospital. ¿Cuál es la probabilidad de que \\(x=6\\) los pacientes sean hombres cuando \\(p=0,9\\)? Intentar \\(n=5\\) veces para dar en el blanco (\\(A:éxito\\) y \\(B:fracaso\\)). ¿Cuál es la probabilidad de que alcance el objetivo \\(x=5\\) veces cuando normalmente lo hago el \\(20\\%\\) de las veces (\\(p=0,25\\))? Transmitiendo \\(n=100\\) píxeles correctamente (\\(A:sí\\) y \\(B:no\\)). ¿Cuál es la probabilidad de que \\(x=2\\) píxeles sean errores, cuando la probabilidad de error es \\(p=0,1\\)? 7.19 Distribución binomial ¿Cuál es la probabilidad de observar \\(X=4\\) errores al transmitir \\(4\\) píxeles, si la probabilidad de error es \\(p\\)? Considere las variables aleatorias \\(4\\): \\(K_1\\), \\(K_2\\), \\(K_3\\) y \\(K_4\\) que registran si se ha cometido un error en el \\(1^{o}\\), \\(2^{o}\\), \\(3^{r}\\) y \\(4^{o}\\) píxel. Por lo tanto \\(k_i\\) toma valores \\(\\{correcto:0; error:1\\}\\) \\(X=\\sum_{i=1}^4 K_i\\) toma valores \\(\\{0,1,2,3,4\\}\\) Entonces la probabilidad de observar \\(4\\) errores es: \\(P(X=4)=P(1,1,1,1)=p*p*p*p=p^4\\) porque \\(K_i\\) son independientes. La probabilidad de observar \\(0\\) errores es: \\(P(X=0)=P(0,0,0,0)=(1-p)(1-p)(1-p)(1-p)=(1-p)^4\\) La probabilidad de errores de \\(3\\) es: \\(P(X=3)=P(0,1,1,1)+P(1,0,1,1)+P(1,1,0,1)+P(1,1,1,0)=4p^3(1-p)^1\\) 7.20 Distribución binomial Por lo tanto, la probabilidad de \\(x\\) errores es \\[ f(x)= \\begin{cases} 1*p^4(1-p)^4 &amp; x=0 \\\\ 4*p^3(1-p)^3,&amp; x=1 \\\\ 6*p^2(1-p)^2,&amp; x=2 \\\\ 4*p^1(1-p)^1,&amp; x=3 \\\\ 1*p^0(1-p)^0 &amp; x=4 \\\\ \\end{cases} \\] o más en breve \\[f(x)=\\binom 4 x p^x(1-p)^{4-x}\\] para \\(x=0,1,2,3,4\\) donde \\(\\binom 4 x\\) es el número de posibles resultados (transmisiones de \\(4\\) píxeles) con \\(x\\) errores. 7.21 Distribución binomial: Definición La función de probabilidad binomial es la función de masa de probabilidad de observar \\(x\\) resultados de tipo \\(A\\) en \\(n\\) ensayos independientes de Bernoulli, donde \\(A\\) tiene la misma probabilidad \\(p\\) en cada ensayo. La función está dada por \\(f(x)=\\binom n x p^x(1-p)^{n-x}\\), \\(x=0,1,...n\\) \\(\\binom n x= \\frac{n!}{x!(n-x)!}\\) se denomina coeficiente binomial y da el número de formas en que se pueden obtener \\(x\\) eventos de tipo \\(A\\) en un conjunto de \\(n\\). Cuando una variable \\(X\\) tiene una función de probabilidad binomial decimos que se distribuye binomialmente y escribimos \\[X\\rightarrow Bin(n,p)\\] donde \\(n\\) y \\(p\\) son parámetros. 7.22 Distribución binomial: Media y Varianza La media y la varianza de \\(X\\rightarrow Bin(n,p)\\) son \\(E(X)=np\\) \\(V(X)=np(1-p)\\) Dado que \\(X\\) es la suma de \\(n\\) variables independientes de Bernoulli \\(E(X)=E(\\sum_{i=1}^n K_i)=np\\) y \\(V(X)=V(\\sum_{i=1}^n K_i)=n(1-p)p\\) Ejemplo: El valor esperado para el número de errores en la transmisión de 4 píxeles es \\(np=4*0.1=0.4\\) cuando la probabilidad de error es \\(0.1\\). La varianza es \\(n(1-p)p=0.36\\) Recuerde: Podemos especificar los parámetros \\(n\\) y \\(p\\), o los parámetros \\(E(X)\\) y \\(V(X)\\) 7.23 Ejemplo 1 Ahora respondamos: ¿Cuál es la probabilidad de observar \\(4\\) de al transmitir \\(4\\) píxeles, si la probabilidad de error es de \\(0.1\\)? Dado que estamos repitiendo una prueba de Bernoulli \\(n=4\\) veces y contando el número de eventos de tipo \\(A\\) (errores), cuando \\(P(A)=p=0.1\\) entonces \\[X \\rightarrow Bin(n=4, p=0.1)\\] Eso es \\[f(x)=\\binom 4 x 0.1^x(1-0.1)^{4-x}\\] 7.24 Ejemplo 1 Queremos calcular: \\(P(X=4)=f(4)=\\binom 4 4 0.1^4 0.9^{0}=0.1^4=10^{-4}\\) En R dbinom(4,4,0.1) También podemos calcular: \\(P(X=2)=\\binom 4 2 0.1^2 0.9^2=0.0486\\) En R dbinom(2,4,0.1) 7.25 Ejemplo 2 ¿Cuál es la probabilidad de observar al menos \\(8\\) votantes del partido de gobierno en una encuesta electoral de tamaño \\(10\\), si la probabilidad de un voto positivo es de \\(0.9\\)? Para este caso \\[X \\rightarrow Bin(n=8, p=0.9)\\] Eso es \\[f(x)=\\binom {10} x 0.9^x(0.1)^{4-x}\\] Queremos calcular: \\(P(X\\le 8)=F(8)= \\sum_{i=1..8} f(x_i)=0.2639011\\) en R pbinom(8,10, 0.9) 7.26 Distribución binomial 7.27 Distribución binomial negativa Ahora imaginemos que estamos interesados en contar los píxeles bien transmitidos antes de que ocurra un número dado de errores. Digamos que podemos tolerar \\(r\\) errores en la transmisión. Experimento: Supongamos que realizamos ensayos de Bernoulli hasta que observamos que el resultado \\(A\\) aparece \\(r\\) veces. Variable aleatoria: Contamos el número de eventos \\(B\\) Ejemplo: ¿Cuál es la probabilidad de observar \\(y\\) píxeles bien transmitidos (\\(B\\)) antes de \\(r\\) errores (\\(A\\))? 7.28 Distribución binomial negativa Primero encontremos la probabilidad de una transmisión en particular con \\(y\\) número de píxeles correctos (\\(B\\)) y \\(r\\) número de errores (\\(A\\)). \\((0,0,1,., 0,1,...0,1)\\) (hay \\(y\\) ceros y \\(r\\) unos) Observamos \\(y\\) píxeles correctos en un total de \\(y + r\\) intentos. Por lo tanto \\(P(0,0,1,., 0,1,...0,1)=(1-p)^yp^r\\) (Recuerda: \\(p\\) es la probabilidad de error) ¿Cuántas transmisiones pueden tener \\(y\\) píxeles correctos antes de \\(r\\) errores? Nota: El último bit es fijo (marca el final de la transmisión) El número total de transmisiones con \\(y\\) número de píxeles correctos (\\(B\\)) que podemos obtener en \\(y + r-1\\) intentos es: \\(\\binom {y + r-1} y\\) 7.29 Distribución binomial negativa Por lo tanto, la probabilidad de observar \\(y\\) eventos de tipo \\(B\\) antes de \\(r\\) eventos de tipo \\(A\\) (con probabilidad \\(p\\)) es \\[P(Y=y)=f(y)=\\binom {y+r-1} y (1-p)^yp^r\\] para \\(y=0,1,...\\) Entonces decimos que \\(Y\\) sigue una distribución binomial negativa y escribimos \\[Y\\rightarrow NB(r,p)\\] donde \\(r\\) y \\(p\\) son parámetros que representan la tolerancia y la probabilidad de un solo error. 7.30 Media y Varianza Una variable aleatoria con \\(Y\\rightarrow NB(r,p)\\) tiene media: \\(E(Y)= r\\frac{1-p}{p}\\) varianza: \\(V(Y)= r\\frac{1-p}{p^2}\\) 7.31 Distribución geométrica Llamamos distribución geométrica a la distribución binomial negativa con \\(r=1\\) La probabilidad de observar \\(B\\) eventos antes de observar el primer evento de tipo \\(A\\) es \\[P(Y=y)=f(y)= (1-p)^yp\\] \\[Y\\rightarrow Geom(p)\\] con media media: \\(E(Y)= \\frac{1-p}{p}\\) varianza: \\(V(Y)= \\frac{1-p}{p^2}\\) 7.32 Ejemplo Un sitio web tiene tres servidores. Un servidor opera a la vez y solo cuando falla una solicitud se utiliza otro servidor. Si se sabe que la probabilidad de que falle una solicitud es \\(p=0.0005\\), entonces ¿Cuál es el número esperado de solicitudes exitosas antes de que los tres servidores fallen? 7.33 Ejemplo Ya que estamos repitiendo un ensayo de Bernoulli hasta que se observan \\(r=3\\) eventos de tipo \\(A\\) (cada uno con \\(P(A)=p=0.0005\\)) y estamos contando el número de eventos de tipo \\(B\\) (errores) después \\[Y \\rightarrow NB(r=3, p=0.0005)\\] Por lo tanto, el número esperado de solicitudes antes de que el sistema falle es: \\(E(Y)=r\\frac{1-p}{p}=3\\frac{1-0.0005}{0.0005}=5997\\) Tenga en cuenta que para enviar este número de solicitures hemos enviado un total de \\(6000=5997+3\\) 7.34 Ejemplo ¿Cuál es la probabilidad de tratar con éxito como máximo \\(5\\) solicitudes antes de que el sistema falle? Recuerde la función de distribución: \\(F(y)=P(Y\\leq 5)\\) \\(F(5)=P(Y\\leq 5)=\\Sigma_{y=0}^5 f(y)\\) \\(=\\sum_{y=0}^5\\binom {y+2} y 0.9995^y 0.0005^r\\) \\(=\\binom{2} 0 0.9995^0 0.0005^3 +\\binom{3} 1 0.9995^1 0.0005^3\\) \\(+\\binom{4} 2 0.9995^2 0.0005^3 +\\binom{5} 3 0.9995^3 0.0005^3\\) \\(+\\binom {6} 4 0.9995^4 0.0005^3 +\\binom {7} 5 0.9995^5 0.0005^3\\) \\(= 6.9\\times 10^{-9}\\) En R pnbinom(5,3,0.0005) 7.35 Ejemplos Con la función de probabilidad binomial negativa: \\[f(y)=\\binom {y+r-1} y (1-p)^yp^r\\] Ahora podemos responder preguntas como: ¿Cuál es la probabilidad de observar \\(10\\) píxeles correctos antes de \\(2\\) errores, si la probabilidad de error es \\(0.1\\)? \\(f(10; r=2, p=0.1)=0.03835463\\) en R dnbinom(10, 2, 0.1) ¿Cuál es la probabilidad de que entren \\(2\\) chicas antes que \\(4\\) chicos si la probabilidad de que entre una chica es de \\(0.5\\)? \\(f(2; r=4, p=0.5)=0.15625\\) en R dnbinom(2, 4, 0.5) 7.36 Distribución binomial negativa 7.37 Resumen de modelos de probabilidad modelo X rango de x f(x) E(X) V(X) R Uniforme número entero o real \\([a,b]\\) \\(\\frac{1}{n}\\) \\(\\frac{b+a}{2}\\) \\(\\frac{(b-a+1)^2-1}{12}\\) rep(1/n, n), dunif(x, a, b) Bernoulli evento A 0,1 \\((1-p)^{1-x}p^x\\) \\(p\\) \\(p(1-p)\\) c(1-p,p) binomial # de eventos A en \\(n\\) repeticiones de ensayos de Bernoulli 0,1, \\(\\binom n x (1-p)^{n-x}p^x\\) \\(np\\) \\(np(1-p)\\) dbimon(x,n,p) Binomial negativo para eventos # de eventos B en repeticiones de Bernoulli antes de \\(r\\) Como se observan 0,1,.. \\(\\binom {x+r-1} x (1-p)^xp^r\\) \\(\\frac{r(1-p)}{p}\\) \\(\\frac{r(1-p)}{p^2}\\) dnbinom(x,r,p) "],["ejercicios.html", "Chapter 8 Ejercicios 8.1 Descripción de datos 8.2 Probabilidad 8.3 La probabilidad condicional 8.4 Variables aleatorias 8.5 Modelos de probabilidad 8.6 Estimadores puntuales 8.7 Muestreo y teorema del límite central 8.8 Máxima verosimilitud 8.9 Método de los momentos", " Chapter 8 Ejercicios 8.1 Descripción de datos 8.1.0.1 Ejercicio 1 Hemos realizado un experimento 12 veces con los siguientes resultados ## [1] 3 3 10 2 6 11 5 4 Responde las siguientes preguntas: Calcula las frecuencias relativas de cada resultado. Calcula las frecuencias acumuladas de cada resultado. ¿Cuál es el promedio de las observaciones? ¿Qué es la mediana? ¿Qué es el tercer cuartil? ¿Cuál es el primer cuartil? 8.1.0.2 Ejercicio 2 Hemos realizado un experimento 10 veces con los siguientes resultados ## [1] 2.875775 7.883051 4.089769 8.830174 9.404673 0.455565 5.281055 8.924190 ## [9] 5.514350 4.566147 Considere 10 contenedores de tamaño 1: [0,1], (1,2](9,10). Responde las siguientes preguntas: Calcula las frecuencias relativas de cada resultado y dibuje el histograma Calcula las frecuencias acumulativas de cada resultado y dibuje la gráfica acumulativa. Dibuja un diagrama de caja. 8.2 Probabilidad 8.2.0.1 Ejercicio 1 El resultado de un experimento aleatorio es medir la gravedad de la misofonía y el estado de depresión de un paciente. Gravedad de la misofonía: \\(x\\in \\{0,1,2,3,4\\}\\) Depresión: \\(y\\in \\{0,1\\}\\) (no:\\(0\\), si:\\(1\\)) ## Misofonia.dic depresion.dic ## 1 4 1 ## 2 2 0 ## 3 0 0 ## 4 3 0 ## 5 0 0 ## 6 0 0 Un estudio en 123 pacientes mostró las frecuencias \\(n_{x,y}\\) dadas en la tabla de contingencia: ## ## Depression:0 Depression:1 ## Misophonia:4 0 9 ## Misophonia:3 25 6 ## Misophonia:2 34 3 ## Misophonia:1 5 0 ## Misophonia:0 36 5 Supongamos que \\(N&gt;&gt;0\\) y que las frecuencias estiman las probabilidades \\(f_{x,y}=\\hat{P}(X, Y)\\) ## ## Depression:0 Depression:1 ## Misophonia:4 0.00000000 0.07317073 ## Misophonia:3 0.20325203 0.04878049 ## Misophonia:2 0.27642276 0.02439024 ## Misophonia:1 0.04065041 0.00000000 ## Misophonia:0 0.29268293 0.04065041 ¿Cuál es la probabilidad marginal de misofonía de gravedad 3? ¿Cuál es la probabilidad de no ser misofónico y no estar deprimido? ¿Cuál es la probabilidad de ser misofónico o deprimido? ¿Cuál es la probabilidad de ser misofónico y deprimido? Describir en palabras los resultados con probabilidad 0. 8.2.0.2 Ejercicio 2 Hemos realizado un experimento 10 veces con los siguientes resultados ## A B ## 1 male dead ## 2 male dead ## 3 male dead ## 4 female alive ## 5 male dead ## 6 female alive ## 7 female dead ## 8 female alive ## 9 male alive ## 10 male alive Crear la tabla de contingencia para el número (\\(n_{i,j}\\)) de observaciones de cada resultado (\\(A,B\\)) Crear la tabla de contingencia para la frecuencia relativa (\\(f_{i,j}\\)) de los resultados ¿Cuál es la frecuencia marginal de ser hombre? ¿Cuál es la frecuencia marginal de estar vivo? ¿Cuál es la frecuencia de estar vivo o mujer? 8.3 La probabilidad condicional 8.3.0.1 Ejercicio 1 Se prueba el rendimiento de una máquina para producir varillas de torneado de alta calidad. Estos son los resultados de las pruebas Redondeado: Sí Redondeado: No superficie lisa: sí 200 1 superficie lisa: no 4 2 ¿Cuál es la probabilidad estimada de que la máquina produzca una varilla que no satisfaga ningún control de calidad? ¿Cuál es la probabilidad estimada de que la máquina produzca una varilla que no satisfaga al menos un control de calidad? ¿Cuál es la probabilidad estimada de que la máquina produzca varillas de superficie redondeada y alisada? ¿Cuál es la probabilidad estimada de que la barra sea redondeada si la barra es lisa? ¿Cuál es la probabilidad estimada de que la varilla sea lisa si es redondeada? ¿Cuál es la probabilidad estimada de que la varilla no sea ni lisa ni redondeada si no cumple al menos un control de calidad? ¿Son eventos independientes la suavidad y la redondez? 8.3.0.2 Ejercicio 2 Desarrollamos un test para detectar la presencia de bacterias en un lago. Encontramos que si el lago contiene la bacteria, la prueba es positiva el 70% de las veces. Si no hay bacterias, la prueba es negativa el 60% de las veces. Implementamos la prueba en una región donde sabemos que el 20% de los lagos tienen bacterias. ¿Cuál es la probabilidad de que un lago que dé positivo esté contaminado con bacterias? 8.3.0.3 Ejercicio 3 Se prueba el rendimiento de dos máquinas para producir varillas de torneado de alta calidad. Estos son los resultados de las pruebas Máquina 1 Redondeado: Sí Redondeado: No superficie lisa: sí 200 1 superficie lisa: no 4 2 Máquina 2 Redondeado: Sí Redondeado: No superficie lisa: sí 145 4 superficie lisa: no 8 6 ¿Cuál es la probabilidad de que la barra sea redondeada? ¿Cuál es la probabilidad de que la varilla haya sido producida por la máquina 1? ¿Cuál es la probabilidad de que la varilla no sea lisa? ¿Cuál es la probabilidad de que la varilla sea lisa o redondeada o producida por la máquina 1? ¿Cuál es la probabilidad de que la varilla quede redondeada si es alisada y de la máquina 1? ¿Cuál es la probabilidad de que la varilla no esté redondeada si no está alisada y es de la máquina 2? ¿Cuál es la probabilidad de que la varilla haya salido de la máquina 1 si está alisada y redondeada? ¿Cuál es la probabilidad de que la varilla haya venido de la máquina 2 si no pasa al menos uno de los controles de calidad? 8.3.0.4 Ejercicio 4 Queremos cruzar una avenida con dos semáforos. La probabilidad de encontrar el primer semáforo en rojo es 0,6. Si paramos en el primer semáforo, la probabilidad de parar en el segundo es 0,15. Mientras que la probabilidad de detenernos en el segundo si no nos detenemos en el primero es 0,25. Cuando intentamos cruzar ambos semáforos: ¿Cuál es la probabilidad de tener que detenerse en cada semáforo? ¿Cuál es la probabilidad de tener que parar en al menos un semáforo? ¿Cuál es la probabilidad de tener que detenerse en un solo semáforo? Si paré en el segundo semáforo, ¿cuál es la probabilidad de que hubiera tenido que parar en el primero? Si tuviera que parar en cualquier semáforo, ¿cuál es la probabilidad de que tuviera que hacerlo dos veces? ¿Parar en el primer semáforo es un evento independiente de detenerse en el segundo semáforo? Ahora, queremos cruzar una avenida con tres semáforos. La probabilidad de encontrar un semáforo en rojo solo depende de la anterior. En concreto, la probabilidad de encontrar un semáforo en rojo dado que el anterior estaba en rojo es de 0,15. Mientras que la probabilidad de encontrar un tráfico justo en rojo dado que el anterior estaba en verde es de 0,25. Además, la probabilidad de encontrar el primer semáforo en rojo es de 0,6. ¿Cuál es la probabilidad de tener que parar en cada semáforo? ¿Cuál es la probabilidad de tener que parar en al menos un semáforo? ¿Cuál es la probabilidad de tener que detenerse en un solo semáforo? consejos: Si la probabilidad de que un semáforo esté en rojo depende únicamente del anterior, entonces \\(P(R_3|R_2,R_1)=P(R_3|R_2,\\bar{R}_1)=P(R_3|R_2)\\) y \\(P(R_3|\\bar{R}_2,R_1)=P(R_3 |\\bar{R}_2,\\bar{R}_1)=P(R_3|\\bar{R}_2)\\) La probabilidad conjunta de encontrar tres semáforos en rojo se puede escribir como: \\(P(R_1,R_2,R_3)=P(R_3|R_2)P(R_2|R_1)P(R_1)\\) 8.3.0.5 Ejercicio 5 Una prueba de calidad en un ladrillo aleatorio se define por los eventos: Pasar la prueba de calidad: \\(E\\), no pasar la prueba de calidad: \\(\\bar{E}\\) Defectuoso: \\(D\\), no defectuoso: \\(\\bar{D}\\) Si la prueba diagnóstica tiene sensibilidad \\(P(E|\\bar{D})=0.99\\) y especificidad \\(P(\\bar{E}|D)=0.98\\), y la probabilidad de pasar la prueba es \\(P(E) =0.893\\) entonces ¿Cuál es la probabilidad de que un ladrillo elegido al azar sea defectuoso \\(P(D)\\)? ¿Cuál es la probabilidad de que un ladrillo que ha pasado la prueba sea realmente defectuoso? La probabilidad de que un ladrillo no sea defectuoso y que no pase la prueba ¿Son \\(D\\) y \\(\\bar{E}\\) estadísticamente independientes? 8.4 Variables aleatorias 8.4.0.1 Ejercicio 1 Dada la función de masa de probabilidad \\(x\\) \\(f(x)=P(X=x)\\) 10 0.1 12 0.3 14 0.25 15 0.15 17 ? 20 0.15 ¿Cuál es su valor esperado y su desviación estándar? 8.4.0.2 Ejercicio 2 Dada la distribución de probabilidad para una variable discreta \\(X\\) \\[ F(x)= \\begin{cases} 0, &amp; x &lt; -1 \\\\ 0.2,&amp; x \\in [-1,0)\\\\ 0.35,&amp; x \\in [0,1)\\\\ 0.45,&amp; x \\in [1,2)\\\\ 1,&amp; x \\geq 2\\\\ \\end{cases} \\] encuentra \\(f(X)\\) encuentra \\(E(X)\\) y \\(V(X)\\) cuál es el valor esperado y la varianza de \\(Y=2X+3\\) ¿Cuál es la mediana y el primer y tercer cuartil de \\(X\\)? 8.4.0.3 Ejercicio 3 Estamos probando un sistema para transmitir imágenes digitales. Primero consideramos el experimento de enviar \\(3\\) píxeles y tener por ejemplo eventos como \\((0,1,1)\\). Este es el evento de recibir el primer píxel sin error, el segundo con error y el tercero con error. Enumere en una columna el espacio muestral del experimento aleatorio. En la segunda columna asigne la variable aleatoria que cuenta el número de errores transmitidos para cada resultado Considere que tenemos un canal totalmente ruidoso, es decir, cualquier resultado de tres píxeles es igualmente probable. ¿Cuál es la probabilidad de recibir errores de \\(0\\), \\(1\\), \\(2\\) o \\(3\\) en la transmisión de \\(3\\) píxeles? Dibuje la función de masa de probabilidad para el número de errores ¿Cuál es el valor esperado para el número de errores? ¿Cuál es su varianza? Dibujar la distribución de probabilidad ¿Cuál es la probabilidad de transmitir al menos \\(1\\) error? 8.4.0.4 Ejercicio 4 Para la densidad de probabilidad \\[ f(x)= \\begin{cases} \\frac{1}{100},&amp; \\text{si } x\\in (0,100)\\\\ 0, de\\, lo\\, contrario \\end{cases} \\] calcular la media calcular la varianza usando \\(E(X^2)=V(X)+E(X)^2\\) calcular \\(P(\\mu-\\sigma\\leq X \\leq \\mu+\\sigma)\\) ¿Cuáles son el primer y tercer cuartiles? 8.4.0.5 Ejercicio 5 Dado \\[ f(x)= \\begin{cases} 0, &amp; x &lt; 0 \\\\ ax, &amp; x \\in [0,3] \\\\ b, &amp; x \\in (3,5) \\\\ \\frac{b}{3}(8-x),&amp; x \\in [5,8]\\\\ 0, &amp; x &gt; 8 \\\\ \\end{cases} \\] ¿Cuáles son los valores de \\(a\\) y \\(b\\) tales que \\(f(x)\\) es una función de densidad de probabilidad continua? ¿Cuál es la media de \\(X\\)? 8.4.0.6 Ejercicio 6 Para la densidad de probabilidad \\[ f(x)= \\begin{cases} \\lambda e^{-\\lambda x},&amp; \\text{if } x \\geq 0\\\\ 0,&amp; si\\,no \\end{cases} \\] Confirmar que se trata de una densidad de probabilidad Calcular la media Calcule el valor esperado de \\(X^2\\) Calcular la varianza Hallar la distribución de probabilidad \\(F(a)\\) Encuentra la mediana 8.4.0.7 Ejercicio 7 Dada la distribución de probabilidad de una variable aleatoria \\(X\\) \\[ F(x)= \\begin{cases} 0, &amp; x &lt; -1 \\\\ \\frac{1}{80}(17+16x-x^2),&amp; x \\in [-1,7)\\\\ 1,&amp; x \\geq 7\\\\ \\end{cases} \\] calcular: \\(P(X&gt;0)\\) \\(E(X)\\) \\(P(X&gt;0|X&lt;2)\\) 8.5 Modelos de probabilidad 8.5.0.1 Ejercicio 1 Un motor de búsqueda falla al recuperar información con una probabilidad de \\(0.1\\) Si nuestro sistema recibe solicitudes de búsqueda de \\(50\\), ¿cuál es la probabilidad de que el sistema no responda a tres de ellas? ¿Cuál es la probabilidad de que el motor complete con éxito búsquedas de \\(15\\) antes de la primera falla? Consideramos que un buscador funciona suficientemente bien cuando es capaz de encontrar información para mas de \\(10\\) solicitudes por cada \\(2\\) fallos. ¿Cuál es la probabilidad de que en un ensayo de fiabilidad nuestro motor de búsqueda sea satisfactorio? 8.5.0.2 Ejercicio 2 En una población, la probabilidad de que nazca un niño es \\(p=0,51\\). Considere una familia de 4 hijos. ¿Cuál es la probabilidad de que una familia tenga un solo niño? ¿Cuál es la probabilidad de que una familia tenga una sola niña? ¿Cuál es la probabilidad de que una familia tenga solo un niño o solo una niña? ¿Cuál es la probabilidad de que la familia tenga al menos dos niños? ¿Cuál es el número de hijos que debe tener una familia para que la probabilidad de tener al menos una niña sea superior a 0,75? 8.5.0.3 Ejercicio 3 La cantidad promedio de partículas radiactivas que golpean un contador Geiger es de \\(2.3\\) por segundo. ¿Cuál es la probabilidad de contar exactamente 2 partículas en un segundo? ¿Cuál es la probabilidad de detectar exactamente \\(10\\) partículas en \\(5\\) segundos? ¿Cuál es la probabilidad de al menos un conteo en dos segundos? ¿Cuál es la probabilidad de tener que esperar \\(2,5\\) segundos después de que encendemos el detector? 8.5.0.4 Ejercicio 4 ¿Cuál es la probabilidad de que la altura de un hombre sea al menos \\(165\\)cm si la media poblacional es \\(175\\)cm y la desviación estándar es \\(10\\)cm? ¿Cuál es la probabilidad de que la altura de un hombre esté entre \\(165\\)cm y \\(180\\)cm. ¿Cuál es la altura que define el \\(5\\%\\) de los hombres más pequeños? 8.6 Estimadores puntuales 8.6.0.1 Ejercicio 1 Considere el modelo de probabilidad \\[ f(x)= \\begin{cases} 1/2-a,&amp; \\text{si } x=-1 \\\\ 1/2,&amp; \\text{si } x=0\\\\ a,&amp; 1 \\text{si } x=1\\\\ \\end{cases} \\] donde \\(a\\) es un parámetro. Calcule la media y la varianza de la estadística: \\[T=\\frac{\\bar{X}}{2}+\\frac{1}{4}\\] donde \\(\\bar{X}=\\frac{1}{N}\\sum_{i=1}^N X_i\\) ¿\\(T\\) es un estimador sesgado de \\(a\\)? ¿Es \\(T\\) consistente? es decir, \\(V(T) \\rightarrow 0\\) cuando \\(N\\rightarrow \\infty\\) 8.6.0.2 Ejercicio 2 ¿Es \\(\\bar{X}^2=(\\frac{1}{N}\\sum_{i=1}^N X_i)^2\\) un estimador imparcial de \\(E(X)^2\\)? 8.7 Muestreo y teorema del límite central 8.7.0.1 Ejercicio 1 Un modelo de batería carga hasta \\(75\\%\\) de su capacidad en una hora con una desviación estándar de \\(15\\%\\). Si cobramos \\(25\\), ¿cuál es la probabilidad de que el promedio de la muestra esté a una distancia de \\(5\\%\\) del cargo de la media? Si cobramos \\(100\\), ¿cuál es esa probabilidad? Si, en cambio, solo cargamos baterías de \\(9\\), ¿cuál es la carga que es superada por el promedio de la muestra con solo \\(0.015\\) de probabilidad? 8.7.0.2 Ejercicio 2 Se necesita un componente electrónico para el correcto funcionamiento de un telescopio. Necesita ser reemplazado inmediatamente cuando se desgasta. La vida media del componente (\\(\\mu\\)) es de \\(100\\) horas y su desviación estándar \\(\\sigma\\) es de \\(30\\) horas. ¿Cuál es la probabilidad de que el promedio de la vida media de \\(50\\) componentes esté dentro de \\(1\\) hora de la vida media de un solo componente? ¿Cuántos componentes necesitamos para que el telescopio esté operativo \\(2750\\) horas consecutivas con una probabilidad de \\(0,95\\)? 8.7.0.3 Ejercicio 3 Una máquina automática llena tubos de ensayo con muestras biológicas con una media de \\(\\mu=130\\)mg y una desviación estándar de \\(\\sigma=5\\)mg. para una muestra aleatoria de tamaño \\(50\\). ¿Cuál es la probabilidad de que la media muestral (promedio) está entre \\(128\\) y \\(132\\)gr? ¿Cuál debe ser el tamaño de la muestra (\\(n\\)) para que la media muestral \\(\\bar{X}\\) sea mayor a \\(131\\)gr con una probabilidad menor o igual a \\(0.025\\)? 8.7.0.4 Ejercicio 4 En el Caribe, parece haber un promedio de huracanes de \\(6\\) por año. Teniendo en cuenta que la formación de huracanes es un proceso de Poisson, los meteorólogos planean estimar el tiempo medio entre la formación de dos huracanes. Planean recolectar una muestra de tamaño \\(36\\) para los tiempos entre dos huracanes. ¿Cuál es la probabilidad de que su promedio muestral esté entre \\(45\\) y \\(60\\) días? ¿Cuál debe ser el tamaño de la muestra para que tengan una probabilidad de \\(0.025\\) de que la media muestral sea mayor a \\(70\\) días? 8.7.0.5 Ejercicio 5 La probabilidad de que se encuentre una mutación particular en la población es de \\(0.4\\). Si probamos \\(2000\\) personas para la mutación: ¿Cuál es la probabilidad de que el número total de personas con la mutación esté entre \\(791\\) y \\(809\\)? sugerencia: use el CLT con una muestra de ensayos de Bernoulli de \\(2000\\). Esto se conoce como la aproximación normal de la distribución binomial. 8.8 Máxima verosimilitud 8.8.0.1 Ejercicio 1 Para una variable aleatoria con una función de probabilidad binomial \\[f(x; p)=\\binom n x p^x(1-p)^{n-x}\\] ¿Cuál es el estimador de máxima verosimilitud de \\(p\\) para una muestra de tamaño \\(1\\) de esta variable aleatoria? En un examen de \\(100\\) estudiantes observamos \\(x_1=68\\) estudiantes que aprobaron el examen. ¿Cuál es la estimación de \\(p\\)? 8.8.0.2 Ejercicio 2 Tome una variable aleatoria con la siguiente función de densidad de probabilidad \\[ f(x)= \\begin{cases} (1+\\theta)x^\\theta,&amp; \\text{si } x\\in (0,1)\\\\ 0,&amp; x\\notin (0,1) \\end{cases} \\] ¿Cuál es la estimación de máxima verosimilitud para \\(\\theta\\)? Si tomamos una muestra de \\(5\\) con observaciones \\(x_1 = 0,92; \\qquad x_2 = 0,79; \\qquad x_3 = 0,90; \\qquad x_4 = 0,65; \\qquad x_5 = 0,86\\) ¿Cuál es el valor estimado del parámetro \\(\\theta\\)? 8.8.0.3 Ejercicio 3 Tome una variable aleatoria con la siguiente función de densidad de probabilidad \\[ f(x)= \\begin{cases} \\lambda e^{-\\lambda x},&amp; \\text{si } 0 \\leq x\\\\ 0,&amp; si \\, no \\end{cases} \\] ¿Cuál es la estimación de máxima verosimilitud para \\(\\lambda\\)? Si tomamos una muestra de \\(5\\) con observaciones \\(x_1 = 0.223 \\qquad x_2 = 0.681; \\qquad x_3 = 0,117; \\qquad x_4 = 0,150; \\qquad x_5 = 0.520\\) ¿Cuál es el valor estimado del parámetro \\(\\lambda\\)? 8.9 Método de los momentos 8.9.0.1 Ejercicio 1 ¿Cuáles son los estimadores de los siguientes modelos paramétricos dados por el método de los momentos? modelo f(x) E(X) Bernoulli \\(p^x(1-p)^{1-x}\\) \\(p\\) binomial \\(\\binom n x p^x(1-p)^{n-x}\\) \\(np\\) Geométrico desplazado \\(p(1-p)^{x-1}\\) \\(\\frac{1}{p}\\) Binomial negativo \\(\\binom {x+r-1} x p^r(1-p)^x\\) \\(r\\frac{1-p}{p}\\) Veneno \\(\\frac{e^{-\\lambda}\\lambda^x}{x!}\\) \\(\\lambda\\) Exponencial \\(\\lambda e^{-\\lambda x}\\) \\(\\frac{1}{\\lambda}\\) normales \\(\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\) \\(\\mu\\) 8.9.0.2 Ejercicio 2 Tome una variable aleatoria con la siguiente función de densidad de probabilidad \\[ f(x)= \\begin{cases} (1+\\theta)x^\\theta,&amp; \\text{si } x\\in (0,1)\\\\ 0,&amp; x\\notin (0,1) \\end{cases} \\] Calcule \\(E(X)\\) como una función de \\(\\theta\\) ¿Cuál es la estimación de \\(\\theta\\) utilizando el método de los momentos? Si tomamos una muestra de \\(5\\) con observaciones \\(x_1 = 0,92; \\qquad x_2 = 0,79; \\qquad x_3 = 0,90; \\qquad x_4 = 0,65; \\qquad x_5 = 0,86\\) ¿Cuál es el valor estimado del parámetro \\(\\theta\\)? 8.9.0.3 Ejercicio 3 Considere una variable aleatoria discreta \\(X\\) que sigue una distribución binomial negativa con función de masa de probabilidad: \\[f(x) = \\binom{x+r-1}{x}p^r(1-p)^x\\] Dado que \\(E(X)=\\dfrac{r(1-p)}{p}\\) \\(V(X) =\\dfrac{r(1-p)}{p^2}\\) calcular: Una estimación del parámetro \\(r\\) y una estimación del parámetro \\(p\\) obtenidas a partir de una muestra aleatoria de tamaño \\(n\\) por el método de los momentos. Los valores de las estimaciones de \\(r\\) y \\(p\\) para la siguiente muestra aleatoria: \\[x_1 = 27; \\qquad x_2 = 8; \\qquad x_3 = 22; \\qquad x_4 = 29; \\qquad x_5 = 19; \\qquad x_5 = 32\\] "]]
