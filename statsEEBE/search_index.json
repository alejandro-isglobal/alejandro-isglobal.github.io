[["index.html", "EEBE Estadística Chapter 1 Objetivo 1.1 Lectura recomendada", " EEBE Estadística Alejandro Cáceres (alejandro.caceres.dominguez@upc.edu) 2025-05-15 Chapter 1 Objetivo Este es el curso de introducción a la estadística de la EEBE (UPC). La estadística es un lenguaje que permite afrontar problemas nuevos, sobre los que no tenemos solución, y en donde interviene la aleatoridad. En este curso trataremos los conceptos fundamentales de estadística. 3 horas de teoría por semana: Explicaremos los conceptos, haremos ejercicios. 6 horas de estudio individual por semana: Notas notas de curso y los recursos en ATENEA. 2 horas de Solución de problemas con R: Sesiones presenciales con ordenador (Prácticas). Las fechas de exámenes y material de estudio adicional se pueden encontrar en ATENEA metacurso. Objetivos de evaluación: Q1 (10%): Prueba en ordenador duración 2h en las fechas indicadas. Dominio de comandos básicos en R (Prácticas) Capacidad de calcular estadísticos descriptivos y gráficos, en situaciones concretas (Teoría/Práctica) Conocimiento sobre la regresión lineal (Prácticas) EP1 (25%): Prueba escrita (2-3 problemas) Capacidad de interpretación de enunciados en fórmulas de probabilidad (Teoría). Conocimiento de las herramientas básicas para solucionar problemas de probabilidad conjunta y probabilidad condicional (Teoría). Dominio matemático de funciones de probabilidad para calcular sus propiedades básicas (Teoría). Q2 (20%): Prueba en ordenador duración 2h en horario de clase en las fechas indicadas Capacidad de identificación de modelos de probabilidad en problemas concretos (Teoría/Práctica). Uso de funciones de R para calcular probabilidades de modelos probabilísticos (Práctica/Teoría) Capacidad de identificación de un estadístico de muestreo y sus propiedades (Teoría/Práctica) Conocimiento de cómo calcular la probabilidad de los estadísticos de muestreo (Teoría/Práctica) Uso de comandos en R para calcular probabilidades y hacer simulaciones de muestras aleatorias (Prácticas) EP2 (40%): Prueba escrita (2-3 problemas) Capacidad matemática para determinar estimadores puntuales de modelos de probabilidad. Conociemiento de las propiedades de los estimadores puntuales. Conocimiento de los intervalos de confianza y sus propiedades (Teoría). Capacidad de identificar el tipo de intervalo de confianza en un problema concreto (Teoría). Capacidad de interpretación del tipo de hipótesis a usar en un problema concreto (Teoría). Uso de comandos en R para resolver problemas de intervalos de confianza y prueabas de hipótesis (Práctica). CG (5%): Prueba escrita (2 preguntas sobre un texto) Capacidad de expresión escrita sobre un tema relacionado a la estadística. coordinadores: Luis Mujica (luis.eduardo.mujica@upc.edu) Pablo Buenestado (pablo.buenestado@upc.edu) 1.1 Lectura recomendada Los apuntes de clase se nuestra sección estarán accesibles en ATENEA en pdf y en html. Douglas C. Montgomery and George C. Runger. “Applied Statistics and Probability for Engineers” 4th Edition. Wiley 2007. "],["descripción-de-datos.html", "Chapter 2 Descripción de datos 2.1 Método científico 2.2 Estadística 2.3 Datos 2.4 Tipos de resultado 2.5 Experimentos aleatorios 2.6 Frecuencias absolutas 2.7 Frecuencias relativas 2.8 Gráfico de barras 2.9 Gráfico de sectores (pie) 2.10 Variables categóricas ordinales 2.11 Frecuencias acumuladas absolutas y relativas 2.12 Gráfica de frecuencia acumulada 2.13 Variables numéricas 2.14 Transformando datos continuos 2.15 Tabla de frecuencias para una variable continua 2.16 Histograma 2.17 Gráfica de frecuencia acumulada 2.18 Estadísticas de resumen 2.19 Promedio (media muestral) 2.20 Mediana 2.21 Dispersión 2.22 Varianza muestral 2.23 Rango intercuartílico (IQR) 2.24 Diagrama de caja 2.25 Preguntas 2.26 Ejercicios", " Chapter 2 Descripción de datos En este capítulo, presentaremos herramientas para describir datos. Lo haremos utilizando tablas, figuras y estadísticos descriptivos de tendencia central y dispersión. También presentaremos conceptos clave en estadística como experimentos aleatorios, observaciones, resultados y frecuencias absolutas y relativas. 2.1 Método científico Uno de los objetivos del método científico es proporcionar un marco para resolver los problemas que surgen en el estudio de los fenómenos naturales o en el diseño de nuevas tecnologías. Los humanos modernos han desarrollado un método durante miles de años que todavía está en desarrollo. El método tiene tres actividades humanas principales: Observación caracterizada por la adquisición de datos Razón caracterizada por el desarrollo de modelos matemáticos Acción caracterizada por el desarrollo de nuevos experimentos (tecnología) Su compleja interacción y resultados son la base de la actividad científica. 2.2 Estadística La estadística se ocupa de la interacción entre modelos y datos (la parte inferior de la figura). Las preguntas de tipo estadístico son: ¿Cuál es el mejor modelo para mis datos (inferencia)? ¿Cuáles son los datos que produciría un determinado modelo (predicción)? 2.3 Datos Los datos se presentan en forma de observaciones. Una Observación o Realización es la adquisición de un número o una característica de un experimento. Por ejemplo, tomemos la serie de números que se producen por la repetición de un experimento (1: éxito, 0: fracaso) … 1 0 0 1 0 1 0 1 1 … El número en negrita es una observación en una repetición del experimento. Un resultado es una posible observación que es el resultado de un experimento. 1 es un resultado, 0 es el otro resultado del experimento. Recuerdemos que la observación es concreta. Es el número que obtenemos un día en el laboratorio. El resultado, por el contrario, es abstracto es una de las características del experimento que estamos realizando. No necesitamos ir al laboratorio para conocer los posibles resultados de un experimento. El razonamiento basta. 2.4 Tipos de resultado En estadística nos interesan principalmente dos tipos de resultados. Categóricos: Si el resultado de un experimento es una cualidad. Pueden ser nominales (binario: sí, no; múltiple: colores) u ordinales cuando las cualidades pueden jerarquizarse (gravedad de una enfermedad). Numéricos: Si el resultado de un experimento es un número. El número puede ser discreto (número de correos electrónicos recibidos en una hora, número de leucocitos en sangre) o continuo (estado de carga de la batería, temperatura del motor). 2.5 Experimentos aleatorios Se puede decir que el tema de estudio de la estadística son los experimentos aleatorios, el medio por el cual producimos datos. Definición: Un experimento aleatorio es un experimento que produce diferentes resultados cuando se repite de la misma manera. Los experimentos aleatorios son de diferentes tipos, dependiendo de cómo se realicen: en el mismo objeto (persona): temperatura, niveles de azúcar. sobre objetos diferentes pero de la misma medida: el peso de un animal. sobre eventos: el número de huracanes por año. 2.6 Frecuencias absolutas Cuando repetimos un experimento aleatorio con resultados categóricos, lo peimero que hacemos es una lista con los posibles resultados. Resumimos las observaciones contando cuántas veces vimos cada uno de los resultados. Frecuencia absoluta: \\[n_i\\] es el número de veces que observamos el resultado \\(i\\). Ejemplo (leucocitos) Extraigamos un leucocito de la muestra de sangre de un donante y anotemos su tipo. Repitamos el experimento \\(N=119\\) veces. (célula T, célula T, neutrófilo, ..., célula B) La segunda célula T en negrita es la segunda observación. La última célula B es la observación número \\(119\\). Podemos listar los resultados (categorías) en una tabla de frecuencia: ## outcome ni ## 1 T Cell 34 ## 2 B cell 50 ## 3 basophil 20 ## 4 Monocyte 5 ## 5 Neutrophil 10 De la tabla, podemos decir que, por ejemplo, \\(n_1=34\\) es el número total de células T observadas en la repetición del experimento. También notamos que el número total de repeticiones \\[\\sum_{i=1}^M n_i=N=119\\]. Donde \\(M\\) es el número de resultados (número de filas en la table) y \\(N\\) el número de observaciones (repeticiones del experimento aleatorio). La tabla puede interpretarse como el perfil inmune del donante. 2.7 Frecuencias relativas También podemos resumir las observaciones calculando la proporción de cuántas veces vimos un resultado en particular. \\[f_i=n_i/N\\] En nuestro ejemplo se registraron \\(n_1=34\\) células T, por lo que su frecuencia relativa nos da la proporción de células T del total de \\(119\\). Podemos agregar estas proporciones \\(f_i\\) en la tabla de frecuencias. ## outcome ni fi ## 1 T Cell 34 0.28571429 ## 2 B cell 50 0.42016807 ## 3 basophil 20 0.16806723 ## 4 Monocyte 5 0.04201681 ## 5 Neutrophil 10 0.08403361 Las frecuencias relativas son fundamentales en estadística. Dan la proporción de un resultado en relación con los otros resultados. Más adelante las entenderemos como observaciones de probabilidades, o mas precisamente, como estimaciones de las probabilidades. Para las frecuencias absolutas y relativas tenemos las propiedades \\(\\sum_{i=1}^M n_i = N\\) \\(\\sum_{i=1}^M f_i = 1\\) donde \\(M\\) es el número de resultados. 2.8 Gráfico de barras Cuando tenemos muchos resultados y queremos ver cuáles son los más probables, podemos usar un gráfico de barras que es un grafico de \\(n_i\\) versus los resultados. 2.9 Gráfico de sectores (pie) También podemos visualizar las frecuencias relativas con un gráfico de sectores. El área del círculo representa el \\(100\\%\\) de las observaciones (proporción = 1) y las secciones las frecuencias relativas de cada resultado. 2.10 Variables categóricas ordinales El tipo de leucocito de los ejemplos anteriores es una variable nominal categórica. Cada observación pertenece a una categoría (cualidad). Las categorías no siempre tienen un orden determindado. A veces, las variables categóricas se pueden ordenar cuando cumplen una clasificación natural. Esto permite calcular frecuencias acumuladas. Ejemplo (misofonía) Este es un estudio clínico de \\(123\\) pacientes que fueron examinados por su grado de misofonía. La misofnía es ansiedad/ira descontrolada producida por ciertos sonidos. Cada paciente fue evaluado con un cuestionario (AMISO) y se clasificaron en 4 grupos diferentes según la gravedad. Los resultados del estudio son ## [1] 4 2 0 3 0 0 2 3 0 3 0 2 2 0 2 0 0 3 3 0 3 3 2 0 0 0 4 2 2 0 2 0 0 0 3 0 2 ## [38] 3 2 2 0 2 3 0 0 2 2 3 3 0 0 4 3 3 2 0 2 0 0 0 2 2 0 0 2 3 0 1 3 2 4 3 2 3 ## [75] 0 2 3 2 4 1 2 0 2 0 2 0 2 2 4 3 0 3 0 0 0 2 2 1 3 0 0 3 2 1 3 0 4 4 2 3 3 ## [112] 3 0 3 2 1 2 3 3 4 2 3 2 Cada observación es el resultado de un experimento aleatorio: medición del nivel de misofonía en un paciente. Esta serie de datos se puede resumir en la siguente tabla de frecuencias ## outcome ni fi ## 1 0 41 0.33333333 ## 2 1 5 0.04065041 ## 3 2 37 0.30081301 ## 4 3 31 0.25203252 ## 5 4 9 0.07317073 2.11 Frecuencias acumuladas absolutas y relativas La gravedad de la misofonía es categórica ordinal porque sus resultados pueden ordenarse en relación con su grado. Cuando los resultados se pueden ordenar, es útil preguntarnos por cuántas observaciones se obtuvieron hasta un resultado dado. Llamamos a este número la frecuencia acumulada absoluta hasta el resultado \\(i\\): \\[N_i=\\sum_{k=1}^i n_k\\] También es útil calcular la proporción de las observaciones que se obtuvo hasta un resultado dado \\[F_i=\\sum_{k=1}^i f_k\\] Podemos agregar estas frecuencias a la tabla de frecuencias ## outcome ni fi Ni Fi ## 0 0 41 0.33333333 41 0.3333333 ## 1 1 5 0.04065041 46 0.3739837 ## 2 2 37 0.30081301 83 0.6747967 ## 3 3 31 0.25203252 114 0.9268293 ## 4 4 9 0.07317073 123 1.0000000 Vemos que el 67% de los pacientes tenían misofonía hasta la gravedad 2 y el 37 % de los pacientes tenían una gravedad inferior o igual a 1. 2.12 Gráfica de frecuencia acumulada \\(F_i\\) es una cantidad importante porque nos permite definir la acumulación de probabilidades para niveles intermedios. La acumulaci'on de frecuancia hasta un nivel intermedio \\(x\\) (\\(i\\leq x&lt; i+1\\)) de resultados \\(i\\) e \\(i+1\\) es la acumulación hasta el nivel inferior \\[F_x=F_i\\] \\(F_x\\) es por lo tanto una función de rango continuo. Podemos dibujarla con respecto a los resultados. Podemos decir, por ejemplo, que el 67 % de los pacientes tenían misofonía hasta gravedad \\(2.3\\), aunque \\(2.3\\) no sea un posible resultado del experimento aleatorio. 2.13 Variables numéricas El resultado de un experimento aleatorio puede producir un número. Si el número es discreto, podemos generar una tabla de frecuencias, con frecuencias absolutas, relativas y acumuladas, e ilustrarlas con gráficos de barras, de sectores y acumulativos. Cuando el número es continuo las frecuencias no son útiles, lo más probable es que observemos o no un número contínuo en particular. Ejemplo (misofonía) Los investigadores se preguntaron si la convexidad de la mandíbula afectaría la gravedad de la misofonía. La hipótesis científica es que el ángulo de convexidad de la mandíbula puede influir en el oído y su sensibilidad. Estos son los resultados de la convexidad de la mandíbula en grados para cada paciente: ## [1] 7.97 18.23 12.27 7.81 9.81 13.50 19.30 7.70 12.30 7.90 12.60 19.00 ## [13] 7.27 14.00 5.40 8.00 11.20 7.75 7.94 16.69 7.62 7.02 7.00 19.20 ## [25] 7.96 14.70 7.24 7.80 7.90 4.70 4.40 14.00 14.40 16.00 1.40 9.76 ## [37] 7.90 7.90 7.40 6.30 7.76 7.30 7.00 11.23 16.00 7.90 7.29 6.91 ## [49] 7.10 13.40 11.60 -1.00 6.00 7.82 4.80 11.00 9.00 11.50 16.00 15.00 ## [61] 1.40 16.80 7.70 16.14 7.12 -1.00 17.00 9.26 18.70 3.40 21.30 7.50 ## [73] 6.03 7.50 19.00 19.01 8.10 7.80 6.10 15.26 7.95 18.00 4.60 15.00 ## [85] 7.50 8.00 16.80 8.54 7.00 18.30 7.80 16.00 14.00 12.30 11.40 8.50 ## [97] 7.00 7.96 17.60 10.00 3.50 6.70 17.00 20.26 6.64 1.80 7.02 2.46 ## [109] 19.00 17.86 6.10 6.64 12.00 6.60 8.70 14.05 7.20 19.70 7.70 6.02 ## [121] 2.50 19.00 6.80 Si bien, en este conjunto de observaciones, vemos la repetición del grado de convexidad \\(7,90\\), esto es el resultado del redondeo de las mediciones o de la resolución limitada del instrumento. Los grados son continuos y es poco probable que los pacientes tengan el mismo valor de convexidad, si tuviéramos suficiente resolución. 2.14 Transformando datos continuos Como los resultados continuos no se pueden contar, los transformamos en variables categóricas ordenadas. Primero cubrimos el rango de las observaciones en intervalos regulares del mismo tamaño (bins) ## [1] &quot;[-1.02,3.46]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; &quot;(16.8,21.3]&quot; Luego mapeamos cada observación a su intervalo: creando una variable categórica ordenada; en este caso con 5 resultados posibles ## [1] &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; ## [6] &quot;(12.4,16.8]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; ## [11] &quot;(12.4,16.8]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; ## [16] &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; ## [21] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; ## [26] &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [31] &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; &quot;[-1.02,3.46]&quot; ## [36] &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [41] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; ## [46] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; ## [51] &quot;(7.92,12.4]&quot; &quot;[-1.02,3.46]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [56] &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; ## [61] &quot;[-1.02,3.46]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; ## [66] &quot;[-1.02,3.46]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;[-1.02,3.46]&quot; ## [71] &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; ## [76] &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; ## [81] &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; ## [86] &quot;(7.92,12.4]&quot; &quot;(12.4,16.8]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; ## [91] &quot;(3.46,7.92]&quot; &quot;(12.4,16.8]&quot; &quot;(12.4,16.8]&quot; &quot;(7.92,12.4]&quot; &quot;(7.92,12.4]&quot; ## [96] &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(16.8,21.3]&quot; &quot;(7.92,12.4]&quot; ## [101] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; ## [106] &quot;[-1.02,3.46]&quot; &quot;(3.46,7.92]&quot; &quot;[-1.02,3.46]&quot; &quot;(16.8,21.3]&quot; &quot;(16.8,21.3]&quot; ## [111] &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; &quot;(3.46,7.92]&quot; &quot;(7.92,12.4]&quot; ## [116] &quot;(12.4,16.8]&quot; &quot;(3.46,7.92]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; &quot;(3.46,7.92]&quot; ## [121] &quot;[-1.02,3.46]&quot; &quot;(16.8,21.3]&quot; &quot;(3.46,7.92]&quot; Por lo tanto, en lugar de decir que el primer paciente tuvo un ángulo de convexidad de \\(7.97\\), decimos que su ángulo estuvo entre el intervalo (o bin) \\((7.92,12.4]\\). Ningún otro paciente tuvo un ángulo de \\(7.97\\), pero muchos tuvieron ángulos entre \\((7.92,12.4]\\). 2.15 Tabla de frecuencias para una variable continua Para una partición regular dada del rango de los resultados, podemos producir una tabla de frecuencias como antes ## outcome ni fi Ni Fi ## 1 [-1.02,3.46] 8 0.06504065 8 0.06504065 ## 2 (3.46,7.92] 51 0.41463415 59 0.47967480 ## 3 (7.92,12.4] 26 0.21138211 85 0.69105691 ## 4 (12.4,16.8] 20 0.16260163 105 0.85365854 ## 5 (16.8,21.3] 18 0.14634146 123 1.00000000 2.16 Histograma El histograma es el gráfico de \\(n_i\\) o \\(f_i\\) versus los resultados en intervalos (bins). El histograma depende del tamaño de los los intervalos Este es un histograma con 20 intervalos Vemos que la mayoría de las personas tienen ángulos entre \\((7, 8]\\). 2.17 Gráfica de frecuencia acumulada También podemos graficar \\(F_x\\) contra los resultados. Como \\(F_x\\) es de rango continuo, podemos ordenar las observaciones (\\(x_1 &lt;... x_j &lt; x_{j+1} &lt; x_n\\)) y por lo tanto \\[F_x = \\frac{k}{n}\\] para \\(x_{k} \\leq x &lt; x_{k+1}\\). \\(F_x\\) se conoce como la distribución de los datos. \\(F_x\\) no depende del tamaño del bin. Sin embargo, su resolución depende de la cantidad de datos. 2.18 Estadísticas de resumen Las estadísticas de resumen son números calculados a partir de los datos que nos dicen características importantes de las variables numéricas (discretas o continuas). Por ejemplo, tenemos estadísticas que describen los valores extremos: mínimo: el resultado mínimo observado máximo: el resultado máximo observado 2.19 Promedio (media muestral) Una estadística importante que describe el valor central de los resultados (dónde esperar la mayoría de las observaciones) es el promedio \\[\\bar{x}=\\frac{1}{N} \\sum_{j=1}^N x_j\\] donde \\(x_j\\) es la observación \\(j\\) de un total de \\(N\\) observaciones. Ejemplo (misofonía) La convexidad promedio se puede calcular directamente a partir de las observaciones \\(\\bar{x}= \\frac{1}{N}\\sum_j x_j\\) \\(= \\frac{1}{123}(7.97 + 18.23 + 12.27... + 6.80) = 10.19894\\) Para variables categóricamente ordenadas, podemos usar las frecuencias relativas para calcular el promedio \\(\\bar{x}=\\frac{1}{N}\\sum_{i=1}^N x_j=\\frac{1}{N}\\sum_{i=1}^M x_in_ {i}\\) \\[=\\sum_{i=1}^M x_if_{i}\\] donde pasamos de sumar \\(N\\) observaciones a sumar \\(M\\) resultados. La forma \\(\\bar{x}= \\sum_{i = 1}^M x_i f_i\\) muestra que el promedio es el centro de gravedad de los resultados. Como si cada resultado tuviera una densidad de masa dada por \\(f_i\\). Ejemplo (misofonía) La severidad promedio de la misofonía en el estudio se puede calcular a partir de las frecuencias relativas de los resultados ## outcome ni fi ## 1 0 41 0.33333333 ## 2 1 5 0.04065041 ## 3 2 37 0.30081301 ## 4 3 31 0.25203252 ## 5 4 9 0.07317073 \\(\\bar{x}=0\\times f_{0}+1\\times f_{1}+2\\times f_{2}+3\\times f_{3}+4\\times f_{4}=1.691057\\) El promedio es también el centro de gravedad de las variables continuas. Ese es el punto donde las frecuencias reativas se equilibran. 2.20 Mediana Otra medida de centralidad es la mediana. La mediana \\(x_m\\), o \\(q_{0.5}\\), es el valor por debajo del cual encontramos la mitad de las observaciones. Cuando ordenamos las observaciones \\(x_1 &lt;... x_j &lt; x_{j+1} &lt; x_N\\), las contamos hasta encontrar la mitad de ellas. \\(x_m\\) es tal que \\[\\sum_{i\\leq m} 1 = \\frac{N}{2}\\] Ejemplo (misofonía) Si ordenamos los ángulos de convexidad, vemos que \\(62\\) observaciones (individuos) (\\(N/2 \\sim 123/2\\)) están por debajo de \\(7.96\\). La convexidad mediana es por lo tanto \\(q_{0.5}=x_{62}=7.96\\) ## [1] -1.00 -1.00 1.40 1.40 1.80 2.46 2.50 3.40 3.50 4.40 4.60 4.70 ## [13] 4.80 5.40 6.00 6.02 6.03 6.10 6.10 6.30 6.60 6.64 6.64 6.70 ## [25] 6.80 6.91 7.00 7.00 7.00 7.00 7.02 7.02 7.10 7.12 7.20 7.24 ## [37] 7.27 7.29 7.30 7.40 7.50 7.50 7.50 7.62 7.70 7.70 7.70 7.75 ## [49] 7.76 7.80 7.80 7.80 7.81 7.82 7.90 7.90 7.90 7.90 7.90 7.94 ## [61] 7.95 7.96 ## [1] 7.96 7.97 8.00 8.00 8.10 8.50 8.54 8.70 9.00 9.26 9.76 9.81 ## [13] 10.00 11.00 11.20 11.23 11.40 11.50 11.60 12.00 12.27 12.30 12.30 12.60 ## [25] 13.40 13.50 14.00 14.00 14.00 14.05 14.40 14.70 15.00 15.00 15.26 16.00 ## [37] 16.00 16.00 16.00 16.14 16.69 16.80 16.80 17.00 17.00 17.60 17.86 18.00 ## [49] 18.23 18.30 18.70 19.00 19.00 19.00 19.00 19.01 19.20 19.30 19.70 20.26 ## [61] 21.30 En términos de frecuencias, \\(q_{0.5}\\) hace que la frecuencia acumulada \\(F_x\\) sea igual a \\(0.5\\) \\[\\sum_{i = 1}^m f_i =F_{q_{0.5}}=0.5\\] o \\[q_{0.5}=F^{-1}(0.5)\\] En el gráfico de distribución, la mediana es el valor de \\(x\\) en el que se encuentra la mitad del máximo de \\(F\\). El promedio y la mediana no siempre son iguales. 2.21 Dispersión Otras estadísticas de resumen importantes de las observaciones son las de dispersión. Muchos experimentos pueden compartir su media, pero difieren en cuán dispersos son los valores. La dispersión de las observaciones es una medida del ruido o de la aleatoridad del experimento. 2.22 Varianza muestral La dispersión sobre la media se mide con la varianza muestral \\[s^2=\\frac{1}{N-1} \\sum_{j=1}^N (x_j-\\bar{x})^2\\] Este número, mide la distancia cuadrada promedio de las observaciones al promedio. La razón de dividir por \\(N-1\\) se explicará cuando hablemos de inferencia, cuando estudiemos la dispersión de \\(\\bar{x}\\). En términos de las frecuencias relativas categóricas y ordenadas \\[s^2=\\frac{N}{N-1} \\sum_{i=1}^M (x_i-\\bar{x})^2 f_i\\] \\(s^2\\) se puede considerar como el momento de inercia de las observaciones. La raíz cuadrada de la varianza de la muestra se denomina desviación estándar \\(s\\). Ejemplo (misofonía) La desviación estándar del ángulo de convexidad es \\(s= [\\frac{1}{123-1}((7.97-10.19894)^2+ (18.23-10.19894)^2\\) \\(+ (12.27-10.19894)^2 + ...)]^{1/2} = 5.086707\\) La convexidad de la mandíbula se desvía de su media en \\(5.086707\\). 2.23 Rango intercuartílico (IQR) La dispersión de los datos también se puede medir con respecto a la mediana usando el rango intercuartílico: Definimos el primer cuartil como el valor \\(x_m\\) que hace que la frecuencia acumulada \\(F_{q_{0.25}}\\) sea igual a \\(0.25\\) (\\(x\\) es donde hemos acumulado una cuarta parte de las observaciones) \\[F_{q_{0.25}}=0.25\\] Definimos el tercer cuartil como el valor \\(x_m\\) que hace que la frecuencia acumulada \\(F_{q_{0.75}}\\) sea igual a \\(0.75\\) (\\(x\\) es donde hemos acumulado tres cuartos de observaciones) \\[F_{q_{0.75}}=0.75\\] El rango intercuartílico (IQR) es \\[IQR=q_{0.75} - q_{0.25}\\] Es la distancia entre el tercer y el primer cuartil y captura el \\(50\\%\\) central de las observaciones. 2.24 Diagrama de caja El rango intercuartílico, la mediana, el \\(5\\%\\) y el \\(95\\%\\) de los datos se pueden visualizar en un diagrama de caja. En el diagrama de caja, los valores de los resultados están en el eje y. El IQR es la caja, la mediana es la línea del medio y los bigotes marcan los \\(5\\%\\) y \\(95\\%\\) de los datos. 2.25 Preguntas 1) En el siguiente diagrama de caja, el primer cuartil y el segundo cuartil de los datos son: \\(\\qquad\\)a: \\((-1.00, 21.30)\\); \\(\\qquad\\)b: \\((-1.00, 7.02)\\); \\(\\qquad\\)c: \\((7.02, 7.96)\\); \\(\\qquad\\)d: \\((7.02, 14.22)\\) 2) La principal desventaja de un histograma es que: \\(\\qquad\\)a: Depende del tamaño del bin; \\(\\qquad\\)b: No se puede utilizar para variables categóricas; \\(\\qquad\\)c: No se puede usar cuando el tamaño del bin es pequeño; \\(\\qquad\\)d: Se usa solo para frecuencias relativas; 3) Si las frecuencias acumuladas relativas de un experimento aleatorio con resultados \\(\\{1,2,3,4\\}\\) son: \\(F(1)=0.15, \\qquad F(2)=0.60, \\qquad F(3)=0.85, \\qquad F(4)=1\\). Entonces la frecuencia relativa para el resultado \\(3\\) es \\(\\qquad\\)a: \\(0.15\\); \\(\\qquad\\)b: \\(0.85\\); \\(\\qquad\\)c: \\(0.45\\); \\(\\qquad\\)d: \\(0.25\\) 4) En una muestra de tamaño \\(10\\) de un experimento aleatorio obtuvimos los siguientes datos: \\(8,\\qquad 3,\\qquad 3,\\qquad 7,\\qquad 3,\\qquad 6,\\qquad 5,\\qquad 10,\\qquad 3,\\qquad 8\\). El primer cuartil de los datos es: \\(\\qquad\\)a: \\(3.5\\); \\(\\qquad\\)b: \\(4\\); \\(\\qquad\\)c: \\(5\\); \\(\\qquad\\)d: \\(3\\) 5) Imaginemos que recopilamos datos para dos cantidades que no son mutuamente excluyentes, por ejemplo, el sexo y la nacionalidad de los pasajeros de un vuelo. Si queremos hacer un solo gráfico circular para los datos, ¿cuál de estas afirmaciones es verdadera? \\(\\qquad\\)a: Solo podemos hacer un gráfico circular de nacionalidad porque tiene más de dos resultados posibles; \\(\\qquad\\)b: Podemos hacer un gráfico circular para una variable nueva que marca el sexo y la nacionalidad; \\(\\qquad\\)c: Podemos hacer un gráfico circular para la variale sexo o la variable nacionalidad; \\(\\qquad\\)d: Solo podemos elegir si hacemos un gráfico circular para el sexo o un gráfico circular para la nacionalidad. 2.26 Ejercicios 2.26.0.1 Ejercicio 1 Hemos realizado un experimento 8 veces con los siguientes resultados ## [1] 3 3 10 2 6 11 5 4 Responde las siguientes cuestiones: Calcula las frecuencias relativas de cada resultado. Calcula las frecuencias acumuladas de cada resultado. ¿Cuál es el promedio de las observaciones? ¿Cuál es la mediana? ¿Cuál es el tercer cuartil? ¿Cuál es el primer cuartil? 2.26.0.2 Ejercicio 2 Hemos realizado un experimento 10 veces con los siguientes resultados ## [1] 2.875775 7.883051 4.089769 8.830174 9.404673 0.455565 5.281055 8.924190 ## [9] 5.514350 4.566147 Considera 10 bins de tamaño 1: [0,1], (1,2]…(9,10). Responde las siguientes cuestiones: Calcula las frecuencias relativas de cada resultado y dibuja el histograma Calcula las frecuencias acumuladas de cada resultado y dibuja la gráfica acumulativa. Dibuja un diagrama de caja . "],["probabilidad.html", "Chapter 3 Probabilidad 3.1 Experimentos aleatorios 3.2 Medida de probabilidad 3.3 Probabilidad clásica 3.4 Frecuencias relativas 3.5 Frecuencias relativas en el infinito 3.6 Probabilidad frecuentista 3.7 Probabilidades clásicas y frecuentistas 3.8 Definición de probabilidad 3.9 Tabla de probabilidades 3.10 Espacio muestral 3.11 Eventos 3.12 Álgebra de eventos 3.13 Resultados mutuamente excluyentes 3.14 Probabilidades conjuntas 3.15 Tabla de contingencia 3.16 La regla de la suma: 3.17 Preguntas 3.18 Ejercicios", " Chapter 3 Probabilidad En este capítulo introduciremos el concepto de probabilidad a partir de frecuencias relativas. Definiremos los eventos como los elementos sobre los que se aplica la probabilidad. Los eventos compuestos se definirán usando álgebra de conjuntos. Luego discutiremos el concepto de probabilidad condicional derivado de la probabilidad conjunta de dos eventos. 3.1 Experimentos aleatorios Recordemos el objetivo básico de la estadística. La estadística se ocupa de los datos que se presentan en forma de observaciones. Una observación es la adquisición de un número o una característica de un experimento Las observaciones son las realizaciones de los resultados posibles de un experimento. Un resultado es una posible observación que es el resultado de un experimento. Al realizar un experimento multiples veces, a menudo obtenemos resultados diferentes. La descripción de la variabilidad de los resultados es uno de los objetivos de la estadística. Un experimento aleatorio es un experimento que da diferentes resultados cuando se repite de la misma manera. La pregunta filosófica detrás es ¿Cómo podemos llegar a conocer cualquier cosa si cada vez que observamos las cosas estas cambian? 3.2 Medida de probabilidad Nos gustaría tener una medida para un resultado de un experimento aleatorio de tal forma que nos diga cuán seguros estamos de observar el resultado cuando realicemos un futuro experimento aleatorio. Llamaremos a esta medida la probabilidad del resultado y le asignaremos valores: 0, cuando estamos seguros de que la observación no ocurrirá. 1, cuando estamos seguros de que la observación sí sucederá. 3.3 Probabilidad clásica Siempre que un experimento aleatorio tenga \\(M\\) resultados posibles que son todos igualmente probables, la probabilidad de cada resultado \\(i\\) es \\[P_i=\\frac{1}{M}\\]. La probabilidad clásica fue defendida por Laplace (1814). Dado que cada resultado es igualmente probable en este tipo de experimento, declaramos una completa ignorancia y lo mejor que podemos hacer es distribuir equitativamente la misma probabilidad para cada resultado. Notemos que No observamos \\(P_i\\). Deducimos \\(P_i\\) de nuestra razón y no necesitamos realizar ningún experimento para conocerla. Ejemplo (dado) ¿Cuál es la probabilidad de que obtengamos \\(2\\) en el lanzamiento de un dado? \\[P_2=1/6=0.166666\\] Diremos que estamos seguros de que uno entre seis lanzamientos es un \\(2\\) si lanzamos el dado muchas veces. También podemos decir que tenemos una seguridad del \\(16\\%\\) de que el pr'oximo lanzamiento es en \\(2\\). O finalmente, decimos que la probabilidad de obtener \\(2\\) es de \\(1/6\\). 3.4 Frecuencias relativas ¿Qué sucede con los experimentos aleatorios cuyos posibles resultados no son igualmente probables? ¿Cómo podemos entonces definir las probabilidades de los resultados? Ejemplo (experimento aleatorio) Imaginemos que repetimos un experimento aleatorio \\(8\\) veces y obtenemos las siguientes observaciones 8 4 12 7 10 7 9 12 ¿Qué tan seguros estamos de obtener el resultado \\(12\\) en la siguiente observación? La tabla de frecuencias es ## outcome ni fi ## 1 4 1 0.125 ## 2 7 2 0.250 ## 3 8 1 0.125 ## 4 9 1 0.125 ## 5 10 1 0.125 ## 6 12 2 0.250 La frecuencia relativa \\(f_i=\\frac{n_i}{N}\\) parece una medida razonable de probabilidad porque es un número entre \\(0\\) y \\(1\\); y mide la proporción del total de observaciones para un resultado particular. Como observamos \\(12\\) un \\(25\\%\\) de las veces, entonces \\(f_{12}=0.25\\) y estaríamos un cuarto seguros, una de cada 4 observaciones, de obtener \\(12\\). Pregunta: ¿Qué tan buena es \\(f_i\\) como medida de certeza del resultado \\(i\\)? Ejemplo (experimento aleatorio con mas repeticiones) Digamos que repetimos el experimento 100000 veces más: La tabla de frecuencias es ahora ## outcome ni fi ## 1 2 2807 0.02807 ## 2 3 5607 0.05607 ## 3 4 8435 0.08435 ## 4 5 11070 0.11070 ## 5 6 13940 0.13940 ## 6 7 16613 0.16613 ## 7 8 13806 0.13806 ## 8 9 10962 0.10962 ## 9 10 8402 0.08402 ## 10 11 5581 0.05581 ## 11 12 2777 0.02777 y el gráfico de barras es Aparecieron nuevos resultados y \\(f_{12}\\) ahora es solo \\(0.027\\), y entonces estamos sólo un \\(\\sim 3\\%\\) seguros de obtener \\(12\\) en el próximo experimento. Por lo tanto, las probabilidades medidas por \\(f_i\\) cambian con \\(N\\). 3.5 Frecuencias relativas en el infinito Una hecho crucial es que si calculamos \\(f_i\\) con valores crecientes de \\(N\\) ¡\\(f_i\\) converge! En este gráfico cada sección vertical da la frecuencia relativa de cada observación. Vemos que después de \\(N=1000\\) (\\(log10(N)=3\\)) las proporciones de las secciones apenas varían con mas \\(N\\). Encontramos que cada una de las frecuencias relativas \\(f_i\\) converge a un valor constante \\[lim_{N\\rightarrow \\infty} f_i = P_i\\] Observemos que \\(f_i\\) es una cantidad derivada de experiencias conctretas y que en infinito se convierte en una cantidad abstracta \\(P_i\\). Esta relación es fundamental. Nos dice que podemos usar la experiencia para acceder a conceptos generales. 3.6 Probabilidad frecuentista Llamamos Probabilidad \\(P_i\\) al límite cuando \\(N \\rightarrow \\infty\\) de la frecuencia relativa de observar el resultado \\(i\\) en un experimento aleatorio. Defendida por Venn (1876), la definición frecuentista de probabilidad se deriva de datos/experiencia (empírica). No observamos \\(P_i\\), observamos \\(f_i\\) Estimamos \\(P_i\\) con \\(f_i\\) (normalmente cuando \\(N\\) es grande), y escribimos: \\[\\hat{P_i}=f_i\\] Similar a la relación entre observación y resultado, tenemos la relación entre frecuencia relativa y probabilidad como un valor concreto de una cantidad abstracta. 3.7 Probabilidades clásicas y frecuentistas Tenemos situaciones en las que se puede usar la probabilidad clásica para encontrar el límite de frecuencias relativas. Si los resultados son igualmente probables, la probabilidad clásica nos da el límite: \\[P_i=lim_{N\\rightarrow \\infty} \\frac{n_i}{N}=\\frac{1}{M}\\] Si los resultados en los que estamos interesados pueden derivarse de otros resultados igualmente probables. Veremos más sobre esto cuando estudiemos los modelos de probabilidad. Ejemplo (suma de dos dados) Nuestro ejemplo anterior se basa en la suma de dos dados. Si bien realizamos el experimento muchas veces, anotamos los resultados y calculamos las frecuencias relativas, podemos conocer el valor exacto de probabilidad. Esta probabilidad se deduce del hecho de que el resultado de cada dado es igualmente probable. A partir de esta suposición, podemos encontrar que (Ejercicio 1) \\[ P_i= \\begin{cases} \\frac{i-1}{36},&amp; i \\in \\{2,3,4,5,6, 7\\} \\\\ \\frac{13-i}{36},&amp; i \\in \\{8,9,10,11,12\\} \\\\ \\end{cases} \\] La motivación de la definición frecuentista es empírica (datos) mientras que la de la definición clásica es racional (modelos). A menudo combinamos ambos enfoques (inferencia y deducción) para conocer las probabilidades de nuestro experimento aleatorio. 3.8 Definición de probabilidad Una probabilidad es un número que se asigna a cada resultado posible de un experimento aleatorio y satisface las siguientes propiedades o axiomas: Cuando los resultados \\(E_1\\) y \\(E_2\\) son mutuamente excluyentes; es decir, solo uno de ellos puede ocurrir, entonces la probabilidad de observar \\(E_1\\) o \\(E_1\\), escrito como \\(E_1\\cup E_2\\), es su suma: \\[P(E_1\\cup E_2) = P(E_1) + P(E_2)\\] Cuando \\(S\\) es el conjunto de todos los resultados posibles, entonces su probabilidad es \\(1\\) (al menos se observa algo): \\[P(S)=1\\] La probabilidad de cualquier resultado está entre 0 y 1 \\[P(E) \\in [0,1]\\] Propuesto por Kolmogorov’s hace menos de 100 años (1933). 3.9 Tabla de probabilidades Las propiedades de Kolmogorov son las reglas básicas para construir una tabla de probabilidad, de manera similar a la tabla de frecuencia relativa. Ejemplo (Dado) La tabla de probabilidad para el lanzamiento de un dado Resultado Probabilidad \\(1\\) 1/6 \\(2\\) 1/6 \\(3\\) 1/6 \\(4\\) 1/6 \\(5\\) 1/6 \\(6\\) 1/6 \\(P(1 \\cup 2\\cup ... \\cup 6)\\) 1 Verifiquemos los axiomas: Donde \\(1 \\cup 2\\) es, por ejemplo, el evento de lanzar un \\(1\\) o un \\(2\\). Entonces \\[P(1 \\cup 2)=P(1)+P(2)=2/6\\] Como \\(S=\\{1,2,3,4,5,6\\}\\) se compone de resultados mutuamente excluyentes, entonces \\[P(S)=P(1\\cup 2\\cup ... \\cup 6) = P(1)+P(2)+ ...+P(6)=1\\] Las probabilidades de cada uno de resultados están entre \\(0\\) y \\(1\\), como se verifica en la tabla. 3.10 Espacio muestral El conjunto de todos los resultados posibles de un experimento aleatorio se denomina espacio muestral y se denota como \\(S\\). El espacio muestral puede estar formado por resultados categóricos o numéricos. Por ejemplo: temperatura humana: \\(S = (36, 42)\\) grados Celsius. niveles de azúcar en humanos: \\(S=(70-80) mg/dL\\) el tamaño de un tornillo de una línea de producción: \\(S=(70-72) mm\\) número de correos electrónicos recibidos en una hora: \\(S =\\{0, ...\\infty \\}\\) el lanzamiento de un dado: \\(S=\\{1, 2, 3, 4, 5, 6\\}\\) 3.11 Eventos Un evento \\(A\\) es un subconjunto del espacio muestral. Es una colección de resultados. Ejemplos de eventos: El evento de una temperatura normal: \\(A=37-38\\) grados Celsius El evento de producir un tornillo con un tamaño: \\(A=71.5mm\\) El evento de recibir más de 4 emails en una hora: \\(A=\\{4, \\infty \\}\\) El evento de obtener un número menor o igual a 3 en el lanzamiento de un dado: \\(A=\\{1,2,3\\}\\) Un evento se refiere a un posible conjunto de resultados. 3.12 Álgebra de eventos Para dos eventos \\(A\\) y \\(B\\), podemos construir los siguientes eventos compuestos utilizando las operaciones básicas de conjuntos: Complemento \\(A&#39;\\): el evento de no \\(A\\) Unión \\(A \\cup B\\): el evento de \\(A\\) o \\(B\\) Intersección \\(A \\cap B\\): el evento de \\(A\\) y \\(B\\) Ejemplo (dado) Lancemos un dado y veamos los eventos (resultados compuestos): un número menor o igual a tres \\(A:\\{1,2,3\\}\\) un número par \\(B:\\{2,4,6\\}\\) Veamos como podemos construir nuevos eventos con las operaciones de conjuntos: un número no menor de tres: \\(A&#39;:\\{4,5,6\\}\\) un número menor o igual a tres o par: \\(A \\cup B: \\{1,2,3,4,6\\}\\) un número menor o igual a tres y par \\(A \\cap B: \\{2\\}\\) 3.13 Resultados mutuamente excluyentes Los resultados como lanzar \\(1\\) y \\(2\\) en un dado son eventos que no pueden ocurrir al mismo tiempo. Decimos que son mutuamente excluyentes. En general, dos eventos denotados como \\(E_1\\) y \\(E_2\\) son mutuamente excluyentes cuando no tienen nada en común \\[E_1\\cap E_2=\\emptyset\\] Ejemplos: Los siguientes eventos son mutamente excluyentes. El resultado de tener misofonía de gravedad \\(1\\) y gravedad \\(4\\). Sólo es posibl tener una severidad. Los resultados de obtener \\(12\\) y \\(5\\) al sumar el lanzamiento de dos dados. Si obtenemos \\(12\\) no obtenemos \\(5\\). De acuerdo con las propiedades de Kolmogorov, solo los resultados mutuamente excluyentes se pueden organizar en tablas de probabilidad, como en las tablas de frecuencias relativas. 3.14 Probabilidades conjuntas La probabilidad conjunta de \\(A\\) y \\(B\\) es la probabilidad de \\(A\\) y \\(B\\). Eso es \\[P(A \\cap B)\\] o \\(P(A,B)\\). Para escribir probabilidades conjuntas de eventos no mutuamente excluyentes (\\(A \\cap B \\neq \\emptyset\\)) en una tabla de probabilidad, notamos que siempre podemos descomponer el espacio muestral en conjuntos mutuamente excluyentes que involucran las intersecciones: \\[S=\\{A\\cap B, A \\cap B&#39;, A&#39;\\cap B, A&#39;\\cap B&#39;\\}\\] Consideremos el diagrama de Ven para el ejemplo donde \\(A\\) es el evento que corresponde a sacar número menor o igual que \\(3\\) y \\(B\\) corresponde a un número par: Las marginales de \\(A\\) y \\(B\\) son la probabilidad de \\(A\\) y la probabilidad de \\(B\\), respectivamente: \\(P(A)=P(A\\cap B&#39;) + P(A \\cap B)=2/6+1/6=3/6\\) \\(P(B)=P(A&#39;\\cap B) +P(A \\cap B)=2/6+1/6=3/6\\) Podemos ahora escribir la tabla de probabilidad para las probabilidades conjuntas Resultado Probabilidad \\((A \\cap B)\\) \\(P(A \\cap B)=1/6\\) \\((A\\cap B&#39;)\\) \\(P(A \\cap B&#39;)=2/6\\) \\((A&#39; \\cap B)\\) \\(P(A&#39; \\cap B)=2/6\\) \\((A&#39; \\cap B&#39;)\\) \\(P(A&#39; \\cap B&#39;)=1/6\\) suma \\(1\\) Cada resultado tiene \\(dos\\) valores (uno para la característica del tipo \\(A\\) y otro para el tipo \\(B\\)) 3.15 Tabla de contingencia La tabla de probabilidad conjunta también se puede escribir en una tabla de contingencia \\(B\\) \\(B&#39;\\) suma \\(A\\) \\(P(A \\cap B )\\) \\(P(A\\cap B&#39; )\\) \\(P(A)\\) \\(A&#39;\\) \\(P(A&#39;\\cap B )\\) \\(P(A&#39;\\cap B&#39; )\\) \\(P(A&#39;)\\) suma \\(P(B)\\) \\(P(B&#39;)\\) 1 Donde las marginales son las sumas en las márgenes de la tabla, por ejemplo: \\(P(A)=P(A \\cap B&#39;) + P(A \\cap B)\\) \\(P(B)=P(A&#39; \\cap B) +P(A \\cap B)\\) En nuestro ejemplo, la tabla de contingencia es \\(B\\) \\(B&#39;\\) suma \\(A\\) \\(1/6\\) \\(2/6\\) \\(3/6\\) \\(A&#39;\\) \\(2/6\\) \\(1/6\\) \\(3/6\\) suma \\(3/6\\) \\(3/6\\) 1 3.16 La regla de la suma: La regla de la suma nos permite calcular la probabilidad de \\(A\\) o \\(B\\), \\(P(A \\cup B)\\), en términos de la probabilidad de \\(A\\) y \\(B\\), \\(P(A \\cap B)\\). Podemos hacer esto de tres maneras equivalentes: Usando las marginales y la probabilidad conjunta \\[P(A \\cup B)=P(A) + P(B) - P(A\\cap B)\\] Usando solo probabilidades conjuntas \\[P(A \\cup B)=P(A \\cap B)+P(A\\cap B&#39;)+P(A&#39;\\cap B)\\] Usando el complemento de la probabilidad conjunta \\[P(A \\cup B)=1-P(A&#39;\\cap B&#39;)\\] Ejemplo (dado) Tomemos los eventos \\(A:\\{1,2,3\\}\\), sacar un número menor o igual que \\(3\\), y \\(B:\\{2,4,6\\}\\), sacar un número par en el lanzamiento de un dado. Por lo tanto: \\(P(A \\cup B)=P(A) + P(B) - P(A\\cap B)=3/6+3/6-1/6=5/6\\) \\(P(A \\cup B)=P(A \\cap B)+P(A\\cap B&#39;)+P(A&#39;\\cap B)=1/6+2/6+2/6=5/6\\) \\(P(A \\cup B)=1-P(A&#39;\\cap B&#39;)= 1-1/6=5/6\\) En la tabla de contingencia \\(P(A \\cup B)\\) corresponde a las casillas en negrita (método 2 arriba), o todas menos el 1/6 de abajo a la derecha (método 3). \\(B\\) \\(B&#39;\\) \\(A\\) 1/6 2/6 \\(A&#39;\\) 2/6 1/6 3.17 Preguntas Recopilamos la edad y categoría de 100 deportistas en una competición \\(edad:junior\\) \\(edad:senior\\) \\(categoria:1ra\\) \\(14\\) \\(12\\) \\(categoria:2a\\) \\(21\\) \\(18\\) \\(categoria:3a\\) \\(22\\) \\(13\\) 1) ¿Cuál es la probabilidad estimada de que un deportista sea de 2ª categoría y senior? \\(\\qquad\\)a: \\(18/100\\); \\(\\qquad\\)b: \\(18/43\\); \\(\\qquad\\)c: \\(18\\); \\(\\qquad\\)d: \\(18/39\\) 2) ¿Cuál es la probabilidad estimada de que el atleta no esté en la tercera categoría y sea senior? \\(\\qquad\\)a: \\(35/100\\); \\(\\qquad\\)b: \\(30/100\\); \\(\\qquad\\)c: \\(22/100\\); \\(\\qquad\\)d: \\(13/100\\) 3) ¿Cuál es la probabilidad marginal de la tercera categoría? \\(\\qquad\\)a: \\(13/100\\); \\(\\qquad\\)b: \\(35/100\\); \\(\\qquad\\)c: \\(22/100\\); \\(\\qquad\\)d: \\(13/22\\) 4) ¿Cuál es la probabilidad marginal de ser senior? \\(\\qquad\\)a: \\(13/100\\); \\(\\qquad\\)b: \\(43/100\\); \\(\\qquad\\)c: \\(43/57\\); \\(\\qquad\\)d: \\(57/100\\) 5) ¿Cuál es la probabilidad de ser senior o de tercera categoría? \\(\\qquad\\)a: \\(65/100\\); \\(\\qquad\\)b: \\(86/100\\); \\(\\qquad\\)c: \\(78/100\\); \\(\\qquad\\)d: \\(13/100\\) 3.18 Ejercicios 3.18.0.1 Probabilidad clásica: Ejercicio 1 Escribe la tabla de probabilidad conjunta para los resultados de lanzar dos dados; en las filas escribe los resultados del primer dado y en las columnas los resultados del segundo dado. ¿Cuál es la probabilidad de sacar \\((3,4)\\)? (R:1/36) ¿Cuál es la probabilidad de tirar \\(3\\) y \\(4\\) con cualquiera de los dos dados? (R:2/36) ¿Cuál es la probabilidad de tirar \\(3\\) en el primer dado o \\(4\\) en el segundo? (A:11/36) ¿Cuál es la probabilidad de tirar \\(3\\) o \\(4\\) con cualquier dado? (R:20/36) Escribe la tabla de probabilidad para el resultado de la suma de dos dados. Supon que el resultado de cada dado es igualmente probable. Verifica que es: \\[ P_i= \\begin{cases} \\frac{i-1}{36},&amp; i \\in \\{2,3,4,5,6, 7\\} \\\\ \\frac{13-i}{36},&amp; i \\in \\{8,9,10,11,12\\} \\\\ \\end{cases} \\] 3.18.0.2 Probabilidad frecuentista: Ejercicio 2 El resultado de un experimento aleatorio es medir la gravedad de la misofonía y el estado de depresión de un paciente. Gravedad de la misofonía: \\(S_M:\\{M_0,M_1,M_2,M_3,M_4\\}\\) Depresión: \\(S_D:\\{D&#39;, D\\}\\)) Escribe la tabla de contingencia para las frecuencias absolutas (\\(n_{M,D}\\)) para un estudio sobre un total de 123 pacientes en el que se observó 100 individuos no tuvieron depresión. Ningún individuo con misofonía 4 y sin depresión. 5 individuos con misofonía de grado 1 y sin depresión. El mismo número que el caso anterior para individuos con depresión y sin misofonía. 25 individuos sin depresión y grado 3 de misofonía. El número de misofónicos sin depresión para los grados 2 y 0 se repartieron a cantiaddes iguales. El número de individuos con depresión y misofonía incrementó progresivamente en múltiplos de tres, empezando en 0 individuos para grado 1. Reponde las siguientes preguntas: ¿Cuantos individuos tuvieron misofonía? (R:83) ¿Cuantos individuos tuvieron misofonía de grado 3? (R:31) ¿Cuantos individuos tuvieron misofonía de grado 2 sin depresión? (R:35) Escribe las tabla de consingencia para frecuencias relativas \\(f_{M,D}\\). Supongamos que \\(N\\) es grande y que las frecuencias absolutas estiman las probabilidades \\(f_{M,D}=\\hat{P}(M \\cap D)\\). Responde las siguientes preguntas: ¿Cuál es la probabilidad marginal de misofonía de gravedad 2? (R: 0.3) ¿Cuál es la probabilidad de no ser misofónico y no estar deprimido? (R:0.284) ¿Cuál es la probabilidad de ser misofónico o estar deprimido? (R: 0.715) ¿Cuál es la probabilidad de ser misofónico y estar deprimido? (R: 0.146) Describir en lenguaje hablado los resultados con probabilidad 0. 3.18.0.3 Ejercicio 3 Hemos realizado un experimento aleatorio \\(10\\) veces, que consiste en anotar el sexo y el estado vital de pacientes con algún tipo de cáncer después de 10 años del diagnóstico. Obtuvimos los siguientes resultados ## A B ## 1 male dead ## 2 male dead ## 3 male dead ## 4 female alive ## 5 male dead ## 6 female alive ## 7 female dead ## 8 female alive ## 9 male alive ## 10 male alive Crea la tabla de contingencia para el número (\\(n_{i,j}\\)) de observaciones de cada resultado (\\(A,B\\)) Crea la tabla de contingencia para la frecuencia relativa (\\(f_{i,j}\\)) de los resultados ¿Cuál es la frecuencia marginal de ser hombre? (R/0.6) ¿Cuál es la frecuencia marginal de estar vivo? (R/0.5) ¿Cuál es la frecuencia de estar vivo o ser mujer? (R/0.6) 3.18.0.4 Teoría: Ejercicio 4 De la segunda forma de la regla de la suma, obtener la primera y la tercera forma. ¿Cuál es la regla de la suma de la tercera forma para la probabilidad de tres eventos \\(P(A \\cup B \\cup C)\\)? "],["probabilidad-condicional.html", "Chapter 4 Probabilidad condicional 4.1 Probabilidad conjunta 4.2 Independencia estadística 4.3 La probabilidad condicional 4.4 Tabla de contingencia condicional 4.5 Independencia estadística 4.6 Dependencia estadística 4.7 Prueba de diagnóstico 4.8 Probabilidades inversas 4.9 Teorema de Bayes 4.10 Preguntas 4.11 Ejercicios", " Chapter 4 Probabilidad condicional En este capítulo, introduciremos la probabilidad condicional. Usaremos la probabilidad condicional para definir la independencia estadística. Discutiremos el teorema de Bayes y discutiremos una de sus principales aplicaciones: la eficacia de predicción de una herramienta de diagnóstico. 4.1 Probabilidad conjunta Recordemos que la probabilidad conjunta de dos eventos \\(A\\) y \\(B\\) se define como su intersección \\[P( A,B )=P(A \\cap B)\\] Ahora, imagina experimentos aleatorios que miden dos tipos diferentes de resultados. altura y peso de un individuo: \\((h, w)\\) tiempo y posición de una carga eléctrica: \\((p, t)\\) el lanzamiento de dos dados: (\\(n_1\\),\\(n_2\\)) cruzar dos semáforos en verde (no rojo): (\\(R&#39;_1\\) , \\(R&#39;_2\\)) A menudo nos interesa saber si los valores de un resultado condicionan los valores del otro. 4.2 Independencia estadística En muchos casos, estamos interesados en saber si dos eventos a menudo tienden a ocurrir juntos. Queremos poder discernir entre dos casos. Independencia entre eventos. Por ejemplo, sacar un 1 en un dado no hace más probable sacar otro 1 en un segundo dado. Correlación entre eventos. Por ejemplo, si un hombre es alto, probablemente sea pesado. Ejemplo (conductor) Realizamos un experimento para averiguar si observar fallas estructurales en un material afecta su conductividad. Los datos se verían como Conductor Estructura Conductividad \\(c_1\\) con fallas defectuosa \\(c_2\\) sin fallas sin defectos \\(c_3\\) con fallas defectuosa … … … \\(c_i\\) sin fallas defectuosa* … … … … … … \\(c_n\\) con fallas sin defectos* Podemos esperar que la conductividad defectuosa ocurra más a menudo con fallas que sin fallas si las fallas afectan la conductividad. Imaginemos que a partir de los datos obtenemos la siguiente tabla de contingencia de probabilidades conjuntas estimadas con fallas (F) sin fallas (F’) suma defectuoso (D) \\(0.005\\) \\(0.045\\) \\(0.05\\) sin defectos (D’) \\(0.095\\) \\(0.855\\) \\(0.95\\) suma \\(0.1\\) \\(0.9\\) 1 donde, por ejemplo, la probabilidad conjunta de \\(F\\) y \\(D\\) es \\(P(D,F)=0.005\\) y las probabilidades marginales son \\(P(D)=P(D, F) + P(D, F&#39;)=0.05\\) \\(P(F)=P(D, F) + P(D&#39;, F)= 0.1\\). 4.3 La probabilidad condicional La conductividad defectuosa es independiente de tener fallas estructurales en el material si la probabilidad de tener conductividad defectuosa (\\(D\\)) es la misma ya sea que tenga fallas (\\(F\\)) o no (\\(F&#39;\\)) . Consideremos primero solamente los materiales que tienen fallas. Dentro de aquellos materiales que tienen fallas (\\(F\\)), ¿cuál es la probabilidad estimada de que tengan conductividad defectuosa? \\(\\hat{P}(D|F)=\\frac{n_{F,D}}{n_{F}}=\\frac{n_{F,D}/n}{n_{F}/n}= \\frac{f_{F,D}}{f_{F}}\\) \\[= \\frac{\\hat{P}(D,F)}{\\hat{P}(F)}\\] Por lo tanto, en el límite cuando \\(N \\rightarrow \\infty\\), tenemos \\[P(D|F)=\\frac{P(F, D)}{P(F)}=\\frac{P(F \\cap D)}{P(F)}\\] Definición: La probabilidad condicional de un evento \\(B\\) dado un evento \\(A\\), indicado como \\(P(A|B)\\), es \\[P(A|B) = \\frac{P(A\\cap B)}{P(B)}\\] Podemos probar que la probabilidad condicional satisface los axiomas de probabilidad. La probabilidad condicional se puede entender como una probabilidad con un espacio muestral dado por \\(B\\): \\(S_B\\). En nuestro ejemplo, los materiales con fallas. 4.4 Tabla de contingencia condicional Si dividimos las columnas de la tabla de probabilidad conjunta por las probabilidades marginales de los efectos condicionantes (\\(F\\) y \\(F&#39;\\)), podemos escribir una tabla de contingencia condicional F F’ D P( D | F) P(D | F’) D’ P(D’ | F) P(D’ | F’) suma 1 1 Donde las probabilidades por columnas suman uno. La primera columna muestra las probabilidades de ser defectuoso o no solo de que los materiales que tienen fallas (primera condición: \\(F\\)). La segunda columna muestra las probabilidades solo para los materiales que no tienen fallas (segunda condición: \\(F&#39;\\)). Las probabilidades condicionales son las probabilidades del evento dentro de cada condición. Las leemos como: \\(P(D|F)\\): Probabilidad de tener conductividad defectuosa si tiene fallas \\(P(D&#39;|F)\\): Probabilidad de no tener conductividad defectuosa si tiene fallas \\(P(D|F&#39;)\\): Probabilidad de tener conductividad defectuosa si no tiene fallas \\(P(D&#39;|F&#39;)\\) Probabilidad de no tener conductividad defectuosa si no tiene fallas 4.5 Independencia estadística En nuestro ejemplo, la tabla de contingencia condicional es F F’ D P(D|F) = 0.05 P(D|F’)=0.05 D’ P(D’|F)=0.95 P(D’|F’)=0.95 suma 1 1 ¡Observamos que las probabilidades marginales y condicionales son las mismas! \\(P(D|F)=P(D|F&#39;)=P(D)\\) \\(P(D&#39;|F)=P(D&#39;|F&#39;)=P(D&#39;)\\) Esto quiere decir que la probabilidad de observar un conductor defectuoso no depende tener una falla estructural o no. Concluimos que la conductividad defectuosa no se ve afectada por tener una falla estructural. Definición Dos eventos \\(A\\) y \\(B\\) son estadísticamente independientes si ocurre cualquiera de los casos equivalentes \\(P(A|B)=P(A)\\); \\(A\\) es independiente de \\(B\\) \\(P(B|A)=P(B)\\); \\(B\\) es independiente de \\(A\\) y por la definición de probabilidad condicional \\(P(A\\cap B)=P(A|B)P(B)=P(A)P(B)\\) Esta tercera forma es un enunciado sobre las probabilidades conjuntas. Dice que podemos obtener probabilidades conjuntas por la multiplicación de las marginales. En nuestra tabla de probabilidad conjunta original F F’ suma D \\(0.005\\) \\(0.045\\) \\(0.05\\) D’ \\(0.095\\) \\(0.855\\) \\(0.95\\) suma \\(0.1\\) \\(0.9\\) 1 podemos confirmar que todas las entradas de la matriz son el producto de las marginales. Por ejemplo: \\(P(F)P(D)= P(D \\cap F)\\) y \\(P(D&#39;)P(F&#39;)=P(D&#39; \\cap F&#39;)\\). Por lo tanto, tener una falla estructural es independiente de tener conductividad defectuosa. Ejemplo (Monedas) Queremos confirmar que los resultados de lanzar dos monedas son independientes. Consideramos que todos los resultados son igualmente probables: resultado Probabilidad \\((H,T)\\) 1/4 \\((H,H)\\) 1/4 \\((T,T)\\) 1/4 \\((T,H)\\) 1/4 suma 1 donde \\((H,T)\\) es, por ejemplo, el evento de cara en la primera moneda y cruz en la segunda moneda. La tabla de contingencia para las probabilidades conjuntas es: H T suma H \\(1/4\\) \\(1/4\\) \\(1/2\\) T \\(1/4\\) \\(1/4\\) \\(1/2\\) suma \\(1/2\\) \\(1/2\\) 1 De esta tabla vemos que la probabilidad de obtener una cara y luego una cruz es el producto de las marginales \\(P(H, T)=P(H)P(T)=1/4\\). Por lo tanto, el evento de cara en la primera moneda y cruz en la segunda son independientes. Si elaboramos la tabla probabilidad condicional sobre el lanzamiento de la primera moneda veremos que obtener cruz en la segunda moneda no está condicionado por haber obtenido cara en la primera moneda: \\(P(T|H)=P(T) =1/2\\). 4.6 Dependencia estadística Un ejemplo importante de dependencia estadística se encuentra en el desempeño de herramientas de diagnóstico, donde queremos determinar el estado de un sistema (s) con resultados inadecuado (si) adecuado (no) con una prueba (t) con resultados positivo negativo Por ejemplo, probamos una batería para saber cuánto tiempo puede durar. Tensamos un cable para saber si resiste llevar cierta carga. Realizamos una PCR para ver si alguien está infectado. 4.7 Prueba de diagnóstico Consideremos diagnosticar una infección con una nueva prueba. Estado de infección: si (infectado) no (no infectado) Prueba: positivo negativo La tabla de contingencia condicional es lo que obtenemos en un ambiente controlado (laboratorio) Infección: sí Infección: no Test: positivo P(pos | sí) P(pos | no) Test: negativo P(neg | sí) P(neg | no) suma 1 1 Miremos las entradas de la tabla Tasa de verdaderos positivos (Sensibilidad): La probabilidad de dar positivo si tiene la enfermedad \\(P(pos|sí)\\) Tasa de verdaderos negativos (Especificidad): La probabilidad de dar negativo si no tiene la enfermedad \\(P(neg|no)\\) Tasa de falsos positivos: la probabilidad de dar positivo si no tiene la enfermedad \\(P(pos|no)\\) Tasa de falsos negativos: la probabilidad de dar negativo si tiene la enfermedad \\(P(neg|si)\\) Alta correlación (dependencia estadística) entre la prueba y la infección significa valores altos de las probabilidades 1 y 2 y valores bajos para las probabilidades 3 y 4. Ejemplo (COVID) Ahora consideremos una situación real. En los días iniciales de la pandemia de coronavirus no había una medida de la eficacia de las PCR para detectar el virus. Uno de los primeros estudios publicados (https://www.nejm.org/doi/full/10.1056/NEJMp2015897) encontró que Las PCR tuvieron una sensibilidad del 70%, en condición de infección. Las PCR tuvieron una especificidad del 94%, en condición de no infección. La tabla de contingencia condicional es Infección: sí Infección: no Test: positivo P(pos | sí)=0.7 P(pos | no)=0.06 Test: negativo P(neg | sí)=0.3 P(neg | no)=0.94 suma 1 1 Por lo tanto, los errores en las pruebas de diagnóstico fueron: La tasa de falsos positivos es \\(P(pos|no)=0.06\\) La tasa de falsos negativos es \\(P(neg|si)=0.3\\) 4.8 Probabilidades inversas Nos interesa encontrar la probabilidad de estar infectado si la prueba da positivo: \\[P(sí|pos)\\] Para eso: Recuperamos la tabla de contingencia para probabilidades conjuntas, multiplicando por las marginales Infección: sí Infección: No suma Test: positivo P(pos | sí)P(sí) P(pos | no)P(no) P(pos) Test: negativo P(neg | sí)P(sí) P(neg | no) P(no) P(neg) suma P(sí) P(no) 1 Usamos la definición de probabilidades condicionales para filas en lugar de columnas (dividimos por la marginal de los resultados de la prueba) Infección: sí Infección: no sum Test: positivo P(sí|pos) P(no|pos) 1 Test: negative P(sí|neg) P(no|neg) 1 Donde ahora las filas suman 1. Para encontrar las entradas de esta tabla debemos dividir la probabilidad conjunta por las marginales de las filas. Por ejemplo: \\[P(si|pos)=\\frac{P(pos|si)P(si)}{P(pos)}\\] \\(P(pos|yes)\\) was the result of the study (0.7). Sin embargo, para aplicar esta fórmula necesitamos las marginales \\(P(si)\\) (incidencia) y \\(P(pos)\\). \\(P(si)\\) tiene que ser dada. Por ejemplo, necesitamos un nuevo estudio: el primer estudio de prevalencia en España mostró que durante el confinamiento \\(P(si)=0.05\\), \\(P(no)=0.95\\), antes del verano de 2020. Para encontrar \\(P(pos)\\), podemos usar la definición de probabilidad marginal en términos de las condicionales: \\(P(pos)=P(pos \\cap sí) + P(pos \\cap no)\\) \\[= P(pos|sí)P(sí)+P(pos|no)P(no)\\] Esta última relación de las marginales se llama regla de probabilidad total. 4.9 Teorema de Bayes Después de sustituir la regla de probabilidad total en \\(P(sí|pos)\\), tenemos \\[P(sí|pos)=\\frac{P(pos|sí)P(sí)}{P(pos|sí)P(sí)+P(pos|no)P(no)}\\] Esta expresión se conoce como teorema de Bayes. Nos permite invertir los condicionales: \\[P(pos|si) \\rightarrow P(si|pos)\\] O evaluar una prueba en una condición controlada (infección) y luego usarla para inferir la probabilidad de la condición cuando la prueba es positiva. Ejemplo (COVID): El rendimiento de la prueba fue: Sensibilidad: \\(P(pos|si)=0.70\\) Tasa de falsos positivos: \\(P(pos|no)=1- P(neg|no)=0.06\\) El estudio en población española dio: \\(P(sí)=0.05\\) \\(P(no)=1-P(sí)=0.95\\). Por lo tanto, la probabilidad de estar infectado en caso de dar positivo era: \\[P(si|pos)=0.38\\] Concluimos que en ese momento las PCR no eran muy buenas para confirmar infecciones. Sin embargo, apliquemos ahora el teorema de Bayes a la probabilidad de no estar infectado si la prueba fue negativa. \\[P(no|neg) = \\frac{P(neg|no) P(no)}{P(neg|no) P(no)+P(neg|sí)P(sí)}\\] La sustitución de todos los valores da \\[P(no|neg)=0.98\\] Por lo tanto, las pruebas eran buenas para descartar infecciones y un justo requisito para viajar. Teorema de Bayes En general, podemos tener más de dos eventos condicionantes. Pensemos por ejemplo en diferentes grados de enfermedad (leve, moderado, crítico). Por lo tanto, el teorema de Baye dice: Si \\(E_1, E_2, ..., E_k\\) son \\(k\\) eventos mutuamente excluyentes y exhaustivos (grados de enfermedad) y \\(B\\) es cualquier evento (hospitalización), entonces la probabilidad inversa \\(P(E_i|B)\\) (la probabilidad de encontar pacientes con severidad \\(i\\) en en hospital) es \\[P(E_i| B)= \\frac{P(B|E_i)P(E_i)}{P(B|E_0)P(E_0) +...+ P(B|E_k)P(E_k)}\\] El denominador es la regla de probabilidad total para la marginal \\(P(B)\\) (probabiliadd de hospitalización), en términos de las marginales \\(P(E_1), P(E_2), ... P(E_k)\\) (probabilidad de cada severidad). \\[P(B)=P(B|E_0)P(E_0) +...+ P(B|E_k)P(E_k)\\] Árbol condicional La regla de probabilidad total también se puede ilustrar usando un árbol condicional. Regla de probabilidad total para la marginal de \\(B\\) se puede resumir resolviendo esta pregunta ¿De cuántas maneras puedo obtener el resultado \\(B\\), si conozco \\(A\\) y sus probabilidades condicionales? \\(P(B)=P(B|A)P(A)+P(B|A&#39;)P(A&#39;)\\) 4.10 Preguntas Recopilamos la edad y categoría de 100 deportistas en una competición \\(junior\\) \\(senior\\) \\(1er\\) \\(14\\) \\(12\\) \\(2do\\) \\(21\\) \\(18\\) \\(3er\\) \\(22\\) \\(13\\) 1) ¿Cuál es la probabilidad estimada de que el atleta esté en la tercera categoría si el atleta es junior? \\(\\qquad\\)a: \\(22\\); \\(\\qquad\\)b: \\(22/100\\); \\(\\qquad\\)c: \\(22/57\\); \\(\\qquad\\)d: \\(22/35\\); 2) ¿Cuál es la probabilidad estimada de que el atleta sea junior y esté en 1ra categoría si el atleta no está en 3ra categoría? \\(\\qquad\\)a: \\(14/35\\); \\(\\qquad\\)b: \\(14/65\\); \\(\\qquad\\)c: \\(14/100\\); \\(\\qquad\\)d: \\(14/26\\) 3) Una prueba diagnóstica tiene una probabilidad de \\(8/9\\) de detectar una enfermedad si los pacientes están enfermos y una probabilidad de \\(3/9\\) de detectar la enfermedad si los pacientes están sanos. Si la probabilidad de estar enfermo es \\(1/9\\). ¿Cuál es la probabilidad de que un paciente esté enfermo si una prueba detecta la enfermedad? \\(\\qquad\\)a: \\(\\frac{8/9}{8/9+3/9}*1/9\\); \\(\\qquad\\)b: \\(\\frac{3/9}{8/9+3/9}*1/9\\); \\(\\qquad\\)c: \\(\\frac{3/9*8/9}{8/9*1/9+3/9*8/9}\\); \\(\\qquad\\)d: \\(\\frac{8/9*1/9}{8/9*1/9+3/9*8/9}\\); 4) Como se comenta en las notas, una prueba PCR para coronavirus tenía una sensibilidad del 70 % y una especificidad del 94 % y en España durante el confinamiento hubo una incidencia del 5 %. Con estos datos, ¿cuál era la probabilidad de dar positivo en España (\\(P(pos)\\)) \\(\\qquad\\)a: \\(0.035\\); \\(\\qquad\\)b: \\(0.092\\); \\(\\qquad\\)c: \\(0.908\\); \\(\\qquad\\)d: \\(0.95\\) 5) Con los mismos datos que en la pregunta 4, dar positivo en la PCR y estar infectado no son eventos independientes porque: \\(\\qquad\\)a: La sensibilidad es del 70%; \\(\\qquad\\)b: La sensibilidad y la tasa de falsos positivos son diferentes; \\(\\qquad\\)c: La tasa de falsos positivos es del 0.06%; \\(\\qquad\\)d: la especificidad es del 96% 4.11 Ejercicios 4.11.0.1 Ejercicio 1 Se prueba el rendimiento de una máquina para producir varillas de torneado de alta calidad. Estos son los resultados de las pruebas Redondeado: si Redondeado: No superficie lisa: si 200 1 superficie lisa: no 4 2 ¿Cuál es la probabilidad estimada de que la máquina produzca una varilla que no satisfaga ningún control de calidad? (R: 2/207) ¿Cuál es la probabilidad estimada de que la máquina produzca una varilla que no satisfaga al menos un control de calidad? (R: 7/207) ¿Cuál es la probabilidad estimada de que la máquina produzca varillas de superficie redondeada y alisada? (R: 200/207) ¿Cuál es la probabilidad estimada de que la barra sea redondeada si la barra es lisa? (R: 200/201) ¿Cuál es la probabilidad estimada de que la varilla sea lisa si es redondeada? (R: 200/204) ¿Cuál es la probabilidad estimada de que la varilla no sea ni lisa ni redondeada si no satisface al menos un control de calidad? (R: 2/7) ¿Son eventos independientes la lisa y la redondez? (No) 4.11.0.2 Ejercicio 2 Desarrollamos un test para detectar la presencia de bacterias en un lago. Encontramos que si el lago contiene la bacteria, la prueba es positiva el 70% de las veces. Si no hay bacterias, la prueba es negativa el 60% de las veces. Implementamos la prueba en una región donde sabemos que el 20% de los lagos tienen bacterias. ¿Cuál es la probabilidad de que un lago que dé positivo esté contaminado con bacterias? (R: 0.30) 4.11.0.3 Ejercicio 3 Se prueba el rendimiento de dos máquinas para producir varillas de torneado de alta calidad. Estos son los resultados de las pruebas Máquina 1 Redondeado: si Redondeado: No superficie lisa: si 200 1 superficie lisa: no 4 2 Máquina 2 Redondeado: si Redondeado: No superficie lisa: si 145 4 superficie lisa: no 8 6 ¿Cuál es la probabilidad de que la barra sea redondeada? (R: 357/370) ¿Cuál es la probabilidad de que la varilla haya sido producida por la máquina 1? (R: 207/370) ¿Cuál es la probabilidad de que la varilla no sea lisa? (R: 20/370) ¿Cuál es la probabilidad de que la varilla sea lisa o redondeada o producida por la máquina 1? (R: 364/370) ¿Cuál es la probabilidad de que la varilla quede redondeada si es alisada y de la máquina 1? (R: 200/201) ¿Cuál es la probabilidad de que la varilla no esté redondeada si no está alisada y es de la máquina 2? (R: 6/14) ¿Cuál es la probabilidad de que la varilla haya salido de la máquina 1 si está alisada y redondeada? (R: 200/345) ¿Cuál es la probabilidad de que la varilla haya venido de la máquina 2 si no pasa al menos uno de los controles de calidad? (R:0.72) 4.11.0.4 Ejercicio 4 Queremos cruzar una avenida con dos semáforos. La probabilidad de encontrar el primer semáforo en rojo es 0.6. Si paramos en el primer semáforo, la probabilidad de parar en el segundo es 0.15. Mientras que la probabilidad de detenernos en el segundo si no nos detenemos en el primero es 0.25. Cuando intentamos cruzar ambos semáforos: ¿Cuál es la probabilidad de tener que detenerse en cada semáforo? (R:0.09) ¿Cuál es la probabilidad de tener que parar en al menos un semáforo? (R:0.7) ¿Cuál es la probabilidad de tener que detenerse en un solo semáforo? (R:0.61) Si paré en el segundo semáforo, ¿cuál es la probabilidad de que tuviera que parar en el primero? (R: 0.47) Si tuviera que parar en cualquier semáforo, ¿cuál es la probabilidad de que tuviera que hacerlo dos veces? (R: 0.12) ¿Parar en el primer semáforo es un evento independiente de detenerse en el segundo semáforo? (No) Ahora, deseamos cruzar una avenida con tres semáforos. La probabilidad de encontrarnos con el primer semáforo en rojo es del 0.6, y la probabilidad de encontrar el segundo semáforo en rojo depende únicamente de la probabilidad del primer semáforo. De manera similar, la probabilidad de encontrar un semáforo en rojo en el tercer semáforo depende solo de las probabilidades del segundo. Como antes, la probabilidad de detenernos en un semáforo es del 0.15 si nos detuvimos en el semáforo anterior. Si no nos detuvimos en el semáforo anterior, la probabilidad de detenernos en el siguiente semáforo es del 0.25. ¿Cuál es la probabilidad de tener que parar en cada semáforo? (R:0.013) ¿Cuál es la probabilidad de tener que parar en al menos un semáforo? (R:0.775) ¿Cuál es la probabilidad de tener que detenerse en un solo semáforo? (R:0.5425) consejos: Si la probabilidad de que un semáforo esté en rojo depende únicamente del anterior, entonces \\(P(R_3|R_2,R_1)=P(R_3|R_2,R&#39;_1)=P(R_3|R_2)\\) y \\(P(R_3|R&#39;_2,R_1)=P(R_3 |R&#39;_2,R&#39;_1)=P(R_3|R&#39;_2)\\) La probabilidad conjunta de encontrar tres semáforos en rojo se puede escribir como: \\(P(R_1,R_2,R_3)=P(R_3|R_2)P(R_2|R_1)P(R_1)\\) 4.11.0.5 Ejercicio 5 Una prueba de calidad en un ladrillo aleatorio se define por los eventos: Pasar la prueba de calidad: \\(E\\), no pasar la prueba de calidad: \\(E&#39;\\) Defectuoso: \\(D\\), no defectuoso: \\(D&#39;\\) Si la prueba diagnóstica tiene sensibilidad \\(P(E|D&#39;)=0.99\\) y especificidad \\(P(E&#39;|D)=0.98\\), y la probabilidad de pasar la prueba es \\(P(E) =0.893\\) entonces ¿Cuál es la probabilidad de que un ladrillo elegido al azar sea defectuoso \\(P(D)\\)? (R:0.1) ¿Cuál es la probabilidad de que un ladrillo que ha pasado la prueba sea realmente defectuoso? (R:0.022) La probabilidad de que un ladrillo no sea defectuoso y que no pase la prueba (R:0.009) ¿Son \\(D\\) y \\(E&#39;\\) estadísticamente independientes? (No) "],["variables-aleatorias-discretas.html", "Chapter 5 Variables aleatorias discretas 5.1 Objetivo 5.2 Frecuencias relativas 5.3 Variable aleatoria 5.4 Eventos de observar una variable aleatoria 5.5 Probabilidad de variables aleatorias 5.6 Funciones de probabilidad 5.7 Funciones de probabilidad 5.8 Probabilidades y frecuencias relativas 5.9 La media o el valor esperado 5.10 Varianza 5.11 Funciones de probabilidad para funciones de \\(X\\) 5.12 Distribución de probabilidad 5.13 Función de probabilidad y distribución de probabilidad 5.14 Cuantiles 5.15 Resumen 5.16 Preguntas 5.17 Ejercicios", " Chapter 5 Variables aleatorias discretas 5.1 Objetivo En este capítulo definiremos las variables aleatorias y estudiaremos variables aleatorias discretas. Definiremos la función de masa de probabilidad y sus principales propiedades de media y varianza. Siguiendo el proceso de abstracción de las frecuencias relativas en probabilidades, también definimos la distribución de probabilidad como el caso límite de la frecuencia relativa acumulada. 5.2 Frecuencias relativas Las frecuencias relativas de los resultados de un experimento aleatorio son una medida de su propensión. Podemos usarlos como estimadores de sus probabilidades, cuando repetimos el experimento aleatorio muchas veces (\\(n \\rightarrow \\infty\\)). Definimos tendencia central (promedio), dispersión (varianza muestral) y la distribución de frecuencias de los datos (\\(F_i\\)). En términos de probabilidades, ¿cómo se definen estas cantidades? 5.3 Variable aleatoria Definimos las frecuencias relativas sobre las observaciones de los experimentos. Ahora definimos las cantidades equivalentes para las probabilidades en términos de los resultados de los experimentos. Nos ocuparemos únicamente de resultados de tipo numérico. Una variable aleatoria es un símbolo que representa un resultado numérico de un experimento aleatorio. Escribimos la variable aleatoria en mayúsculas (es decir, \\(X\\)). Definición: Una variable aleatoria es una función que asigna un número real a un evento del espacio muestral de un experimento aleatorio. Recuerda que un evento puede ser un resultado o una colección de resultados. Cuando la variable aleatoria toma un valor, indica la realización de un evento de un experimento aleatorio. Ejemplo: Si \\(X \\in \\{0,1\\}\\), entonces decimos que \\(X\\) es una variable aleatoria que puede tomar los valores \\(0\\) o \\(1\\). 5.4 Eventos de observar una variable aleatoria Hacemos la distinción entre variables en el espacio modelo con letras mayúsculas, como entidades abstractas, y la realización de un evento o resultado particular. Por ejemplo: \\(X=1\\) es el evento de observar la variable aleatoria \\(X\\) con valor \\(1\\) \\(X=2\\) es el evento de observar la variable aleatoria \\(X\\) con valor \\(2\\) … En general: \\(X=x\\) es el evento de observar la variable aleatoria \\(X\\) (\\(X\\) mayúscula) con valor \\(x\\) (\\(x\\) pequeño). 5.5 Probabilidad de variables aleatorias Nos interesa asignar probabilidades a los eventos de observar un valor particular de una variable aleatoria. Por ejemplo, para los dados escribiremos la tabla de probabilidad como \\(X\\) Probabilidad \\(1\\) \\(P(X=1)=1/6\\) \\(2\\) \\(P(X=2)=1/6\\) \\(3\\) \\(P(X=3)=1/6\\) \\(4\\) \\(P(X=4)=1/6\\) \\(5\\) \\(P(X=5)=1/6\\) \\(6\\) \\(P(X=6)=1/6\\) donde hacemos explícitos los eventos de que la variable toma un resultado dado \\(X=x\\). 5.6 Funciones de probabilidad Debido a que \\(x\\) (minúscula) es una variable numérica, las probabilidades de la variable aleatoria se pueden dibujar o escrir como una función \\[f(x)=P(X=x)=1/6\\] 5.7 Funciones de probabilidad Podemos crear cualquier tipo de función de probabilidad si satisfacemos las reglas de probabilidad de Kolmogorov: Para una variable aleatoria discreta \\(X \\in \\{x_1 , x_2 , .. , x_M\\}\\), una función de masa de probabilidad que se usa para calcular probabilidades \\(f(x_i)=P(X=x_i)\\) siempre es positiva \\(f(x_i)\\geq 0\\) y su suma sobre todos los valores de la variable es \\(1\\): \\(\\sum_{i=1}^M f(x_i)=1\\) Donde \\(M\\) es el número de resultados posibles. Ten en cuenta que la definición de \\(X\\) y su función de masa de probabilidad es general sin referencia a ningún experimento. Las funciones viven en el espacio modelo (abstracto). Aquí tenemos un ejemplo \\(X\\) y \\(f(x)\\) son objetos abstractos que pueden corresponder o no a un experimento. Tenemos la libertad de construirlos como queramos siempre que respetemos su definición. Las funciones de masa de probabilidad tienen algunas propiedades que se derivan exclusivamente de su definición. 5.8 Probabilidades y frecuencias relativas Considera el ejemplo Haz el siguiente experimento: En una urna pon \\(8\\) bolas y: marca \\(1\\) bola con el número \\(-2\\) marca \\(2\\) bolas con el número \\(-1\\) marca \\(2\\) bolas con el número \\(0\\) marca \\(2\\) bolas con el número \\(1\\) marca \\(1\\) bolas con el número \\(2\\) Y considere realizar el siguiente experimento aleatorio: Tome una bola y lea el número. A partir de la probabilidad clásica, podemos escribir la tabla de probabilidades, para lo cual no necesitamos realizar ningún experimento \\(X\\) \\(P(X=x)\\) \\(-2\\) \\(1/8=0.125\\) \\(-1\\) \\(2/8=0.25\\) \\(0\\) \\(2/8=0.25\\) \\(1\\) \\(2/8=0.25\\) \\(2\\) \\(1/8=0.125\\) Ahora, realicemos el experimento \\(30\\) veces y escribamos la tabla de frecuencia \\(X\\) \\(f_i\\) \\(-2\\) \\(0.132\\) \\(-1\\) \\(0.262\\) \\(0\\) \\(0.240\\) \\(1\\) \\(0.248\\) \\(2\\) \\(0.118\\) La probabilidad frecuentista nos dice \\[lim_{N \\rightarrow \\infty} f_i = f(x_i)=P(X=x_i)\\] Entonces, si no conocíamos el montaje del experimento (caja negra), lo mejor que podemos hacer es estimar las probabilidades con las frecuencias, obtenidas de \\(N\\) repeticiones del experimento aleatorio: \\[f_i = \\hat{P}_i\\] Cada vez que estimamos las probabilidades, nuestras estimaciones \\(\\hat{P}_i=f_i\\) cambian. Pero \\(P_i\\) es una cantidad abstracta que nunca cambia. A medida que aumenta \\(N\\), nos acercamos más a ella. 5.9 La media o el valor esperado Cuando discutimos las estadísticas de resumen de los datos, definimos el centro de las observaciones como un valor alrededor del cual se concentran las frecuencias de los resultados. Usamos el promedio para medir el centro de gravedad de los datos. En términos de las frecuencias relativas de los valores de los resultados discretos, escribimos el promedio como \\(\\bar{x}= \\sum_{i=1}^M x_i \\frac{n_i}{N}=\\) \\[\\sum_{i=1}^M x_i f_i\\] Definición La media (\\(\\mu\\)) o valor esperado de una variable aleatoria discreta \\(X\\), \\(E(X)\\), con función de masa \\(f(x)\\) está dada por \\[ \\mu = E(X)= \\sum_{i=1}^M x_i f(x_i) \\] Es el centro de gravedad de las probabilidades: El punto donde se equilibran las cargas de probabilidad. De la definición tenemos \\[\\bar{x} \\rightarrow \\mu\\] en el límite cuando \\(N \\rightarrow \\infty\\) como la frecuencia tiende a la función de masa de probabilidad \\(f_i \\rightarrow f(x_i)\\). Ejemplo ¿Cuál es la media de \\(X\\) si su función de masa de probabilidad \\(f(x)\\) está dada por \\(X\\) \\(f(x)=P(X=x)\\) \\(0\\) \\(1/16\\) \\(1\\) \\(4/16\\) \\(2\\) \\(6/16\\) \\(3\\) \\(4/16\\) \\(4\\) \\(1/16\\) \\[ \\mu =E(X)=\\sum_{i=1}^m x_i f(x_i) \\] \\(E(X)=\\)0 * 1/16 + 1 * 4/16 + 2 * 6/16 + 3 * 4/16 + 4 * 1/16 =2 La media \\(\\mu\\) es el centro de gravedad de la función de masa de probabilidad y no cambia. Sin embargo, el promedio \\(\\bar{x}\\) es el centro de gravedad de las observaciones (frecuencias relativas) cambia con diferentes datos. 5.10 Varianza Cuando discutimos los estadísticos de resumen, también definimos la dispersión de las observaciones como una distancia promedio de los datos al promedio. Definición La varianza, escrita como \\(\\sigma^2\\) o \\(V(X)\\), de una variable aleatoria discreta \\(X\\) con función de masa \\(f(x)\\) viene dada por \\[\\sigma^2 = V(X)= \\sum_{i=1}^M (x_i-\\mu)^2 f(x_i)\\] \\(\\sigma=\\sqrt{V(X)}\\) se llama la desviación estándar de la variable aleatoria. La varianza es la dispersión de las probabilidades con respecto a la media: El momento de inercia de las probabilidades sobre la media. Ejemplo ¿Cuál es la varianza de \\(X\\) si su función de masa de probabilidad \\(f(x)\\) está dada por \\(X\\) \\(f(x)=P(X=x)\\) \\(0\\) \\(1/16\\) \\(1\\) \\(4/16\\) \\(2\\) \\(6/16\\) \\(3\\) \\(4/16\\) \\(4\\) \\(1/16\\) \\[\\sigma^2 =V(X)=\\sum_{i=1}^m (x_i-\\mu)^2 f(x_i)\\] \\(V(X)=\\)(0-2)\\(^2\\)* 1/16 + (1-2)\\(^2\\)* 4/16 + (2- 2)\\(^2\\)* 6/16 + (3-2)\\(^2\\)* 4/16 + (4-2)\\(^2\\)* 1/ 16 = 1 \\[V(X)=\\sigma^2=1\\] \\[\\sigma=1\\] 5.11 Funciones de probabilidad para funciones de \\(X\\) En muchas ocasiones, estaremos interesados en resultados que sean función de las variables aleatorias. Quizás nos interese el cuadrado del número de contagios de gripe, o la raíz cuadrada del número de correos electrónicos en una hora. Definición Para cualquier función \\(h\\) de una variable aleatoria \\(X\\), con función de masa \\(f(x)\\), su valor esperado viene dado por \\[ E[h(X)]= \\sum_{i=1}^M h(x_i) f(x_i) \\] Esta es una definición importante que nos permite probar tres propiedades de la media y la varianza que se usan con frecuencia: La media de una función lineal es la función lineal de la media: \\[E(a\\times X +b)= a\\times E(X) +b\\] para \\(a\\) y \\(b\\) escalares (números ). La varianza de una función lineal de \\(X\\) es:\\[V(a\\times X +b)= a^2\\times V(X)\\] La varianza con respecto al origen es la varianza con respecto a la media más la media al cuadrado: \\[E(X^2)=V(X)+E(X)^2\\] Ejemplo ¿Cuál es la varianza \\(X\\) con respecto al origen, \\(E(X^2)\\), si su función de masa de probabilidad \\(f(x)\\) está dada por \\(X\\) \\(f(x)=P(X=x)\\) \\(0\\) \\(1/16\\) \\(1\\) \\(4/16\\) \\(2\\) \\(6/16\\) \\(3\\) \\(4/16\\) \\(4\\) \\(1/16\\) \\[E(X^2) =\\sum_{i=1}^m x_i^2 f(x_i)\\] \\(E(X^2)=\\)(0)\\(^2\\)* 1/16 + (1)\\(^2\\)* 4/16 + (2) \\(^2\\)* 6/16 + (3)\\(^2\\)* 4/16 + (4)\\(^2\\)* 1/16 =5 También podemos verificar: \\[E(X^2)=V(X)+E(X)^2\\] \\(5=1+2^2\\) 5.12 Distribución de probabilidad Cuando discutimos las estadísticas de resumen, también definimos la distribución de frecuencias (o la frecuencia acumulada relativa) \\(F_i\\). \\(F_i\\) es una cantidad importante porque es una función continua \\(F_x\\) es por lo tanto una función de rango continuo, incluso si los resultados son discretos. Definición: La función de distribución de probabilidad se define como \\[F(x)=P(X\\leq x)=\\sum_{x_i\\leq x} f(x_i) \\] Esa es la probabilidad acumulada hasta un valor dado \\(x\\) \\(F(x)\\) satisface por lo tanto satisface: \\(0\\leq F(x) \\leq 1\\) Si \\(x \\leq y\\), entonces \\(F(x) \\leq F(y)\\) Para la función de masa de probabilidad: \\(X\\) \\(f(x)=P(X=x)\\) \\(0\\) \\(1/16\\) \\(1\\) \\(4/16\\) \\(2\\) \\(6/16\\) \\(3\\) \\(4/16\\) \\(4\\) \\(1/16\\) La distribución de probabilidad es: \\[ F(x)= \\begin{cases} 1/16,&amp; \\text{if } 0 \\leq x &lt; 1\\\\ 5/16,&amp; 1\\leq x &lt; 2\\\\ 11/16,&amp; 2\\leq x &lt; 3\\\\ 15/16,&amp; 4\\leq x &lt; 5\\\\ 16/16,&amp; x \\leq 5\\\\ \\end{cases} \\] Para \\(X \\in \\mathbb{Z}\\) 5.13 Función de probabilidad y distribución de probabilidad La función de probabilidad y la distribución son equivalentes. Podemos obtener uno del otro y viceversa. \\[f(x_i)=F(x_i)-F(x_{i-1})\\] con \\[f(x_1)=F(x_1)\\] para \\(X\\) tomando valores en \\(x_1 \\leq x_2 \\leq ... \\leq x_n\\) Ejemplo De la distribución de probabilidad: \\[ F(x)= \\begin{cases} 1/16,&amp; \\text{if } 0 \\leq x &lt; 1\\\\ 5/16,&amp; 1\\leq x &lt; 2\\\\ 11/16,&amp; 2\\leq x &lt; 3\\\\ 15/16,&amp; 4\\leq x &lt; 5\\\\ 16/16,&amp; x \\leq 5\\\\ \\end{cases} \\] Podemos obtener la función masa de probabilidad. \\(f(0)=F(0)=1/16\\) \\(f(1)=F(1)-f(0)=5/32-1/32=4/16\\) \\(f(2)=F(2)-f(1)-f(0)=F(2)-F(1)=6/16\\) \\(f(3)=F(3)-f(2)-f(1)-f(0)=F(3)-F(2)=4/16\\) \\(f(4)=F(4)-F(3)=1/16\\) 5.14 Cuantiles Finalmente, podemos usar la distribución de probabilidad \\(F(x)\\) para definir la mediana y los cuartiles de la variable aleatoria \\(X\\). En general, definimos el q-cuantil como el valor \\(x_{p}\\) bajo el cual hemos acumulado q*100% de la probabilidad \\[q=\\sum_{i=1}^pf(x_i) = F (x_p)\\] La mediana es valor \\(x_m\\) tal que \\(q=0.5\\) \\[F(x_{m})=0.5\\] El cuantil \\(0.05\\) es el valor \\(x_{r}\\) tal que \\(q=0.05\\) \\[F(x_{r})=0.05\\] El cuantil de \\(0,25\\) es el primer cuartil el valor \\(x_{s}\\) tal que \\(q=0.25\\) \\[F(x_{s})=0.25\\] 5.15 Resumen nombres de cantidades modelo (no observado) datos (observados) función de masa de probabilidad // frecuencia relativa \\(f(x_i)=P(X=x_i)\\) \\(f_i=\\frac{n_i}{N}\\) distribución de probabilidad // frecuencia relativa acumulada \\(F(x_i)=P(X \\leq x_i)\\) \\(F_i=\\sum_{k\\leq i} f_k\\) media // promedio \\(\\mu=E(X)=\\sum_{i=1}^M x_i f(x_i)\\) \\(\\bar{x}=\\sum_{j=1}^N x_j/N\\) varianza // varianza de la muestra \\(\\sigma^2=V(X)=\\sum_{i=1}^M (x_i-\\mu)^2 f(x_i)\\) \\(s^2=\\sum_{j=1}^N (x_j-\\bar{x})^2/(N-1)\\) desviación estándar // muestra sd \\(\\sigma=\\sqrt{V(X)}\\) \\(s\\) varianza con respecto al origen // 2º momento muestral \\(E(X^2)=\\sum_{i=1}^M x_i^2 f(x_i)\\) \\(m_2= \\sum_{j=1}^N x_j^2/n\\) Ten en cuenta: \\(i=1...M\\) es un resultado de la variable aleatoria \\(X\\). \\(j=1...N\\) es una observación de la variable aleatoria \\(X\\). Propiedades: \\(\\sum_{i=1...N} f(x_i)=1\\) \\(f(x_i)=F(x_i)-F(x_{i-1})\\) \\(E(a\\times X +b)= a\\times E(X) +b\\); for \\(a\\) and \\(b\\) scalars. \\(V(a\\times X +b)= a^2\\times V(X)\\) \\(E(X^2)=V(X)+E(X)^2\\) 5.16 Preguntas 1) Para una función de masa de probabilidad no es cierto que \\(\\qquad\\)a: la suma de los valores de su imagen es 1; \\(\\qquad\\)b: sus valores pueden interpretarse como probabilidades de eventos; \\(\\qquad\\)c: siempre es positiva; \\(\\qquad\\)d: no puede tomar el valor 1; 2) El valor de una variable aleatoria representa \\(\\qquad\\)a: una observación de un experimento aleatorio; \\(\\qquad\\)b: la frecuencia de un resultado de un experimento aleatorio; \\(\\qquad\\)c: un resultado de un experimento aleatorio; \\(\\qquad\\)d: una probabilidad de un resultado; 3) El valor estimado de una probabilidad \\(\\hat{P_i}\\) es igual a la probabilidad \\(P_i\\) cuando el número de repeticiones del experimento aleatorio es \\(\\qquad\\)a: grande; \\(\\qquad\\)b: infinito; \\(\\qquad\\)c: pequeño \\(\\qquad\\)d: cero; 4) Si una función de masa de probabilidad es simétrica alrededor de \\(x=0\\) \\(\\qquad\\)a: La media es menor que la mediana; \\(\\qquad\\)b: La media es mayor que la mediana; \\(\\qquad\\)c: La media y la mediana son iguales; \\(\\qquad\\)d: La media y la mediana son diferentes de 0; 5) La media y la varianza \\(\\qquad\\)a: son inversamente proporcionales; \\(\\qquad\\)b: son valores esperados de funciones de \\(X\\); \\(\\qquad\\)c: de una función lineal son la función lineal de la media y la función lineal de la varianza; \\(\\qquad\\)d: cambia cuando repetimos el experimento aleatorio; 5.17 Ejercicios 5.17.0.1 Ejercicio 1 Ponemos en una urna papeletas con letras de la a la f. Considera el sorteo que da \\(0\\) euros a las dos primeras letras del abecedario, \\(1.5\\) euros a las dos siguientes, y \\(2\\) y \\(3\\) euros a las siguientes. ¿cuál es la función de masa de probabilidad y función de distribución de probabilidad de para los premios en dinero del juego? ¿cuál es el valor esperado del premio? (R:1.3) ¿cuál es la varianza del premio? (R:1.13) ¿cuál es la probabilidad de ganar 2 o mas euros? (R:2/6) 5.17.0.2 Ejercicio 2 Dada la función de masa de probabilidad \\(x\\) \\(f(x)=P(X=x)\\) 10 0.1 12 0.3 14 0.25 15 0.15 17 ? 20 0.15 ¿Cuál es su valor esperado y su desviación estándar? (R: 14,2; 2,95) 5.17.0.3 Ejercicio 3 Dada la distribución de probabilidad para una variable discreta \\(X\\) \\[ F(x)= \\begin{cases} 0, &amp; x &lt; -1 \\\\ 0.2,&amp; x \\in [-1,0)\\\\ 0.35,&amp; x \\in [0,1)\\\\ 0.45,&amp; x \\in [1,2)\\\\ 1,&amp; x \\geq 2\\\\ \\end{cases} \\] encuentra \\(f(x)\\) encuentra \\(E(X)\\) y \\(V(X)\\) (R:1; 1.5) cuál es el valor esperado y la varianza de \\(Y=2X+3\\) (R:5, 6) ¿Cuál es la mediana y el primer y tercer cuartil de \\(X\\)? (R:2,0,2) 5.17.0.4 Ejercicio 4 Estamos probando un sistema para transmitir imágenes digitales. Primero consideramos el experimento de enviar \\(3\\) píxeles y tener como resultados posibles eventos como \\((0,1,1)\\). Este es el evento de recibir el primer píxel sin error, el segundo con error y el tercero con error. Enumera en una columna el espacio muestral del experimento aleatorio. En la segunda columna asigna la variable aleatoria que cuenta el número de errores transmitidos para cada resultado Considera que tenemos un canal totalmente ruidoso, es decir, cualquier resultado de tres píxeles es igualmente probable. ¿Cuál es la probabilidad de recibir errores de \\(0\\), \\(1\\), \\(2\\) o \\(3\\) en la transmisión de \\(3\\) píxeles? (R: 1/8; 3/8; 3/8; 1/8) Dibuja la función de masa de probabilidad para el número de errores ¿Cuál es el valor esperado para el número de errores? (R:1.5) ¿Cuál es su varianza? (R: 0,75) Dibuja la distribución de probabilidad ¿Cuál es la probabilidad de transmitir al menos 1 error? (R:7/8) "],["variables-aleatorias-continuas.html", "Chapter 6 Variables aleatorias continuas 6.1 Objetivo 6.2 Variables aleatorias continuas 6.3 frecuencias relativas 6.4 función de densidad de probabilidad 6.5 Área total bajo la curva 6.6 Probabilidades de variables continuas 6.7 Distribución de probabilidad 6.8 Gráficas de probabilidad 6.9 Media 6.10 Varianza 6.11 Funciones de \\(X\\) 6.12 Ejercicios", " Chapter 6 Variables aleatorias continuas 6.1 Objetivo En este capítulo estudiaremos variables aleatorias continuas. Definiremos la función de densidad de probabilidad, su media y varianza. De forma similar a las variables aleatorias discretas, definiremos la función de distribución de probabilidad. 6.2 Variables aleatorias continuas En el capítulo pasado usamos las probabilidades de variables aleatorias discretas para definir la función de masa de probabilidad \\[f(x)=P(X= x)\\] Donde la probabilidad de que la variable aleatoria tome el valor \\(x\\) la entendemos como el valor de su frecuencia relativa, cuando el número de repeticiones del experimiento aleatorio tiende a infinito. Cuando hablamos de datos continous vimos que teníamos que transformarlos en variables discretas (bins) para producir tablas de frecuencias relativas o histogramas. Veamos cómo definir las probabilidades de las variables continuas teniendo en cuenta estas particiones. Ejemplo (misofonía) Reconsideremos el ángulo de convexidad de los pacientes con misofonía (Sección 2.21). El ángulo de convexidas de 123 pacientes fue medido. Entendimos cada medición como el resultado de un experimento aleatorio que repetimos 123 veces y que podíamos describir en una tabla de frecuencias o en un histograma. Para hacer esto, redefinimos los resultados como pequeños intervalos regulares (bins) y calculamos la frecuencia relativa de cada intervalo. ## outcome ni fi ## 1 [-1.02,3.46] 8 0.06504065 ## 2 (3.46,7.92] 51 0.41463415 ## 3 (7.92,12.4] 26 0.21138211 ## 4 (12.4,16.8] 20 0.16260163 ## 5 (16.8,21.3] 18 0.14634146 6.3 frecuencias relativas Por lo tanto, definimos la probabilidad de observar un intervalo \\(i\\) como la frecuencia relativa del intervalo cuando \\(N \\rightarrow \\infty\\) \\[ f_i =\\frac{ n_ i }{ N} \\rightarrow P( x_i \\leq X \\leq x_i + \\Delta x)\\] Esta probabilidad depende de la longitud de los bins \\(\\Delta x\\). Si hacemos los bins cada vez más pequeños, las frecuencias se hacen más pequeñas y, por lo tanto, \\[P(x_i \\leq X \\leq x_i + \\Delta x) \\rightarrow 0\\] cuando \\(\\Delta x \\rightarrow 0\\) porque \\(n_i \\rightarrow 0\\) Veamos cómo las frecuencias relativas se hacen más pequeñas cuando dividimos el rango de \\(X\\) en \\(20\\) bins ## outcome ni fi ## 1 [-1.02,0.115] 2 0.01626016 ## 2 (0.115,1.23] 0 0.00000000 ## 3 (1.23,2.34] 3 0.02439024 ## 4 (2.34,3.46] 3 0.02439024 ## 5 (3.46,4.58] 2 0.01626016 ## 6 (4.58,5.69] 4 0.03252033 ## 7 (5.69,6.8] 11 0.08943089 ## 8 (6.8,7.92] 34 0.27642276 ## 9 (7.92,9.04] 12 0.09756098 ## 10 (9.04,10.2] 4 0.03252033 ## 11 (10.2,11.3] 3 0.02439024 ## 12 (11.3,12.4] 7 0.05691057 ## 13 (12.4,13.5] 2 0.01626016 ## 14 (13.5,14.6] 6 0.04878049 ## 15 (14.6,15.7] 4 0.03252033 ## 16 (15.7,16.8] 8 0.06504065 ## 17 (16.8,18] 4 0.03252033 ## 18 (18,19.1] 9 0.07317073 ## 19 (19.1,20.2] 3 0.02439024 ## 20 (20.2,21.3] 2 0.01626016 6.4 función de densidad de probabilidad Definimos una cantidad en un punto \\(x\\) que es la probabilidad por unidad de distancia de que una observación esté en el bin infinitesimal entre \\(x\\) y \\(x+dx\\) \\[f(x)= \\frac{ P(x\\leq X \\leq x+dx )}{dx}\\] \\(f(x)\\) se llama la función de densidad de probabilidad. Por lo tanto, la probabilidad de observar \\(X\\) entre \\(x\\) y \\(x+dx\\) está dada por \\[P( x\\leq X \\leq x+dx )= f(x) dx\\] Definición Para una variable aleatoria continua \\(X\\), una función de densidad de probabilidad es tal que Es positiva: \\[f(x) \\geq 0\\] La probabilidad de observar cualquier valor de \\(x\\) es 1: \\[\\int_{-\\infty}^{\\infty} f(x) dx = 1\\] La probabilidad de observar un valor dentro de un intervalo es el área bajo la curva: \\[ P( a\\leq X \\leq b)=\\int_{a}^{b} f(x) dx\\] Las propiedades aseguran que \\(f(x)dx\\) satisfacen las propiedades de probabilidad de Kolmogorov. La función de densidad de probabilidad es un paso más en la abstracción de probabilidades en la que añadimos el límite continuo \\[dx \\rightarrow 0\\] Todas las propiedades de las probabilidades se traducen en términos de densidades y por lo tanto cambiamos sumatorios por integrales \\[\\sum \\rightarrow \\int\\] Las densidades de probabilidad son cantidades matemáticas que no necesariamente representan experimientos aleatorios. Un interés fundamental en estadística es describir las densidades que describen nuestro experimento aleatorio concreto. 6.5 Área total bajo la curva Ejemplo (gotas de lluvia) tomemos una densidad de probabilidad que podría describir la variable aleatoria que mide dónde cae una gota de lluvia en una canaleta de \\(100\\) cm de longitud. \\[ f(x)= \\begin{cases} \\frac{1}{100},&amp; \\text{si } x\\in (0,100)\\\\ 0,&amp; de\\, otra\\, forma \\end{cases} \\] Verifiquemos que la función satisface las tres propiedades de una densidad de probabilidad. es evidente a partir de la definición que \\(f(x) \\geq 0\\) La probabilidad de observar cualquier valor de \\(X\\) es el área total bajo la curva \\(P( -\\infty \\leq X \\leq \\infty )= \\int_{-\\infty }^{\\infty } f(x) dx = 100*0.01= 1\\) La probabilidad de observar \\(X\\) en un intervalo es el área bajo la curva dentro del intervalo \\(P( 20 \\leq X \\leq 60) = \\int_{20}^{60} f(x) dx = (60-20)*0.01=0.4\\) 6.6 Probabilidades de variables continuas Para variables continuas, calculamos la probabilidad de que la variable esté en un intervalo dado. Eso es \\[P( a \\leq X \\leq b)\\] Recordemos que para variables continuas, la probabilidad de que el experimento nos dé un número real particular es cero: \\(P(X= a)= 0\\) La probabilidad \\(P( a \\leq X \\leq b)\\) es el área bajo la curva de \\(f(x)\\) entre \\(a\\) y \\(b\\) \\(P( a \\leq X \\leq b) = \\int_{a}^{b} f(x) dx\\) 6.7 Distribución de probabilidad La distribución de probabilidad \\(F(c)\\) definida como la acumulación de probabilidad hasta el resultado \\(C\\) \\(F(c) = P( X \\leq c)\\) se puede utilizar para calcular la probabilidad \\(P( a \\leq X \\leq b)\\). Si consideramos que: la probabilidad acumulada hasta \\(b\\) está dada por \\(F(b) = P( X \\leq b)=\\int_{-\\infty }^bf(x)dx\\) la probabilidad acumulada hasta \\(a\\) es \\(F(a) = P( X \\leq a)\\) Entonces, la probabilidad entre \\(a\\) y \\(b\\) viene dada por la diferencia en el valor de la distribución de probabilidad \\(P( a\\leq X \\leq b) = \\int_a^b f(x)dx=F(b)-F(a)\\) Definición La distribución de probabilidad de una variable aleatoria continua se define como \\[F(a)= P( X\\leq a) =\\int_{-\\infty } ^af(x)dx\\] y tiene las siguientes propiedades: Está entre \\(0\\) y \\(1\\): \\[F(-\\infty )= 0\\,\\, y \\,\\,F(\\infty )=1\\] Siempre aumenta: \\[F(a)\\leq F(b)\\] si \\(a \\leq b\\) Se puede utilizar para calcular probabilidades: \\[P( a \\leq X \\leq b)=F(b)-F(a)\\] Recupera la densidad de probabilidad: \\[f(x)=\\frac{ dF (x )}{ dx}\\] Usamos distribuciones de probabilidad para calcular probabilidades de una variable aleatoria dentro de intervalos, y su derivada es la función de densidad de probabilidad. Ejemplo (gotas de lluvia) Para la función de densidad uniforme: \\[ f(x)= \\begin{cases} \\frac{1}{100},&amp; \\text{si } x\\in (0,100)\\\\ 0,&amp; de\\, otra\\, forma \\end{cases} \\] Encontramos que la distribución de probabilidad es \\[ F(a)= \\begin{cases} 0,&amp; \\text{si } a \\leq 0 \\\\ \\frac{a}{100},&amp; \\text{si } a\\in (0,100)\\\\ 1, &amp; \\text{si } 100 \\leq a \\\\ \\\\ \\end{cases} \\] 6.8 Gráficas de probabilidad Podemos dibujar la probabilidad de una variable aleatoria en un intervalo como el área bajo la curva de la densidad. Por ejemplo \\[P(20&lt;X&lt; 60)\\] También podemos dibujar la probabilidad \\(P(20&lt;X&lt; 60)\\) como la diferencia en los valores de la distribución 6.9 Media Como en el caso discreto, la media mide el centro de masa de las probabilidades Definición Supongamos que \\(X\\) es una variable aleatoria continua con función de probabilidad densidad \\(f(x)\\). El valor medio o esperado de \\(X\\), denotado como \\(\\mu\\) o \\(E(X)\\), es \\[\\mu=E(X)=\\int_{-\\infty}^\\infty x f(x) dx\\] Es la versión continua del centro de gravedad. Ejemplo (gotas de lluvia) La variable aleatoria con densidad de probabilidad \\[ f(x)= \\begin{cases} \\frac{1}{100},&amp; \\text{si } x\\in (0,100)\\\\ 0,&amp; de\\, otra\\, forma \\end{cases} \\] Tiene un valor esperado en \\[E(X)=50\\] 6.10 Varianza Como en el caso discreto, la varianza mide la dispersión de probabilidades sobre la media Definición Supongamos que \\(X\\) es una variable aleatoria continua con función de densidad de probabilidad \\(f(x)\\). La varianza de \\(X\\), denotada como \\(\\sigma^2\\) o \\(V(X)\\), es \\[\\sigma^2=V(X)=\\int_{-\\infty}^\\infty (x-\\mu)^2 f(x) dx\\] Es la versión continua del momento de inercia. 6.11 Funciones de \\(X\\) En muchas ocasiones, estaremos interesados en resultados que sean función de las variables aleatorias. Tal vez nos interese el cuadrado de la elongación de un muelle, o la raíz cuadrada de la temperatura de un motor. Definición Para cualquier función \\(h\\) de una variable aleatoria \\(X\\), con función de masa \\(f(x)\\), su valor esperado viene dado por \\[E[h(X)]= \\int_{-\\infty}^{\\infty} h(x) f(x)dx\\] De esta definición recuperamos las mismas propiedades que en el caso discreto La media de una función lineal es la función lineal de la media: \\[ E( a\\times X +b)= a\\times E(X) +b\\] para \\(a\\) y \\(b\\) escalares. La varianza de una función lineal de \\(X\\) es: \\[V(a\\times X +b)= a^2\\times V(X)\\] La varianza sobre el origen es la varianza sobre la media más la media al cuadrado: \\[E(X^2)= V(X)+E(X)^2\\] 6.12 Ejercicios 6.12.0.1 Ejercicio 1 Para la densidad de probabilidad \\[ f(x)= \\begin{cases} \\frac{1}{100},&amp; \\text{si } x\\in (0,100)\\\\ 0,&amp; de\\, otra\\, forma \\end{cases} \\] calcular la media (R:50) calcular la varianza usando \\(E(X^2)= V(X)+E(X)^2\\) (R:100^2/12) calcula \\(P( \\mu-\\sigma \\leq X \\leq \\mu+\\sigma)\\) (R: 0.57) ¿Cuáles son el primer y tercer cuartiles? (R: 25; 75) 6.12.0.2 Ejercicio 2 Dado \\[ f(x)= \\begin{cases} 0, &amp; x &lt; 0 \\\\ ax, &amp; x \\in [0,3] \\\\ b, &amp; x \\in (3,5) \\\\ \\frac{b}{3}(8-x),&amp; x \\in [5,8]\\\\ 0, &amp; x &gt; 8 \\\\ \\end{cases} \\] ¿Cuáles son los valores de \\(a\\) y \\(b\\) tales que \\(f(x)\\) es una función de densidad de probabilidad continua ? (R: 1/15; 1/5) ¿Cuál es la media de \\(X\\)? (R:4) 6.12.0.3 Ejercicio 3 Para la densidad de probabilidad \\[ f(x)= \\begin{cases} \\lambda e^{-\\lambda x},&amp; \\text{si } x \\geq 0\\\\ 0,&amp; de\\, otra\\, forma \\end{cases} \\] Confirmar que se trata de una densidad de probabilidad Calcular la media (R: 1/\\(\\lambda\\)) Calcule el valor esperado de \\(X^2\\) (R: 2/\\(\\lambda^2\\)) Calcular la varianza (R: 1/\\(\\lambda^2\\)) Hallar la distribución de probabilidad \\(F(a)\\) (R: \\(1- exp( -\\lambda a)\\)) Encuentra la mediana (R: \\(\\log{ 2}\\) /\\(\\lambda\\)) 6.12.0.4 Ejercicio 4 Dada la distribución acumulativa de una variable aleatoria \\(X\\) \\[ F(x)= \\begin{cases} 0, &amp; x &lt; -1 \\\\ \\frac{1}{80}(17+16x-x^2),&amp; x \\in [-1,7)\\\\ 1,&amp; x \\geq 7\\\\ \\end{cases} \\] calcular: \\(P(X&gt; 0)\\) (R:63/80) \\(E(X)\\) (R:1.93) \\(P(X&gt;0|X&lt; 2)\\) (R:28/45) "],["modelos-de-probabilidad-para-variables-aletorias-discretas.html", "Chapter 7 Modelos de probabilidad para variables aletorias discretas 7.1 Objetivo 7.2 Función de probabilidad 7.3 Modelo de probabilidad 7.4 Modelos paramétricos 7.5 Distribución uniforme (un parámetro) 7.6 Distribución uniforme (dos parámetros) 7.7 Ensayo de Bernoulli 7.8 Experimento binomial 7.9 Función de probabilidad binomial 7.10 Función de probabilidad binomial negativa 7.11 Distribución geométrica 7.12 Modelo hipergeométrico 7.13 Preguntas 7.14 Ejercicios", " Chapter 7 Modelos de probabilidad para variables aletorias discretas 7.1 Objetivo En este capítulo veremos algunas funciones de masa de probabilidad que se utilizan para describir experimentos aleatorios comunes. Introduciremos el concepto de parámetro y por tanto de modelos paramétricos. En particular, discutiremos las funciones de probabilidad uniforme y de Bernoulli y cómo se usan para derivar las funciones de probabilidad binomial y binomial negativa. También hableramos del modelo hipergeométrico. 7.2 Función de probabilidad Recordemos que una función de masa de probabilidad de una variable aleatoria discreta \\(X\\) con valores posibles \\(x_1 , x_2 , .. , x_M\\) es cualquier función tal que Nos permite calcular probabilidades para todos los resultados \\[f(x_i)=P(X=x_i)\\] Siempre es positiva: \\[f(x_i)\\geq 0\\] La probabilidad de obtener algo en el experimento aleatorio es \\(1\\) \\[\\sum_{i=1}^M f(x_i)=1\\] Estudiamos dos propiedades importantes: La media como medida de tendencia central: \\[E(X)= \\sum_{i=1}^M x_i f(x_i)\\] La varianza como medida de dispersión: \\[V(X)= \\sum_{i=1}^M (x_i-\\mu)^2 f(x_i)\\] 7.3 Modelo de probabilidad Un modelo de probabilidad es una función de masa de probabilidad que puede representar las probabilidades de un experimento aleatorio. Ejemplos: La función de masa de probabilidad definida por \\(X\\) \\(f(x)\\) \\(-2\\) \\(1/8\\) \\(-1\\) \\(2/8\\) \\(0\\) \\(2/8\\) \\(1\\) \\(2/8\\) \\(2\\) \\(1/8\\) Representa la probabilidad de sacar una bola de una urna donde hay dos bolas con etiquetas: \\(-1, 0, 1\\) y una bola con etiquetas: \\(-2, 2\\). \\(f(x)=P(X=x)=1/6\\) representa la probabilidad de los resultados de un lanzamiento de un dado. 7.4 Modelos paramétricos Cuando tenemos un experimento aleatorio con \\(M\\) resultados posibles, necesitamos encontrar \\(M\\) números para determinar la función de masa de probabilidad. Como en el ejemplo 1 anterior, necesitábamos \\(5\\) valores en la columna \\(f(x)\\) de la tabla de probabilidad. Sin embargo, en muchos casos, podemos formular funciones de probabilidad \\(f(x)\\) que dependen únicamente de muy pocos números. Al igual que en el ejemplo 2 anterior, solo necesitábamos saber cuántos resultados posibles puede dar un dado. Ejemplo (probabilidad clásica) Un experimento aleatorio con \\(M\\) resultados igualmente probables tiene una función de masa de probabilidad: \\[f(x)=P(X=x)=1/M\\] Sólo necesitamos saber \\(M\\). Los números que necesitamos saber para determinar completamente una función de probabilidad se llaman parámetros. 7.5 Distribución uniforme (un parámetro) El ejemplo anterior es la interpretación clásica de la probabilidad y define nuestro primer modelo paramétrico. Definición Una variable aleatoria \\(X\\) con resultados \\(\\{1,...M\\}\\) tiene una distribución uniforme discreta si todos sus resultados \\(M\\) tienen la misma probabilidad \\[f(x)=\\frac{1}{M}\\] \\(M\\) es el parámetro natural del modelo. Una vez que definimos \\(M\\) para un experimento, elegimos una función de masa de probabilidad particular. La función anterior es realmente una familia de funciones que dependen de \\(M\\): \\(f(x; M)\\). La media y la varianza de una variable que sigue una distribución uniforme son: \\[E(X)= \\frac{M+1}{2}\\] y \\[V(X)= \\frac{M^2-1}{12}\\] Nota: \\(E(X)\\) y \\(V(X)\\) también son parámetros. Si conocemos alguno de ellos, entonces podemos determinar completamente la distribución. Por ejemplo: \\[f(x)=\\frac{1}{2E(X)-1}\\] Veamos algunas funciones de masa de probabilidad en la familia de modelos paramétricos uniformes: 7.6 Distribución uniforme (dos parámetros) Consideremos ahora un nuevo modelo de probabilidad uniforme con dos parámetros: el mínimo y máximo de los resultados. Si la variable aleatoria toma valores en \\(\\{a, a+1, ...b\\}\\), donde \\(a\\) y \\(b\\) son números enteros y todos los resultados son igualmente probables, entonces \\[f(x)=\\frac{1}{b-a+1}\\] porque \\(M=b-a+1\\). Entonces decimos que \\(X\\) se distribuye uniformemente entre \\(a\\) y \\(b\\) y escribimos \\[X \\rightarrow Unif(a,b)\\] Propiedades: Si \\(X\\) se distribuye uniformemente entre \\(a\\) y \\(b\\) \\[X \\rightarrow Unif(a,b)\\] Su media es \\[E(X)= \\frac{b+a}{2}\\] Su varianza es \\[V(X)= \\frac{(b-a+1)^2-1}{12}\\] Para probar esto cambia las variables \\(X=Y+a-1\\), \\(y \\in \\{1,...M\\}\\). Funciones de masa de probabilidad Veamos algunas funciones de masa de probabilidad en la familia de modelos paramétricos uniformes: Ejemplo (clases escolares) ¿Cuál es la probabilidad de observar a un niño de una edad particular en una escuela primaria (si todas las clases tienen la misma cantidad de niños)? Del diseño del experimento sabemos: \\(a=6\\) y \\(b=11\\) entonces \\[X \\rightarrow Unif(a=6, b=11)\\] eso es \\[f(x)=\\frac{1}{6}\\] para \\(x\\in \\{6,7,8,9,10,11\\}\\), y \\(0\\) en caso contrario. Por lo tanto la probabilidad de que un niño tenga edad de \\(9\\) años es \\(1/6\\), igual que cualquier otra edad. La media y la varianza de esta función de masa de probabilidad es: \\(E(X)=8.5\\) \\(V(X)=2.916667\\) Recuerda El valor esperado es la media \\(\\mu=8.5\\) La desviación estándar \\(\\sigma=1.707825\\) es la distancia promedio desde la media y se calcula a partir de la raíz cuadrada de la varianza. Parámetros y Modelos: Un modelo es una función particular \\(f(x)\\) que describe nuestro experimento. Si el modelo es una función conocida que depende de algunos parámetros, al cambiar el valor de los parámetros producimos una familia de modelos: \\(f(x; a,b)\\). El conocimiento de \\(f(x)\\) se reduce al conocimiento del valor de los parámetros \\(a\\), \\(b\\). Idealmente, el modelo y los parámetros son interpretables. En nuestro ejemplo, \\(a\\) representa la edad mínima en la escuela y \\(b\\) la edad máxima. Pueden considerarse como las propiedades físicas del experimento. 7.7 Ensayo de Bernoulli Ahora considermos un modelo con solo dos resultados posibles (\\(A\\) y \\(A&#39;\\)) que tienen probabilidades desiguales Ejemplos: Anotar el sexo de un paciente que acude a urgencias de un hospital (\\(A:masculino\\) y \\(A&#39;:femenino\\)). Registrar si una máquina fabricada es defectuosa o no (\\(A:defectuosa\\) y \\(A&#39;:no\\,\\,defectuosa\\)). Dar en el blanco (\\(A:éxito\\) y \\(A&#39;:fracaso\\)). Transmitir un píxel correctamente (\\(A:sí\\) y \\(A&#39;:no\\)). En estos ejemplos, la probabilidad del resultado \\(A\\) suele ser desconocida. Modelo de probabilidad: Introduciremos la probabilidad de un resultado (\\(A\\)) como el parámetro del modelo. El modelo se puede escribir en diferentes formas. Como una tabla de probabilidad: \\(Resultado\\) \\(P_i\\) \\(A\\) \\(p\\) \\(A&#39;\\) \\(1-p\\) \\(i \\in \\{A,A&#39;\\}\\) resultado \\(A\\) (éxito): tiene probabilidad \\(p\\) (parámetro) resultado \\(A&#39;\\) (fracaso): tiene una probabilidad \\(1-p\\) Como función de masa de probabilidad de la variable aleatoria \\(K\\) tomando valores \\(\\{0, 1\\}\\) para \\(A&#39;\\) y \\(A\\), respectivamente. \\[ f(k)= \\begin{cases} 1-p,&amp; k=0\\, (evento\\, A&#39;)\\\\ p,&amp; k=1\\, (evento\\, A) \\end{cases} \\] Como una función de \\(k\\) \\[f(k; p)=p^k(1-p)^{1-k} \\] para \\(k=(0,1)\\). Entonces decimos que \\(K\\) sigue una distribución de Bernoulli con parámetro \\(p\\) \\[K \\rightarrow Bernoulli(p)\\] Propiedades: Si \\(K\\) sigue una distribución de Bernoulli, entonces su media es \\[E(K)=p\\] su varianza es \\[V(K)=(1-p)p\\] Ten en cuenta que la probabilidad del resultado \\(A\\) es el parámetro \\(p\\) que es lo mismo que su valor en \\(k=1\\): \\(f(1)=P(k=1)\\). El parámetro determina completamente la función de masa de probabilidad, incluidas su media y varianza. Veamos algunas funciones de masa de probabilidad en la familia de modelos paramétricos uniformes: 7.8 Experimento binomial Si estamos interesados en predecir frecuencias absolutas cuando conocemos el parámetro \\(p\\) de un ensayo particular de Bernoulli, entonces repetimos el ensayo de Bernoulli \\(n\\) veces y contamos cuantas veces obtuvimos \\(A\\); es decir, calculamos la frecuencia absoluta de \\(A\\): \\(N_A\\). definimos una variable aleatoria \\(X=N_A\\) tomando valores \\(x \\in {0,1,...n}\\) Cuando repetimos \\(n\\) veces una ensayo de Bernoulli, observamos un valor para \\(n_A\\). Si realizamos otros \\(n\\) ensayos de Bernoulli, entonces \\(n_A\\) cambia de valor. \\(X=N_A\\) es por lo tanto una variable aleatoria y \\(X=n_A\\) es su observación. Ejemplos (Algunos experimentos binomiales) Anotamos el sexo de \\(n=10\\) pacientes que acuden a urgencias de un hospital. ¿Cuál es la probabilidad de que \\(9\\) (\\(X=9\\)) pacientes sean hombres cuando \\(p=0.8\\)? Intentamos \\(n=5\\) veces de dar en un blanco (\\(A:éxito\\) y \\(A&#39;:fracaso\\)). ¿Cuál es la probabilidad de que alcancemos el objetivo \\(5\\) (\\(X=5\\)) veces cuando normalmente lo hacemos el \\(25\\%\\) de las veces (\\(p=0.25\\))? Transmitimos \\(n=100\\) píxeles correctamente (\\(A:sí\\) y \\(A&#39;:no\\)). ¿Cuál es la probabilidad de que \\(2\\) (\\(X=2\\)) píxeles sean errores, cuando la probabilidad de error es \\(p=0.1\\)? 7.9 Función de probabilidad binomial Supongamos que sabemos el valor real del parámetro del ensayo de Bernoulli \\(p\\). Cuando repetimos un ensayo de Bernoulli y paramos en la repetición número \\(n\\), ¿el valor \\(x\\) que obtenemos es un valor común o es raro? ¿cuál es su función de masa de probabilidad \\(P(X=x)=f(x)\\)? Ejemplo (transmisión de píxeles): ¿Cuál es la probabilidad de observar errores \\(X=x\\) al transmitir \\(n=4\\) píxeles, si la probabilidad de error es \\(p\\)? Consideremos que Una variable aleatoria del experimento de transmisión es el vector \\[T=(K_1, K_2, K_3, K_4)\\] donde una observación de \\(T\\) puede ser \\((K_1=0, K_2=1, K_3=0, K_4= 1)\\) o \\(t=(0, 1, 0, 1)\\). Cada \\[K_i \\rightarrow Bernoulli(p)\\] \\(k_i \\in \\{0, 1\\}\\) \\(X=N_A\\) se puede calcular como la suma \\[X=\\sum_{i=1}^4 K_i\\] \\(x\\in \\{0,1,2,3,4\\}\\). Por ejemplo \\(X=2\\) para el resultado \\((0, 1, 0, 1)\\). Ahora veamos las probabilidades del número de errores y luego las generalizaremos. ¿Cuál es la probabilidad de observar \\(4\\) errores (\\(X=4\\))? La probabilidad de observar \\(4\\) errores es la probabilidad de observar un error en \\(1^{er}\\) y \\(2^{o}\\) y \\(3^{o}\\) y \\(4 ^{o}\\) píxel: \\[P(X=4)=P(1,1,1,1)=p\\times p\\times p\\times p=p^4\\] porque \\(K_i\\) son independientes. ¿Cuál es la probabilidad de observar \\(0\\) errores (\\(X=0\\))? La probabilidad de errores \\(0\\) es la probabilidad conjunta de observar ningún error en cualquier transmisión: \\[P(X=0)=P(0,0,0,0)=(1-p)(1-p)(1-p)(1-p)=(1-p)^4\\] ¿Cuál es la probabilidad de observar \\(3\\) errores? La probabilidad de \\(3\\) errores es la suma de la probabilidad de observar \\(3\\) errores en eventos diferentes: \\[P(X=3)=P(0,1,1,1)+P(1,0,1,1)+P(1,1,0,1)+P(1,1,1, 0)=4p^3(1-p)^1\\] porque todos estos eventos son mutuamente excluyentes. Por lo tanto, la probabilidad de \\(x\\) errores es \\[ f(x)= \\begin{cases} 1\\times p^0(1-p)^4,&amp; x=0 \\\\ 4\\times p^1(1-p)^3,&amp; x=1 \\\\ 6\\times p^2(1-p)^2,&amp; x=2 \\\\ 4\\times p^3(1-p)^1,&amp; x=3 \\\\ 1\\times p^4(1-p)^0,&amp; x=4 \\\\ \\end{cases} \\] o más brevemente \\[f(x)=\\binom 4 xp^x(1-p)^{4-x}\\] para \\(x=0,1,2,3,4\\) donde \\(\\binom 4 x\\) es el número de posibles resultados (transmisiones de \\(4\\) píxeles) con \\(x\\) errores. Definición: La función de probabilidad binomial es la función de masa de probabilidad de observar \\(x\\) resultados de tipo \\(A\\) en \\(n\\) ensayos independientes de Bernoulli, donde \\(A\\) tiene la misma probabilidad \\(p\\) en cada ensayo. La función está dada por \\(f(x)=\\binom nxp^x(1-p)^{nx}\\), \\(x=0,1,...n\\) \\(\\binom nx= \\frac{n!}{x!(nx)!}\\) se denomina coeficiente binomial y da el número de formas en que se pueden obtener \\(x\\) eventos de tipo \\(A\\) en un conjunto de \\(n\\). Cuando una variable \\(X\\) tiene una función de probabilidad binomial decimos que se distribuye binomialmente y escribimos \\[X\\rightarrow Bin(n,p)\\] donde \\(n\\) y \\(p\\) son parámetros. Veamos algunas funciones de masa de probabilidad en la familia de modelos paramétricos binomiales: Propiedades: Si una variable aleatoria \\(X\\rightarrow Bin(n,p)\\) entonces su media es \\[E(X)=np\\] su varianza es \\[V(X)=np(1-p)\\] Estas propiedades se pueden demostrar por el hecho de que \\(X\\) es la suma de \\(n\\) variables de Bernoulli independientes. Por lo tanto, \\(E(X)=E(\\sum_{i=1}^n K_i)=np\\) y \\(V(X)=V(\\sum_{i=1}^n K_i)=n(1-p)p\\) Ejemplo (transmisión de píxeles) El valor esperado para el número de errores en la transmisión de \\(4\\) píxeles es \\(np=4*0.1=0.4\\) cuando la probabilidad de error es \\(0.1\\). La varianza es \\(n(1-p)p=0.36\\) ¿Cuál es la probabilidad de observar \\(4\\) errores? Dado que estamos repitiendo una ensayo de Bernoulli \\(n=4\\) veces y contando el número de eventos de tipo \\(A\\) (errores), cuando \\(P(A)=p=0.1\\) entonces \\[X \\rightarrow Bin(n=4, p=0.1)\\] Eso es \\[f(x)=\\binom 4 x 0.1^x(1-0.1)^{4-x}\\] \\(P(X=4)=f(4)=\\binom 4 4 0.1^4 0.9^{0}=0.1^4=10^{-4}\\) En R dbinom(4,4,0.1) ¿Cuál es la probabilidad de observar \\(2\\) errores? \\(P(X=2)=\\binom 4 2 0.1^2 0.9^2=0.0486\\) En R dbinom(2,4,0.1) Ejemplo (encuestas de opinión): ¿Cuál es la probabilidad de observar como máximo \\(8\\) votantes del partido de gobierno en una encuesta electoral de tamaño \\(10\\), si la probabilidad de un voto para el partido es de \\(0.9\\)? Para este caso \\[X \\rightarrow Bin(n=10, p=0.9)\\] Eso es \\[f(x)=\\binom {10} x 0.9^x(0.1)^{4-x}\\] Queremos calcular: \\(P(X\\le 8)=F(8)= \\sum_{i=1..8} f(x_i)=0.2639011\\) en R pbinom(8,10, 0.9) 7.10 Función de probabilidad binomial negativa Ahora imaginemos que estamos interesados en contar los píxeles bien transmitidos antes de que ocurra un número dado de errores. Digamos que podemos tolerar \\(r\\) errores en la transmisión. Nuestro experimento aleatorio ahora es: Repetir las pruebas de Bernoulli hasta que observemos que el resultado \\(A\\) aparece \\(r\\) veces. El resultado del experimento es el número de eventos \\(A&#39;\\) es decir \\(n_{A&#39;}=y\\). Estamos interesados en encontrar la probabilidad de observar un número particular de eventos \\(A&#39;\\), \\(P(Y=y)\\), donde \\(Y=N_{A&#39;}\\) es la variable aleatoria. Ejemplo (transmisión de píxeles): ¿Cuál es la probabilidad de observar \\(y\\) píxeles bien transmitidos (\\(A&#39;\\)) antes de \\(r\\) errores (\\(A\\))? Primero encontremos la probabilidad de un evento de transmisión en particular con \\(y\\) número de píxeles correctos (\\(A&#39;\\)) y \\(r\\) número de errores (\\(A\\)). \\[(0,0,1,., 0,1,...0,1)\\] donde consideramos que hay \\(y\\) ceros y \\(r\\) unos. Por lo tanto, observamos \\(y\\) píxeles correctos en un total de \\(y + r\\) píxeles. La probabilidad de este evento es: \\[P(0,0,1,., 0,1,...0,1)=p^r(1-p)^y\\] Recuerda que \\(p\\) es la probabilidad de error (\\(A\\)). ¿Cuántos eventos de transmisión pueden tener \\(y\\) píxeles correctos (0) antes de \\(r\\) errores (1)? Ten en cuenta que El último pixel es fijo (marca el final de la transmisión) El número total de formas en que \\(y\\) el número de ceros se puede asignar en \\(y + r-1\\) píxeles es: \\(\\binom {y + r-1} y\\) Por lo tanto, la probabilidad de observar \\(y\\) 1 antes de \\(r\\) 0 (cada 1 con probabilidad \\(p\\)) es \\[P(Y=y)=f(y)=\\binom {y+r-1} yp^r(1-p)^y\\] para \\(y=0,1,...\\) Entonces decimos que \\(Y\\) sigue una distribución binomial negativa y escribimos \\[Y\\rightarrow NB(r,p)\\] donde \\(r\\) y \\(p\\) son parámetros que representan la tolerancia y la probabilidad de un solo error (evento \\(A\\)). Propiedades: Una variable aleatoria \\(Y\\rightarrow NB(r,p)\\) tiene media \\[E(Y)= r\\frac{1-p}{p}\\] y varianza \\[V(Y)= r\\frac{1-p}{p^2}\\] Veamos algunas funciones de masa de probabilidad en la familia de modelos paramétricos binomiales negativos: Ejemplo (sitio web) Un sitio web tiene tres servidores. Un servidor opera a la vez y solo cuando falla una solicitud se usa otro servidor. Si se sabe que la probabilidad de que falle una solicitud es \\(p=0.0005\\), entonces ¿Cuál es el número esperado de solicitudes exitosas antes de que las tres computadoras fallen? Ya que estamos repitiendo un ensayo de Bernoulli hasta \\(r=3\\) se observan eventos de tipo \\(A\\) (fallo) (cada uno con \\(P(A)=p=0.0005\\)) y estamos contando el número de eventos de tipo \\(B\\) (solicitudes exitosas) entonces \\[Y \\rightarrow NB(r=3, p=0.0005)\\] Por lo tanto, el número esperado de solicitudes exitosas antes de que el sistema falle es: \\(E(Y)=r\\frac{1-p}{p}=3\\frac{1-0.0005}{0.0005}=5997\\) Si \\(Z\\) es el número total de solicitudes entonces \\(Z=Y+r\\). Y el valor esperado del totalde solicitudes es \\(E(Z)=E(Y)+r=5997+3=6000\\) ¿Cuál es la probabilidad de observar \\(5\\) solicitudes exitosas antes de que el sistema falle? \\(f(5)=\\binom {7} 5 0.0005^3 0.9995^5=2.618444 \\times 10^{-9}\\) En R esto se calcula con dnbinom(5,3,0.0005) Sin embargo si queremos la probabilidad de observar un total de \\(5\\) antes de que el sistema falle entonces: \\[P(Z=5)=P(Y+3=5)=P(Y=2)=f(2)=7.49 \\times10^{-10}\\] En R dnbinom(2,3,0.0005) ¿Cuál es la probabilidad de tratar con un máximo de \\(5\\) solicitudes exitosas antes de que el sistema falle? Por lo tanto, queremos calcular la distribución de probabilidad en \\(5\\): \\(F(5)=P(Y\\leq 5)=\\Sigma_{y=0}^5 f(y)\\) \\(=\\sum_{y=0}^5\\binom {y+2} y 0.0005^r0.9995^y\\) \\(=\\binom{2} 0 0.0005^3 0.9995^0 +\\binom{3} 1 0.0005^3 0.9995^1\\) \\(+\\binom {4} 2 0.0005^3 0.9995^2 +\\binom {5} 3 0.0005^3 0.9995^3\\) \\(+\\binom {6} 4 0.0005^3 0.9995^4 +\\binom {7} 5 0.0005^3 0.9995^5\\) \\(= 6.9\\times 10^{-9}\\) En R esto se calcula con pnbinom(5,3,0.0005) Ejemplos ¿Cuál es la probabilidad de observar \\(10\\) píxeles correctos antes de \\(2\\) errores, si la probabilidad de error es \\(0.1\\)? \\(f(10; r=2, p=0.1)=0.03835463\\) en R dnbinom(10, 2, 0.1) ¿Cuál es la probabilidad de que entren \\(2\\) chicas antes que \\(4\\) chicos entren a clase si la probabilidad de que entre un chico es de \\(0.55\\)? \\(f(2; r=4, p=0.55)=0.1853\\) en R dnbinom(2, 4, 0.55) 7.11 Distribución geométrica Llamamos distribución geométrica a la distribución binomial negativa con \\(r=1\\) La probabilidad de observar \\(A&#39;\\) eventos antes de observar el primer evento de tipo \\(A\\) es \\[P(Y=y)=f(y)= p(1-p)^y\\] \\[Y\\rightarrow Geom(p)\\] que tiene media \\[E(Y)= \\frac{1-p}{p}\\] y varianza \\[V(Y)= \\frac{1-p}{p^2}\\] 7.12 Modelo hipergeométrico El modelo hipergeométrico surge cuando queremos contar el número de eventos de tipo \\(A\\) que se extraen de una población finita. El modelo general es considerar \\(N\\) bolas totales en una urna. Marquemos \\(K\\) con la etiqueta \\(A\\) y \\(K\\) con la etiqueta \\(A&#39;\\). Saquemos \\(n\\) bolas una por una sin reemplazo en la urna y luego contemos cuántos \\(A\\) obtuvimos. El modelo Binomial se puede derivar del modelo Hipergeométrico cuando consideramos que \\(N\\) es infinito, o que cada vez que sacamos una bola la volvemos a colocar en la urna. Ejemplo (varicela): Una escuela de \\(N=600\\) niños tiene una epidemia de varicela. Testamos a \\(n=200\\) niños y observamos que \\(x=17\\) dieron positivo. Si supiéramos que un total de \\(K=64\\) estaban realmente infectados en la escuela, ¿cuál es la probabilidad de nuestra observación? Definición: La probabilidad de obtener \\(x\\) casos (tipo \\(A\\)) en una muestra de \\(n\\) extraída de una población de \\(N\\) donde \\(K\\) son casos (tipo \\(A\\)). \\(P(X=x)=P(una\\,muestra) \\times (Número\\, de\\, formas\\, de\\, obtener\\, x)\\) \\[=\\frac{1}{\\binom N n}\\binom K x \\binom {N-K} {n-x}\\] donde \\(k \\in \\{\\max(0, n+KN), ... \\min(K, n) \\}\\) Entonces decimos que \\(X\\) sigue una distribución hipergeométrica y escribimos \\[X \\rightarrow Hipergeom(N,K,n)\\] El modelo hipergeométrico tiene tres parámetros. Propiedades: Si \\(X \\rightarrow Hypergeometric(N,K,n)\\) entonces tiene media \\[E(X) = n \\frac{K}{N} = np\\] y varianza \\[V(X) = np(1-p)\\frac{Nn}{N-1}\\] cuando \\(p=\\frac{K}{N}\\) es la proporción de casos (\\(A\\)) en una población de tamaño \\(N\\). Ten en cuenta que cuando \\(N \\rightarrow \\infty\\) recuperamos las propiedades binomiales. Veamos algunas funciones de masa de probabilidad en la familia de modelos paramétricos hipergeométricos: Ejemplo (varicela): ¿Cuál es la probabilidad ver como mucho \\(17\\) casos de varicela en una muestra de \\(200\\) alumnos de una ecuela de \\(600\\) alumnos donde \\(64\\) están infectados? La probabilidad que necesitamos calcular es \\(P(X \\leq 17)=F(17)\\) donde \\(X \\rightarrow Hypergeometric(N=600,K=64,n=200)\\) en R phyper(17, 64, 600-64, 200)=0.140565 La solución es la adición de las agujas azules en el gráfico. 7.13 Preguntas 1) ¿Cuál es el valor esperado y la varianza del número de fallos en \\(100\\) prototipos, cuando la probabilidad de un fallo es de \\(0.25\\)? \\(\\qquad\\)a: \\(0.25\\), \\(0.1875\\); \\(\\qquad\\)b: \\(25\\), \\(0.1875\\); \\(\\qquad\\)c: \\(0.25\\), \\(18.75\\); \\(\\qquad\\)d: \\(25\\), \\(18.75\\) 2) ¿Qué modelo de probabilidad describe mejor el número de mesas disponibles a la hora de la cena en un restaurante? \\(\\qquad\\)a: Binomial; \\(\\qquad\\)b: Uniforme; \\(\\qquad\\)c: Binomial negativo; \\(\\qquad\\)d: Hipergeométrico 3) El valor esperado de una distribución Binomial no es \\(\\qquad\\)a: \\(n\\) veces el valor esperado de un Bernoulli; \\(\\qquad\\)b: el valor esperado de un Hipergeométrico, cuando la población es muy grande; \\(\\qquad\\)c: \\(np\\); \\(\\qquad\\)d: el límite de la frecuencia relativa cuando el número de repeticiones es grande 4) Las encuestas de opinión para las elecciones de EE. UU. dan una probabilidad de \\(0.55\\) de que un votante esté a favor del partido republicano. Si realizamos nuestra propia encuesta y preguntamos a 100 personas al azar en la calle, ¿cómo calcularías la probabilidad de que en nuestra encuesta los demócratas ganen las elecciones? \\(\\qquad\\)a:pbinom(x=49, n=100, p=0.55)=0.13; \\(\\qquad\\)b:1-pbinom(x=49, n=100, p=0.55)=0.86; \\(\\qquad\\)c:pbinom(x=51, n=100, p=0.45)=0.90; \\(\\qquad\\)d:1-pbinom(x=51, n=100, p=0.45)=0.095 5) En un examen un alumno cuando no sabe la respuesta elige al azar una de las cuatro respuestas en una pregunta de selección múltiple. Si no sabe \\(10\\) preguntas, ¿cuál es la probabilidad de que al mas de \\(5\\) preguntas (\\(&gt;5\\)) sean correctas? \\(\\qquad\\)a:dbinom(x=4, n=10, p=0.25); \\(\\qquad\\)b:pbinom(x=4, n=10, p=0.75); \\(\\qquad\\)c:dbinom(x=4, n=10, p=0.75); \\(\\qquad\\)d:1-pbinom(x=4, n=10, p=0.25) 7.14 Ejercicios 7.14.0.1 Ejercicio 1 Si el 25% de los tornillo producidos por una máquina son defectuosos, determina la probabilidad de que, de 5 tornillos elegidos al azar ningún tornillo sea defectuoso (R:0.2373) 1 tornillo sea defectuoso (R:0.3955) 2 tornillos sean defectuosos (R:0.2636) como máximo 2 tornillos sean defectuosos (R:0.8964) 7.14.0.2 Ejercicio 2 En una población, la probabilidad de que nazca un niño es \\(p=0.51\\). Considera una familia de 4 hijos. ¿Cuál es la probabilidad de que una familia tenga un solo niño? (R: 0.240) ¿Cuál es la probabilidad de que una familia tenga una sola niña? (R: 0.259) ¿Cuál es la probabilidad de que una familia tenga solo un niño o solo una niña? (R: 0.4999) ¿Cuál es la probabilidad de que la familia tenga por mucho dos niños? (R: 0.6723) ¿Cuál es la probabilidad de que la familia tenga al menos dos niños? (R: 0.7023) ¿Cuál es el mínimo número de hijos que debe tener una familia para que la probabilidad de tener al menos una niña sea mayor a \\(0.75\\)?(R:\\(n=3 \\geq\\log(0.25)/\\log(0.51)=2.05\\)) 7.14.0.3 Ejercicio 3 Un motor de búsqueda falla al recuperar información con una probabilidad de \\(0.1\\) Si nuestro sistema recibe \\(50\\) solicitudes de búsqueda, ¿cuál es la probabilidad de que el sistema no responda a tres de ellas? (R: 0.1385651) ¿Cuál es la probabilidad de que el motor complete con éxito \\(15\\) búsquedas antes del primer fallo? (R:0.020) Consideramos que un motor de búsqueda funciona suficientemente bien cuando es capaz de encontrar información como mínimo para \\(10\\) solicitudes por cada \\(2\\) fallos. ¿Cuál es la probabilidad de que en un ensayo de fiabilidad nuestro buscador sea satisfactorio? (R: 0.659) "],["modelos-de-poisson-y-exponencial.html", "Chapter 8 Modelos de Poisson y Exponencial 8.1 Objetivo 8.2 Modelos de probabilidad para variables discretas 8.3 Experimento de Poissson 8.4 Función de masa de probabilidad de Poisson 8.5 Modelos de probabilidad para variables continuas 8.6 Experimento exponencial 8.7 Densidad de probabilidad exponencial 8.8 Distribución exponencial 8.9 Preguntas 8.10 Ejercicios", " Chapter 8 Modelos de Poisson y Exponencial 8.1 Objetivo En este capítulo veremos dos modelos de probabilidad estrechamente relacionados: los modelos Poisson y exponencial. El modelo de Poisson es para variables aleatorias discretas, mientras que la función exponencial es para variables aleatorias continuas 8.2 Modelos de probabilidad para variables discretas En el capítulo anterior construimos modelos complejos a partir de modelos simples. En cada etapa, introdujimos algún concepto novedoso: Uniforme: interpretación clásica de la probabilidad \\(\\downarrow\\) Bernoulli: Introducción de un parámetro \\(p\\) (familia de modelos) \\(\\downarrow\\) Binomial: Introducción a la Repetición de un experimento aleatorio (\\(n\\) ensayos de Bernoulli) \\(\\downarrow\\) Poisson: Repetición de un experimento aleatorio dentro de un intervalo continuo, sin control sobre cuándo/dónde ocurre el ensayo de Bernoulli. El último modelo es para procesos de Poisson que describen la repetición de un experimento aleatorio con la aleatoriedad adicional del momento en que la repeticiones se producen. 8.3 Experimento de Poissson Imaginemos que estamos observando eventos que ocurren aleatoriamente en un intervalo de tiempo o distancia. Por ejemplo: coches que llegan a un semáforo mensajes que recibimos en el teléfono móvil impurezas que ocurren al azar en un alambre de cobre Supongamos que los eventos son resultados de ensayos de Bernoulli independientes, cada uno de los cuales aparece aleatoriamente en un intervalo continuo, y queremos contarlos. ¿Cuál es la probabilidad de observar \\(X\\) eventos en una unidad de intervalo (tiempo o distancia)? Ejemplo (Impurezas en un alambre) Imaginemos que algunas impurezas se depositan al azar a lo largo de un cable de cobre. Queremos contar el número de impurezas en un centímetro de alambre (\\(X\\)). Considera que sabemos que en promedio hay \\(10\\) impurezas por centímetro \\(\\lambda=10/cm\\). ¿Cuál es la probabilidad de observar \\(X=5\\) impurezas en una muestra de un centímetro en particular? 8.4 Función de masa de probabilidad de Poisson Para calcular la función masa de probabilidad \\(f(x)=P(X=x)\\) del ejemplo anterior dividimos el centímetro en micrómetros (\\(0.0001cm\\)). Los micrómetros son lo suficientemente pequeños como para que no haya una impureza en cada micrómetro que cada micrómetro se pueda considerar como un ensayo de Bernoulli De la función binomial a la función de probabilidad de Poisson La probabilidad de observar \\(X\\) impurezas en \\(n=10,000\\mu\\) (1cm) sigue aproximadamente una distribución binomial \\[f(x) \\sim \\binom nxp^x(1-p)^{nx}\\] donde \\(p\\) es la probabilidad de encontrar una impureza en un micrómetro. Dado que el valor esperado de una variable Binomial es \\(E(X)=np\\). Este es el número promedio de impurezas por 1cm o \\(\\lambda=np\\). Por lo tanto, sustituimos \\(p=\\lambda/n\\) \\[f(x) \\sim \\binom nx \\big(\\frac{\\lambda}{n}\\big)^x(1-\\frac{\\lambda}{n})^{nx}\\] Como podría haber todavía dos impurezas en un micrómetro, necesitamos aumentar la partición del alambre y \\(n \\rightarrow \\infty\\). Por lo tanto en el límite: \\[f(x)= \\frac{e^{-\\lambda}\\lambda^x}{x!}\\] Donde \\(\\lambda\\) es constante porque es la densidad de impurezas por centímetro, una propiedad física del sistema. \\(\\lambda\\) es por lo tanto el parámetro del modelo de probabilidad. Detalles de la derivación: Para \\(f(x) \\sim \\binom nx \\big(\\frac{\\lambda}{n}\\big)^x(1-\\frac{\\lambda}{n})^{nx}\\) en el límite (\\(n \\rightarrow \\infty\\)) \\(\\frac{1}{n^x}\\binom n x =\\frac{1}{n^x}\\frac{n!}{x! (n-x)!}=\\frac{(n-x)!(n-x+1)...(n-1)n}{n^x x! (n-x)!}=\\frac{n(n-1)..(n-x+1)}{n^x x!} \\rightarrow \\frac{1}{x!}\\) \\((1-\\frac{\\lambda}{n})^{n} \\rightarrow e^{-\\lambda}\\) (definición de la función exponencial) \\((1-\\frac{\\lambda}{n})^{-x} \\rightarrow 1\\) Poniendo todo junto entonces: \\(f(x)= \\frac{e^{-\\lambda}\\lambda^x}{x!}\\) Definición Dado un intervalo en los números reales eventos que ocurren al azar en el intervalo que se conoce el número promedio de eventos en el intervalo (\\(\\lambda\\)) que se puede encontrar una pequeña partición regular del intervalo tal que en cada partición la podamos considerar como un ensayo de Bernoulli. Entonces, la variable aleatoria \\(X\\) que cuenta eventos a lo largo del intervalo es una variable Poisson con función de masa de probabilidad \\[f(x)= \\frac{e^{-\\lambda}\\lambda^x}{x!}, \\lambda&gt;0\\] Propiedades: Cuando \\(X \\rightarrow Poiss(\\lambda)\\) tiene media \\[E(X)= \\lambda\\] y varianza \\[V(X)= \\lambda\\] Ejemplos ¿Cuál es la probabilidad de recibir 4 correos electrónicos en una hora, cuando el promedio de correos electrónicos en una hora es de \\(1\\)? Tenemos que la varible es de Poisson: \\(X \\rightarrow Poiss(\\lambda)\\) con \\(\\lambda=1\\) y su función de masa de probabilidad es: \\[f(x)= \\frac{e^{-1}1^x}{x!}\\] Por lo tanto la probabilidad de que la variable tome valor 4 es \\(P(X=4)\\): \\(f(4; \\lambda=1)= \\frac{e^{-1}1^4}{4!}=0.01532831\\) in R dpois(4,1) ¿Cuál es la probabilidad de recibir 4 correos electrónicos en tres horas, cuando el promedio de correos electrónicos en una hora es de \\(1\\)? La unidad sobre la cual hacemos los conteos ha cambiado de 1 hora a 3 horas, por lo tanto tenemos que re-escalar \\(\\lambda\\). Si antes el promedio de correos era \\(\\lambda=1\\) en una hora, el promedio de correos en tres horas es ahora 3: \\(\\lambda_{3h}=3\\lambda_{1h}=3*1=3\\) Tenemos que la varible es de Poisson: \\(X \\rightarrow Poiss(\\lambda_{3h})\\) con \\(\\lambda_{3h}=3\\) y su función de masa de probabilidad es: \\[f(x)= \\frac{e^{-3}3^x}{x!}\\] Por lo tanto la probabilidad de que la variable tome valor 4 es \\(P(X=4)\\): \\(f(4; \\lambda=3)= \\frac{e^{-3}3^4}{4!}=0.1680314\\) in R dpois(4,3) ¿Cuál es la probabilidad de contar al menos \\(10\\) automóviles que llegan a un peaje en un minuto, cuando el promedio de automóviles que llegan a un peaje en un minuto es de \\(5\\); Tenemos que la varible es de Poisson: \\(X \\rightarrow Poiss(\\lambda)\\) con \\(\\lambda=5\\) y su función de masa de probabilidad es: \\[f(x)= \\frac{e^{-5}5^x}{x!}\\] \\(P(X\\geq 10)=1-P(X &lt; 10)=1-P(X \\leq 9)=1-F(9; \\lambda=5)=1-\\sum_{x=0, ...10}f(x; \\lambda=5)=0.03182806\\) en R 1-ppois(9,5) Veamos algunas funciones de masa de probabilidad en la familia de modelos paramétricos de Poisson: 8.5 Modelos de probabilidad para variables continuas Los modelos de probabilidad para variables continuas son funciones de densidad de probabilidad \\(f(x)\\) que creemos describen experimentos aleatorios reales. La función de densidad de probabilidad \\(f(x)\\) es positiva \\[f(x) \\geq 0\\] nos permite calcular probabilidades usando el área bajo la curva: \\[P(a\\leq X \\leq b)=\\int_{a}^{b} f(x) dx\\] es tal que la probabilidad de que obtengamos cualquier resultado es \\(1\\): \\[\\int_{-\\infty}^{\\infty} f(x) dx = 1\\] 8.6 Experimento exponencial Volvamos a un proceso de Poisson definido por la probabilidad \\[f(k)=\\frac{e^{-\\lambda}\\lambda^k}{k!}, \\lambda&gt;0\\] para el número de eventos (\\(k\\)) en un intervalo. Consideremos ahora que estamos interesados en la duración/tiempo que debemos esperar hasta que ocurra el primer evento. Podemos preguntarnos por la probabilidad de que el primer evento ocurra después de la duración/tiempo \\(X\\). Por lo tanto, dado que \\(X\\) es una variable aleatoria continua, buscamos su función de densidad de probabilidad \\(f(x)\\). 8.7 Densidad de probabilidad exponencial La probabilidad de \\(0\\) eventos si un intervalo tiene unidad \\(x\\) (rescalando como en el ejemplo 2) es \\[f(0|x)=\\frac{e^{-x\\lambda}(x\\lambda)^0}{0!}\\] o \\[f(0|x)=e^{-x\\lambda}\\] Podemos tratar esto como la probabilidad condicional de \\(0\\) eventos dada una distancia \\(x\\): \\(f(K=0|X=x)\\) y aplicar el teorema de Bayes para invertirlo: \\[f(x|0)=C f(0|x)=C e^{-x\\lambda}\\] Esta es la probabilidad de observar una distancia \\(x\\) para \\(0\\) eventos, o la distancia hasta el primer evento.La constante \\(C\\) es simplemente función de las marginales que podemos encontrar fácilmente porque la función debe integrar a 1. Definición En un proceso de Poisson con parámetro \\(\\lambda\\) la probabilidad de esperar una distancia/tiempo \\(X\\) entre dos conteos viene dada por la densidad de probabilidad \\[f(x)= C e^{-x\\lambda}\\] \\(C\\) es una constante que asegura: \\(\\int_{-\\infty}^{\\infty} f(x) dx =1\\) por integración \\(C=\\lambda\\) Por lo tanto: \\[f(x)=\\lambda e^{-\\lambda x}, x\\geq 0\\] \\(\\lambda\\) es el parámetro del modelo, también conocido como tasa de decaimiento. Propiedades: Cuando \\(X \\rightarrow Exp(\\lambda)\\) entonces tiene media \\[E(X)=\\frac{1}{\\lambda}\\] y varianza \\[V(Y)=\\frac{1}{\\lambda^2}\\] Veamos un par de densidades de probabilidad en la familia exponencial 8.8 Distribución exponencial Consideremos las siguientes preguntas: En un proceso de Poisson ¿Cuál es la probabilidad de observar un intervalo menor que \\(a\\) hasta el primer evento? Recuerda que esta probabilidad \\(F(a)=P(X \\leq a)\\) es la densidad de probabilidad \\[F(a)=\\lambda\\int_\\infty^ae^{-x\\lambda}dx=1-e^{-a\\lambda}\\] 2) En un proceso de Poisson ¿Cuál es la probabilidad de observar un intervalo mayor que \\(a\\) hasta el primer evento? \\[P(X &gt; a)=1- P(X \\leq a)= 1- F(a) = e^{-a\\lambda}\\] Veamos un par de distribuciones exponenciales de la familia exponencial La mediana \\(x_m\\) es tal que \\(F(x_m)=0.5\\). Eso es \\(x_m=\\frac{\\log(2)}{\\lambda}\\) o el tiempo/distancia media de decaimiento. Ejemplos ¿Cuál es la probabilidad de que tengamos que esperar un bus por más de \\(1\\) hora cuando en promedio hay dos buses por hora? \\[P(X &gt; 1)=1-P(X \\le 1) = 1-F(1,\\lambda=2)=0.1353353\\] En R 1-pexp(1,2) ¿Cuál es la probabilidad de tener que esperar menos de \\(2\\) segundos para detectar una partícula cuando la tasa de desintegración radiactiva es de \\(2\\) partículas por segundo; \\(F(2,\\lambda=2)\\) \\[P(X\\le 2)=F(2,\\lambda=2)=0.9816844\\] En R pexp(2,2) 8.9 Preguntas 1) Durante la Segunda Guerra Mundial, en un día de bombardeo sobre Londres, el valor eperado de que cayera una bomba en \\(1.5km^2\\) era de \\(0.92\\). La probabilidad de que en Hyde Park, de área aproximadamente \\(1.5km^2\\), cayeran como mucho dos bombas era de \\(\\qquad\\)a:1-ppois(x=2, lambda=0.92); \\(\\qquad\\)b:ppois(x=2, lambda=0.92); \\(\\qquad\\)c:1-dpois(x=2, lambda=0.92); \\(\\qquad\\)d:dpois(x=2, lambda=0.92) 2) La probabilidad de que un pasajero tenga que esperar menos de 20 minutos hasta que llegue el próximo taxi a su parada está mejor descrita por \\(\\qquad\\)a: Un modelo de Poisson sobre el número de taxis que pasan cada 20 minutos; \\(\\qquad\\)b: Una distribución exponencial con \\(\\lambda=1/20\\) ; \\(\\qquad\\)c: Un modelo binomial que cuenta el número de taxis cada 20 minutos \\(\\qquad\\)d: Una distribución uniforme entre 0 y 20 minutos; 3) A partir de la distribución de probabilidad exponencial de la siguiente figura, ¿cuál es el valor más posible de la mediana? \\(\\qquad\\)a: \\(2\\); \\(\\qquad\\)b: \\(3\\); \\(\\qquad\\)c: \\(4\\); \\(\\qquad\\)d: \\(5\\) 8.10 Ejercicios 8.10.0.1 Ejercicio 1 El promedio de llamadas telefónicas por hora que ingresan a la centralita de una empresa es de \\(150\\). Encuentra la probabilidad de que durante un minuto en particular haya 0 llamadas telefónicas (R:0.082) 1 llamada telefónica (R:0.205) 4 o menos llamadas (R:0.891) más de 6 llamadas telefónicas (R:0.0141) 8.10.0.2 Ejercicio 2 La cantidad promedio de partículas radiactivas que golpean un contador Geiger en una planta de energía nuclear bajo control es de \\(2.3\\) por minuto. ¿Cuál es la probabilidad de contar exactamente \\(2\\) partículas en un minuto? (R:0.265) ¿Cuál es la probabilidad de detectar exactamente \\(10\\) partículas en \\(5\\) minutos? (R:0.112) ¿Cuál es la probabilidad de al menos un conteo en dos minutos? (R:0.9899) ¿Cuál es la probabilidad de tener que esperar menos de \\(1\\) segundo para detectar una partícula radiactiva, después de encender el detector? (R:0.037) Sospechamos que una planta nuclear tiene una fuga radiactiva si esperamos menos de \\(1\\) segundo para detectar una partícula radiactiva, después de encender el detector. ¿Cuál es la probabilidad de que cuando visitemos \\(5\\) plantas que están bajo control, sospechemos que al menos una tiene una fuga? (R:0.1744). "],["distribución-normal.html", "Chapter 9 Distribución normal 9.1 Objetivo 9.2 Historia 9.3 Densidad normal 9.4 Definición 9.5 Distribución de probabilidad 9.6 Densidad normal estándar 9.7 Distribución estándar 9.8 Estandarización 9.9 Resumen de modelos de probabilidad 9.10 Funciones R de modelos de probabilidad 9.11 Preguntas 9.12 Ejercicios", " Chapter 9 Distribución normal 9.1 Objetivo En este capítulo introduciremos la distribución de probabilidad normal. Hablaremos de su origen y de sus principales propiedades. 9.2 Historia En 1801, Gauss analizó los datos obtenidos sobre la posición de Ceres, un gran asteroide entre Marte y Júpiter. En ese momento, la gente sospechaba que era un nuevo planeta, ya que se movía día a día contra las estrellas fijas. En enero, se podía ver en el horizonte justo antes del amanecer. Sin embargo, a medida que pasaban los días, Ceres salía cada vez más tarde hasta que ya no se pudo ver más debido a la salida del Sol. Gauss entendió que las medidas para la posición de Ceres tenían errores. Por lo tanto, estaba interesado en descubrir cómo se distribuían las observaciones para poder encontrar la órbita más probable. Con la órbita, podía derivar la masa del objeto y luego decidir si era un planeta o sólo un gran asteroide. Los datos estaban disponibles sólo para el mes de enero. Después de lo cual Ceres desaparecería. Quería predecir hacia dónde deberían apuntar los astrónomos sus telescopios para encontrarlo seis meses después al anochecer, una vez que hubiera pasado por detrás del Sol. Gauss tuvo que dar cuenta de los errores en la posición de ceres en un día determinado debido a la medición Gauss supuso que los errores pequeños eran más probables que los errores grandes el error a una distancia \\(-\\epsilon\\) del varlor en la posición de Ceres era igualmente probable que una distancia \\(\\epsilon\\) la pisición más verosimil (que nos creemos más) de Ceres en un momento dado en el cielo era el promedio de múltiples mediciones de la altitud de Ceres desde el horizonte. Eso fue suficiente para mostrar que las desviaciones de las observaciones \\(y\\) de la órbita satisfacian la ecuación \\[\\frac{df(y)}{dy}=-Cyf(y)\\] con \\(C\\) una costante positiva. La solución de esta ecuación diferencial es: \\[f(y)=\\frac{\\sqrt{C}}{\\sqrt{2\\pi}}e^{-\\frac{Cy^2}{2}}\\] *The evolution of the normal distribution, Saul Stahl, Mathemetics Magazine, 2006. 9.3 Densidad normal Densidad de probabilidad de Gauss da la distribución de los errores de medición desde la posición real pero desconocida de Ceres en el cielo. Hagamos un par de cambios en la función. 1- Escribamos la densidad de errores desde el horizonte usando la variable aleatoria \\(X\\), o sea \\(y=x-\\mu\\). \\(\\mu\\) es la posición real pero desconocida de Ceres desde el horizonte. Después de un cambio de variable encontramos la función de densidad de probabilidad: \\[f(x)=\\frac{\\sqrt{C}}{\\sqrt{2\\pi}}e^{-C(x-\\mu)^2}\\] Cambiemos de nombre la variable \\(C\\) por \\(\\frac{1}{\\sigma^2}\\) Entonces, llegamos a la siguente definición. 9.4 Definición Una variable aleatoria \\(X\\) definida en los números reales tiene una densidad Normal si toma la forma \\[f(x; \\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, x \\in {\\mathbb R}\\] La variable tiene media \\[E(X) = \\mu\\] que para Gauss representaba la posición real de Ceres desde el horizonte. y varianza \\[V (X) = \\sigma^2\\] que representaba la dispersión del error en las observaciones, que dependía de la calidad del telescopio. \\(\\mu\\) y \\(\\sigma\\) son los dos parámetros que describen completamente la función de densidad normal y su interpretación depende del experimento aleatorio. Cuando \\(X\\) sigue una densidad Normal, es decir, se distribuye normalmente, escribimos \\[X\\rightarrow N(\\mu,\\sigma^2)\\] Veamos algunas densidades de probabilidad en el modelo paramétrico normal 9.5 Distribución de probabilidad La distribución de probabilidad de la densidad Normal: \\[F(a)=P(Z \\leq a)\\] es la función de error definida por el área bajo la curva de \\(-\\infty\\) a \\(a\\) \\[F(a)=\\int_{-\\infty}^{a}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu) ^2}{2\\sigma^2}} dx\\] La función se encuentra en la mayoría de los programas de software estadístico y no tiene una forma cerrada en términos de funciones conocidas. Ejemplo (altura de mujeres) ¿Cuál es la probabilidad de que una mujer de la población tenga una altura máxima de \\(150cm\\) si las mujeres tienen una altura media de \\(165cm\\) con una desviación estándar de \\(8cm\\)? \\(P(X\\le 150)=F(150, \\mu=165, \\sigma=8)=0.03039636\\) en R pnorm(150, 165, 8) ¿Cuál es la probabilidad de que la altura de una mujer en la población esté entre \\(165cm\\) y \\(170cm\\)? \\(P(165 \\le X \\le 170)=F(170, \\mu=165, \\sigma=8)-F(165, \\mu=165, \\sigma=8)=0.2340145\\) en R pnorm(170, 165, 8)-pnorm(165, 165, 8) Veamos la función de distribución de probabilidad ¿Cuál es el primer cuartíl para altura de las mujeres? El primer cuartíl se define como: \\(F(x_{0.25}, \\mu=165, \\sigma=8)=0.25\\) o \\(x_{0.25}=F^{-1}(0.25, \\mu=165, \\sigma=8)=159.6041\\) en R qnorm(0.25, 165, 8) Propiedades de la distribución Normal la media \\(\\mu\\) es también la mediana ya que divide los resultados del experimento aleatorio en dos Los valores de \\(x\\) que caen más allá de 2\\(\\sigma\\) se consideran raros \\(5\\%\\) Los valores de \\(x\\) que caen más allá de 3\\(\\sigma\\) se consideran extremadamente raros \\(0.2\\%\\) Ejemplo (altura de mujeres) Podemos definir los límites de observaciones comunes para la distribución de la altura de las mujeres en la población. a una distancia de una desviación estándar de la media, encontramos \\(68\\%\\) de la población \\[P(165-8 \\leq X \\leq 165+8)=P(157 \\leq X \\leq 173)=F(173)-F(157)=0.68\\] a una distancia de dos desviaciones estándar de la media, encontramos \\(95\\%\\) de la población \\[P(165-2 \\times 8 \\leq X \\leq 165+2\\times 8)=F(181)-F(149)=0.95\\] a una distancia de tres desviaciones estándar de la media, encontramos \\(99.7\\%\\) de la población \\[P(165-3 \\times 8 \\leq X \\leq 165+3\\times 8)=F(189)-F(141)=0.997\\] 9.6 Densidad normal estándar La densidad normal estándar es la densidad particular de la familia normal \\[f(x; \\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, x \\in {\\mathbb R}\\] Con media cero \\[E(X)= \\mu = 0\\] y varianza uno \\[V (X)= \\sigma^2 =1\\] Cuando una variable aleatoria sigue una densidad de probabilidad estandar decimos que es normal estandar y escribimos \\[X \\rightarrow N(0,1)\\] 9.7 Distribución estándar La distribución estándar es: \\[\\phi(a)=F_{N(0,1)}(a)=P(Z \\leq a)\\] es la función error definida por \\[\\phi(a)=\\int_{-\\infty}^{a} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{z^2}{2}} dz\\] Debido a que la distribución estándar es especial y aparecerá con frecuencia, usamos la letra \\(\\phi\\) para referirnos a ella Esta función se puede encontrar en la mayoría de los programas estadísticos de ordenador. En R es pnorm(x) con los parámetros predeterminados, 0 y 1. Normalmente definimos los límites de las observaciones más comunes para la variable estándar El rango intercuartílico \\[P(-0.67 \\leq X \\leq 0.67)=0.50\\] en R: c(qnorm(0.25), qnorm(0.75)) El rango del \\(95\\%\\) \\[P(-1.96 \\leq X \\leq 1.96)=0.95\\] en R: c(qnorm(0.025), qnorm(0.975)) El rango del \\(99\\%\\) \\[P(-2.58 \\leq X \\leq 2.58)=0.99\\] en R: c(qnorm(0.005), qnorm(0.995)) 9.8 Estandarización Todas las variables normales se pueden estandarizar. Esto significa que si \\(X \\rightarrow N(\\mu, \\sigma^2)\\), entonces podemos transformar la variable a una variable estandarizada \\[Z=\\frac{X-\\mu}{\\sigma}\\] que tendrá densidad: \\[f(z)=\\frac{1}{ \\sqrt{2\\pi}}e^{-\\frac{z^2}{2}}\\] Por lo tanto, para cualquier \\(X \\rightarrow N(\\mu, \\sigma^2)\\) \\[Z=\\frac{X-\\mu}{\\sigma} \\rightarrow N(0, 1) \\] Podemos demostrar esto reemplazando \\(x=\\sigma z+\\mu\\) y \\(dx=\\sigma dz\\) en la probabilidad \\(P(x\\leq X \\leq x +dx)=P(z\\leq Z \\leq z +dz)\\) \\[=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}dx\\] \\[=\\frac{1}{ \\sqrt{2\\pi}}e^{-\\frac{z^2}{2}} dz\\] La probabilidad de cualquier variable normal \\(X\\rightarrow N(\\mu, \\sigma^2)\\) se puede calcular usando la distribución estándar \\(F(a)=P(X&lt;a)=P(\\frac{X-\\mu}{\\sigma}&lt;\\frac{a-\\mu}{\\sigma})\\) \\[=P(Z &lt; \\frac{a-\\mu}{\\sigma})= \\phi \\big(\\frac{a-\\mu}{\\sigma}\\big)\\] Para calcular \\(P(a\\leq X \\leq b)\\), usamos la propiedad de las distribuciones de probabilidad \\(F(b)-F(a)=P(X\\leq b)-P(X\\leq a)\\) \\[=\\phi \\big(\\frac{b-\\mu}{\\sigma}\\big)-\\phi \\big(\\frac{a-\\mu}{\\sigma}\\big)\\] 9.9 Resumen de modelos de probabilidad Modelo X rango de x f(x) E(X) V(X) Uniforme número entero o real \\([a,b]\\) \\(\\frac{1}{n}\\) \\(\\frac{b+a}{2}\\) \\(\\frac{(b-a+1)^2-1}{12}\\) Bernoulli evento A (1) 0,1 \\((1-p)^{1-x}p^x\\) \\(p\\) \\(p(1-p)\\) Binomial # de eventos \\(A\\) en \\(n\\) repeticiones de ensayos de Bernoulli 0,1,… \\(\\binom nx (1-p)^{nx}p^x\\) \\(np\\) \\(np(1-p)\\) Binomial negativo para eventos # de eventos \\(A&#39;\\) (0) en repeticiones de Bernoulli antes de \\(r\\) eventos \\(A\\) (1) 0,1,.. \\(\\binom {x+r-1} x (1-p)^xp^r\\) \\(\\frac{r(1-p)}{p}\\) \\(\\frac{r(1-p)}{p^2}\\) Hipergeométrico # de eventos \\(A\\) en una muestra \\(n\\) de la población \\(N\\) con \\(K\\) numero de \\(A\\)s \\(\\max(0, n+KN)\\), … \\(\\min(K, n)\\) \\(\\frac{1}{\\binom N n}\\binom K x \\binom {N-K} {n-x}\\) \\(n*\\frac{N}{K}\\) \\(n \\frac{N}{K} (1-\\frac{N}{K})\\frac{Nn}{N-1}\\) Poisson # de eventos \\(A\\) en un intervalo 0,1, .. \\(\\frac{e^{-\\lambda}\\lambda^x}{x!}\\) \\(\\lambda\\) \\(\\lambda\\) Exponencial Intervalo entre dos eventos \\(A\\) \\([0,\\infty)\\) \\(\\lambda e^{-\\lambda x}\\) \\(\\frac{1}{\\lambda}\\) \\(\\frac{1}{\\lambda^2}\\) Normal medida con errores simétricos cuyo valor más probable es la media \\((-\\infty, \\infty)\\) \\(\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2 }}\\) \\(\\mu\\) \\(\\sigma^2\\) 9.10 Funciones R de modelos de probabilidad modelo R f(x) R F(x) Uniforme (continuo) dunif(x, a, b) punif(x, a, b) Binomial dbinom(x,n,p) pbimon(x,n,p) Binomial negativo para eventos dnbinom(x,r,p) pnbinom(x,r,p) Hipergeométrico dhyper(x, K, N-K, n) phyper(x, K, N-K, n) Poisson dpois(x, lambda) ppois(x, lambda) Exponencial dexp(x, lambda) pexp(x, lambda) normales dnorm(x, mu, sigma) pnomr(x, mu, sigma) 9.11 Preguntas 1) Para una variable normal estándar \\(\\qquad\\)a: \\(50\\%\\) de sus observaciones están entre \\((-0.67,0.67)\\); \\(\\qquad\\)b: \\(2\\%\\) de sus observaciones son inferiores a \\(-2.58\\); \\(\\qquad\\)c: \\(5\\%\\) de sus observaciones son superiores a \\(1.96\\); \\(\\qquad\\)d: \\(25\\%\\) de sus observaciones están entre \\((-1.96,-0.67)\\) 2) si sabemos que \\(\\phi(-0.8416212)=0.2\\) entonces que es \\(\\phi(0.8416212)\\) \\(\\qquad\\)a: \\(0.1\\); \\(\\qquad\\)b: \\(0.2\\); \\(\\qquad\\)c: \\(0.8\\); \\(\\qquad\\)d: \\(0.9\\) 3) el tercer cuartil de una variable normal con media \\(10\\) y desviación estándar \\(2\\) es \\(\\qquad\\)a: qnorm(1/3, 10, 2)=9.138545; \\(\\qquad\\)b: qnorm(1-0.75, 10, 2)=8.65102 ; \\(\\qquad\\)c: qnorm(1-1/3, 10, 2)=10.86145 ; \\(\\qquad\\)d: qnorm(0.75, 10, 2)= 11.34898 4) la probabilidad de que una variable normal con media \\(10\\) y desviación estándar \\(2\\) esté en \\((-\\infty,10)\\) es \\(\\qquad\\)a: 0.25; \\(\\qquad\\)b: 0.5; \\(\\qquad\\)c: 0.75; \\(\\qquad\\)d: 1: 5) No es cierto que para una variable normal estándar \\(\\qquad\\)a: su media y mediana son iguales; \\(\\qquad\\)b: la distribución de probabilidad estándar se puede utilizar para calcular sus probabilidades; \\(\\qquad\\)c: su rango intercuartílico es el doble de su desviación estándar; \\(\\qquad\\)d: \\(5\\%\\) de sus observaciones están a una distancia desde \\(0\\) mas grande que su desviación estándar 9.12 Ejercicios 9.12.0.1 Ejercicio 1 Encuentra el área bajo la curva normal estándar en los siguientes casos: Entre \\(z=0.81\\) y \\(z=1.94\\) (R:0.182) A la derecha de \\(z=-1.28\\) (R:0.899) A la derecha de \\(z=2.05\\) o a la izquierda de \\(z=-1.44\\) (R:0.0951) 9.12.0.2 Ejercicio 2 ¿Cuál es la probabilidad de que la altura de un hombre sea al menos \\(165\\)cm si la media poblacional es \\(175\\)cm y la desviación estándar es \\(10\\)cm? (R:0.841) ¿Cuál es la probabilidad de que la altura de un hombre esté entre \\(165\\)cm y \\(185\\)cm? (R:0.682) ¿Cuál es la altura que define el \\(5\\%\\) de los hombres más pequeños? (R:158.55) "],["distribuciones-de-muestreo.html", "Chapter 10 Distribuciones de muestreo 10.1 Objetivo 10.2 Muestra aleatoria 10.3 Cálculo de probabilidades 10.4 Estimación de los parámetros 10.5 Ley de los grandes números 10.6 Inferencia 10.7 Distribución media muestral 10.8 Suma muestral 10.9 Variaza muestral 10.10 Probabilidades de la varianza muestral 10.11 \\(\\chi^2\\)-estadística 10.12 Preguntas 10.13 Ejercicios", " Chapter 10 Distribuciones de muestreo 10.1 Objetivo En este capítulo, vamos a estudiar las estimaciones de la media y la varianza de las distribuciones normales utilizando muestras aleatorias. Introduciremos la media muestral y la varianza muestral como variables aleatorias que estiman los parámetros de la distribución normal. La media muestral y la varianza muestral tienen funciones de densidad de probabilidad, estas se denominan funciones de densidad muestral. 10.2 Muestra aleatoria Ejemplo (Cables) Imaginemos que un cliente le pide a un empresa metalúrgica que le venda cables a \\(8\\) que pueden transportar hasta \\(96\\) Toneladas; eso es \\(12\\) Toneladas cada uno. La empresa debe garantizar que ninguno de ellos romperá con este peso. La empresa tiene en existencia un conjunto de cables que podrían servir, pero el ingeniero de materiales no está seguro. Por lo que toma \\(8\\) cables aleatoriamente, y los carga hasta romperlos. Decimos que el ingeniero toma una muestra aleatoria de tamaño \\(8\\), lo que significa que repite el experimento aleatorio \\(8\\) veces. Aquí están los resultados ## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747 Definición: Una muestra aleatoria de tamaño \\(n\\) es la repetición de un experimento aleatorio \\(n\\) independientes veces. Una muestra aleatoria es una variable aleatoria \\(n\\)-dimensional \\[(X_1, X_2, ... X_n)\\] donde \\(X_i\\) es la i-ésima repetición del experimento aleatorio con distribución común \\(f(x; \\theta)\\) para cualquier \\(i\\) Una observación de una muestra aleatoria es el conjunto de \\(n\\) valores obtenidos de los experimentos \\[(x_1, x_2, ... x_n)\\] La observación de la muestra de tamaño \\(8\\) cables hecha por el ingeniero fue ## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747 Ejemplo (Cables) En la muestra observada de la carga de rotura de los cables se observó que Ninguno de ellos rompió en \\(12\\) Toneladas. Hubo uno que rompió en \\(12.62747\\) Toneladas. ¿Es razonable que la empresa arriesgue y venda una muestra aleatoria de \\(8\\) cables de su stock? La empresa es responsable de la rotura de un cable y tiene que pagar los daños y una multa elevada si esto ocurre. Para garantizarle al cliente que los cables no se romperán a \\(12\\) Toneladas, nos gustaría ver que \\(P(X \\leq 12)\\) es razonablemente bajo. 10.3 Cálculo de probabilidades Para calcular probabilidades necesitamos: Un modelo de probabilidad (función de probabilidad) Los parámetros del modelo (los valores de la función de probabilidad) Vamos a suponer que la carga de rotura de los cables sigue una función de densidad de probabilidad normal. \\[X \\rightarrow N(x; \\mu, \\sigma^2)\\] Para calcular \\(P(X \\leq 12)\\), necesitamos los parámetros \\(\\mu\\) y \\(\\sigma^2\\). ¿Cómo podemos estimar los parámetros de la muestra observada? 10.4 Estimación de los parámetros Para encontrar valores plausibes para los parámetros usamos datos. Por lo tanto, tomamos una muestra aleatoria. Es decir, repetimos el experimento \\(n\\) veces, recolectamos datos y los usamos para estimar los parámetros. Estimación de la media y la varianza Recordemos que para una variable aleatoria discreta, definimos la media como \\[\\mu=\\sum_{i}^m x_if(x_i)\\] que es el centro de gravedad de las probabilidades, donde \\(f(x_i)\\) es la función de probabilidad. Esta definición fue motivada por el centro de gravedad de las observaciones \\[\\bar{x}= \\frac{1}{n} \\sum_{i}^n x_i = \\sum_{i}^m x_if_i\\] que definimos como promedio, y donde \\(f_i\\) son las frecuencias relativas. Recordemos que \\(n\\) es el número de observaciones (puede ser tan grande como queramos) y \\(m\\) es el número de resultados posibles (normalmente fijado por el espacio muestral). Habíamos dicho entonces que cuando \\(n \\rightarrow \\infty\\) entonces \\[\\hat{P}(X=x)=f_i\\] Esto significa que las probabilidades pueden ser estimadas (poniéndoles un sombrero) por las frecuencias relativas cuando \\(n\\) es grande, porque \\(lim_{n\\rightarrow \\infty}f_i=f(x_i)\\). Por lo tanto, también deberíamos tener que la media \\(\\mu\\) puede ser estimada por el promedio \\(\\bar{x}\\) \\[\\hat{\\mu}=\\bar{x}= \\sum_{i}^m x_i\\hat{P}(X=x)\\] Asi pues, podemos tomar el centro de gravedad de la función de probabilidad como el centro de gravedad de los datos. Al hacer esto cometeremos un error que podemos asumir, como explicaremos luego. Insistamos aquí que \\(\\bar{\\mu}\\) es una cantidad obtenida de los datos mientras que \\(\\mu\\) es un parámetro desconocido de la funcioón de probabilidad. Con la varianza \\[\\sigma^2=\\sum_{i}^m (x_i-\\mu)^2f(x_i)\\] tenemos una situación similar. En el límite cuando \\(n \\rightarrow \\infty\\) \\[\\hat{\\sigma}^2=s^2=\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar{x})^2\\] y suponemos que el momento de inercia de los datos es cercano al momento de inercia de las probabilidades. Ejemplo (Cables) Suponiendo que la carga de rotura de nuestro cable es una variable aleatoria normal \\[X \\rightarrow N(x; \\mu, \\sigma^2)\\] usamos las estimaciones \\(\\bar{x}_{stock}=13.21\\) (mean(x)) y \\(s^2=0.3571565^2\\) (sd( x)^2) como los valores de \\(\\mu\\) y \\(\\sigma^2\\). De tal forma que el modelo ajustado es \\[X \\rightarrow N(x; \\mu=13.21, \\sigma^2=0.3571565^2)\\] En este problema no sabíamos \\(\\mu\\) o \\(\\sigma\\) y, por lo tanto, estamos adivinando sus valores y, en consequencia, el modelo subyacente. ¿Cuál es la probabilidad de que el cable se rompa a \\(12\\) Toneladas? Como \\[X \\rightarrow N(x; \\mu=13.21, \\sigma^2=0.3571565^2)\\] entonces \\[P(X \\leq 12)= F(12; \\mu=13.21, \\sigma^2=0.1275608)\\] En R pnorm(12,13.21, 0.3571565)\\(=0.000352188\\) Dada la muestra observada, existe una probabilidad estimada de \\(0.03\\%\\) de que un cable se rompa a las \\(12\\) toneladas. Tenemos un argumento probabilístico para vender los cables. Hay muchas otras actividades avaladas con mayor riesgo que este. 10.5 Ley de los grandes números Cuando estimamos los parámetros usando datos, como al tomar el valor de \\[\\hat{\\mu}=\\bar{x}\\] por el valor de \\(\\mu\\); y el valor de \\[\\hat{\\sigma}^2=s^2\\] por el valor de \\(\\sigma^2\\), sabemos que estamos cometiendo un error. Sabemos que si tomamos otra muestra diferente de \\(8\\) cables la estimación cambiará, porque el promedio \\(\\bar{x}\\) cambiará. ¿Podemos tener una idea de qué tan grande es el error de nuestra estimación? Lo primero que debemos darnos cuenta es que el valor numérico que obtenemos para \\[\\bar{x}\\] es la observación de una variable aleatoria \\[\\bar{X}\\] Definición La media muestral (o promedio) de una muestra aleatoria de tamaño \\(n\\) se define como \\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^n X_i\\] El promedio es una variable aleatoria que en nuestra muestra de tamaño \\(8\\) tomó el valor \\[\\bar{x}_{stock}=13.21\\] Si tomamos otra muestra, este número cambiará. La media como estimador El número \\(\\bar{x}\\) se puede usar para estimar el parámetro desconocido \\(\\mu\\) porque la variable aleatoria \\(\\bar{X}\\) satisface estas dos propiedades importantes El valor esperado de \\(\\bar{X}\\) es precisamente el parámetro \\(\\mu\\): \\[E(\\bar{X})=\\mu\\] La varianza de \\(\\bar{X}\\) se hace pequeña con valores grandes de la muestra: \\[lim_{n \\rightarrow \\infty} V(\\bar{X}) = 0\\] La primera propiedad se tiene porque \\[E(\\bar{X})=E\\big(\\frac{1}{n}\\sum_{i=1}^n X_i\\big)=E(X)=\\mu\\] La segunda propiedad se tien porque \\[V(\\bar{X})=V\\big(\\frac{1}{n}\\sum_{i=1}^n X_i\\big)=\\frac{V(\\sum_{i=1}^ nX_i)}{n^2}=\\frac{V(X)}{n}=\\frac{\\sigma^2}{n}\\] aquí usamos el hecho de que cada experimento aleatorio en la muestra es independiente y por lo tanto \\(V(\\sum_{i=1}^n X_i)=nV(X)\\). Estimación de \\(\\mu\\) Como consecuencia de las propiedades 1 y 2, entendemos que el valor \\(\\bar{x}\\) se concentra más y más cerca de \\(\\mu\\) a medida que aumenta \\(n\\). Esto significa que el error que cometemos cuando tomamos un valor de \\(\\bar{x}\\) como la estimación de \\(\\mu\\) \\[\\bar{x}=\\hat{\\mu}\\] se vuelve más y más pequeño a medida que la muestra se hace más y más grande. Esto es porque la varianza de \\(\\bar{x}\\) disminuye cuando \\(n\\) aumenta. 10.6 Inferencia Sabemos que cuando tomamos muestras grandes, nuestro error es pequeño. Sin embargo, para un valor dado de \\(n\\) queremos tener una medida del error. Por lo tanto, nos preguntamos por la probabilidad de cometer un error de un tamaño dado cuando estimamos \\(\\mu\\) con \\(\\bar{x}\\). Cuando calculamos probabilidades en un estimador, decimos que estamos haciendo una inferencia. Los problemas de inferencia suelen surgir cuando nos interesa calcular la probabilidad de cometer un error de magnitud \\(m\\) al estimar \\(\\mu\\) con \\(\\bar{x}\\). En términos matemáticos, nos interesa la probabilidad \\[P(-m \\leq \\bar{X}-\\mu \\leq m) \\] Esto es la probabilidad de que la diferencia entre el promedio y la media este a una distacia de \\(m\\). Para calcular estaprobabilidades de \\(\\bar{X}\\) necesitamos Un modelo de probabilidad (función de probabilidad) Los parámetros del modelo (los valores de la función de probabilidad) ¿Cuáles son las funciones de probabilidad de \\(\\bar{X}\\) y \\(S^2\\) para que podamos calcular sus probabilidades? Estas funciones de probabilidad se denominan funciones de probabilidad de muestreo, porque se derivan de un experimento de muestreo. Ejemplo (cables) Hagamos una pregunta de inferencia. Imagina que nuestros cables están certificados para romperse con una carga promedio de \\(\\mu = 13\\) toneladas con varianza \\(\\sigma^2=0.35^2\\). Ahora vamos a pretender que conocemos los parámtros del modelo. Si tomamos una muestra aleatoria de \\(8\\) cables, ¿Cuál es la probabilidad de que la media de la muestra \\(\\bar{X}\\) esté dentro de un margen de error de \\(0.25\\) toneladas de la media \\(\\mu\\)? \\[P(- 0.25\\leq \\bar{X}-\\mu \\leq 0.25)\\] Para calcular esta probabilidad, necesitamos conocer la función de probabilidad de \\(\\bar{X}\\). 10.7 Distribución media muestral Teorema: Si \\(X\\) sigue una distribución normal \\[X \\rightarrow N(\\mu, \\sigma^2)\\] entonces \\(\\bar{X}\\) es normal \\[\\bar{X} \\rightarrow N(\\mu, \\frac{\\sigma^2}{n})\\] y \\(\\bar{X}\\) tiene media \\[E(\\bar{X})=\\mu\\] decimos que \\(\\bar{X}\\) es insesgado porque su valor esperado es \\(\\mu\\) varianza \\[V(\\bar{X})=\\frac{\\sigma^2}{n}\\] Decimos que \\(\\bar{X}\\) es consistente porque tiende a cero cuando \\(n\\) es grande. La media y la varianza de \\(\\bar{X}\\) es por lo tanto un caso particular la ley de los grandes números, para distribuciones normales. Llamamos \\[se=\\sqrt{V(\\bar{X})}=\\sigma/\\sqrt{n}\\] al error estándar de la media muestral. El error estándar también se escribe como \\(\\sigma_{\\bar{x}}\\). Remarquemos aquí que este es el error que esperamos cuando usamos \\(\\bar{x}\\) como el valor de \\(\\mu\\), y es el sesgo que necesitábamos corregir cuando estmamos la varianza (Seccion 10.9). Entonces, si sabemos \\(\\mu\\) y \\(\\sigma\\), podemos calcular las probabilidades de \\(\\bar{X}\\) usando la distribución normal. Recuerda que tenemos dos funciones de probabilidad: La función de probabilidad de \\(X\\) también se conoce como la función de probabilidad de la población La función de probabilidad de \\(\\bar{X}\\) es una función de probabilidad de la muestra. Ejemplo (cables) Densidades de probabilidad para \\(X\\) y \\(\\bar{X}\\) En nuestro nuevo problema, ahora sabemos \\(\\mu\\) y \\(\\sigma\\) y la función de probabilidad de la población \\[X \\rightarrow N(\\mu=13, \\sigma^2=0.35^2)\\] Dado que \\(X\\) es normal, entonces \\(\\bar{X}\\) es normal y, por lo tanto, también conocemos la función de probabilidad de la media muestral \\(\\bar{X}\\) \\[\\bar{X} \\rightarrow N(13, \\frac{0.35^2}{8})\\] que tiene media y varianza \\(E(\\bar{X})=\\mu=13\\) \\(V(\\bar{X})=\\frac{\\sigma^2}{n}=\\frac{0.35^2}{8}=0.01530169\\) Finalmente queremos calcular la probabilidad de que nuestra estimación tenga un margen de error de \\(0.25\\). Esa es una distancia de \\(0.25\\) de la media. Eso es \\[P(-0.25 \\leq \\bar{X} - 13\\leq 0.25)=P(12.75 \\leq \\bar{X} \\leq 13.25)\\] \\(=F(13.25; \\mu, \\sigma^2/n)-F(12.75; \\mu, \\sigma^2/n)\\) En R podemos calcularlo como: pnorm(13.25, 13, 0.1237)-pnorm(12.75, 13, 0.1237)=0.956. Recuerda: \\(se=\\sigma_{\\bar{x}}=\\sqrt{0.01530169}=0.1237\\) Por lo tanto el \\(95.6\\%\\) de los promedios \\(\\bar{X}\\) de muestras aleatorias de tamaño \\(8\\) están a una distancia de \\(0.25\\) de la media \\(\\mu=13\\). Si vendemos nuestro proceso para construir los cables, podemos decirles a los nuevos fabricantes que cuando sigan nuestras instrucciones, pueden probar el proceso tomando una muestra de tamaño \\(8\\) cables. En ese caso, pueden esperar que el promedio de la muestra caiga entre \\((12.75, 13.25)\\) alrededor de \\(95\\%\\) de las veces. Cuando realizamos el muestreo aleatorio observamos: ## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747 Asumiendo que \\(\\mu=13\\) entonces nuestro error observado en la estimación de la media es la diferencia \\[\\bar{x}_{stock}-\\mu=13.21-13=0.21\\] El cual está dentro del margen de error de \\(95.6\\%\\) y por lo tanto, debemos considerar que el proceso de fabricación está funcionando como se espera dada la certificación. 10.8 Suma muestral Si estamos interesados en usar todos los \\(8\\) cables al mismo tiempo para transportar un total de \\(96\\) toneladas, entonces deberíamos considerar sumar sus contribuciones individuales. La suma muestral es la estadística \\[Y=n \\bar{X}=\\sum_{i=1}^n X_i\\] Una estadística es cualquier función de la muestra aleatoria \\((X_1, ... X_n)\\). Teorema: si \\(X\\) sigue una distribución normal \\[X \\rightarrow N(\\mu, \\sigma^2)\\] entonces \\(Y\\) es normal \\[Y \\rightarrow N(n\\mu, n\\sigma^2)\\] \\(Y\\) tiene media \\[E(Y)=n\\mu\\] varianza \\[V(Y)=n\\sigma^2\\] Ejemplo (suma de cables) ¿Cuál es la probabilidad de que cuando juntamos todos los cables, puedan llevar un peso total entre \\(102=8(13 - 0.25)\\) y \\(106=8(13+ 0.25)\\) Toneladas? Sabemos que para nuestros cables \\[X \\rightarrow N(\\mu=13, \\sigma^2=0.35^2)\\] entonces \\[Y \\rightarrow N(n\\mu=104, n\\sigma^2=8\\times 0.35^2)\\] con media y varianza \\(E(Y)=n\\mu=104\\) \\(V(Y)=n\\sigma^2=8\\times 0.35^2=0.98\\); \\(\\sqrt{V(Y)}=0.9899495\\) Queremos calcular \\[P(102 \\leq Y \\leq 106)\\] \\(=F(102; n\\mu, n\\sigma^2)-F(106; n\\mu, n\\sigma^2)\\) En R podemos calcularlo como: pnorm(106, 104, 0.9899495)-pnorm(102, 104, 0.9899495)=0.956. Por lo tanto, \\(95.6\\%\\) de los pesos totales que pueden cargar \\(8\\) cables están entre \\(102\\) y \\(106\\) toneladas, o a una distancia de \\(8*0.25=2\\) toneladas de la media total \\(n\\mu=104\\). 10.9 Variaza muestral Al estimar la varianza \\[s^2=\\hat{\\sigma}^2\\] También cometemos un error. ¿Cómo podemos estimar el error que cometemos? Definición La varianza muestral \\(S^2\\) de una muestra aleatoria de tamaño \\(n\\) \\[S^2= \\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})^2\\] es la dispersión de las medidas al rededor de \\(\\bar{X}\\). En nuestra muestra de tamaño \\(8\\), \\(S^2\\) tomó el valor \\[s_{stock}^2=\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar{x})^2=0.1275608\\] \\(S^2\\) es insesgada: \\(E(S^2)=V(X)=\\sigma^2\\) consistente: \\(n \\rightarrow \\infty\\), \\(V(\\bar{S^2}) \\rightarrow 0\\) y por lo tanto \\(S^2\\) estima consistentemente \\(\\sigma^2\\). Podemos tomar un valor de \\(s^2\\) como estimación para \\(\\sigma^2\\) o \\[s^2=\\hat{\\sigma}^2\\] De manera similar a \\(\\hat{\\mu}\\), el error de esta estimación se hace cada vez más pequeño a medida que \\(n\\) se hace cada vez más grande. La varianza muestral insesgada (¿por qué dividimos entre n-1?) Podríamos proponer estimar \\(\\sigma^2\\) dividiendo las diferencias cuadráticas de \\(\\bar{X}\\) por \\(n\\) \\[S_n^2=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar{X})^2\\] \\(S_n^2\\) es por lo tanto sesgada: \\(E(S_n^2) = \\sigma^2-\\frac{\\sigma^2}{n} \\neq \\sigma^2\\) pero consistente \\(V(S_n^2) \\rightarrow 0\\) cuando \\(n\\rightarrow \\infty\\) El término de sesgo \\(\\frac{\\sigma^2}{n}\\) surge porque \\(S_n^2\\) mide la dispersión al rededor de \\(\\bar{X}\\) y no al rededor de \\(\\mu\\). Recordemos que el error que cometemos cuando sustituimos \\(\\bar{x}\\) por \\(\\mu\\) es la varianza de \\(\\bar{X}\\): \\(\\sigma^2/n\\). Corrijamos el sesgo, escribiendo la ecuación 1 anterior como: \\[E(\\frac{n}{n-1}S_n^2)=\\sigma^2\\] Podemos definir la varianza muestral (corregida) \\[S^2=\\frac{n}{n-1}S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})^2\\] que es un estimador insesgado de \\(\\sigma^2\\) porque \\(E(S^2)=\\sigma^2\\). Ejemplo (control de calidad) Nos encotramos un problema de inferencia cuando estamos interesados en calcular probabilidades de la varianza muestral \\(S^2\\). Considera un proceso de control de calidad que requiera que los cables se produzcan cerca del valor especificado \\(\\mu\\). No queremos cables que se rompan demasiado lejos de la media. Si una muestra de tamaño \\(8\\) cables está muy dispersa (\\(S^2&gt;0.3\\)), detenemos la producción: el proceso está fuera de control. ¿Cuál es la probabilidad de que la varianza muestral de una muestra de tamaño \\(8\\) cables sea mayor que \\(0.3\\)? 10.10 Probabilidades de la varianza muestral Teorema: Si \\(X\\) sigue una distribución normal \\[X \\rightarrow N(\\mu, \\sigma^2)\\] La estadística: \\[W=\\frac{(n-1)S^2}{\\sigma^2} \\rightarrow \\chi^2(n-1)\\] tiene una distribución \\(\\chi^2\\) (chi-cuadrado) con \\(df=n-1\\) grados de libertad dada por \\[f(w)=C_n w^{\\frac{n-3}{2}} e^{-\\frac{w}{2}}\\] dónde: \\(C_n=\\frac{1}{2^{(n-1)/2\\sqrt{\\pi(n-1)}}}\\) asegura \\(\\int_{-\\infty}^{\\infty} f (t)dt=1\\) \\(\\Gamma(x)\\) es el factorial de Euler para números reales Si sabemos el valor de \\(\\sigma\\), podemos calcular las probabilidades de \\(S^2\\) usando la distribución \\(\\chi^2\\) para \\(W\\). 10.11 \\(\\chi^2\\)-estadística La densidad de probabilidad \\(\\chi^2\\) tiene un parámetro \\(df=n-1\\), llamado grados de libertad. Veamos algunas densidades de probabilidad en la familia de modelos de probabilidad \\(\\chi^2\\) Ejemplo (variaciones en la rotura del cable) Si sabemos que nuestros cables están certificados como \\[X \\rightarrow N(\\mu=13, \\sigma^2=0.35^2)\\] entonces \\[W=\\frac{(n-1)S^2}{\\sigma^2}= \\frac{7S^2}{0.35^2} \\rightarrow \\chi^2(n-1)\\] podemos calcular \\[P(S^2 &gt; 0.3)=P(\\frac{(n-1)S^2}{\\sigma^2} &gt; \\frac{(n-1)0.3}{\\sigma^2 })\\] \\(=P(W &gt; \\frac{7\\times0.3}{0.35^2})=P(W &gt; 17.14286)\\) \\(=1-P(W \\leq 17.14286)\\) \\(= 1- F_{\\chi^2,df=7}(17.14286)=0.016\\) En R 1-pchisq(17.14286, df=7)=0.016 Solo hay una probabilidad del \\(1\\%\\) de obtener un valor superior a \\(s^2=0.3\\). Por lo tanto, \\(s^2&gt;0.3\\) parece ser un buen criterio para detener la producción y revisar el proceso. Imaginamos ahora que la muetra de \\(8\\) cables fue usada como una muestra de control de calidad ## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747 Por lo tanto, nuestro valor observado fue \\(s^2_{stock}=0.1275608\\) La muestra no está muy dispersa porque \\(s^2_{stock} &lt; 0.3\\) y creemos que todo está bien y por lo tanto la producción está bajo control. 10.12 Preguntas 1) La media muestral es un estimador insesgado de la media poblacional porque \\(\\qquad\\)a: El valor esperado de la media muestral es la media poblacional; \\(\\qquad\\)b: El valor esperado de la media poblacional es la media muestral; \\(\\qquad\\)c: El error estándar tiende a cero cuando \\(n\\) tiende a infinito; \\(\\qquad\\)d: La varianza de la media muestral tiende a cero cuando \\(n\\) tiende a infinito; 2) ¿Por qué se usa la estadística \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i -\\bar{X})^2\\) en su lugar de \\(S_n^2=\\frac{1}{n}\\sum_{i=1}^{n}(X_i -\\bar{X})^2\\) para estimar la varianza de una variable aleatoria? \\(\\qquad\\)a: porque su varianza es \\(0\\); \\(\\qquad\\)b: porque es un estimador consistente de \\(\\sigma^2\\); \\(\\qquad\\)c: porque es un estimador insesgado de \\(\\sigma^2\\); \\(\\qquad\\)d: porque es la distancia cuadrática promedio a la media muestral (\\(\\bar{X}\\)); 3) ¿Cuál es la varianza de la media muestral \\(\\bar{X}=\\frac{1}{n}\\sum_{i=1}^n X_i\\)? \\(\\qquad\\)a:\\(\\sigma\\); \\(\\qquad\\)b:\\(\\frac{\\sigma}{\\sqrt{n}}\\); \\(\\qquad\\)c:\\(\\sigma^2\\); \\(\\qquad\\)d:\\(\\frac{\\sigma^2}{n}\\); 4) ¿Cuál es la media y la varianza de la suma muestral? \\(\\qquad\\)a:\\(\\mu\\), \\(n\\sigma\\); \\(\\qquad\\)b:\\(n\\mu\\),\\(n\\sigma\\); \\(\\qquad\\)c:\\(\\mu\\), \\(n\\sigma^2\\); \\(\\qquad\\)d:\\(n\\mu\\), \\(n\\sigma^2\\); 5) Una pregunta de inferencia implica: \\(\\qquad\\)a: calcular el valor esperado de un estimador; \\(\\qquad\\)b: estimar el valor de un parámetro; \\(\\qquad\\)c: calcular una probabilidad de un estimador; \\(\\qquad\\)d: ajustar un modelo de probabilidad; 10.13 Ejercicios 10.13.0.1 Ejercicio 1 Una empresa de electrónica fabrica resistencias que tienen una resistencia media de 100 ohmios y una desviación estándar de 10 ohmios. La distribución de la resistencia es normal. ¿Cuál es el valor esperado de la media muestral de \\(n=25\\) resistencias? (R:100) ¿Cuál es la varianza de la media muestral de \\(n=25\\) resistencias? (R:4) ¿Cuál es el error estándar de la media muestral de \\(n=25\\) resistencias? (R:2) Encuentra la probabilidad que una muestra aleatoria de \\(n = 25\\) resistencias tenga una resistencia promedio de menos de \\(95\\) ohmios (R: 0.0062) 10.13.0.2 Ejercicio 2 Un modelo de batería carga una media de \\(75\\%\\) de su capacidad en una hora con una desviación estándar de \\(15\\%\\). Si la carga de la batería es una variable normal, ¿cuál es la probabilidad de que la diferencia de carga entre la media muestral de \\(25\\) baterías y la carga media sea como mucho de \\(5\\%\\)? (R:0.9044) Si cargamos \\(100\\) baterías, ¿cuál es esa probabilidad? (R:0.9991) Si en cambio solo cargamos \\(9\\) baterías, ¿qué carga \\(c\\) es superada por la media muestral con probabilidad de \\(0.015\\)? (R:85.850) "],["teorema-central-del-límite.html", "Chapter 11 Teorema central del límite 11.1 Objetivo 11.2 Margen de error 11.3 Ejemplo (cables) 11.4 Teorema central del límite 11.5 Suma muestral y CLT 11.6 Preguntas 11.7 Ejercicios", " Chapter 11 Teorema central del límite 11.1 Objetivo En este capítulo introduciremos el margen de errores al estimar la media de la distribución de la población por el promedio. Discutiremos cómo el Teorema central del límite nos permitirá calcular el margen de error para cualquier tipo de distribución si la muestra es grande. También introducirá la estadística t, para calcular el margen de error cuando la muestra es pequeña pero la distribución de la población es normal. 11.2 Margen de error Al decidir si el error de estimación de \\(\\mu\\) por la media muestral \\(\\bar{x}\\) es grande o no, generalmente lo comparamos con una tolerancia predefinida. El margen de error a nivel de \\(5\\%\\) es la distancia \\(m\\) de \\(\\bar{X}\\) de \\(\\mu\\) que captura \\(95\\%\\) de las estimaciones: \\[P(-m \\leq \\bar{X}-\\mu \\leq m)=P(\\mu-m \\leq \\bar{X} \\leq\\mu + m)=0.95\\] Esto significa que \\(95\\%\\) de los posibles resultados de \\(\\bar{X}\\) están a una distancia \\(m\\) de \\(\\mu\\) 11.3 Ejemplo (cables) Si tomamos una muestra de cables de \\(8\\) de una población de cables cuya carga de ruptura sigue una distribución normal con parámetros conocidos \\(\\mu=13\\) y \\(\\sigma^2=0.35^2\\), \\[X \\rightarrow N(\\mu=13, \\sigma^2=0.35^2)\\] ¿Cuál es el margen de error cuando estimamos \\(\\mu\\) por \\(\\bar{x}\\)? Calculando el maging de error de una variable normal Queremos saber el número \\(m\\) en la ecuación \\[P(\\mu-m \\leq \\bar{X} \\leq\\mu + m)=0.95\\] Para resolver esta ecuación necesitamos dos pasos. Primero, necesitamos saber la distribución de \\(\\bar{X}\\). Cuando \\(X\\) es normal \\(X \\rightarrow N(\\mu, \\sigma^2)\\) entonces \\[\\bar{X} \\rightarrow N(\\mu, \\frac{\\sigma^2}{n})\\] Entonces necesitamos estandarizar \\(\\bar{X}\\). Recuerda que para estandarizar una variable normal restamos su media y la dividimos por su desviación estándar. \\[Z=\\frac{\\bar{X}-E(\\bar{X})}{\\sqrt{V(\\bar{X})}} =\\frac{\\bar{X}-\\mu}{ \\frac{\\sigma}{\\sqrt{n}}} \\rightarrow N(0,1)\\] Sustituyendo la media de \\(\\bar{X}\\) y su desviación estándar en la ecuación del margen de error, tenemos: \\(P(\\mu-m \\leq \\bar{X} \\leq\\mu + m)=P(-\\frac{m}{\\sigma/\\sqrt{n}} \\leq \\frac{\\bar{X} -\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\leq\\frac{m}{\\sigma/\\sqrt{n}})\\) \\[=P(-\\frac{m}{\\sigma/\\sqrt{n}} \\leq Z \\leq\\frac{m}{\\sigma/\\sqrt{n}})=0.95\\] Compáralo con el gráfico anterior: \\(\\frac{m}{\\sigma/\\sqrt{n}}\\) es la distancia alrededor de \\(0\\) que captura \\(95\\%\\) de la variable normal estándar de distribución. La distancia deja una probabilidad de \\(2.5\\%\\) en cada extremo de la distribución. Para la parte superior de la cola, esto es \\[\\frac{m}{\\sigma/\\sqrt{n}}=\\phi^{-1}(0.975)=1.96\\] donde \\(\\phi^{-1}\\) es la inversa de la distribución normal estándar (qnorm(0.975)). Por lo tanto \\[m=1.96 \\frac{\\sigma}{\\sqrt{n}}\\] Ejemplo (cables) La media muestral \\(\\bar{X}\\) de una muestra de cables de \\(8\\) sigue una distribución normal con: media \\(E(\\bar{X})=\\mu\\) y error estándar \\(se=\\sqrt{V(\\bar{X})}=\\frac{\\sigma}{\\sqrt{n}}=\\frac{0.35}{\\sqrt{8}}\\) Entonces el margen de error en \\(5\\%\\) es: \\[m=1.96\\frac{0.35}{\\sqrt{8}}=0.24\\] Podemos esperar que \\(95\\%\\) de los promedios (\\(\\bar{x}\\)) para la carga de rotura de los cables de \\(8\\) caigan entre \\((13-0.24, 13+0.24)=(12.76, 13.24)\\) 11.4 Teorema central del límite Pudimos resolver el margen de error porque asumimos que esa variable \\(X\\) era normal. ¿Qué pasa si \\(X\\) sigue cualquier otra distribución de probabilidad? Teorema: Para cualquier variable aleatoria \\(X\\) con cualquier tipo de distribución \\[X \\rightarrow f(x; \\theta)\\] la estadística estandarizada \\[Z=\\frac{\\bar{X}-E(\\bar{X})}{\\sqrt{V(\\bar{X})}}\\] se aproxima a una distribución estándar \\[Z \\rightarrow_d N(0,1)\\] cuando \\(n\\rightarrow \\infty\\) Consecuencia: Podemos calcular las probabilidades de \\(\\bar{X}\\) si \\(n\\) es grande, usando la distribución normal: \\[\\bar{X} \\sim_{aprox} N(\\mu, \\frac{\\sigma^2}{n})\\] Ejemplo (fármaco en concentración sanguínea): Considera un experimento en el que queremos medir la concentración en sangre de un fármaco después de \\(10\\) horas de administración en \\(30\\) pacientes. Si sabemos que los niveles siguen una distribución exponencial \\[X \\rightarrow exp(\\lambda=2)\\] La media y la varianza son: \\(\\mu=\\frac{1}{\\lambda}=0.5\\) \\(\\sigma^2=\\frac{1}{\\lambda^2}=0.25\\) Por lo tanto, la media y el error estándar de \\(\\bar{X}\\) son: \\(E(\\bar{X})=\\frac{1}{\\lambda}=0.5\\) \\(se=\\sqrt{\\frac{\\sigma^2}{n}}=\\sqrt{\\frac{1}{n\\lambda^2}}=0.091\\) Como \\(n \\geq 30\\) \\[Z=\\frac{\\bar{X}-\\frac{1}{\\lambda}}{\\sqrt{\\frac{1}{n\\lambda^2}}}\\] es una variable normal estándar y: \\(\\bar{X} \\sim_{aprox} N(\\frac{1}{\\lambda}, \\frac{1}{n\\lambda^2})\\) El margen de error al nivel de \\(5\\%\\) se puede calcular de nuevo con la distribución estándar \\[m=\\phi^{-1}(0.975) \\frac{\\sigma}{\\sqrt{n}}=1.96\\frac{0.25}{\\sqrt{30}}=0.1789227\\] Podemos esperar que \\(95\\%\\) de los promedios de muestras de pacientes de \\(30\\) caigan entre \\((0.5-0.178, 0.5+0.178)= (0.322, 0.678)\\) 11.5 Suma muestral y CLT La suma muestral es la estadística \\[Y=X_1+X_2+...X_n=\\sum_{i=1}^n X_i=n \\bar{X}\\] con media \\[E(Y)=nE(\\bar{X})=n\\mu\\] variancia \\[V(Y)=n^2V(\\bar{X})=n\\sigma^2\\] El TCL nos dice que para cualquier variable aleatoria \\(X\\) con distribución desconocida (cualquier tipo de) \\[X \\rightarrow f(x; \\theta)\\] la estadística estandarizada \\[Z=\\frac{\\bar{X}-E(\\bar{X})}{\\sqrt{V(\\bar{X})}}\\] se aproxima a una distribución estándar \\[Z \\rightarrow_d N(0,1)\\] cuando \\(n\\rightarrow \\infty\\). \\(Z\\) tembién se puede escribir como \\[Z=\\frac{n\\bar{X}-nE(\\bar{X})}{\\sqrt{n^2V(\\bar{X})}}=\\frac{Y-E(Y)}{\\sqrt{V(Y)}}=\\frac{Y-n\\mu}{\\sqrt{n}\\sigma}\\] Consecuencia: Podemos calcular probabilidades para la suma muestral \\(Y=n\\bar{X}\\) si \\(n\\) es grande, usando la distribución normal: \\[Y \\sim_{aprox} N(n\\mu, n\\sigma^2)\\] Ejemplo (Ensayo de Bernoulli) Para el ensayo de Bernoulli \\(X \\rightarrow Bernoulli(p)\\), el promedio de una muestra \\(n\\) de ensayos de Bernoulli es \\(\\bar{X}=\\sum_i^n X_i\\). Por lo tanto \\[Z=\\frac{\\bar{X}-p}{\\sqrt{\\frac{p(1-p)}{n}}}\\] es normal estándar, porque \\(E(\\bar{X})=p\\) y \\(V(\\bar{X})=\\frac{p(1-p)}{n}\\). Ahora, la suma muestral \\(Y=n\\bar{X}\\) es una variable aleatoria que cuenta el número de eventos con probabilidad \\(p\\) en una repetición de \\(n\\) intentos, por lo tanto \\[Y \\rightarrow Binom(n, p)\\] con media \\(E(Y)=np\\) y viarianza \\(V(Y)=np(1-p)\\). Como podemos escribir \\[Z=\\frac{Y-np}{\\sqrt{np(1-p)}}\\] entonces usando el TCL, podemos aproximar la función de masa de probabilidad binomial con la densidad de probabilidad normal cuando \\(n\\) es grande \\[Y \\rightarrow Binom(n, p) \\sim_{aprox} N(np, np(1-p))\\] Esta aproximación es buena cuando tanto \\(np\\) como \\(n(1-p)\\) son mayores que \\(5\\). 11.6 Preguntas 1) La importancia del Teorema central del límite es que se aplica a la estandarización de \\(\\qquad\\)a: Una variable aleatoria; \\(\\qquad\\)b: La media muestral de una variable normal; \\(\\qquad\\)c: La media muestral de una variable aleatoria; \\(\\qquad\\)d: Una variable normal; 2) Cuando \\(n&gt;30\\), la media muestral de una variable aleatoria (\\(\\frac{1}{n}\\sum_i Xi\\)) y su suma muestral (\\(\\sum_i Xi\\)) se pueden aproximar a \\(\\qquad\\)a: \\(N(\\frac{\\mu}{n}, \\frac{\\sigma^2}{n})\\) y \\(N(\\mu, \\sigma^2)\\); \\(\\qquad\\)b: \\(N(\\mu, n\\sigma^2)\\) y \\(N(\\mu, \\frac{\\sigma^2}{n})\\); \\(\\qquad\\)c: \\(N(\\mu, \\frac{\\sigma^2}{n})\\) y \\(N(\\mu, \\frac{\\sigma^2}{n})\\); \\(\\qquad\\)d: \\(N(\\mu, \\frac{\\sigma^2}{n})\\) y \\(N(n\\mu, n\\sigma^2)\\) 3) Para una variable normal estándar, si ponemos el número \\(z_{0.025}\\) en la definición del margen de error \\(m=z_{0.025} \\frac{\\sigma}{\\sqrt{n}}\\), entonces se referirá a \\(\\qquad\\)a: El primer cuartil; \\(\\qquad\\)b: El número en el que la distribución ha acumulado \\(0.975\\) de probabilidad; \\(\\qquad\\)c: El número en el que la distribución ha acumulado una probabilidad de \\(0.025\\); \\(\\qquad\\)d: El tercer cuartil; 4) La probabilidad de que la media muestral de \\(50\\) observaciones esté una distancia de un error estandar (\\(se=\\frac{\\sigma}{\\sqrt{n}}\\)) de de la media poblacional \\(\\mu\\) es: \\(\\qquad\\)a:2*(1-pnorm(1))= 0.3173105; \\(\\qquad\\)c:2*(1-pnorm(2))=0.04550026; \\(\\qquad\\)b:-1+2*pnorm(1)=0.6826895; \\(\\qquad\\)d:-1+2*pnorm(2)=0.9544997 5) Una resonancia magnética del hipocampo del cerebro tiene \\(100\\) píxeles. Esperamos que \\(90\\%\\) de los píxeles sean blancos (tejido cerebral). Según el Teorema central del límite , ¿cuál es la probabilidad de que el escaneo de un paciente tenga como máximo \\(85\\%\\) de píxeles blancos? \\(\\qquad\\)a:pnorm(0.9, 0.85, sqrt(0.85*0.15)/10); \\(\\qquad\\)b:dnorm(0.85, 0.9, sqrt(0.9*0.1)/10); \\(\\qquad\\)c:pnorm(0.85, 0.9, sqrt(0.9*0.1)/10); \\(\\qquad\\)d:dnorm(0.9, 0.85, sqrt(0.85*0.15)/10) 11.7 Ejercicios 11.7.0.1 Ejercicio 1 Se necesita un componente electrónico para el correcto funcionamiento de un telescopio. Necesita ser reemplazado inmediatamente cuando se desgasta. La vida media del componente (\\(\\mu\\)) es de \\(100\\) horas y su desviación estándar \\(\\sigma\\) es de \\(30\\) horas. ¿Cuál es la probabilidad de que el promedio de la vida media de \\(50\\) componentes esté dentro de \\(1\\) hora de la vida media de un solo componente? (R:0.1863) ¿Cuántos componentes necesitamos para que el telescopio esté operativo \\(2750\\) horas consecutivas con al menos \\(0.95\\) de probabilidad? (R:31) 11.7.0.2 Ejercicio 2 La probabilidad de que se encuentre una mutación particular en la población es de \\(0.4\\). Si testamos \\(2000\\) personas por la mutación: ¿Cuál es la probabilidad de que el número total de personas con la mutación esté entre \\(791\\) y \\(809\\)? (R:0.31) sugerencia: usa el CLT con una muestra de ensayos de Bernoulli de \\(2000\\). Esto se conoce como la aproximación normal de la distribución binomial que es buena cuando \\(p\\) y \\(1-p\\) son ambis mayor que \\(5\\). 11.7.0.3 Ejercicio 3 Una máquina automática llena tubos de ensayo con muestras biológicas con una media de \\(\\mu=130\\)mg y una desviación estándar de \\(\\sigma=5\\)mg. para una muestra aleatoria de tamaño \\(50\\). ¿Cuál es la probabilidad de que la media muestral (promedio) esté entre \\(128\\)mg y \\(132\\)mg? (R:0.995) ¿Cuál debería ser el tamaño de la muestra (\\(n\\)) tal que la media muestral \\(\\bar{X}\\) sea superior a \\(131\\)mg con una probabilidad menor o igual a \\(0.025\\)?(R:97) 11.7.0.4 Ejercicio 4 En el Caribe, parece haber un promedio de huracanes de \\(6\\) por año. Teniendo en cuenta que la formación de huracanes es un proceso de Poisson, los meteorólogos planean estimar el tiempo medio entre la formación de dos huracanes. Planean recolectar una muestra de tamaño \\(36\\) para los tiempos entre dos huracanes. ¿Cuál es la probabilidad de que su promedio muestral esté entre \\(50\\) y \\(60\\) días? (R:0.385975) ¿Cuál debe ser el tamaño de la muestra para que tengan una probabilidad de \\(0.025\\) de que la media muestral sea mayor a \\(70\\) días? (R:169) "],["máxima-verosimilitud-y-método-de-los-momentos.html", "Chapter 12 Máxima verosimilitud y Método de los Momentos 12.1 Objetivo 12.2 Estadística 12.3 Propiedades 12.4 Máxima verosimilitud 12.5 Máxima verosimilitud 12.6 Método de los Momentos 12.7 Método de Momentos para varios parámetros 12.8 Preguntas 12.9 Ejercicios 12.10 Método de los momentos", " Chapter 12 Máxima verosimilitud y Método de los Momentos 12.1 Objetivo En este capítulo discutiremos qué es un estimador y daremos algunos ejemplos. Luego introduciremos dos métodos para obtener estimadores de los parámetros de los modelos de probabilidad. Estos son la máxima verosimilitud y el método de los momentos. 12.2 Estadística Definición Una estadística es cualquier función de una muestra aleatoria \\[T(X_1,X_2, ..., X_n)\\] Por lo general, devuelve un número. Las estadísticas son variables aleatorias y sus distribuciones de probabilidad se llaman distribuciones de muestreo Las estadísticas tienen diferentes funciones: Descripción de los datos de una muestra ubicación: \\(\\bar{X}\\) Mínimo: \\(\\min\\{X_i\\}\\) Máximo: \\(\\max\\{X_i\\}\\) Estimación de los parámetros de un modelo de probabilidad media: \\(\\bar{X}\\) para \\(\\mu\\) varianza: \\(S^2\\) para \\(\\sigma^2\\) Inferencia para decir algo sobre los parámetros dados los datos media: \\(Z\\), \\(T\\) varianza: \\(\\chi^2\\) Recuerda: Todas son variables aleatorias. Cada vez que tomamos otra muestra cambian su valor. Definición de estimadores Un estimador es un estadístico cuyos valores observados se utilizan para estimar los parámetros de la distribución de la población sobre la que se define la muestra. Si escribimos la distribución de la población como \\[X \\rightarrow f(x; \\theta)\\] entonces \\(\\theta\\) es un parámetro y \\(\\Theta\\) es una variable aleatoria cuyas observaciones \\(\\hat{\\theta}\\) tomamos como estimaciones de \\(\\theta\\) \\[\\hat{\\theta} \\sim \\theta\\] Por lo tanto hay tres cantidades diferentes que debemos considerar: \\(\\theta\\) es un parámetro de la distribución de la población \\(f(x; \\theta)\\) \\(\\Theta\\) es un estimador de \\(\\theta\\): Una variable aleatoria \\(\\hat{\\theta}\\) es la estimación de \\(\\theta\\): un valor realizado de \\(\\Theta\\) Ejemplo (media de la muestra) Cuando tenemos una variable aleatoria normal \\[X \\rightarrow N(\\mu, \\sigma^2)\\] identificamos las tres cantidades diferentes: \\(\\mu\\) es un parámetro de la distribución de la población: \\(N(\\mu, \\sigma^2)\\) \\(\\bar{X}=\\frac{1}{n} \\sum_{i=1}^n X_i\\) es un estimador de \\(\\mu\\) \\(\\bar{x}=\\hat{\\mu}\\) es la estimación de \\(\\mu\\) Ejemplo (varianza de la muestra) Cuando tenemos una variable aleatoria normal \\[X \\rightarrow N(\\mu, \\sigma^2)\\] \\(\\sigma^2\\) es un parámetro de la distribución de la población \\(S^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar{X})^2\\) es un estimador de \\(\\sigma^2\\) \\(s^2=\\hat{\\sigma}^2\\) es la estimación de \\(\\sigma^2\\) 12.3 Propiedades Un estimador es insesgado si su valor esperado es el parámetro \\[E(\\Theta)=\\theta\\] Por ejemplo: \\(\\bar{X}\\) es un estimador insesgado de \\(\\mu\\) porque \\(E(\\bar{X})=\\mu\\) \\(S^2\\) es un estimador insesgado de \\(\\sigma^2\\) porque \\(E(S^2)=\\sigma^2\\) Un estimador es consistente cuando sus valores observados están cada vez menos dispersos al rededor de la medida a medida que el tamaño de la muestra crece \\[lim_{n\\rightarrow \\infty} V(\\Theta) = 0\\] Por ejemplo: \\(\\bar{X}\\) es consistente porque \\(V(\\bar{X})=\\frac{\\sigma^2}{n}\\rightarrow 0\\) cuando \\(n \\rightarrow \\infty\\). 12.4 Máxima verosimilitud ¿Cómo se pueden obtener estimadores de los parámetros de cualquier modelo de probabilidad? Ejemplo (Láser) Imagina que diseñamos un láser con un diámetro de \\(1 mm\\) que queremos usar para aplicaciones clínicas. Queremos caracterizar el diámetro de un piercing en un tejido realizado con láser y tomar una muestra aleatoria de \\(30\\) cortes realizados con láser. Aquí están los resultados ## [1] 1.11 1.64 1.20 1.79 1.89 1.01 1.31 1.81 1.34 1.25 1.92 1.24 1.49 1.36 1.03 ## [16] 1.82 1.09 1.01 1.14 1.91 1.80 1.51 1.44 1.98 1.46 1.53 1.33 1.39 1.12 1.04 y el histograma ¿Cuál sería una función de probabilidad que podría describir los datos? Para ello seguimos el siguiente proceso: Proponemos un modelo que depende de parámetros Derivamos los estimadores para los parámetros, por máxima verosimilitud o el método de momentos. Finalmente usamos el estimador para estimar los parámetros con los datos. Proponiendo una densidad de probabilidad En muchas aplicaciones, podemos proponer la forma de una densidad de probabilidad que depende de algunos parámetros. Proponer un modelo de probabilidad se hace siguiendo propiedades generales de las observaciones, o lo que esperamos observar. El modelado requiere experiencia, habilidad y conocimiento de varias funciones matemáticas. Sin embargo, en la mayoría de los casos se suelen aplicar modelos bien conocidos. Ejemplo (Láser) En nuestro ejemplo, podemos considerar, por ejemplo, que se debe dar la máxima probabilidad a los diámetros de \\(x=1 mm\\), y que los diámetros deben disminuir como la potencia inversa de algún parámetro desconocido \\(\\alpha\\), con un límite de \\(2mm\\) más allá del cual la probabilidad es de \\(0\\). Una distribución de densidad de probabilidad adecuada es \\[ f(x)= \\begin{cases} \\frac{1}{\\alpha}(x-1)^{\\frac{1}{\\alpha}-1},&amp; \\text{if } x \\in (1,2)\\\\ 0,&amp; x \\notin (1,2)\\\\ \\end{cases} \\] Donde \\(\\alpha\\) es un parámetro. Esta es una densidad de probabilidad porque se integra a uno y es positiva. En particular, para \\(\\alpha=2\\) podemos graficarlo Derivar los estimadores Si realizamos una muestra de tamaño \\(n\\): \\((X_1,...X_n)\\), ¿Cómo debemos combinar los datos para obtener el mejor valor de \\(\\alpha\\)? Muchos valores de para el parámetro podrían explicar los datos. Nos interesa un criterio para elegir un valor en particular. El método de máxima verosimilitud nos da un estimador para \\(\\alpha\\) \\[\\hat{\\alpha}_{ml}\\] 12.5 Máxima verosimilitud El objetivo es encontrar el valor del parámetro que creemos es el que mejor representa los datos. El método de máxima verosimilitud se basa en la búsqueda del valor del parámetro que hace más probable la observación de la muestra. Máxima verosimilitud paso 1 Calculamos la probabilidad de haber observado la muestra \\(n\\): \\(x_1,...x_n\\). Es el producto de probabilidades porque las observaciones son independientes entre sí: \\(P(M=x_1,...x_n)=P(X=x_1)P(X=x_2)...P(X=x_n)\\) \\[=f(x_1;\\alpha)f(x_2;\\alpha) ...f(x_n;\\alpha)\\] A esta función la llamamos función de verosimilitud y consideramos que: Una vez observados los datos estos son fijos La incógnita es \\(\\alpha\\) \\[L(\\alpha)= \\Pi_{i=1..n} f(x_i; \\alpha)\\] Ejemplo (Láser) Para el experimento con láser la función de verosimilitud es \\(L(\\alpha;x_1,..x_n)= \\frac{1}{\\alpha^n} \\Pi_{i=1..n} (x_i-1)^{\\frac{1-\\alpha}{ \\alpha}}= \\frac{1}{\\alpha^n} \\{(x_1-1)(x_2-1)...(x_n-1)\\}^{\\frac{1-\\alpha}{\\alpha}}\\) Máxima verosimilitud paso 2 Entonces nos preguntamos: ¿cuál es el valor de \\(\\alpha\\) que hace que la muestra observada sea el evento más probable? Por lo tanto, queremos maximizar \\(L(\\alpha)\\) con respecto a \\(\\alpha\\). Como tenemos la multiplicación de muchos factores es más fácil maximizar el logaritmo de \\(L(\\alpha)\\). Esto se llama la función logaritmo de verosimilitud: \\[\\ln L(\\alpha;x_1,..x_n)\\] Ejemplo (Láser) En el ejemplo del láser, tomamos el logaritmo y obtenemos la Logaritmo de verosimilitud \\[\\ln L(\\alpha;x_1,..x_n)= -n \\ln(\\alpha) + {\\frac{1-\\alpha}{\\alpha}} \\Sigma_{i=1...n} \\ln (x_i-1)\\] Máxima verosimilitud paso 3 Finalmente maximizamos el logaritmo de verosimilitud con respecto al parámetro. Por lo tanto, diferenciamos el log-verosimilitud con respecto al parámetro \\(\\alpha\\), igualamos a cero y resolvemos para el máximo. \\[\\frac{d \\ln L(\\alpha)}{d \\alpha} \\big|_{\\hat{\\alpha}}=0 \\] El valor máximo del parámetro se denomina estimación de máxima verosimilitud para el parámetro y se escribe con un sombrero \\(\\hat{\\alpha}\\). Ejemplo (Láser) Derivamos la función log-verosimilitud \\[\\frac{d \\ln L(\\alpha)}{d \\alpha}= -\\frac{n}{\\alpha} - \\frac{1}{\\alpha^2} \\Sigma_{i=1.. .n} \\ln (x_i-1)\\] El máximo es donde la derivada es \\(0\\). Este máximo es el valor de nuestro estimador \\(\\hat{\\alpha}_{ml}\\). \\[\\hat{\\alpha}_{ml}=-\\frac{1}{n}\\sum_{i=1}^n \\ln (x_i-1)\\] El estimador del parámetro es por lo tanto (nótese las letras mayúsculas) \\[A=-\\frac{1}{n}\\sum_{i=1}^n \\ln (X_i-1)\\] Que es una variable aleatoria, función de la muestra aleatoria \\[(X_1, X_2, ... X_n)\\] Estimando los parámetros con los datos En nuestro ejemplo, tenemos entonces la observación de la muestra aleatoria como un conjunto de 30 números \\((x_1, x_2, ...x_{30})\\), por lo tanto sustituimos los números en el estimador y esto nos dará su valor observado. \\(\\hat{\\alpha}_{ml}=-\\frac{1}{30}\\{ \\ln (1.11-1)+ \\ln (1.64-1)+...\\ln (1.04-1)\\}=1.320\\) Por lo tanto, la estimación de máxima verosimilitud del parámetro es \\(1.320\\). Si sustituimos este valor en la función de probabilidad y lo superponemos con el histograma, podemos ver que nos da una descripción adecuada de los datos. Veamos la función del logaritmo de la verosimilitud para nuestros \\(30\\) cortes láser. Recuerda, los datos son fijos para nuestro experimento y \\(\\alpha\\) varía. La función tiene un máximo. Sin embargo, si tomamos otra muestra, esta función cambia y también su lo hará su máximo. Máxima verosimilitud: Historia Para inferir la verdadera posición de Ceres en un momento dado, Gauss derivó la función de error \\[f(x; \\mu, \\sigma^2)= \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2\\sigma^2} (x- \\mu)^2}\\] Donde la posición verdadera de Ceres era la media \\(\\mu\\). ¿Cómo podemos combinar los datos para tener la mejor estimación de la posición de Ceres? ¿Cuál es la estadística que mejor puede describir su posición? Esta pregunta se puede formular como: ¿Cuál es la estimación de máxima verosimilitud de \\(\\mu\\) para una variable aleatoria normal? Máxima verosimilitud de la distribución normal Para una variable aleatoria normal \\[X \\rightarrow N(\\mu, \\sigma^2)\\]. ¿Cuáles son los estimadores de \\(\\mu\\) y \\(\\sigma^2\\) que maximizan la probabilidad de los datos observados? Seguimos el método de máxima verosimilitud: La función de verosimilitud, o la probabilidad de haber observado la muestra \\((x_1, ....x_n)\\) es \\(L(\\mu, \\sigma^2)=\\Pi_{i=1..n} f(x_i;\\mu,\\sigma)\\) \\[=\\big( \\frac{1}{\\sigma \\sqrt{2 \\pi}}\\big)^ne^{-\\frac{1}{2\\sigma^2} \\sum_i(x_i-\\mu) ^2}\\] Tomamos el logaritmo de \\(L\\) y calculamos la función log-verosimilitud \\[\\ln L(\\mu, \\sigma^2)=-n \\ln(\\sigma \\sqrt{2 \\pi})-\\frac{1}{2\\sigma^2} \\Sigma_i(x_i-\\mu )^2\\] Las estimaciones de \\(\\mu\\) y \\(\\sigma^2\\) son donde la probabilidad es máxima. Dan la probabilidad más alta para los datos de la muestra. Diferenciamos con respecto a \\(\\mu\\) y \\(\\sigma^2\\). Estas dos derivadas nos dan dos ecuaciones, una para cada uno de los parámetros. Para derivar con respecto a \\(\\sigma^2\\), es más fácil hacer una sustitución \\(t=\\sigma^2\\). \\(\\frac{d \\ln L(\\mu, \\sigma^2)}{d\\mu}=\\frac{1}{\\sigma^2} \\sum_i(x_i-\\mu)\\) \\(\\frac{d \\ln L(\\mu, \\sigma^2)}{d\\sigma^2}=-\\frac{n}{2 \\sigma^2}+\\frac{1}{2\\sigma^4} \\sum_i(x_i-\\mu)^2\\) Las derivadas son \\(0\\) en el máximo \\(\\frac{1}{\\hat{\\sigma}^2} \\sum_i(x_i-\\hat{\\mu})=0\\) \\(-\\frac{n}{2 \\hat{\\sigma}^2}+\\frac{1}{2\\hat{\\sigma}^4} \\sum_i(x_i-\\hat{\\mu})^ 2=0\\) resolviendo ambas ecuaciones para los parámetros, encontramos para \\(\\mu\\) \\[\\hat{\\mu}_{ml}=\\frac{1}{n}\\sum_i x_i=\\bar{x}\\] y para \\(\\sigma^2\\) \\[\\hat{\\sigma}^2_{ml}=\\frac{1}{n}\\sum_i(x_i-\\bar{x})^2\\] Por lo tanto la media mestral o promedio \\(\\bar{X}\\) es el estimador de máxima verosimilitud de la media \\(\\mu\\) de la población. Gauss demostró que la estadística en la que más deberíamos confiar (la que tienen la mayor verosimilitud) para la posición real de Ceres era el promedio. Gauss, al resolvier la posición de Ceres, no solo descubrió la distribución normal, sino que también creó el análisis de regresión y mostró la importancia del promedio. Es debido a él que usamos el promedio para muchas cosas, y no otro tipo de estadísticas. Además, el estimador de máxima verosimilitud de \\(\\sigma^2\\) es un estimador sesgado porque se puede demostrar que \\[E(\\hat{\\sigma}^2_{ml})=\\sigma^2+ \\frac{\\sigma^2}{n}\\neq\\sigma^2\\] Fue Fisher quien demostró que a pesar de ser sesgado este estimador es importante, ya que lo usó para generalizar el teorema del límite central, donde pierde su sesgo en \\(n\\rightarrow \\infty\\). 12.6 Método de los Momentos El método de máxima verosimilitud tiene como objetivo producir los estimadores de distribuciones de probabilidad a partir de datos. Sin embargo, existe otra forma de producir esos estimadores, que se basa en la idea frecuentista de las probabilidades. Hbíamos visto que las frecuancias relativas tienden a las probabilidades cuando \\(n\\) es grande \\(f_i \\rightarrow f(x_i)\\), y como consequencia \\[\\bar{x} \\rightarrow \\mu\\] El centro de gravedad de los datos tiende al centro de gravedad de la probabilidad. El método de los momentos dice que podemos tomar el valor observado de la media meustral \\(\\bar{X}\\) como estimador de \\(E(X)=\\mu\\) \\[E(X)\\sim \\bar{x}=\\hat{\\mu}\\] Es decir que la variable aleatoria que estima la media de la población es el promedio: \\[\\bar{X}= \\frac{1}{n}\\sum_i X_i\\] que también se denomina el primer momento de muestra En general, si \\(X \\rightarrow f(x, \\theta)\\) el estimador del parámetro \\(\\theta\\) se obtiene entonces de la ecuación: \\[E(X; \\hat{\\theta})=\\bar{x}\\] debido a que el valor esperado de la variable aletoria siempre es función del parámetro \\(\\theta\\). Ejemplo (exponencial) Si una variable aleatoria sigue una distribución exponencial \\[X \\hookrightarrow exp(\\lambda)\\] entonces podemos usar el método de los momentos para estimar \\(\\lambda\\). El método consta de tres pasos: Calcular el valor esperado de la variable \\[E(X; \\lambda)=\\mu\\] Escribir la ecuación donde el valor esperado es igual al primer momento muestral \\[\\frac{1}{\\hat{\\lambda}}=\\bar{x}\\] Resolver para el parámetro \\[\\hat{\\lambda}=\\frac{1}{\\bar{x}}\\] En términos de datos, esto es \\(\\hat{\\lambda}=(\\frac{1}{n}\\sum_i x_i)^{-1}\\). Es decir que si queremos estimar el parámetro \\(\\lambda\\) de una variable exponencial de un experimento aleatorio, debemos tomar una muestra aleatoria, sacar su promedio y tomar su inversa. El resultado es el estimador de parámetro que después, junto al modelo, lo podemos usar para calcular las probabilidades de observaciones futuras. Ejemplo (Baterías) Supongamos que tenemos varias baterías (nuevas y viejas) que cargamos durante el período de 1 hora. Medimos el estado de carga de la batería, siendo 1 un 100% de carga. El estado de carga de una batería es una variable aleatoria que puede tener una distribución uniforme, donde no sabemos el valor mínimo que puede tomar \\(x\\), pero sabemos que el máximo es 1 (\\(100\\%\\) de carga) \\[ f(x)= \\begin{cases} \\frac{1}{1-a},&amp; \\text{if } x\\in (a,1)\\\\ 0,&amp; x\\notin (a,1) \\end{cases} \\] ¿Cuál es el estimador de \\(a\\) (la carga mínima después de una hora)? Si ejecutamos un experimento y obtenemos \\(x_1,...x_n\\), nos preguntamos ¿cómo podemos estimar \\(a\\) a partir de los datos? Seguimos los tres pasos del método de los momentos: Calculamos el valor esperado de la variable aleatoria \\[E(X)=\\frac{a+1}{2}\\] Obtenemos la ecuación para \\(\\hat{a}\\) donde igualamos el valor esperado al primer momento muestral \\[\\frac{\\hat{a}+1}{2}=\\bar{x}\\] Resolvemos para el estimador \\(\\hat{a}\\) \\[\\hat{a}=2\\bar{x}-1\\] Este es el estimador de la carga mínima que podemos observar. Ten en cuenta si tomaramos el mínimo de las observaciones esto sería claramente subóptimo. El método nos dio una respuesta inteligente que también se puede resumir en los siguientes pasos Podemos calcular \\(\\bar{x}\\) con precisión creciente dada por \\(n\\) Sabemos que ninguna medida supera \\(b=1\\) Luego calculamos la distancia entre \\(\\bar{x}\\) y \\(b\\) que es \\(1-\\bar{x}\\) Esta distancia la restamos al promedio \\(\\bar{x}\\) para estimar el valor mínimo de carga: \\[\\bar{x}-(1-\\bar{x})=2\\bar{x}-1\\] Esta debería ser nuestra mejor suposición para \\(\\hat{a}\\). Como tal llegamos a la misma estimación dada por el método de los momentos. 12.7 Método de Momentos para varios parámetros El método dice que se puede encontrar un estimador para el parámetro \\(\\theta\\) de \\(f(x;\\theta)\\) a partir de la ecuación: \\[E(X)=\\frac{1}{n}\\sum_i x_i\\] Si hay más parámetros, usamos los momentos de muestra más altos. Consideremos que el segundo momento muestral es \\[\\frac{1}{n}\\sum_i X^2_i\\] Por lo tanto, una observación de este momento es cercana a \\(E(X^2)\\) \\[E(X ^ 2)=\\frac{1}{n}\\sum_i x^2_i\\] El método para dos parámetros dice que se puede encontrar una estimación para los parámetros \\(\\theta_1\\) y \\(\\theta_2\\) de \\(f(x;\\theta_1,\\theta_2)\\) a partir de las ecuaciones: \\(E(X)= \\frac{1}{n}\\sum_i x_i\\) \\(E(X^2)=\\frac{1}{n}\\sum_i x^2_i\\) Podemos tener tantas ecuaciones como parámetros necesitemos calcular, incrementando el grado de los momentos, es decir las potencias de \\(X\\). Ejemplo (Distribución normal) Si \\(X\\) se distribuye normalmente, tenemos dos parámetros para estimar \\[X \\rightarrow N(\\mu, \\sigma^2)\\] Seguimos los pasos del método de los momentos para dos parámetros: Calculamos el valor esperado de la variable \\[E(X)=\\mu\\] y el valor esperado de \\(X^2\\) \\[E(X^2)=\\sigma^2+\\mu^2\\] \\(E(X^2)\\) se sigue de la propiedad: \\(E(X^2) = V(X)+\\mu^2\\) Obtenemos las ecuaciones para los parámetros donde hacemos (a) el valor esperado de la variable igual al primer momento muestral, y (b) el valor esperado del segundo momento igual al segundo momento muestral \\(E(X)\\) se estima por \\[\\hat{\\mu}=\\frac{1}{n}\\sum_i x_i\\] \\(E(X^2)\\) se estima por \\[\\hat{\\sigma}^2+\\hat{\\mu}^2=\\frac{1}{n}\\sum_i x^2_i\\] Resolvemos los parámetros La primera ecuación da directamente el estimador de la media \\(\\mu\\). \\[\\hat{\\mu}=\\frac{1}{n}\\sum_i x_i\\] Que de nuevo es el promedio. De la segunda ecuación obtenemos \\[\\hat{\\sigma}^2= \\frac{1}{n} \\sum_i x^2_i-\\hat{\\mu}^2\\] que también se puede escribir como: \\[\\hat{\\sigma}^2=\\frac{1}{n} \\sum_i(x_i-\\hat{\\mu})^2\\] Encontramos que el método de los momentos y las estimaciones de máxima verosimilitud para la distribución normal son iguales. Sin embargo, este no siempre es el caso. Ejemplo (láser) ¿Cuál es el estimador del parámetro \\(\\alpha\\) para el corte láser dado por el método de los momentos? \\[ f(x; \\alpha)= \\begin{cases} \\frac{1}{\\alpha}(x-1)^{\\frac{1}{\\alpha}-1},&amp; \\text{if } x \\in (1,2)\\\\ 0,&amp; x \\notin (1,2)\\\\ \\end{cases} \\] Donde \\(\\alpha\\) es un parámetro. El método dice que se puede encontrar un estimador para el parámetro \\(\\alpha\\) de \\(f(x;\\alpha)\\) a partir de la ecuación: \\[E(X)=\\frac{1}{n}\\sum_i x_i\\] por \\(\\hat{\\alpha}\\) Calculamos el valor esperado \\(E(X)\\) \\[E(X)=\\int_{-\\infty}^{\\infty} xf(x;\\alpha)dx\\] Considera un cambio de variables \\(Z=X-1\\) entonces \\(E(X)=E(Z)+1\\) y \\(E(Z)= \\frac{1}{\\alpha} \\int_0^1 zz^{\\frac{1-\\alpha}{\\alpha}}dz= \\frac{1}{\\alpha} \\int_0^1 z^{1+\\frac{1-\\alpha}{\\alpha}}dz\\) \\(= \\frac{1}{\\alpha} \\frac{z^{2+\\frac{1-\\alpha}{\\alpha}}}{{2+\\frac{1-\\alpha}{\\alpha}} } |_0^1=\\frac{1}{1+\\alpha}\\) Por lo tanto, \\[E(X)=E(Z+1)=\\frac{1}{1+\\alpha}+1\\] Obtenemos la ecuación para \\(\\hat{\\alpha}_m\\) donde igualamos el valor esperado al primer momento muestral. Sustituyendo \\(\\hat{\\alpha}_m\\), el método de los momentos nos da la ecuación \\[\\frac{1}{1+\\hat{\\alpha}}+1=\\bar{x}\\] Resolvemos para \\(\\hat{\\alpha}\\) \\[\\hat{\\alpha}_m=\\frac{1}{\\bar{x}-1}-1\\] Calculamos el valor de nuestros datos \\[\\hat{\\alpha}_m=1.314\\] Tenga en cuenta que este es un ejemplo para el cual las estimaciones por máxima verosimilitud y el método de momentos son diferentes. La estimación de máxima verosimilitud fue: \\[\\hat{\\alpha}_{ml}=-\\frac{1}{n}\\sum_{i=1}^n \\ln (x_i-1)=1.320\\] El método de estimación de momentos fue: \\[\\hat{\\alpha}_m=\\frac{1}{\\bar{x}-1}-1=1.314\\] Necesitamos estudios de simulación, donde sabemos el verdadero valor del parámetro \\(\\alpha\\), para encontrar cuál de estas estadísticas tiene menos error. Nota: los datos para perforaciones con láser de \\(30\\) se simularon con \\(\\alpha=2\\), por lo tanto, debemos preferir la estimación de máxima verosimilitud. Para obtener mejores estimaciones de \\(\\alpha\\) necesitamos aumentar el tamaño de la muestra. 12.8 Preguntas 1) Un estimador no es \\(\\qquad\\)a: una estadística; \\(\\qquad\\)b: una variable aleatoria; \\(\\qquad\\)c: discreto; \\(\\qquad\\)d: Una observación de un parámetro; 2) Un estimador es insesgado si \\(\\qquad\\)a: es el parámetro que estima; \\(\\qquad\\)b: depende de \\(1/n\\); \\(\\qquad\\)c: varianza es pequeña; \\(\\qquad\\)d: su valor esperado es el parámetro que estima; 3) Un estimador es consistente si \\(\\qquad\\)a: es el parámetro que estima; \\(\\qquad\\)b: depende de \\(1/n\\); \\(\\qquad\\)c: varianza es pequeña; \\(\\qquad\\)d: su valor esperado es el parámetro que estima; 4) El método de máxima verosimilitud \\(\\qquad\\)a: Produce estimadores basados en la probabilidad de las observaciones; \\(\\qquad\\)b: produce estimadores insesgados; \\(\\qquad\\)c: produce estimadores consistentes; \\(\\qquad\\)d: produce estimadores iguales a los del métoodo de los momentos; 5) El primer momento muestral es \\(\\qquad\\)a: la media; \\(\\qquad\\)b: la varianza; \\(\\qquad\\)c: el valor esperado; \\(\\qquad\\)d: el promedio; 12.9 Ejercicios 12.9.0.1 Ejercicio 1 Toma una variable aleatoria con la siguiente función de densidad de probabilidad \\[ f(x)= \\begin{cases} (1+\\theta)x^\\theta,&amp; \\text{if } x\\in (0,1)\\\\ 0,&amp; x\\notin (0,1) \\end{cases} \\] ¿Cuál es la estimación de máxima verosimilitud para \\(\\theta\\)? (R:\\(\\hat{\\theta}=-n/\\sum_{i=1..n} \\log(x_i)-1\\)) Si tomamos una muestra de \\(5\\) obsevaciones \\(x_1 = 0.92; \\qquad x_2 = 0.79; \\qquad x_3 = 0.90; \\qquad x_4 = 0.65; \\qquad x_5 = 0.86\\) ¿Cuál es el valor estimado del parámetro \\(\\theta\\)? (R: \\(\\hat{\\theta}=3.96\\)) Calcula \\(E(X)=\\mu\\) en función de \\(\\theta\\). ¿Cuál es la estimación de máxima verosimilitud para \\(\\mu\\)? (R:\\(\\hat{\\mu}=(1+\\hat{\\theta})/(2+\\hat{\\theta})=0.832\\)) 12.9.0.2 Ejercicio 2 Para una variable aleatoria con una función de probabilidad binomial \\[f(x; p)=\\binom nxp^x(1-p)^{n-x}\\] ¿Cuál es el estimador de máxima verosimilitud de \\(p\\) para una muestra de tamaño \\(1\\) de esta variable aleatoria? (R:\\(\\hat{p}=x_1/n\\)) ¿Es el estimador insesgado y/o consistente? En un examen de \\(100\\) estudiantes observamos \\(x_1=68\\) estudiantes que aprobaron el examen. ¿Cuál es la estimación de máxima verosimilitud para la probabilidad de pasar el examen? (R:0.68) 12.9.0.3 Ejercicio 3 Toma una variable aleatoria con la siguiente función de densidad de probabilidad \\[ f(x)= \\begin{cases} \\lambda e^{-\\lambda x},&amp; x &gt; 0 \\\\ 0,&amp; x\\leq 0 \\end{cases} \\] ¿Cuál es la estimación de máxima verosimilitud para \\(\\lambda\\)? (R: \\(1/\\bar{x}\\)) Si tomamos una muestra de \\(5\\) observaciones \\(x_1 = 0.223 \\qquad x_2 = 0.681; \\qquad x_3 = 0.117; \\qquad x_4 = 0.150; \\qquad x_5 = 0.520\\) ¿Cuál es el valor estimado del parámetro \\(\\alpha=\\frac{n}{\\lambda}\\)? (R:2.95) ¿Cuál es la estimación de máxima verosimilitud de la varianza de la variable exponencial \\(\\sigma^2=\\frac{1}{\\lambda^2}\\)? (R:1.694) ¿Es \\(\\alpha\\) un estimador insesgado y consistente de \\(E(Y)\\), donde \\(Y=\\sum_1^n X_i\\)? (R: Es insesgado, no es consistente) 12.9.0.4 Ejercicio 4 En 1910, Rutherford y Geiger contaron que el número de partículas \\(\\alpha\\) que se emitían de una muestra de polonio cada 7.5 segundos. Las frequencias relativas reportadas en su artículo original para el número de partículas en 7.5 segundos en 2608 observaciones fueron ## 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## 57 203 383 525 532 408 273 139 45 27 10 4 0 1 1 observaron, por ejemplo, que 57 veces no contaron ninguna partícula en 7.5 segundos, 203 veces contaron 1 partícula en 7.5 segundos, etc. Bateman mostró que la variable que cuenta el número de particulas en 7.5 segundos era una variable aleatoria de Poisson y estimó su valor usando el método de máxima verosimilitud. ¿Cuál es el valor estimado del parámetro \\(\\lambda\\) usando el método de máxima verosimilitud? (R: 3.871549) En este experimento Rutherford y Geiger muestran una gran coincidencia entre los datos y el modelo demostrando que los átomos radioactivos emiten partículas alpha de forma independiente y aleatoria. 12.9.0.5 Ejercicio 5 En 6 pruebas de encendido de un motor, el número de repeticiones que se han tenido que hacer antes de 4 fallos en el encendido han sido ## [1] 49 62 52 92 83 73 estima por el método de máxima verosimilitud la probabilidad de que el motor falle (R:0.05839). 12.10 Método de los momentos 12.10.0.1 Ejercicio 1 ¿Cuáles son los estimadores de los siguientes modelos paramétricos dados por el método de los momentos? Model f(x) E(X) Bernoulli \\(p^x(1-p)^{1-x}\\) \\(p\\) Binomial \\(\\binom n x p^x(1-p)^{n-x}\\) \\(np\\) Geometrica \\(p(1-p)^{x-1}\\) \\(\\frac{1}{p}\\) Binomial Negativa \\(\\binom {x+r-1} x p^r(1-p)^x\\) \\(r\\frac{1-p}{p}\\) Poisson \\(\\frac{e^{-\\lambda}\\lambda^x}{x!}\\) \\(\\lambda\\) Exponencial \\(\\lambda e^{-\\lambda x}\\) \\(\\frac{1}{\\lambda}\\) Normal \\(\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\) \\(\\mu\\) (R: \\(\\hat{p}=\\bar{x}\\), \\(\\hat{p}=\\bar{x}/n\\), \\(\\hat{p}=1/\\bar{x}\\) \\(\\hat{p}=r/(\\bar{x}+1)\\), \\(\\hat{\\lambda}=\\bar{x}\\), \\(\\hat{\\lambda}=1/\\bar{x}\\), \\(\\hat{\\mu}=\\bar{x}\\)) 12.10.0.2 Ejercicio 2 Toma una variable aleatoria con la siguiente función de densidad de probabilidad \\[ f(x)= \\begin{cases} (1+\\theta)x^\\theta,&amp; \\text{if } x\\in (0,1)\\\\ 0,&amp; x\\notin (0,1) \\end{cases} \\] Calcula \\(E(X)\\) como una función de \\(\\theta\\) (R:\\((1+\\theta)/(2+\\theta)\\)) ¿Cuál es la estimación de \\(\\theta\\) utilizando el método de los momentos? (R:\\(\\hat{\\theta}=(2\\bar{x}-1)/(1-\\bar{x})\\)) Si tomamos una muestra de \\(5\\) observaciones \\(x_1 = 0.92; \\qquad x_2 = 0.79; \\qquad x_3 = 0.90; \\qquad x_4 = 0.65; \\qquad x_5 = 0.86\\) ¿Cuál es el valor estimado del parámetro \\(\\theta\\)? (R:3.681) 12.10.0.3 Ejercicio 3 Considera una variable aleatoria discreta \\(X\\) que sigue una distribución binomial negativa con función de masa de probabilidad: \\[f(x) = \\binom{x+r-1}{x}p^r(1-p)^x\\] Dado que \\(E(X)=\\dfrac{r(1-p)}{p}\\) \\(V(X) =\\dfrac{r(1-p)}{p^2}\\) calcular: Una estimación del parámetro \\(r\\) y una estimación del parámetro \\(p\\) obtenidas a partir de una muestra aleatoria de tamaño \\(n\\) por el método de los momentos. (R: \\(\\hat{p}=\\bar{x}/({1/n \\sum x^2}-\\bar{x}^2)\\), \\(\\hat{r}=\\bar{x}\\hat{p}/(1-\\hat{p})\\)) Los valores de las estimaciones de \\(r\\) y \\(p\\) para la siguiente muestra aleatoria: \\[x_1 = 27; \\qquad x_2 = 8; \\qquad x_3 = 22; \\qquad x_4 = 29; \\qquad x_5 = 19; \\qquad x_5 = 32\\] (R: 13.152, 0.365) "],["intervalos-de-confianza.html", "Chapter 13 Intervalos de confianza 13.1 Objetivo 13.2 Estimación de la media 13.3 Margen de error 13.4 Estimación de intervalo para la media 13.5 Margen de error para varianza desconocida 13.6 Estimación de proporciones 13.7 Estimación de la varianza 13.8 Intervalo de confianza para la varianza 13.9 Preguntas 13.10 Ejercicios 13.11 Práctica", " Chapter 13 Intervalos de confianza 13.1 Objetivo En este capítulo, presentaremos el concepto de intervalos de confianza para medias, proporciones y varianzas. Derivaremos las fórmulas para los intervalos de confianza bajo diferentes condiciones, como normalidad con varianza conocida y desconocida, y \\(n\\) grande. 13.2 Estimación de la media Hemos visto que siempre que tomamos una muestra aleatoria \\((X_1, X_2, ... X_n)\\), la media muestral \\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^n X_i\\] es un estimador de la media \\(\\mu\\) de la variable aleatoria \\(X\\). El estimador es insesgado \\(E(\\bar{X})=\\mu\\) y consistente \\(V(\\bar{X})=\\frac{\\sigma^2}{n}\\) donde \\(\\sigma^2\\) es la varianza de \\(X\\). Llamamos a \\(se=\\frac{\\sigma}{\\sqrt{n}}\\) el error estándar. cuando tomamos los valores de la muestra aleatoria, tomamos el valor de \\(\\bar{x}\\) como el valor de la media. Eso es \\[\\bar{x}=\\hat{\\mu}\\] Como \\(\\bar{X}\\) es una variable aleatoria, la estimación de la media cambia cuando tomamos otra muestra. 13.3 Margen de error Al decidir si el error en la estimación \\[\\bar{X}-\\mu\\] es grande o no, generalmente lo comparamos con una tolerancia predefinida. Si sabemos que la distribución de \\(X\\) es normal \\(X \\rightarrow N(\\mu, \\sigma^2)\\) y el valor de \\(\\mu\\), podemos calcular qué tan lejos cae la estimación de \\(\\bar{x}\\) de \\(\\mu\\). Definimos el margen de error al nivel de \\(5\\%\\) como la distancia \\(m\\) tal que la distribución de \\(\\bar{X}\\) captura \\(95\\%\\) de las estimaciones: \\[P(-m \\leq \\bar{X}-\\mu \\leq m)=P(\\mu-m \\leq \\bar{X} \\leq\\mu + m)=0.95\\] Si \\(\\bar{X}\\) se distribuye normalmente, entonces el margen de error es \\[m=z_{0.025} \\frac{\\sigma}{\\sqrt{n}}=1.96\\times se\\] donde \\(z_{0.025}=\\phi^{-1}(0.975)=\\) qnorm(0.975) Ejemplo (cables) Tomamos una muestra aleatoria de tamaño \\(8\\): Cargamos un cable hasta que se rompan y registramos la carga de rotura. Si sabemos que la rotura de los cables realmente se distribuyen normalmente \\[X \\rightarrow N(\\mu=13, \\sigma^2=0.35^2)\\] entonces la media muestral es normal \\[\\bar{X} \\rightarrow N(13, \\frac{0.35^2}{8})\\] Con media \\(E(\\bar{X})=13\\) y error estándar \\(se=\\frac{0.35}{\\sqrt{8}}=0.1237\\) Por lo tanto, el margen de error en \\(95\\%\\) es \\[m=z_{0.025} \\frac{\\sigma}{\\sqrt{n}}=1.96\\times se=1.96\\frac{0.35}{\\sqrt{8}}=0.24\\] Ahora, tomamos la muestra aleatoria y encontramos los resultados. ## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747 El promedio observado es \\(\\bar{x}=13.21\\), y el error que cometeríamos si reemplazamos \\(\\mu\\) por \\(\\bar{x}\\), sería \\[\\bar{x}-\\mu=13.21-13=0.21\\] El error observado está dentro del margen de error \\[\\bar{x}-\\mu &lt;m\\] 13.4 Estimación de intervalo para la media El problema es que en la vida real no sabemos los valores reales de \\(\\mu\\) o \\(\\sigma\\) para \\[X \\rightarrow N(\\mu, \\sigma^2)\\] Empezamos tomando la muestra y luego calcular la media ¿Cuál es el valor de \\(\\mu\\)? Nuestros datos sugieren que es \\(\\bar{x}=13.21\\). Reemplazar \\(\\mu\\) por \\(\\bar{x}\\) se denomina estimación puntual del parámetro. Pero ¿qué tan seguros estamos al hacer este remplazo? después de todo, sabemos que estamos cometiendo un error, pero no sabemos qué tan grande es. Definimos el intervalo de confianza para \\(\\mu\\). De la ecuación del margen de error \\[P(-m \\leq \\bar{X} - \\mu \\leq m)=0.95\\] resolvamos para \\(\\mu\\) que es la verdadera incógnita \\[P(\\bar{X} - m \\leq \\mu \\leq \\bar{X} + m)=0.95\\] Los límites izquierdo y derecho de la desigualdad son variables aleatorias que motivan la definición del intervalo de confianza aleatorio en \\(95\\%\\): \\[(L,U)=(\\bar{X} - m,\\bar{X} + m)\\] Este intervalo es una nueva variable aleatoria y tiene por definición una probabilidad de \\(0.95\\) de contener \\(\\mu\\). El intervalo observado que obtenemos del experimento es (en minúsculas) \\[(l,u)=(\\bar{x} - m,\\bar{x} + m)\\] Este intervalo contiene o no el parámetro \\(\\mu\\): ¡nunca lo sabremos! Decimos que tenemos una confianza de \\(95\\%\\) en que el intervalo \\((l,u)\\) capturará el verdadero parámetro desconocido \\(\\mu\\). Piensa en comprar un billete de lotería del raspa y gana pero que no podemos raspar para ver el premio. El billete tiene o no el premio sólo que no lo sabemos. 13.4.1 Caso 1 (varianza conocida) Los intervalos de confianza se pueden estimar en diferentes casos. El primer caso es cuando \\(X\\) es una variable normal y conocemos el valor de \\(\\sigma\\) el intervalo de confianza en \\(95\\%\\) es \\[(l,u)=(\\bar{x} - m, \\bar{x} + m)\\] donde \\[m=z_{0.025} \\frac{\\sigma}{\\sqrt{n}}\\] Eso es: \\[(l,u)=(\\bar{x} - z_{0.025} \\frac{\\sigma}{\\sqrt{n}}, \\bar{x} + z_{0.025} \\frac{\\sigma}{ \\sqrt{n}})\\] Ejemplo (cables) En nuestro ejemplo, asumimos que \\(X\\) se distribuye normalmente y que sabemos \\(\\sigma^2=0.35^2\\). Dado que \\(\\bar{X}\\) es normal, el margen de error es \\[m=z_{0.025} \\frac{\\sigma}{\\sqrt{n}}\\] 2. Como sabemos \\(\\sigma^2=0.35^2\\), entonces el intervalo de confianza de \\(95\\%\\) es \\((l,u)=(\\bar{x} - m, \\bar{x} + m)=\\) \\[(\\bar{x}-z_{0.025} \\frac{\\sigma}{\\sqrt{n }}, \\bar{x}+z_{0.025} \\frac{\\sigma}{\\sqrt{n}})= (12.97,13.45)\\] o también podemos escribirlo como \\[\\hat{\\mu}=\\bar{x} \\pm m = 13.21 \\pm 0.24\\] Esto significa que, al estimar la media por el promedio, confiamos en las unidades pero no tanto en los lugares decimales. Recuerda que el intervalo de confianza \\((l,u)\\) es una observación del intervalo de confianza aleatorio \\((L,U)\\). Por lo tanto, cada vez que obtenemos una nueva muestra entonces \\((l,u)\\) cambia. Si realizamos muestras de \\(100\\) de tamaño \\(n\\) entonces \\(95%\\) de los intervalos de confianza contendrán \\(\\mu\\), ¡pero no sabemos cuál! 13.4.2 Nivel de confianza Podemos cambiar nuestra confianza de \\(95\\%\\) a \\(99\\%\\). Cuando calculamos el margen de error en \\(95\\%\\), dejamos por fuera \\(\\alpha=0.05\\) de probabilidad, \\(0.025\\) a cada lado. Ahora, podemos dejar fuera \\(\\alpha=0.01\\) de probabilidad, \\(0.005\\) a cada lado. Por lo tanto, el intervalo de confianza de \\(99\\%\\) es \\((l,u) = (\\bar{x} - z_{0.005}\\frac{\\sigma}{\\sqrt{n}},\\bar{x} + z_{0.005}\\frac{\\sigma}{\\sqrt{n}})\\) \\[= (\\bar{x} - 2.58\\frac{\\sigma}{\\sqrt{n}},\\bar{x} + 2.58\\frac{\\sigma}{\\sqrt{n}})\\] donde \\(z_{0.005}=\\phi^{-1}(0.995)=\\) qnorm(0.995). También podemos escribirlo como \\[\\hat{\\mu}=\\bar{x} \\pm 2.58\\frac{\\sigma}{\\sqrt{n}}\\] Para nuestros cables, el intervalo de confianza de \\(99\\%\\) es \\[\\hat{\\mu}= 13.21 \\pm 0.31\\] Si queremos tener más confianza, ¡necesitamos intervalos de confianza más grandes! Ejemplo (Energía de impacto) Un material metálico se prueba por impacto para medir la energía requerida para cortarlo a una temperatura dada. Se cortaron diez probetas de acero A238 a 60ºC con las siguientes energías de impacto (J): \\[64.1,\\, 64.7,\\, 64.5,\\, 64.6,\\, 64.5,\\, 64.3,\\, 64.6,\\, 64.8,\\, 64.2,\\, 64.3\\] Si suponemos que la energía del impacto se distribuye normalmente con \\(\\sigma=1J\\), ¿cuál es el intervalo de confianza de \\(95\\%\\) para la media de estos datos? Sabemos \\(X \\rightarrow N(\\mu, \\sigma^2)\\) \\(\\sigma=1J\\) \\(\\alpha=0.05\\) (el límite de confianza) El intervalo de confianza de \\(95\\%\\) es entonces \\((l,u)=(\\bar{x}-1.96 \\frac{\\sigma}{\\sqrt{n}}, \\bar{x}+1.96 \\frac{\\sigma}{\\sqrt{n}})\\) \\[=(64.46-1.96 \\frac{1}{\\sqrt{10}}, 64.46+1.96 \\frac{1}{\\sqrt{10}})=(63.84,65.08)\\] o \\[\\hat{\\mu}=64.46 \\pm 0.61\\] esto nos dice que podemos estar seguros del primer dígito (6), algo seguros del segundo (4) e inseguros de los decimales (46). En R: library(BSDA) ## Warning: package &#39;BSDA&#39; was built under R version 4.3.3 ## Loading required package: lattice ## ## Attaching package: &#39;BSDA&#39; ## The following object is masked from &#39;package:datasets&#39;: ## ## Orange z.test(c(64.1, 64.7, 64.5, 64.6, 64.5, 64.3, 64.6, 64.8, 64.2, 64.3), sigma.x=1) ## ## One-sample z-Test ## ## data: c(64.1, 64.7, 64.5, 64.6, 64.5, 64.3, 64.6, 64.8, 64.2, 64.3) ## z = 203.84, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 63.8402 65.0798 ## sample estimates: ## mean of x ## 64.46 ¿Qué pasa si \\(\\sigma^2\\) es desconocido? 13.5 Margen de error para varianza desconocida Pudimos calcular el intervalo de confianza \\((l,u)=(\\bar{x} -m, \\bar{x} +m)\\) porque pudimos encontrar el margen de error \\[m=1.96 \\frac{\\sigma}{\\sqrt{n}}\\] desde que sabíamos \\(\\sigma\\). \\(\\sigma\\) es un parámetro de la distribución que normalmente no conocemos, así cmo \\(\\mu\\). Para encontrar el margen de error con varianza desconocida, necesitamos el siguiente teorema, debido a Gosset 13.5.1 Teorema (estadística T) Cuando \\(X\\) es normal, entonces la estadística estandarizada \\[T=\\frac{\\bar{X}-\\mu}{\\frac{S}{\\sqrt{n}}}\\] Sigue una distribución \\(t\\) con \\(n-1\\) grados de libertad, donde \\(S^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar{X})^2\\). Por lo tanto, podemos calcular probabilidades para \\(\\bar{X}\\), incluso si no conocemos \\(\\sigma\\). Veamos algunas densidades de probabilidad en la familia de las distribuciones \\(T\\). Ahora necesitamos volver a calcular el margen de error \\(m\\) al nivel de \\(5\\%\\) cuando usamos la distribución \\(t\\) \\(P(\\mu-m \\leq \\bar{X} \\leq\\mu + m)\\) \\[=P(-\\frac{m}{s/\\sqrt{n}} \\leq T \\leq\\frac{m}{s/\\sqrt{n}})=0.95\\] \\[m=t_{0.025, n-1} \\frac{s}{\\sqrt{n}}\\] donde \\(t_{0.025, n-1}\\) es el valor \\(T\\) que deja \\(2.5\\%\\) de probabilidad en el lado derecho de la distribución \\(t\\) con \\(n-1\\) grados de libertad. 13.5.2 Caso 2 (varianza desconocida) El segundo caso para calcular los intervalos de confianza es más realista. Si \\(X\\) es una variable normal el intervalo de confianza en \\(95\\%\\) es \\[(l,u)=(\\bar{x} - m, \\bar{x} + m)\\] donde \\[m=t_{0.025, n-1} \\frac{s}{\\sqrt{n}}\\] Eso es: \\[(l,u)=(\\bar{x} - t_{0.025, n-1} \\frac{s}{\\sqrt{n}}, \\bar{x} + t_{0.025, n-1} \\frac{s}{\\sqrt{n}})\\] donde \\(t_{0.025, n-1}=F^{-1}(0.975)\\)=qt(0.975, n-1) Ejemplo (Energía de impacto) Un material metálico se prueba por impacto para medir la energía requerida para cortarlo a una temperatura dada. Se cortaron diez probetas de acero A238 a 60ºC con las siguientes energías de impacto (J): \\[64.1,\\, 64.7,\\, 64.5,\\, 64.6,\\, 64.5,\\, 64.3,\\, 64.6,\\, 64.8,\\, 64.2,\\, 64.3\\] Si suponemos que la energía del impacto se distribuye normalmente pero no conocemos la varianza, ¿cuál es el intervalo de confianza de \\(95\\%\\) para la media de estos datos? Sabemos \\(\\bar{x}=64.46\\) \\(s=0.227\\) \\(\\alpha=0.05\\) \\(t_{0.025,9}=2.26\\) obtenido de \\(t_{0.025,9}=\\) qt(0.975, 9) El intervalo de confianza es entonces \\((l,u)=(\\bar{x}- t_{0.025,9}\\frac{s}{\\sqrt{n}},\\bar{x}+t_{0.025,9} \\frac{s} {\\sqrt{n}})\\) \\[=(64.46-2.26 \\frac{0.227}{\\sqrt{10}},64.46+2.26 \\frac{0.227}{\\sqrt{10}})\\] \\[=(64.29,64.62)\\] Tengamos en cuenta que cuando asumimos \\(\\sigma=1\\) el intervalo de confianza \\((63.84,65.08)\\) era mayor. Por lo tanto, los datos sugieren que \\(\\sigma&lt;1\\). En R, podemos calcular el intervalo de confianza con: t.test(c(64.1,64.7,64.5,64.6,64.5,64.3,64.6,64.8,64.2,64.3)) ## ## One Sample t-test ## ## data: c(64.1, 64.7, 64.5, 64.6, 64.5, 64.3, 64.6, 64.8, 64.2, 64.3) ## t = 897.74, df = 9, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 64.29757 64.62243 ## sample estimates: ## mean of x ## 64.46 13.6 Estimación de proporciones Ejemplo (vacuna) Se seleccionó una muestra aleatoria de \\(400\\) pacientes para probar una nueva vacuna contra el virus de la influenza, después de \\(6\\) meses de vacunación, \\(134\\) estaban enfermos. ¿Cuál es la eficacia esperada de la vacuna? Dado que cada vacunación \\(X_i\\) es un ensayo de Bernoulli \\[X \\rightarrow Bernoulli(p)\\] Con media \\(\\mu=p\\) y varianza \\(\\sigma^2=p(1-p)\\). La muestra sería algo como \\[(x_1,x_2, x_3, ...x_n)=(0,1,0,.. 1, 0)_{400}\\] con \\(134\\) unos y en un total de \\(400\\) repeticiones. La muestra tiene un promedio de \\[\\bar{x}=\\frac{1}{400}\\sum_i^{400} x_i=134/400=0.34\\] . Dado que la media muestral es un estimador insesgado de \\(\\mu\\), entonces podemos tener una estimación puntual para \\(p\\) \\[\\hat{p}=\\bar{x}=134/400=0.34\\] Esto tiene sentido porque \\(\\bar{x}\\) es la frecuencia relativa observada para el número de unos en la muestra (\\(f_1\\)). Y como tal, es un estimador de la probabilidad de observar un uno en un ensayo de Bernoulli. \\[f_1 =\\hat{P}(X=1)\\] Esto es consistente con la definición frecuentista de probabilidad que vimos en el capítulo 2. Pero, ¿qué confianza tenemos en esta estimación? Queremos un intervalo de confianza para \\(p\\). 13.6.1 Caso 3 (proporciones) Cuando \\(\\hat{p}n&gt;5\\) y \\((1-\\hat{p})n&gt;5\\), la estadística estandarizada de \\(\\bar{X}\\) se puede aproximar a una variale normal estándar por el TCL \\[Z=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}= \\frac{\\bar{X}-p}{\\big[\\frac{p(1-p)}{n} \\big]^{1/2}}\\rightarrow N(0,1)\\] Entonces el intervalo de confianza al \\(95\\%\\) de \\(p\\) es: \\[CI=(l,u)=(\\bar{x}-z_{0.025}\\big[\\frac{\\bar{x}(1-\\bar{x})}{n} \\big]^{ 1/2}, \\bar{x}+z_{0.025}\\big[\\frac{\\bar{x}(1-\\bar{x})}{n} \\big]^{1/2})\\] Donde estimamos la varianza de Bernoulli \\(\\sigma^2=p(1-p)\\) por \\(\\hat{\\sigma}^2=\\bar{x}(1-\\bar{x})\\). Es decir \\(\\hat{\\sigma}=\\sqrt{\\bar{x}(1-\\bar{x})}\\). Ejemplo (vacuna) En nuestro caso, estamos contando \\(134\\) fallos de inmunización de \\(400\\) inoculaciones. sabemos \\(\\bar{x}=134/400=0.34\\) \\(z_{0.025}=1.96\\) Por lo tanto, el intervalo de confianza de \\(95\\%\\) para \\(p\\) es \\((l,u)=(\\bar{x}-1.96 \\big[\\frac{\\bar{x}(1-\\bar{x})}{n} \\big]^{1/2}, \\bar{x}+1.96 \\big[\\frac{\\bar{x}(1-\\bar{x})}{n} \\big]^{1/2})\\) \\[=(0.29,0.38)\\] La probabilidad de fracaso de la vacuna es \\[\\hat{p}=0.34 \\pm 0.05\\] en R prop.test(134, 400, correct=FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: 134 out of 400, null probability 0.5 ## X-squared = 43.56, df = 1, p-value = 4.112e-11 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.2905091 0.3826300 ## sample estimates: ## p ## 0.335 Nota: Las encuestas de intención de voto (ensayo de Bernoulli) en una muestra de \\(n\\) individuos reportan este tipo de estimación con su margen de error. No significa que el valor verdadero de \\(p\\) esté dentro de este intervalo con una probabilidad de \\(95\\%\\). Significa que tenemos una confianza del \\(95\\%\\) de haber atrapado al \\(p\\) que representa esta muestra en particular. 13.7 Estimación de la varianza Hemos visto que siempre que tomamos una muestra aleatoria \\((X_1, X_2, ... X_n)\\), la varianza muestral \\[S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})^2\\] es un estimador de la media \\(\\sigma^2\\) de la variable aleatoria \\(X\\). El estimador es insesgado \\(E(S^2)=\\sigma^2\\) y también es consistente. Cuando tomamos los valores de la muestra aleatoria, tomamos el valor de \\(S^2\\) como el valor de la varianza; eso es \\[s^2=\\hat{\\sigma}^2\\] Dado que \\(S^2\\) es una variable aleatoria, la estimación de la varianza cambia cuando tomamos otra muestra. Ejemplo (energía de impacto) Un material metálico se prueba por impacto para medir la energía requerida para cortarlo a una temperatura dada. Se cortaron diez probetas de acero A238 a 60ºC con las siguientes energías de impacto (J): \\[64.1,\\, 64.7,\\, 64.5,\\, 64.6,\\, 64.5,\\, 64.3,\\, 64.6,\\, 64.8,\\, 64.2,\\, 64.3\\] ¿Cuál es la estimación de la varianza de estos datos? \\[s^2=0.05155556\\] En R: sd(c(64.1, 64.7, 64.5, 64.6, 64.5, 64.3, 64.6, 64.8, 64.2, 64.3))^2 ¿Cuánta confianza tenemos en los decimales de la estimación? 13.8 Intervalo de confianza para la varianza Para calcular un intervalo de confianza para la varianza, necesitamos una estadística que sea una función de \\(S^2\\) y nos permita calcular probabilidades. Usaremos el siguiente teorema 13.8.1 Teorema (\\(\\chi^2\\)): Cuando \\(X\\) es normal, entonces la estadística estandarizada \\[W=\\frac{S^2(n-1)}{\\sigma^2}\\] sigue una distribución \\(\\chi^2\\) con \\(n-1\\) grados de libertad \\[\\frac{S^2}{\\sigma^2}(n-1)\\rightarrow \\chi^2_{n-1}\\] Por lo tanto, podemos calcular probabilidades para \\(W\\). Veamos algunas densidades de probabilidad en la familia de las distribuciones \\(\\chi^2\\). 13.8.2 Intervalo de confianza para la varianza Buscamos el intervalo de confianza de \\(\\sigma^2\\) con confianza \\(95\\%\\) \\((L,U)\\) tal que \\[P(L \\leq \\sigma^2 \\leq U)=0.95\\] Entonces podemos usar el \\(\\chi^2\\) para determinar el \\(95\\%\\) de la distribución alrededor de \\(W\\). Comencemos definiendo los valores que capturan los \\(95\\%\\) de la distribución \\[P(\\chi^2_{0.975,n-1} \\leq W \\leq \\chi^2_{0.025,n-1})=0.95\\] Reemplazando el valor de \\(W\\) \\[P(\\chi^2_{0.975,n-1} \\leq \\frac{S^2}{\\sigma^2}(n-1) \\leq \\chi^2_{0.025,n-1})= 0.95\\] y resolviendo para \\(\\sigma^2\\) \\[P(\\frac{S^2 (n-1)}{\\chi^2_{0.025,n-1}}\\leq \\sigma^2 \\leq \\frac{S^2(n-1)}{ \\chi^2_{0.975,n-1}})=0.95\\] Encontramos un intervalo aleatorio que captura \\(\\sigma^2\\) con \\(95\\%\\) de confianza \\[(L,U) = (\\frac{S^2 (n-1)}{\\chi^2_{0.025,n-1}},\\frac{S^2(n-1)}{\\chi ^2_{0.975,n-1}})\\] 13.8.3 Caso 4 (varianza) Cuando \\(X\\) es una variable normal El intervalo de confianza al \\(95\\%\\) observado (minúsculas) es \\[(l,u) = (\\frac{s^2 (n-1)}{\\chi^2_{0.025,n-1}},\\frac{s^2(n-1)}{\\chi ^2_{0.975,n-1}})\\] dónde \\(\\chi^2_{0.975,n-1}=F^{-1}(0.025)=\\) qchisq(0.025, df=n-1) \\(\\chi^2_{0.025, n-1}=F^{-1}(0,975)=\\)qchisq(0,975, df=n-1) para \\(n=10\\) o \\(df=n-1=9\\) Ejemplo (energía de impacto) Un material metálico se prueba por impacto para medir la energía requerida para cortarlo a una temperatura dada. Se cortaron diez probetas de acero A238 a 60ºC con las siguientes energías de impacto (J): \\[64.1,\\, 64.7,\\, 64.5,\\, 64.6,\\, 64.5,\\, 64.3,\\, 64.6,\\, 64.8,\\, 64.2,\\, 64.3\\] ¿Cuál es el intervalo de confianza para la varianza de estos datos? \\[(l,u) = (\\frac{s^2 (n-1)}{\\chi^2_{0.025,n-1}},\\frac{s^2(n-1)}{\\chi ^2_{0.975,n-1}})\\] La desviación estándar de los datos es \\(s=0.2270585\\) \\(n=10\\) Luego calculamos \\(\\chi^2_{0.025,n-1}\\) y \\(\\chi^2_{0.975,n-1}\\) chi0.975 &lt;- qchisq(0.025, df=9) chi0.975 [1] 2.700389 chi0.025 &lt;- qchisq(0.975, df=9) chi0.025 [1] 19.02277 Por lo tanto \\[(l,u)= (\\frac{0.2270585^2 (10-1)}{19.02277},\\frac{0.2270585^2(10-1)}{2.700389})=(0.02,0.17)\\] Recuerda que cuando habíamos calculado el intervalo de confianza para la media, asumimos \\(\\sigma^2=1\\) (caso 1). Ahora podemos decir que esta elección no fue consistente con los datos porque vemos que el intervalo de confianza no contiene el valor \\(\\sigma^2=1\\) . Según los datos, \\(\\sigma^2 \\neq 1\\) con una confianza de \\(95\\%\\). en R: library(Ecfun) confint.var(0.05155556, 9) ## lower upper ## [1,] 0.02439183 0.1718271 ## attr(,&quot;level&quot;) ## [1] 0.95 El intervalo para la varianza no es simétrico y no podemos formularlo como un margen de error con \\(\\pm\\). 13.9 Preguntas 1) El margen de error a \\(95\\%\\) de confianza de una variable normal es \\(\\qquad\\)a: \\(\\frac{s}{\\sqrt{n}}\\); \\(\\qquad\\)b: \\(1.96\\times se\\); \\(\\qquad\\)c: \\(\\frac{\\sigma}{\\sqrt{n}}\\); \\(\\qquad\\)d: \\(\\sigma\\) 2) cuando hablamos de \\(z_{0.025}\\) queremos encontrar: \\(\\qquad\\)a: El valor de una variable normal estándar que acumula hasta \\(99.75\\%\\) de probabilidad; \\(\\qquad\\)b: El valor de una variable normal estándar que acumula hasta \\(0.25\\%\\) de probabilidad; \\(\\qquad\\)c: La probabilidad de una variable estándar hasta \\(99.75\\%\\); \\(\\qquad\\)d: La probabilidad de una variable estándar hasta \\(0.25\\%\\) 3) El intervalo de confianza aleatorio \\((L,U)\\) para la media al \\(95\\%\\) \\(\\qquad\\)a: es un parámetro bidimensional de la distribución de la muestra; \\(\\qquad\\)b: da los límites donde \\(\\mu\\) tiene una probabilidad de ocurrir el \\(95\\%\\) de las veces; \\(\\qquad\\)c: es una estimación del promedio; \\(\\qquad\\)d: captura \\(\\mu\\) \\(95\\%\\) de las veces 4) Un intervalo de confianza para la media escrito como \\(\\hat{\\mu}=56.99 \\pm 0.01\\) \\(\\qquad\\)a: indica que estamos \\(\\%99\\) seguros de que la media es \\(56.99\\); \\(\\qquad\\)b: indica que no podemos confiar en el último decimal en la estimación de la media; \\(\\qquad\\)c: indica que la media de la población es en \\(56.99\\) con un error de \\(0.01\\); \\(\\qquad\\)d: indica que podemos confiar en la cifra unitaria (\\(6\\)) en la estimación de la media 5)Si conocemos el valor de \\(\\mu\\) y encontramos que el intervalo de confianza no lo atrapó, entonces \\(\\qquad\\)a: el intervalo de confianza no está bien calculado; \\(\\qquad\\)b: tenemos una observación rara del intervalo de confianza; \\(\\qquad\\)c: el intervalo de confianza no estima la media; \\(\\qquad\\)f: hay poca probabilidad de encontrar la media en el intervalo de confianza 13.10 Ejercicios 13.10.0.1 Ejercicio 1 En un artículo científico, los autores reportan un intervalo de confianza al \\(95\\%\\) de \\((228, 232)\\) para la frecuencia natural (Hz) de un haz metálico. Utilizaron una muestra de tamaño \\(25\\) y consideraron que las medidas estaban distribuidas normalmente. ¿Cuál es la media y la desviación estándar de las medidas? (R:230, 4.85) Calcula el intervalo de confianza de \\(99\\%\\). (R:(227.51, 232.48)) 13.10.0.2 Ejercicio 2 calcula el intervalo de confianza al \\(95\\%\\) de la media de una variable normal con varianza conocida \\(\\sigma^2=9\\) y \\(\\bar{x}=22\\), usando una muestra de tamaño \\(36\\). (R:(21.02, 22.97)) 13.10.0.3 Ejercicio 3 Este año, \\(17\\) de \\(1000\\) de pacientes con influenza desarrollaron complicaciones. Calcular el intervalo de confianza al \\(99\\%\\) para la proporción de complicaciones. (R:(0.009, 0.031)) El año anterior \\(2\\%\\) presentó complicaciones. ¿Podemos decir con \\(99\\%\\) de confianza que este año hay una caída significativa en las complicaciones de la influenza? (R:No) 13.10.0.4 Ejercicio 4 ¿Cuál es el intervalo de confianza para la varianza poblacional de una variable normal si tomamos una muestra aleatoria de tamaño \\(n=10\\) y observamos una varianza muestral de \\(0.5\\)? (R:(0.236, 1.666)) 13.11 Práctica Carga datos de misofonía https://alejandro-isglobal.github.io/SDA/data/data_0.txt Calcula el intervalo de confianza para la media de las medidas cefalométricas. (“Angulo_convexidad”, “protusion.mandibular”, “Angulo_cuelloYtercio”, “Subnasal_H”) Calcula el intervalo de confianza para la proporción de misofonía (“Misofonia.dic”) y depresión (“depresion.dic”). Calcula el intervalo de confianza para la varianza de la edad (“Edad”). ¿Cuál es el intervalo de confianza para la desviación estándar de la población? "],["contraste-de-hipótesis.html", "Chapter 14 Contraste de hipótesis 14.1 Objetivo 14.2 Hipótesis 14.3 Contraste de hipótesis 14.4 Caso 1 (para la media con varianza conocida) 14.5 Caso 2 (para la media con varianza desconocida) 14.6 Caso 3 (para proporciones) 14.7 Caso 4 (para la varianza) 14.8 Errores en la prueba de hipótesis 14.9 Ejercicios", " Chapter 14 Contraste de hipótesis 14.1 Objetivo En este capítulo estudiaremos pruebas de hipótesis de medias y proporciones. Definiremos cuál es la hipótesis nula y la alternativa y cómo utilizar los datos para elegir entre ambas. También presentaremos la prueba de hipótesis de varianzas y los errores que se cometen cuando se prueba una hipótesis. Estos errores se conocen como falsos positivos y falsos negativos. 14.2 Hipótesis Cuando llevamos a cabo experimentos aleatorios, a menudo queremos probar si los cambios que hacemos del experimento tienen un efecto real. Queremos, por ejemplo, saber si hemos podido influenciar el experimento. O, si hemos sometido el experimento a una nueva condición, queremos saber si esa condición afecta los resultados del experimento. Usualmente tenemos una idea de cómo deberían ser los datos cuando estos cambios no están presentes. Dado que los resultatos del experimento son aleatorios con y sin el cambio ¿cómo podemos diferenciar entre aplicar el cambio o no? La estrategia es formular el cambio o la nueva condición en términos de los valores que pueden tomar los parámetros de los modelos probabilísticos del experimento aleatorio. Entonces, usamos las observaciones del experimento aleatorio bajo la nueva condición para estimar el parámetro y ver su posible cambio. Ejemplos (Neumáticos) Los fabricantes de neumáticos quieren saber si la vida media de los neumáticos que producen es de al menos 20000 km. Intentemos traducir su interés en términos estadísticos. Imaginemos un experimento alatorio que consiste en medir cuanto dura la vida útil de un neumático en particular. Por lo tanto a los fabricantes les interesa saber si la media de la vida útil de un neumático es de al menos 20000 km. Formulemos dos afirmaciones dicotómicas, es decir dos situaciones excluyentes: La vida media de los neumáticos puede ser menos de 20 000 km La vida media de los neumáticos puede ser superior a 20000 km Esto significa que sólo una puede ser verdadera. La pregunta es entonces ¿Cómo podemos usar los datos para decidir entre la situación a. o situación b? Consideremos que \\(\\mu\\) es la media de la distribución de la población. Por lo tanto, las afirmaciones a. y b. también se pueden escribir como \\(H_0: \\mu \\leq 20000km\\) \\(H_1: \\mu &gt; 20000km\\) Donde la afirmación \\(H_0\\) es para la cual los neumáticos no recorren los 20000 km deseados mientras que la declaración \\(H_1\\) es el caso que sí los recorren. \\(H_0\\) y \\(H_1\\) se llaman hipótesis. Definición En estadística, una afirmación (conjetura) sobre la distribución de una variable aleatoria se denomina hipótesis. Las hipótesis generalmente se escriben en dos afirmaciones dicotómicas. La hipótesis nula \\(H_0\\): cuando la conjetura es falsa. Generalmente se refiere al statu quo. Los datos pueden ser explicados por el satus quo. La hipótesis alternativa \\(H_1\\): cuando la conjetura es verdadera. Generalmente se refiere a la hipótesis de investigación. Los datos pueden ser explicados por la alternativa al estatus quo. Ejemplo (Fertilizante) Los desarrolladores de fertilizantes quieren probar si su nuevo producto tiene un efecto real en el crecimiento de las plantas. ¿Cuáles son la hipótesis nula y la alternativa? Siendo \\(\\mu_0\\) el crecimiento medio de las plantas sin fertilizante (conocido) y \\(\\mu\\) el crecimiento medio de las plantas con el fertilizante (desconocido) \\(H_0:\\mu \\leq \\mu_0\\) (El fertilizante puede que no haga nada: status quo) \\(H_1:\\mu &gt; \\mu_0\\) (El fertilizante puede tener el efecto deseado: interés de investigación) Ejemplo (quimioterapia) Las farmacéuticas necesitan saber si una nueva quimioterapia puede curar al 90% de los pacientes con cáncer. Siendo \\(p_0\\) la proporción de pacientes que se curan sin la quimioterapia (conocido) y \\(p\\) la proporción que se curan con la quimioterapia (desconocido) podemos formular las hipótesis \\(H_0:p \\leq p_0\\) (La quimioterapia puede que no cambie la proporción de curados: status quo) \\(H_1: p &gt; p_0\\) (La quimioterapia puede tener el efecto deseado: interés de investigación) Tengamos en cuenta que nuestro nuevo experimento mejorado tiene el parámetro \\(p\\) y queremos saber cómo se compara con el experimento sin ningúna mejora que tiene el parámetro \\(p_0\\). Queremos decidir entre \\(H_0\\) y \\(H_1\\). Hay dos opciones: Rechazamos la hipótesis alternativa \\(H_1\\); es decir, aceptamos la hipótesis nula \\(H_0\\). Aceptamos la hipótesis alternativa \\(H_1\\) (nuestro interés); es decir, rechazamos la hipótesis nula \\(H_0\\). 14.3 Contraste de hipótesis Resumamos los diferentes casos, formas y tipos de las pruebas de hipótesis. Después discutiremos cada caso con un ejemplo particular. Las hipótesis se pueden probar o decidir utilizando intervalos de confianza. Por lo tanto, vamos a probar hipótesis en los cuatro casos que vimos para los intervalos de confianza, a saber: Caso 1: Prueba de hipótesis para la media \\(\\mu\\), cuando \\(X \\rightarrow N(\\mu, \\sigma^2)\\) y sabemos \\(\\sigma\\) Caso 2: Prueba de hipótesis para la media \\(\\mu\\), cuando \\(X \\rightarrow N(\\mu, \\sigma^2)\\) y no sabemos \\(\\sigma\\) Caso 3: Prueba de hipótesis para la proporción \\(p\\) cuando \\(X \\rightarrow Bernoulli(p)\\) y tanto \\(np\\) como \\(n(1-p)\\) \\(&gt; 5\\). Caso 4: Prueba de hipótesis para la varianza \\(\\sigma^2\\), cuando \\(X \\rightarrow N(\\mu, \\sigma^2)\\) Hay tres formas de probar las hipótesis: Por medio de intervalos de confianza usando una zona de rechazo Con un \\(pvalor\\). Las tres opciones son equivalentes. Finalmente, hay tres tipos de hipótesis que podemos probar: de dos colas de cola superior de cola inferior 14.4 Caso 1 (para la media con varianza conocida) Un contraste de hipótesis de dos colas es de la forma \\(H_0:\\mu = \\mu_0\\) (status quo) \\(H_1:\\mu \\neq \\mu_0\\) (interés de investigación) Este contraste se llama de dos colas porque la hipótesis alternativa \\(H_1\\) requiere que la media \\(\\mu\\) sea o menor o mayor que \\(\\mu_0\\). Esta hipótesis se puede probar en diferentes casos. El caso 1 es cuando \\(X\\) es una variable normal, y conocemos el valor de \\(\\sigma^2\\) 14.4.1 Prueba de hipótesis con un intervalo de confianza Para el caso 1 el intervalo de confianza en \\(95\\%\\) es \\[(l,u)=(\\bar{x}-z_{0.025} \\frac{\\sigma}{\\sqrt{n}}, \\bar{x}+z_{0.025} \\frac{\\sigma}{ \\sqrt{n}})\\] Criterios de decisión: Si el intervalo de confianza contiene la hipótesis nula \\[\\mu_0\\in (l,u)\\] entonces aceptamos \\(H_0\\) con una confianza de \\(95\\%\\). Si el intervalo de confianza no contiene la hipótesis nula\\[\\mu_0\\notin (l,u)\\] entonces rechazamos \\(H_0\\) con \\(95\\%\\) de confianza. Ejemplo (Cables) Compramos \\(8\\) cables a una empresa fabricante que afirma que se rompen con media de \\(\\mu_0=13\\) Toneladas. No estamos seguros y tal vez queramos decidir si los cables se rompen en promedio a \\(13\\) Toneladas o no. Formulamos el contraste de hipótesis \\(H_0:\\mu = 13\\) (Los cables pueden romperse como afirma el fabricante: status quo) \\(H_1:\\mu \\neq 13\\) (Los cables pueden no romperse como afirma el fabricante: interés de investigación) Como no sabemos qué es cierto, empecemos por suponer que el fabricante tiene razón y que los cables realmente satisfacen la hipótesis nula y, por lo tanto, \\(X \\rightarrow N(\\mu=13, \\sigma^2=0.35^2)\\) Sabemos \\(\\sigma^2=0.35^2\\) Para decidir entre \\(H_0\\) o \\(H_1\\), realizamos una muestra de \\(8\\) experimentos aleatorios: Cargamos un cable hasta que se rompe y registramos la carga de rotura. Estos son los resultados. ## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747 El intervalo de confianza si el fabricante tiene razón, es decir, si la hipótesis nula es verdadera, es \\[(l,u)=(\\bar{x}-z_{0.025} \\frac{\\sigma}{\\sqrt{n}}, \\bar{x}+z_{0.025} \\frac{\\sigma}{ \\sqrt{n}})= (12.97,13.45)\\] El intervalo de confianza nos dice que podemos tener un \\(95\\%\\) de confianza en que la media de la carga de rotura \\(\\mu\\) de los cables que probamos está en el intervalo. No sabemos el valor real de \\(\\mu\\) pero entendemos que un valor pausible podría ser \\(\\mu=13\\) toneladas, ya que el intervalo atrapó \\(\\mu_0\\) \\[\\mu_0\\in (12.97,13.45)\\] Nuestra conclusión es aceptar que \\(H_0\\) podría haber producido nuestro intervalo observado. También decimos que los datos respaldan la afirmación del fabricante. Más técnicamente, decimos que no rechazamos la hipótesis nula \\(H_0\\). 14.4.2 Prueba de hipótesis con zonas de aceptación/rechazo Una forma equivalente de probar la hipótesis es ver si nuestro conjunto de observaciones es raro o común si la hipótesis nula fuera verdadera. Recordemos el contraste de hipótesis \\(H_0:\\mu = \\mu_0\\) (status quo) \\(H_1:\\mu \\neq \\mu_0\\) (interés de investigación) Para probar la hipótesis con una zona de rechazo calculamos la estadística estandarizada \\[Z=\\frac{\\bar{X}-\\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\] Cuando la hipótesis nula es verdadera. Tengamos en cuenta que estamos estandarizando con \\(\\mu_0\\) (la hipótesis nula). Este es el error estandarizado que cometemos al estimar \\(\\mu_0\\) con \\(\\bar{X}\\). Luego vemos si el valor observado de \\(Z\\) está dentro del intervalo \\[(-z_{0.025}, z_{0.025})\\] Recordemos que este intervalo define los valores más comunes de \\(Z\\) ya que \\[P(-z_{0.025} \\leq Z \\leq z_{0.025})=0.95\\] El intervalo \\((-z_{0.025}, z_{0.025})\\) se denomina intervalo de aceptación de \\(H_0\\) con un nivel de confianza de \\(95\\%\\). Criterios de decisión: Si la estadística observada \\(z_{obs}\\) bajo la hipótesis nula está en la región de aceptación \\[z_{obs} \\in (-z_{0.025}, z_{0.025})\\] entonces aceptamos \\(H_0\\) con una confianza de \\(95\\%\\). Si la estadística observada \\(z_{obs}\\) bajo la hipótesis nula no está en la región de aceptación \\[z_{obs} \\notin (-z_{0.025}, z_{0.025})\\] entonces rechazamos \\(H_0\\) con una confianza de \\(95\\%\\). La región \\((-z_{0.025}] \\cup[z_{0.025})\\) se denomina zona de rechazo. Este criterio de decición nos dice que tan común es nuestra muestra aleatoria si la hipótesis nula es verdad. Ejemplo (Cables) Si \\(H_0\\) es verdadera entonces \\(\\bar{X}\\) es el estimador de \\(\\mu_0\\) y la estadística estandarizada \\[Z=\\frac{\\bar{X}-13}{\\frac{0.35}{\\sqrt{8}}} \\rightarrow N(0,1)\\] es el error en estimación de \\(\\mu_0\\) que cometemos cuando realizamos la estimación. Debido a que estamos en caso 1, \\(Z\\) es estándar normal. Para nuestros datos, el error observado estandarizado está en la región de aceptación \\[z_{obs}=\\frac{\\bar{x}-13}{\\frac{0.35}{\\sqrt{8}}}=1.7187 \\in (-z_{0.025}, z_{0.025})\\] Concluimos que nuestro error observado es típico al \\(95\\%\\) cuando la hipótesis nula \\(\\mu_0\\) es verdadera. Por lo tanto, nuevamente aceptamos que los datos son consistentes con la afirmación del fabricante. También decimos que no rechazamos \\(H_0\\). 14.4.3 Prueba de hipótesis con un P valor También podemos contrastar la hipótesis de dos colas calculando la probabilidad de que el promedio de una nueva muestra aleatoria sea aún más raro que el promedio que acabamos de observar. Debido a que estamos en el caso 1, sabemos que la estadística estandarizada \\(Z\\) es una variable normal estándar, entonces definimos el \\(pvalor\\) como \\[pvalor = P(Z \\leq -z_{obs}) + P(z_{obs} \\leq Z) = 2 (1-\\phi(|z_{obs}|))\\] Esa es la probabilidad de que cuando tomamos otra muestra del mismo tamaño podamos obtener una observación aún más extrema. Si nuestra observación ya es rara, entonces este valor será pequeño. Criterios de decisión: Si el \\(pvalor\\) observado es \\[pvalor \\geq \\alpha =1-0.95=0.05\\] entonces aceptamos \\(H_0\\) con una confianza de \\(95\\%\\). Si el \\(pvalor\\) observado es \\[pvalor &lt; \\alpha =1-0.95=0.05\\] entonces rechazamos \\(H_0\\) con una confianza de \\(95\\%\\). \\(\\alpha\\) es el nivel de significancia. Nos dice cuánto de la distribución estamos omitiendo y define la región que consideramos como observaciones raras. Recuerdemos que siempre confiamos en nuestros datos. Si la hipótesis nula dice que nuestros datos son una observación rara entonces desconfiamos de la hipótesis nula y la rechazamos. Ejemplo (Cables) Para nuestros datos, la estadística observada \\(z_{obs}=1.718714\\) y su P valor es entonces \\[pvalor=2 (1-\\phi(1.718714))=0.08567\\] R: 2*(1-pnorm(1.718714)) Concluimos que si realizamos una nueva muestra es probable que podamos obtener un resultado más extremo que el que obtuvimos. La hipótesis nula puede tolerar el error observado con una confianza de \\(95\\%\\). Por lo tanto, aceptamos que \\(H_0\\) podría haber producido nuestros datos y decimos nuevamente que es consistente con la afirmación del fabricante. En R todo el contraste de hipótesis se puede realizar con la función z.test de la librería BSDA (que es necesario instalar previamente) install.packages(BSDA) library(BSDA) z.test(c(13.34642, 13.32620, 13.01459, 13.10811, 12.96999, 13.55309, 13.75557, 12.62747), mu=13, sigma.x=0.35) ## ## One-sample z-Test ## ## data: c(13.34642, 13.3262, 13.01459, 13.10811, 12.96999, 13.55309, 13.75557, 12.62747) ## z = 1.7187, p-value = 0.08567 ## alternative hypothesis: true mean is not equal to 13 ## 95 percent confidence interval: ## 12.97015 13.45521 ## sample estimates: ## mean of x ## 13.21268 14.4.4 Hipótesis de la cola superior Es posible que nos interese probar solo el hecho de que la media de nuestro experimento es una media más alta que la media nula. Prueba de cola superior: \\(H_0:\\mu \\leq 13\\) (como mucho los cables se rompen como siempre) \\(H_1:\\mu &gt; 13\\) (los cables se rompen con una carga mayor) Esto se llama de cola superior porque la hipótesis alternativa \\(H_1\\) requiere que la media \\(\\mu\\) sea mayor que \\(\\mu_0\\). Esta hipótesis se puede probar en diferentes casos. El caso 1 es cuando \\(X\\) es una variable normal, y conocemos el valor de \\(\\sigma\\) Criterios de decisión: Intervalo de confianza: Si el intervalo de confianza de cola superior contiene la hipótesis nula \\[\\mu_0\\in (l,u)=(\\bar{x}-z_{0.05} \\frac{\\sigma}{\\sqrt{n}}, \\infty)\\] donde \\(z_{0.05}=\\phi^{-1}(0.95)=\\)qnorm(0.95), entonces aceptamos \\(H_0\\) con una confianza de \\(95\\%\\). Tengamos en cuenta que esta prueba es desde el punto de vista de los datos y que no estamos centrando el intervalo de confianza alrededor de \\(\\bar{x}\\), sino que estamos dejando todo el \\(5\\%\\) que corresponde a los casos raros a la izquierda del promedio. Por lo tanto, nos estamos preguntando si la hipótesis nula es menor que el promedio. Región de rechazo/aceptación: Si la estadística observada \\(z_{obs}\\) bajo la hipótesis nula está en la región de aceptación \\[z_{obs}=\\frac{\\bar{x}-\\mu_0}{\\frac{\\sigma}{\\sqrt{n}}} \\in (-\\infty, z_{0.05})\\] entonces aceptamos \\(H_0\\) con una confianza de \\(95\\%\\). Esta prueba es desde el punto de vista de la hipótesis nula. Dejamos todo el \\(5\\%\\) que corresponde a los promedios raros a la derecha de la hipótesis nula y, por lo tanto, nos preguntamos si el promedio es claramente mayor que la hipótesis nula. \\(pvalor\\): Si el P valor de cola superior \\[pvalor= 1-\\phi(z_{obs})\\] 1-pnorm(zobs) es mayor que \\(\\alpha=1-0.95=0.05\\) \\[pvalor \\geq \\alpha =0.05\\] entonces aceptamos \\(H_0\\) con una confianza del \\(95\\%\\). Ten en cuenta que esta prueba es nuevamente desde el punto de vista de la hipótesis nula. Nos estamos preguntando: si tuviéramos que tomar otro promedio, ¿cuál es la probabilidad de que sea mayor que el observado? Ejemplo (Cables) En el ejemplo de los cables, nos puede interesar solo en el caso de que los cables sean una versión mejorada de lo que afirma el fabricante. Por lo tanto, la hipótesis de cola superior es \\(H_0:\\mu \\leq 13\\) (Los cables pueden romperse como máximo como afirma el fabricante: status quo) \\(H_1:\\mu &gt; 13\\) (Los cables pueden romperse al menos como afirma el fabricante: interés de investigación) Probaremos la cola superior de la distribución. Para los datos que discutimos antes, de tal forma que rechazamos \\(H_0\\) a \\(95\\%\\) de confianza debido a cualquiera de los tres contrastes equivalentes: El intervalo de confianza de cola superior no contiene la hipótesis nula \\(\\mu_0=13\\) \\[\\mu_0=13 \\notin (\\bar{x}-z_{0.05} \\frac{\\sigma}{\\sqrt{n}}, \\infty)=(13.00914, \\infty)\\] donde \\(z_{0.05}=\\)qnorm(0.95)=1.644854 Tenemos que la región de aceptación para \\(H_0\\) es: \\[(-\\infty, z_{0.05})=( -\\infty, 1.644854)\\] y que el error estandarizado observado no está en la región \\[z_{obs} = \\frac{13.21268-13}{\\frac{0.35}{\\sqrt{8}}}=1.7187 \\notin ( -\\infty, 1.644854)\\] El \\(pvalor\\) de cola superior es menor que \\(\\alpha=0.05\\) \\[pvalor=1-\\phi(1.7187)=0.04283451 &lt;0.05\\] donde \\(pvalor=\\)1-pnorm(1.7187). La prueba de hipótesis se puede realizar en R, especificando el parámetro “alternative” como “greater”: z.test(c(13.34642, 13.32620, 13.01459, 13.10811, 12.96999, 13.55309, 13.75557, 12.62747), mu=13, alternative=&quot;greater&quot;, sigma.x=0.35) ## ## One-sample z-Test ## ## data: c(13.34642, 13.3262, 13.01459, 13.10811, 12.96999, 13.55309, 13.75557, 12.62747) ## z = 1.7187, p-value = 0.04283 ## alternative hypothesis: true mean is greater than 13 ## 95 percent confidence interval: ## 13.00914 NA ## sample estimates: ## mean of x ## 13.21268 14.5 Caso 2 (para la media con varianza desconocida) Un contraste de hipótesis de dos colas de la forma \\(H_0:\\mu = \\mu_0\\) (status quo) \\(H_1:\\mu \\neq \\mu_0\\) (interés de investigación) se puede probar cuando no sabemos \\(\\sigma^2\\) a partir del caso 2, es decir, cuando \\(X\\) es una variable normal, \\(X \\rightarrow N(\\mu, \\sigma^2)\\), y no sabemos el valor de \\(\\sigma^2\\) Recordemos que en este caso el error estandarizado con respecto a la desviación estándar de la muestra \\(S\\) \\[T=\\frac{\\bar{X}-\\mu}{\\frac{S}{\\sqrt{n}}}\\] Sigue una distribución \\(t\\) con \\(n-1\\) grados de libertad. Por lo tanto, podemos aplicar los tres criterios como en el caso 1 pero reemplazando \\(s\\) por \\(\\sigma\\) y \\(Z\\) por \\(T\\). Criterios de decisión: Intervalo de confianza: Si el intervalo de confianza contiene la hipótesis nula \\[\\mu_0\\in (l,u)=(\\bar{x}-t_{0.025,n-1} \\frac{s}{\\sqrt{n}}, \\bar{x}+t_{0.025, n-1} \\frac{s}{\\sqrt{n}})\\] entonces aceptamos \\(H_0\\) con una confianza de \\(95\\%\\). Región de rechazo/aceptación: Si la estadística observada \\(t_{obs}\\) bajo la hipótesis nula está en la región de aceptación \\[t_{obs}=\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}} \\in (-t_{0.025}, t_{0.025})\\] donde \\(t_{0.025}=F_t^{-1}(0.975, n-1)=\\)qt(0.975, n-1), entonces aceptamos \\(H_0\\) con \\(95 \\%\\) confianza. \\(pvalor\\): Si el \\[pvalor= 2 (1-F_t(|t_{obs}|))\\] 2*(1-pt(abs(tobs), n-1)) es \\(pvalor \\geq \\alpha =1-0.95=0.05\\) entonces aceptamos \\(H_0\\) con una confianza de \\(95\\%\\). Si se cumple un criterio se cumplen los otros. En el caso de no se cumplan entonces no aceptamos \\(H_0\\) y la rechazamos. Ejemplo (Cables) Para el contraste de hipótesis para la carga de rotura de los cables \\(H_0:\\mu = 13\\) \\(H_1:\\mu\\neq 13\\) Supondremos que la rotura de los cables se distribuye normalmente \\(X \\rightarrow N(\\mu=13, \\sigma^2=?)\\) No sabemos \\(\\sigma^2\\) Habiendo obtenido la muestra ## [1] 13.34642 13.32620 13.01459 13.10811 12.96999 13.55309 13.75557 12.62747 aceptamos \\(H_0\\) con confianza de \\(95\\%\\) debido a cualquiera de los siguientes contrastes: El intervalo de confianza \\[(\\bar{x}-t_{0.025, n-1} \\frac{s}{\\sqrt{n}}, \\bar{x}+t_{0.025, n-1} \\frac{s}{ \\sqrt{n}})=(12.91409, 13.51127)\\] contiene \\(H_0:\\mu=13\\). La región de aceptación para \\(H_0\\) es: \\[(-t_{0.025,7}, t_{0.025,7})=( -2.36, 2.36)\\] y el error estandarizado observado de \\(H_0\\) es \\[t_{obs} = \\frac{13.21268-13}{\\frac{0.3571565}{\\sqrt{8}}}=1.6843\\] el cual está dentro de la región de aceptación. El \\[pvalor=2(1-F_{t,7}(1.6843))=0.136\\] es mayor que \\(\\alpha=0.05\\). El \\(pvalor\\) se calcula R como 2*(1-pt(1.6843,7)) En R estos contrastes se realizan con la función t.test: t.test(c(13.34642, 13.32620, 13.01459, 13.10811, 12.96999, 13.55309, 13.75557, 12.62747), mu=13) ## ## One Sample t-test ## ## data: c(13.34642, 13.3262, 13.01459, 13.10811, 12.96999, 13.55309, 13.75557, 12.62747) ## t = 1.6843, df = 7, p-value = 0.136 ## alternative hypothesis: true mean is not equal to 13 ## 95 percent confidence interval: ## 12.91409 13.51127 ## sample estimates: ## mean of x ## 13.21268 Ejemplo (NaCl) \\(11.6 g\\) de NaCl se disuelven en \\(100 g\\) de agua, correpondiendo a una concentración molar de \\(1.92 mol/L\\). Diseñamos un proceso para eliminar la sal de esta concentración y obtenemos los siguientes resultados ## [1] 1.716 1.889 1.783 1.849 1.891 Queremos probar con una confianza del \\(95\\%\\) si el proceso cambia la concentración de sal en cualquier dirección. Por lo tanto, proponemos una hipótesis de dos colas: \\(H_0:\\mu=1.92\\) \\(H_1:\\mu\\neq 1.92\\) Supondremos que \\(X\\) es normal y que no conocemos la varianza \\(\\sigma^2\\). Por lo tanto, estamos en el caso 2 que probamos con una prueba t. t.test(c(1.716, 1.889, 1.783, 1.849, 1.891), mu=1.92, alternative = &quot;two.sided&quot;) ## ## One Sample t-test ## ## data: c(1.716, 1.889, 1.783, 1.849, 1.891) ## t = -2.8038, df = 4, p-value = 0.04862 ## alternative hypothesis: true mean is not equal to 1.92 ## 95 percent confidence interval: ## 1.732122 1.919078 ## sample estimates: ## mean of x ## 1.8256 De tal forma que rechazamos la hipótesis nula: el \\(pvalor\\) es menor que \\(0.05\\) y el intervalo de confianza no contiene a \\(\\mu_0=1.92\\). Concluimos pues que el proceso altera la concentración de sal. 14.5.1 Hipótesis la cola inferior Si solo estamos interesados en el caso de que podamos eliminar la sal de la concentración, entonces proponemos una hipótesis de cola inferior: \\(H_0:\\mu \\geq 1.92\\) (Después del proceso de desalinización la concentración de sal es al menos la inicial: status quo) \\(H_1:\\mu &lt; 1.92\\) (Después del proceso de desalinización la concentración es menor a la inicial: interés de investigación) Tengamos en cuenta que la cola inferior viene dada por la alternativa \\(H_1\\). Queremos probar que la concentración promedio después del proceso es menor que la concentración inicial. Los criterios de contraste son los mismos que para los otros tipos de hipótesis. Para este tipo de hipótesis, aceptaremos la hipótesis nula si \\(\\mu_0\\) está en el intervalo de confianza: \\[\\mu_0\\in (l,u)=(-\\infty, \\bar{x}+t_{0.05,n-1} \\frac{s}{\\sqrt{n}})\\] o \\(t_{obs}\\) está en la región de aceptación: \\[t_{obs}\\in (t_{0.05,n-1}, \\infty)\\] o el \\(pvalor\\) de la cola inferior \\[pvalor=F_t(t_{obs},n-1)\\] es superior a \\(\\alpha=0.05\\) Si no se complen estos criterios, rechazamos \\(H_0\\) y aceptamos la hipótesis alternativa \\(H_1\\). Ejemplo (NaCl) Para el contraste de cola inferior \\(H_0:\\mu\\geq 1.92\\) \\(H_1:\\mu &lt; 1.92\\) Podemos suponer que la concentración es normal y que no sabemos \\(\\sigma^2\\). Por lo tanto, estamos en el caso 2 para el cual solo necesitamos cambiar el argumento “alternative” en la función t.test. Así pues rechazamos la hipótesis nula. t.test(c(1.716, 1.889, 1.783, 1.849, 1.891), mu=1.92, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: c(1.716, 1.889, 1.783, 1.849, 1.891) ## t = -2.8038, df = 4, p-value = 0.02431 ## alternative hypothesis: true mean is less than 1.92 ## 95 percent confidence interval: ## -Inf 1.897376 ## sample estimates: ## mean of x ## 1.8256 El resultado es mas significativo que el de dos colas, ya que el \\(pvalor\\) es menor que antes. Concluimos con mas confianza que el proceso reduce la concentración de sal. Ejemplo 2 (soporífero) En algunos casos, no estamos seguros del valor numérico de la hipótesis a contrastar, pero sabemos que queremos mejorar el valor de un parámetro en una nueva condición. En su artículo original, Gosset al proponer la distribución t, analizó el efecto de dos medicamentos soporíferos. A 10 individuos se les dio el soporífero 1 y se anotaron las horas adicionales dormidas bajo tratamiento, con una media de \\(0.75\\) medicine1 &lt;- c(0.7,-1.6,-0.2,-1.2,-0.1,3.4,3.7,0.8,0,2) medicine1 ## [1] 0.7 -1.6 -0.2 -1.2 -0.1 3.4 3.7 0.8 0.0 2.0 Los mismos 10 individuos recibieron el soporífero 2 y anotaron las horas adicionales dormidas bajo tratamiento, con una media de \\(2.33\\) medicine2 &lt;- c(1.9,0.8,1.1,0.1,-0.1,4.4,5.5,1.6,4.6,3.4) medicine2 ## [1] 1.9 0.8 1.1 0.1 -0.1 4.4 5.5 1.6 4.6 3.4 La hipótesis científica era que el soporífero 2 era mejor que el soporífero 1. Para cada individuo, Gosset calculó la diferencia entre los tratamientos. Tomando \\(X\\) como la diferencia entre tratamientos, esta fue la muestra observada para \\(X\\) x &lt;- medicine2-medicine1 x ## [1] 1.2 2.4 1.3 1.3 0.0 1.0 1.8 0.8 4.6 1.4 encontrando un promedio de ganancia de tratamiento del soporífero 2 con respecto al soporífero 1 de \\(1.58\\), y \\(s=1.229995\\) La pregunta científica se puede establecer como prueba t pareada de cola superior: \\(H_0:\\mu \\leq \\mu_0=0\\) (sin diferencia de tratamiento: \\(\\mu_2-\\mu_1=0\\)) \\(H_1:\\mu &gt; \\mu_0= 0\\) (tratamiento 2 superior al tratamiento 1: \\(\\mu_2-\\mu_1&gt;0\\)) Donde \\(\\mu\\) es la media de las diferencias entre tratamientos, y la hipótesis nula dice que no hay diferencia. Si suponemos que \\(X\\) es normal y no sabemos \\(\\sigma^2\\) entonces estamos en el caso 2. El error estandarizado es: \\[T=\\frac{\\bar{X}}{\\frac{S}{\\sqrt{n}}}\\] y su observación \\[t_{obs}=\\frac{\\bar{x}}{\\frac{s}{\\sqrt{n}}}\\] que también se conoce como la relación señal a ruido. Podemos probar la hipotesis sobre la diferencia \\(X=medicina_1-medicina_2\\) t.test(x,alternative=&quot;greater&quot;) ## ## One Sample t-test ## ## data: x ## t = 4.0621, df = 9, p-value = 0.001416 ## alternative hypothesis: true mean is greater than 0 ## 95 percent confidence interval: ## 0.8669947 Inf ## sample estimates: ## mean of x ## 1.58 Esto también se conoce como una prueba t pareada, donde introducimos cada condición por separado y afirmamos que las observaciones están pareadas t.test(medicine2, medicine1, paired = TRUE, alternative=&quot;greater&quot;) ## ## Paired t-test ## ## data: medicine2 and medicine1 ## t = 4.0621, df = 9, p-value = 0.001416 ## alternative hypothesis: true mean difference is greater than 0 ## 95 percent confidence interval: ## 0.8669947 Inf ## sample estimates: ## mean difference ## 1.58 14.5.2 Prueba de hipótesis con n grande y cualquier distribución En muchas ocasiones, \\(X\\) no se distribuye normalmente pero si podemos tomar muestras grandes \\(n \\ge 30\\) entonces podemos usar el CLT: De tal forma que el error estandarizado de la hipótesis nula se puede aproximar a una distribución estándar \\[Z=\\frac{\\bar{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\rightarrow N(0,1)\\] Por lo tando procedemos como en el caso 1. Si no se desconoce \\(\\sigma\\), lo reemplazamos por su estimación \\(s\\) y procedemos como en el caso 2 usando la estadística t \\[T=\\frac{\\bar{X}-\\mu_0}{\\frac{S}{\\sqrt{n}}}\\] 14.6 Caso 3 (para proporciones) Si nuestro experimento aleatorio es un ensayo de Bernoulli \\(X \\rightarrow Bernoulli(p)\\), podemos formular contrastes de hipótesis para la probabilidad \\(p\\) de un evento en el ensayo. Considera una hipótesis de cola superior \\(H_0: p \\leq p_0\\) (status quo) \\(H_1: p&gt; p_0\\) (interés de investigación) En este caso 3, podemos probar el valor de la proporción si \\(X\\) es un ensayo de Bernoulli, y \\(np\\), \\(n(1-p)\\) son ambos mayores que \\(5\\), lo que nos permite aplicar el teorema central del límite. Recuerdemos que si tomamos una muestra de \\(n\\) ensayos de Bernoulli \\((1,0,1,...0)\\), \\[\\bar{X}=\\frac{1}{n}\\sum_{i=1 }^n X_i\\] es la frecuencia relativa de “unos” en la muestra. Este es un estimador de \\(p\\). Si asumimos que la hipótesis nula es verdadera entonces \\[X \\rightarrow Bernoulli(p_0)\\] y el error estandarizado que cometemos cuando estimamos \\(p_0\\) con \\(\\bar{X}\\) es \\[Z=\\frac{\\bar{X}-p_0}{\\frac{\\sqrt{p_0(1-p_0)}}{\\sqrt{n}}} \\rightarrow N(0,1)\\] \\(\\sigma=\\sqrt{p_0(1-p_0)}\\) es la desviación estándar de \\(X\\), cuando la hipótesis nula es verdadera: \\(V(X)=\\sigma^2=p_0(1-p_0)\\). Con esta estadística \\(Z\\), podemos aceptar o rechazar la hipótesis nula usando cualquiera de los tres riterios de decisión. Ejemplo (mejora de procesos) Podemos estar satisfechos con un nuevo proceso si el \\(90\\%\\) de las veces mejoramos el proceso anterior. Si tomamos una muestra de \\(200\\) nuevos procesos y encontramos que \\(188\\) veces mejoramos el proceso anterior, ¿podemos estar satisfechos con el nuevo proceso con una confianza de \\(95\\%\\)? Para esto formulamos un contraste de hipótesis de cola superior para la hipótesis nula \\(p_0=0.9\\). Por lo tanto, las hipótesis nula y alternativa son \\(H_0: p \\leq p_0=0.9\\) (No satisfactorio) \\(H_1: p&gt; p_0=0.9\\) (Satisfactorio) Suponemos que si la hipótesis nula es verdadera entonces la distribución de un experimento aleatorio es \\[X \\rightarrow Bernoulli (p_0)\\] además \\(np_0=180&gt;5\\) y \\(n(1-p_0)=20&gt;5\\) Y por lo tanto podemos aplicar caso 3. Podemos usar cualquiera de los tres criterios para probar la hipótesis. En cualquier caso, rechazaremos \\(H_0\\) con una confianza de \\(95\\%\\) porque: El intervalo de confianza de cola superior para \\(p\\) no incluye \\(p_0\\) \\[p_0=0.9 \\notin (\\bar{x}-z_{0.05}\\big[\\frac{\\bar{x}(1-\\bar{x})}{n} \\big]^{1/2},1)= (0.912,1)\\] El error estandarizado observado bajo la hipótesis nula no está en la región de aceptación \\[z_{obs}= \\frac{\\bar{X}-p_0}{\\big[\\frac{p_0(1-p_0)}{n} \\big]^{1/2}} =\\frac{0.94 -0.90}{\\sqrt{0.00045}}=1.88563 \\notin (-\\infty, z_{0.05})=(-\\infty, 1.644)\\] Nota que para este caso usamos \\(p_0\\) para calcular \\(z_{obs}\\) porque estamos suponiendo que la hipótesis nula es verdad. Por lo tanto conocemos exactamente la media y la varianza de las observaciones. El \\(pvalor\\) de cola superior es menor que \\(\\alpha=0.05\\): \\[pvalor=1-\\phi(1.885618)=0.02967323&lt;0.05\\] en R: 1-pnorm(1.885618) La prueba se puede realizar en R usando la función prop.test. Sin embargo nota que los intervalos de confianza no son idénticos a los dados en el criterio 1, porque ahí suponemos que \\(\\bar{X}\\) es exactamente normal mientras que prop.test usa una aproximación mas adecuada prop.test(188, 200, p=0.9, alternative = &quot;greater&quot; , correct=FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: 188 out of 200, null probability 0.9 ## X-squared = 3.5556, df = 1, p-value = 0.02967 ## alternative hypothesis: true p is greater than 0.9 ## 95 percent confidence interval: ## 0.9060689 1.0000000 ## sample estimates: ## p ## 0.94 14.7 Caso 4 (para la varianza) En muchos casos, se realizan experimentos para probar valores específicos de la dispersión de datos. Como al cumplir con estándares de diseño donde las medidas deben estar entre ciertos valores. cuando se aplican diferentes tratamientos a diferentes grupos, queremos ver la dispersión de los resultados entre los grupos. Un contraste de hipótesis de dos colas para la varianza es de la forma \\(H_0:\\sigma^2 = \\sigma^2_0\\) (status quo) \\(H_1:\\sigma^2 \\neq \\sigma^2_0\\) (interés de investigación) Esta hipótesis para \\(\\sigma^2\\) (caso 4) y se puede probar cuando \\(X\\) es una variable normal Recuerda que si tomamos una muestra de \\(n\\) ensayos de Bernoulli \\((1,0,1,...0)\\), \\(S^2=\\frac{1}{n-1}\\sum_{i=1 }^n (X_i-\\bar{X})^2\\) es la varianza de la muestra. Este es un estimador de \\(\\sigma^2\\). Si asumimos que la hipótesis nula es verdadera, entonces \\(X \\rightarrow N(\\mu, \\sigma_0)\\) y la proporción de error que cometemos cuando estimamos \\(\\sigma^2\\) con \\(s^2\\) es \\[W=\\frac{(n-1)S^2}{\\sigma_0^2}\\] cuando \\(W=1\\) no cometemos ningún error. \\(W\\) tiene una distribución \\(\\chi^2\\) (chi-cuadrado) con n-1 grados de libertad. \\[W \\rightarrow \\chi^2(n-1)\\] Con \\(W\\), podemos aceptar o rechazar la hipótesis nula usando cualquiera de los tres criterios de decisión. Ejemplo (semiconductor) La producción de un chip semiconductor está regulada por un proceso que requiere que el espesor de una capa en particular no varíe en más de \\(\\sigma_0=0.6mm\\), de su media de \\(25mm\\). Para llevar el control del proceso cada cierto tiempo se toma una muestra de \\(20\\) especímenes. En una ocasión se toma una muestra del espesor de 20 semiconductores. ## [1] 24.51239 24.79975 26.35608 25.06134 25.11248 26.49211 25.40100 23.89940 ## [9] 24.40244 24.61227 26.06495 25.31304 25.34867 25.09629 24.51642 26.55461 ## [17] 25.43313 23.28904 25.61018 24.58867 La desviación estándar estimada para estos datos es \\(s=0.8462188\\). ¿Estaba el proceso fuera de control con una confianza de \\(99\\%\\) y debería detenerse? sd(x) ## [1] 0.8462188 Por lo tanto, queremos contrastar las hipótesis de la cola superior \\(H_0:\\sigma^2 \\leq \\sigma^2_0=0.6^2\\) (El proceso está bajo control) \\(H_1:\\sigma^2 &gt; \\sigma^2_0=0.6^2\\) (El proceso está fuera de control) Probemos la hipótesis usando la región de aceptación. La estadística de contraste son \\[W=\\frac{(n-1)S^2}{\\sigma_0^2} \\rightarrow \\chi^2(n-1)\\] y el límite de umbral \\(\\alpha=0.01=0-0.99\\). Por lo tanto, la región de aceptación \\(P(W\\leq \\chi^2_{0.01,19})=0.99\\) es \\[(0, \\chi^2_{0.01,19})=(0,36.19)\\] En R: \\(\\chi^2_{0.01,19}=\\)qchisq(0.99,19)\\(= 36.19\\) Para nuestros datos, la proporción de error estandarizado observada es: \\[w_{obs}=\\frac{19 (0.8462188)^2}{0.60^2}=37.79344\\] Que cae fuera de la región de aceptación. \\[w_{obs}=37.79344\\notin (0,36.19)\\] Por lo tanto, rechazamos la hipótesis nula y concluimos que sí! el proceso está fuera de control. Si nosotros, alternativamente, calculamos el \\(pvalor\\) de cola superior \\[pvalor=1-F_{\\chi^2,19}(37.79344)= 0.006\\] vemos que es menor que \\(\\alpha=0.01\\) y rechazamos la hipótesis nula. R: 1-pchisq(37.79344, 19) Para probar la hipótesis, podemos usar la función varTest de la biblioteca EnvStats. Si la biblioteca no está instalada, primero ejecuta install.packages(EnvStats) y después library(EnvStats); varTest(x, sigma.squared = 0.6^2, alternative = &quot;greater&quot;, conf.level = 0.99) ## ## Results of Hypothesis Test ## -------------------------- ## ## Null Hypothesis: variance = 0.36 ## ## Alternative Hypothesis: True variance is greater than 0.36 ## ## Test Name: Chi-Squared Test on Variance ## ## Estimated Parameter(s): variance = 0.7160863 ## ## Data: x ## ## Test Statistic: Chi-Squared = 37.79344 ## ## Test Statistic Parameter: df = 19 ## ## P-value: 0.006304231 ## ## 99% Confidence Interval: LCL = 0.3759412 ## UCL = Inf También vemos que \\(\\sigma_0^2=0.6^2=0.36\\) no está en el intervalo de confianza, por lo que se rechaza la hipótesis nula. 14.8 Errores en la prueba de hipótesis El resultado de una prueba de hipótesis de cola superior puede ser rechazar la hipótesis nula: \\[H_0: \\mu\\leq\\mu_0\\] cuando \\(H_0\\) realmente es verdad. Debemos tener en cuenta que la decisión se toma con base en los datos. Bien puede ser que el estadístico observado haya caido, por aleatoridad, lejos de la hipótesis nula, en la zona de rechazo de \\(H_0\\) aún cuando esta sea verdad. La probabilidad de rechazar \\(H_0\\) cuando es verdadera es precisamente el nivel de significación estadística \\(\\alpha\\). Llamamos a esta probabilidad la probabilidad de cometer un error de tipo 1. Tomando el ejemplo para el caso 1, un test de cola superior, y una confianza del \\(95\\%\\) tenemos que \\[\\alpha = P(Z&gt; z_{0.05})=0.05\\] donde \\(z_{0.05}=\\phi^{-1}(0.95)=\\) qnorm(0.95)=1.644 Un error del tipo 1 también se llama un falso positivo porque nuestro interés de investigación está en \\(H_1\\). Cuando rechazamos \\(H_0\\), aceptamos \\(H_1\\) y decimos que nuestro test es positivo. Aceptar \\(H_1\\) se traduce en anunciar un descubrimiento, por lo que el error de tipo 1 es anunciar un descubrimiento cuando este no existe: lo hemos hecho porque los datos nos lo han sugerido. Existe otro tipo de error. El resultado de una prueba de hipótesis de cola superior puede ser aceptar la hipótesis nula: \\[H_0: \\mu\\leq\\mu_0\\] cuando \\(H_0\\) realmente no es verdad. En este caso, puede ser que el estadístico observado haya caido, por aleatoridad, cerca de la hipótesis nula, en la zona de aceptación de \\(H_0\\), cuando realmente \\(H_1\\) es verdad. Si averiguaramos de alguna forma que, por ejemplo, \\(\\mu\\) realmente tiene un valor \\(\\mu_1\\) entonces la hipóteisis alternativa sería exactamente: \\[H_1: \\mu=\\mu_1\\] Si \\(H_1\\) es realemente verdadera (línea roja, que no sabemos cuando realizamos la prueba de hipótesis) entonces los estadísticos observados son realmente variables aleatorias \\(Y\\) que se distribuyen con media (caso 1) \\[E(Y)=\\frac{\\mu_1-\\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\] y en su mayoría caerán cerca de este valor, por lo tanto, en la zona de rechazo de \\(H_0\\), dando validez a la prueba de hipótesis. Sin embargo existen casos en que el estadístico observado cae en la zona de acepación de \\(H_0\\) por aleatoreidad, apesar de que los estadísticos los produce \\(H_1\\). En estos casos aceptamos \\(H_0\\) cuando no es verdad. Este error se llama un error de tipo 2 o un falso negativo. Para el caso 1, con un test de cola superior y nivel de confianza del \\(95\\%\\) este es \\[\\beta= P(Y &lt; z_{0.05})\\] Donde \\(Y \\rightarrow N(\\frac{\\mu_1-\\mu_0}{\\sigma/\\sqrt{n}},1)\\) es la verdadera distribución de los estadísticos observados. Ejemplo (Bombilla) La eficiencia energética de una nueva bombilla es una variable aleatoria normal con desviación estandar de \\(5\\) vátios. Consideramos que las bombillas que producimos son eficientes si su media no supera los \\(80\\) vátios, por lo que planteamos el contraste de hipótesis \\(H_0 : \\mu \\geq 80\\) (no eficientes) \\(H_1 : \\mu &lt; 80\\) (eficientes) Queremos probar que producimos bombillas eficientes y planteamos realizar una muestra aleatoria de tamaño 100, con un nivel de significación estadística del \\(5\\%\\). Si estudios previos han sugerído que las bombillas puedne tener una media de \\(\\mu=\\mu_1=79\\) vátios ¿Qué error del tipo 2 esperamos? El contraste es para el caso 1 con una cola inferior. Por lo tanto la probabilidad de aceptar la hipótesis nula (no prodicimos bombillas eficientes) es \\[\\alpha = P(Z&lt; z_{0.95})=0.05\\] y \\(z_{0.95}=\\)qnorm(0.05)\\(=-1.644\\). El error de tipo 2 es por lo tanto \\[\\beta= P(Y &gt; -1.644)\\] es decir, la probabilidad de aceptar que no producimos bombillas eficientes cuando en realidad sí lo hacemos. Al tener el caso 1 porque conocemos \\(\\sigma\\) y la variable es normal, entonces los estadísticos observados realmente se distribuyten como \\[Y \\rightarrow N(\\frac{79-80}{5/\\sqrt{100}}=-2,1)\\] y el error de tipo 2 es \\[\\beta = 1-F(-1.644)=0.36\\] calculado en R como 1-pnorm(-1.644,-2,1)=0.36. De tal forma, que sólo un \\(\\alpha=5\\%\\) de las veces anunciaríamos que producimos bombillas eficientes cuando realmente no lo son, mientras que un \\(\\beta=36\\%\\) de las veces anunciaríamos tener una producción que no sirve cuando realmente sí sirve. Cuando realizamos una prueba de hipótesis tenemos dos posibilidades para cada condición \\(H_1\\) es en realidad : verdadera (\\(\\mu=\\mu_1\\)) o falsa (\\(\\mu=\\mu_0\\)) La prueba para \\(H_1\\) es: positiva (\\(z_{obs}\\) en zona de aceptación de \\(H_1\\)) o negativa (\\(z_{obs}\\) en zona de aceptación de \\(H_0\\)) Ejemplo (PCR) Hacemos una PCR para probar una infección. El contraste de hipótesis es \\(H_0\\) no hay infección \\(H_1\\) hay infección Hacemos la prueba PCR y nos da negativa: rechazamos la infección (\\(H_1\\)) positiva: aceptamos la infección (\\(H_1\\)) Podemos escribir la tabla de contingencia para las probabilidades de los resultados de la prueba de hipótesis como \\(H_1\\) es verdadera \\(H_0\\) es verdadera La prueba en \\(H_1\\) es positiva \\(1-\\beta\\) \\(\\alpha\\) La prueba en \\(H_1\\) es negativa \\(\\beta\\) \\(1-\\alpha\\) suma 1 1 De tal forma que tenemos La tasa de error de tipo 2: probabilidad de un falso negativo (ignorar un descubrimiento cuando es verdadero) \\[\\beta=P(negativa|H_1)\\] Tasa de verdaderos positivos: Este es el poder o la sensibilidad de una prueba (afirmar un descubrimiento cuando es verdadero, el objetivo principal) \\[1-\\beta=P(positiva|H_1)\\] Tasa de error de tipo 1: probabilidad de un falso positivo (afirmar un descubrimiento cuando es falso) \\[\\alpha=P(positiva|H_0)\\] Tasa de verdaderos negativos: Esta es la especificidad de una prueba (ignorar un descubrimiento cuando es falso) \\[1-\\alpha=P(negativa|H_0)\\] 14.9 Ejercicios 14.9.0.1 Ejercicio 1 Imagina que tomamos una muestra aleatoria de tamaño \\(n = 41\\) de una variable aleatoria normal \\(X\\) y encontramos que el promedio de la muestra es de \\(10\\) y la varianza de la muestra es de \\(1.5\\). ¿Cuál es entonces el intervalo de confianza para la media de \\(X\\) con un nivel de confianza de \\(95\\%\\)? (R:(9.61, 10.38)) Probar la hipótesis de que la media de \\(X\\) es diferente de \\(10.5\\), utilizando un umbral de significancia de \\(5\\%\\). (R: Reject the null) Escriba el código para calcular el P valor para probar la hipótesis de que la media de \\(\\mu\\) es inferior a \\(10.5\\), utilizando un umbral de significancia de \\(5\\%\\). Considera que el código para la distribución de probabilidad T con \\(n-1\\) grados de libertad es pt(tobs, n-1). (R: pt((10-10.5)/sqrt(1.5/41), 40)) 14.9.0.2 Ejercicio 2 Una muestra de \\(10\\) condensados de gas mostraron las siguientes concentraciones de mercurio (en \\(ng/ml\\)): \\(23.3\\), \\(22.5\\), \\(21.9\\), \\(21.5\\), \\(19.9\\), \\(21.3\\), \\(21.7\\), \\(23.8\\), \\(22.6\\), \\(24.7\\) Suponiendo que la concentración de mercurio se distribuye normalmente entre los condensados de gas, prueba la hipótesis de que un condensado no supera el límite de toxicidad establecido en \\(24 ng/ml\\). (R: p-value = 0.001, Reject the null) 14.9.0.3 Ejercicio 3 El fabricante de microarrays de expresión génica garantiza que al menos el \\(97\\%\\) de los microarrays que producen tienen señales de alta calidad. Un cliente recibe un lote de piezas de \\(200\\) y descubre que \\(8\\) no son buenas. ¿Debe el cliente devolver el lote por mala calidad? (R:No, p-value = 0.20, Accept the null) "],["soluciones-a-las-preguntas.html", "Chapter 15 Soluciones a las preguntas 15.1 Capítulo 2 15.2 Capítulo 3 15.3 Capítulo 4 15.4 Capítulo 5 15.5 Capítulo 7 15.6 Capítulo 8 15.7 Capítulo 9 15.8 Capítulo 10 15.9 Capítulo 11 15.10 Capítulo 12 15.11 Capítulo 13", " Chapter 15 Soluciones a las preguntas 15.1 Capítulo 2 \\(\\qquad\\) 1.c; \\(\\qquad\\) 2.a; \\(\\qquad\\) 3.d; \\(\\qquad\\) 4.d; \\(\\qquad\\) 5.b 15.2 Capítulo 3 \\(\\qquad\\) 1.a; \\(\\qquad\\) 2.b; \\(\\qquad\\) 3.b; \\(\\qquad\\) 4.b; \\(\\qquad\\) 5.a 15.3 Capítulo 4 \\(\\qquad\\) 1.c; \\(\\qquad\\) 2.b; \\(\\qquad\\) 3.d; \\(\\qquad\\) 4.b; \\(\\qquad\\) 5.b 15.4 Capítulo 5 \\(\\qquad\\) 1.d; \\(\\qquad\\) 2.c; \\(\\qquad\\) 3.b; \\(\\qquad\\) 4.c; \\(\\qquad\\) 5.b 15.5 Capítulo 7 \\(\\qquad\\) 1.d; \\(\\qquad\\) 2.a; \\(\\qquad\\) 3.d; \\(\\qquad\\) 4.a; \\(\\qquad\\) 5.d 15.6 Capítulo 8 \\(\\qquad\\) 1.d; \\(\\qquad\\) 2.b; \\(\\qquad\\) 3.a 15.7 Capítulo 9 \\(\\qquad\\) 1.c; \\(\\qquad\\) 2.a; \\(\\qquad\\) 3.c; \\(\\qquad\\) 4.d; \\(\\qquad\\) 5.b 15.8 Capítulo 10 \\(\\qquad\\) 1.a; \\(\\qquad\\) 2.c; \\(\\qquad\\) 3.d; \\(\\qquad\\) 4.d; \\(\\qquad\\) 5.c 15.9 Capítulo 11 \\(\\qquad\\) 1.c; \\(\\qquad\\) 2.b; \\(\\qquad\\) 3.c 15.10 Capítulo 12 \\(\\qquad\\) 1.d; \\(\\qquad\\) 2.d; \\(\\qquad\\) 3.b; \\(\\qquad\\) 4.a 15.11 Capítulo 13 \\(\\qquad\\) 1.b; \\(\\qquad\\) 2.a; \\(\\qquad\\) 3.d; \\(\\qquad\\) 4.b; \\(\\qquad\\) 5.b "]]
